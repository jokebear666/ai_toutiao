{
  "label": "cs.LG",
  "slug": "cslg",
  "week": "20260105-20260111",
  "items": [
    {
      "title": "Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems",
      "authors": "Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi",
      "institution": "McMaster University",
      "link": "https://arxiv.org/pdf/2601.00005",
      "code": null,
      "tags": [
        "anomaly detection",
        "class imbalance",
        "synthetic dataset",
        "generalization error",
        "unsupervised methods",
        "semi-supervised methods"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive, problem-agnostic evaluation of 14 anomaly detectors under simulated industrial constraints of extreme class imbalance. 2. Identified that the best-performing detector depends critically on the absolute number of faulty examples available, not just the imbalance ratio, and provided thresholds for method selection. 3. Demonstrated the nuanced impact of feature dimensionality on method performance, showing semi-supervised methods gain advantage in higher dimensions.",
      "summary": "This paper evaluates anomaly detection algorithms for industrial problems with extreme class imbalance using a synthetic dataset. It benchmarks 14 detectors across varying anomaly rates and training sizes, finding that the optimal detector depends on the absolute number of faulty examples, with unsupervised methods best for very few faults and supervised/semi-supervised methods improving with 30-50 faults. The study highlights performance drops on smaller datasets and provides practical deployment insights.",
      "mindmap": "graph TB\n        A[Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Extreme class imbalance in industrial applications due to limited faulty data]\n        C[主要方法/Method: Benchmark 14 detectors on synthetic hyperspherical dataset with varying anomaly rates and training sizes]\n        D[关键结果/Results: Best detector depends on number of faulty examples; Unsupervised dominates with <20 faults; Supervised/semi-supervised improve with 30-50 faults]"
    },
    {
      "title": "A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system",
      "authors": "Luis M. Moreno-Saavedra, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, David Casillas-Perez, Sancho Salcedo-Sanz",
      "institution": "Universidad de Alcalá, Universidad Rey Juan Carlos",
      "link": "https://arxiv.org/pdf/2601.00023",
      "code": null,
      "tags": [
        "logistics optimization",
        "workload balancing",
        "evolutionary algorithms",
        "k-means",
        "last-mile delivery",
        "hybrid algorithm"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp",
      "contributions": "1. Proposes a multi-algorithm methodology for operational human resources workload balancing in last-mile delivery, moving beyond simple geographical assignment. 2. Introduces and combines several algorithmic approaches, including different versions of k-means, evolutionary algorithms, recursive assignments, and a hybrid evolutionary ensemble. 3. Validates the proposed approach by applying it to a real-world case study of a delivery workforce in Azuqueca de Henares, Spain.",
      "summary": "This paper addresses the problem of unbalanced workload distribution among delivery workers in last-mile urban logistics. It proposes a multi-algorithm approach that uses a combination of distance and workload considerations, including k-means variants and evolutionary algorithms, to assign packages and balance daily effort. The method was successfully tested on a real-world delivery system in Spain.",
      "mindmap": "graph TB\n        Root[”A multi-algorithm approach for operational human resources workload balancing<br>多算法方法用于最后一公里配送的人力资源负载均衡”] --> Problem[”核心问题/Problem: Unbalanced workload among delivery workers in last-mile systems<br>最后一公里配送中快递员工作量不均衡”]\n        Root --> Method[”主要方法/Method: Multi-algorithm approach combining k-means, evolutionary algorithms, and hybrid ensembles<br>结合k-means、进化算法和混合集成的多算法方法”]\n        Root --> Results[”关键结果/Results: Successfully applied to a real-world delivery workforce, balancing workload<br>成功应用于现实配送团队，实现了工作量均衡”]"
    },
    {
      "title": "Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study",
      "authors": "Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin",
      "institution": "Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin",
      "link": "https://arxiv.org/pdf/2601.00004",
      "code": null,
      "tags": [
        "mental health language modeling",
        "large language models",
        "fine-tuning",
        "PHQ-9",
        "Nigerian Pidgin",
        "depression screening"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp",
      "contributions": "1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin.",
      "summary": "This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings.",
      "mindmap": "graph TB\n        A[Finetuning LLMs for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Limited depression screening in Nigeria due to language barriers and lack of clinicians]\n        C[主要方法/Method<br>Fine-tune LLMs on annotated Nigerian Pidgin dataset for PHQ-9 scoring]\n        D[关键结果/Results<br>GPT-4.1 achieved 94.5% accuracy and best cultural appropriateness]"
    },
    {
      "title": "Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games",
      "authors": "Nicholas A. Pape",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2601.00007",
      "code": null,
      "tags": [
        "reinforcement learning",
        "policy gradient",
        "self-play",
        "Markov Decision Process",
        "Advantage Actor-Critic",
        "ablation study"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp",
      "contributions": "1. Formulates the classic stochastic combinatorial game Yahtzee as a Markov Decision Process and establishes it as a mid-scale RL benchmark. 2. Conducts a comprehensive empirical study comparing REINFORCE, A2C, and PPO under a fixed training budget, identifying A2C as the most robust method. 3. Achieves a median score within 5% of the optimal dynamic programming solution, while analyzing persistent challenges like long-horizon credit assignment.",
      "summary": "This paper investigates using deep reinforcement learning to play the full solitaire version of Yahtzee. It trains self-play agents with policy gradient methods (REINFORCE, A2C, PPO) and performs ablation studies on various design choices. The main finding is that A2C robustly achieves near-optimal performance, while all methods struggle with long-term strategic elements like securing the upper section bonus.",
      "mindmap": "graph TB\n        A[Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Yahtzee as a mid-scale RL benchmark with delayed rewards & combinatorial complexity] --> B1[目标/Objectives<br>Can RL achieve near-optimal performance via self-play?]\n        C[主要方法/Method<br>Formulate as MDP, train self-play agents with policy gradient methods (REINFORCE, A2C, PPO)] --> C1[技术/Techniques<br>Ablation on encodings, architecture, estimators, entropy]\n        D[关键结果/Results<br>A2C is robust & achieves median score within 5% of optimal DP score] --> D1[挑战/Challenges<br>Agents struggle with long-horizon strategy (upper bonus)]"
    },
    {
      "title": "Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing",
      "authors": "Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura Bégon-Lours",
      "institution": "ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)",
      "link": "https://arxiv.org/pdf/2601.00020",
      "code": null,
      "tags": [
        "on-device ai",
        "ferroelectric synapses",
        "spiking neural networks",
        "EEG signal processing",
        "adaptive learning",
        "neuromorphic computing"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp",
      "contributions": "1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation.",
      "summary": "This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing.",
      "mindmap": "graph TB\n        A[Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: EEG信号非平稳性限制模型泛化，需在资源受限平台上进行个性化适应/Non-stationary EEG signals limit model generalization, requiring personalized adaptation on resource-constrained platforms]\n        C[主要方法/Method: 在铁电忆阻突触上部署SNN，采用设备感知的权重更新策略/Deploy SNNs on ferroelectric memristive synapses with a device-aware weight-update strategy]\n        D[关键结果/Results: 两种部署策略性能媲美软件SNN，特定对象迁移学习提升准确率/Two deployment strategies achieve performance comparable to software SNNs, subject-specific transfer learning improves accuracy]"
    },
    {
      "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition",
      "authors": "Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao",
      "institution": "Purdue University, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2601.00065",
      "code": "https://github.com/xz-liu/tokenforge",
      "tags": [
        "llm inference",
        "tokenizer transplant",
        "model composition",
        "supply-chain vulnerability",
        "sparse solver",
        "spectral mimicry"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp",
      "contributions": "1. Identifies tokenizer transplant as a novel attack surface in the LLM composition supply chain, 2. Introduces the concept of a \"breaker token\"—a single, engineered token that is inert in a donor model but maliciously activates after transplant, 3. Formalizes and instantiates the attack as a dual-objective optimization problem solved with a sparse solver, demonstrating its training-free nature, stealth, and persistence.",
      "summary": "This paper identifies a security vulnerability in the tokenizer transplant step required for composing different LLMs. The authors propose a method to engineer a single \"breaker token\" that, when added to a donor model, remains harmless but sabotages a base model after transplant by exploiting coefficient reuse. The attack is stealthy, training-free, and persistent, revealing a hidden risk in modular AI pipelines.",
      "mindmap": "graph TB\n        A[The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Tokenizer transplant introduces a supply-chain vulnerability for LLM composition]\n        C[主要方法/Method<br>Engineer a single ”breaker token” exploiting coefficient reuse via sparse solver]\n        D[关键结果/Results<br>Stealthy, training-free attack that persists against fine-tuning and merging]"
    },
    {
      "title": "It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models",
      "authors": "Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros",
      "institution": "UC Berkeley, University of Tübingen (Tübingen AI Center), Technical University of Munich (MCML)",
      "link": "https://arxiv.org/pdf/2601.00090",
      "code": null,
      "tags": [
        "diffusion models",
        "mode collapse",
        "noise optimization",
        "frequency characteristics",
        "text-to-image generation",
        "inference-time scaling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp",
      "contributions": "1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches.",
      "summary": "The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images.",
      "mindmap": "graph TB\n    Root(”It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models”) --> Problem(”核心问题/Problem: Mode collapse in text-to-image models”)\n    Root --> Method(”主要方法/Method: Noise optimization with frequency analysis”)\n    Root --> Results(”关键结果/Results: Improved generation diversity and quality”)"
    },
    {
      "title": "IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business",
      "authors": "Swetha Varadarajan, Abhishek Ray, Lumina Albert",
      "institution": "University of Cape Town, George Mason University, Colorado State University",
      "link": "https://arxiv.org/pdf/2601.00075",
      "code": "https://anonymous.4open.science/r/IMB-GNN-F022",
      "tags": [
        "spatio-temporal graph neural networks",
        "spatio-temporal graph neural network",
        "dynamic graphs",
        "temporal attention",
        "graph convolutional network",
        "network forensics"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef349259f296853dd5ad0527274fd725d811fdf18b16b020222413bfeb7f6f93_w640_q70.webp",
      "contributions": "1. Proposes IMBWatch, a novel ST-GNN framework for detecting Illicit Massage Businesses by modeling them as dynamic, heterogeneous graphs from open-source data. 2. Introduces a method combining graph convolutions with temporal attention to capture evolving spatio-temporal patterns like intercity movement and coordinated advertising. 3. Demonstrates superior performance over baseline models on real-world data and provides an interpretable, scalable tool for proactive anti-trafficking interventions.",
      "summary": "This paper introduces IMBWatch, a spatio-temporal graph neural network framework designed to detect Illicit Massage Businesses by modeling their operations as dynamic graphs from online ads and records. The method uses graph convolutions and temporal attention to learn evolving patterns, and it outperforms baseline models in accuracy and F1-score on real-world data, offering a scalable tool for law enforcement.",
      "mindmap": "graph TB\n    A[IMBWatch: A Spatio-Temporal Graph Neural Network Framework for Detecting Illicit Massage Businesses] --> B(核心问题/Problem: Detecting covert Illicit Massage Businesses is difficult due to encoded ads and dynamic operations.)\n    A --> C(主要方法/Method: Constructs dynamic graphs from open-source data and uses ST-GNN with temporal attention to model spatio-temporal evolution.)\n    A --> D(关键结果/Results: Outperforms baseline models (GCN, GAT, etc.) on real data, offering higher accuracy, interpretability, and scalability.)"
    },
    {
      "title": "Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing",
      "authors": "Manish Bhatt, Adrian Wood, Idan Habler, Ammar Al-Kahfah",
      "institution": "OWASP, Amazon, Dropbox, CISCO, AWS",
      "link": "https://arxiv.org/pdf/2601.00042",
      "code": "https://github.com/mbhatt1/competitionscratch",
      "tags": [
        "LLM Security",
        "Go-Explore",
        "Prompt Injection",
        "Adversarial Testing",
        "Agent Safety",
        "Multi-Hop Attacks"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp",
      "contributions": "1. Adapted the Go-Explore reinforcement learning algorithm for systematic security testing of LLM agents. 2. Conducted a large-scale empirical study revealing that random seed variance dominates algorithmic parameter choices in this domain. 3. Provided actionable insights for practitioners, such as the ineffectiveness of reward shaping and the benefits of using ensembles and simple state signatures.",
      "summary": "This paper adapts the Go-Explore algorithm to test the security of safety-trained LLM agents against prompt injection attacks. Through 28 experimental runs on GPT-4o-mini, the study finds that random seed variance is a major factor, reward shaping is harmful, and simple state signatures work best. The results suggest that managing seed variance and applying domain knowledge are more critical than algorithmic sophistication for effective security testing.",
      "mindmap": "graph TB\n        Root[”Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing<br/>大型实证案例研究：用于AI红队测试的Go-Explore”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Testing security of safety-trained LLM agents<br/>测试经过安全训练的LLM代理的安全性”] --> P1[”Prompt Injection<br/>提示注入”]\n        Method[”主要方法/Method<br/>Adapt Go-Explore algorithm<br/>改编Go-Explore算法”] --> M1[”Systematic exploration from archive<br/>从存档进行系统探索”]\n        Results[”关键结果/Results<br/>Key Findings<br/>关键发现”] --> R1[”Seed variance dominates<br/>种子方差占主导”]\n        Results --> R2[”Reward shaping harms performance<br/>奖励塑形损害性能”]\n        Results --> R3[”Simple signatures outperform<br/>简单签名效果更好”]\n        Results --> R4[”Ensembles provide diversity<br/>集成提供多样性”]"
    },
    {
      "title": "Exploration in the Limit",
      "authors": "Brian M. Cho, Nathan Kallus",
      "institution": "Cornell University, Netflix",
      "link": "https://arxiv.org/pdf/2601.00084",
      "code": null,
      "tags": [
        "bandit algorithms",
        "best arm identification",
        "asymptotic error control",
        "confidence sequences",
        "nonparametric",
        "sample complexity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a27e11a98452705127752d5c92465faa383d4d4bc6f162ce25d8230c0bd85bbd_w640_q70.webp",
      "contributions": "1. Introduces a relaxed, asymptotic formulation for fixed-confidence best arm identification (BAI) that requires valid error control only after a minimum sample size, aligning with long-horizon practical settings. 2. Develops a novel asymptotic anytime-valid confidence sequence for arm indices and uses it to design a new BAI algorithm that flexibly incorporates covariates for variance reduction in fully nonparametric settings. 3. Provides asymptotic sample complexity bounds, showing the worst-case complexity matches the best-case complexity of Gaussian BAI under exact guarantees, and demonstrates reduced average sample complexity in experiments.",
      "summary": "The paper addresses limitations in existing best arm identification (BAI) methods by proposing an asymptotic framework that relaxes the requirement for exact error control to an asymptotic one, enabling the use of tighter bounds and handling nonparametric distributions with covariates. It introduces a new algorithm based on asymptotic anytime-valid confidence sequences. Experiments show this approach reduces average sample complexity while maintaining error control.",
      "mindmap": "graph TB\n        A[Exploration in the Limit] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有BAI方法不实用/Existing BAI methods impractical]\n        B1 --> B2[严格误差控制导致限制/Stringent exact error control causes restrictions]\n        C --> C1[渐近误差控制框架/Asymptotic error control framework]\n        C1 --> C2[新颖置信序列/Novel confidence sequences]\n        C2 --> C3[新BAI算法/New BAI algorithm]\n        D --> D1[样本复杂度匹配最佳情况/Sample complexity matches best-case]\n        D1 --> D2[实验减少平均样本量/Experiments reduce average sample size]"
    },
    {
      "title": "Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery",
      "authors": "Junqi Qu, Yan Zhang, Shangqian Gao, Shibo Li",
      "institution": "Florida State University",
      "link": "https://arxiv.org/pdf/2601.00088",
      "code": null,
      "tags": [
        "symbolic regression",
        "Bayesian Optimization",
        "Instruction Tuning",
        "Partial Differential Equation Discovery",
        "LLM Prompting"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d0309ae3b94e9b3ab379ec45397a3f052c2722939b39c6750528988b21974_w640_q70.webp",
      "contributions": "1. Identifies and formalizes the problem of \"instruction brittleness\" in LLMs for equation discovery, where model outputs are highly sensitive to prompt phrasing. 2. Proposes NeuroSymBO, a novel framework that reframes prompt engineering as a sequential decision problem, using Bayesian Optimization to dynamically select optimal instructions from a library at each step. 3. Demonstrates through experiments on PDE discovery benchmarks that adaptive instruction selection significantly outperforms static prompts, achieving higher recovery rates and more parsimonious solutions.",
      "summary": "The paper addresses the problem of \"instruction brittleness\" in LLMs used for discovering partial differential equations, where fixed prompts lead to suboptimal results. It proposes NeuroSymBO, a framework that uses Bayesian Optimization to dynamically select the best instruction at each step of the generation process. Experiments show this adaptive approach outperforms static prompting, yielding more accurate and simpler equations.",
      "mindmap": "graph TB\n        Root[Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: LLM指令脆弱性<br/>Instruction Brittleness in LLMs] --> P1[静态提示导致次优解<br/>Static prompts cause suboptimal solutions]\n        Method[主要方法/Method: NeuroSymBO框架<br/>NeuroSymBO Framework] --> M1[将提示工程视为序列决策问题<br/>Reframes prompt engineering as sequential decision] --> M2[使用贝叶斯优化动态选择指令<br/>Uses Bayesian Optimization for adaptive instruction selection]\n        Results[关键结果/Results: 实验评估<br/>Experimental Evaluation] --> R1[自适应选择显著优于固定提示<br/>Adaptive selection significantly outperforms fixed prompts] --> R2[更高的恢复率与更简约的解<br/>Higher recovery rates & more parsimonious solutions]"
    },
    {
      "title": "Reinforcement learning with timed constraints for robotics motion planning",
      "authors": "Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao",
      "institution": "University of Iowa, Talus Renewables, Inc., Roma Tre University",
      "link": "https://arxiv.org/pdf/2601.00087",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Metric Interval Temporal Logic (MITL)",
        "Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA)",
        "Q-learning",
        "POMDP"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp",
      "contributions": "1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments.",
      "summary": "This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments.",
      "mindmap": "graph TB\n        A[Reinforcement learning with timed constraints for robotics motion planning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[复杂任务序列与严格时间约束/Complex task sequences & strict temporal constraints]\n        B --> B2[随机动态与部分可观测性/Stochastic dynamics & partial observability]\n        C --> C1[MITL公式转换为Timed-LDGBA/MITL to Timed-LDGBA translation]\n        C --> C2[构建产品定时模型与Q学习/Construct product timed models for Q-learning]\n        C --> C3[简单而富有表现力的奖励结构/Simple yet expressive reward structure]\n        D --> D1[满足时间约束的策略/Policies satisfy time-bounded requirements]\n        D --> D2[扩展到更大状态空间/Scales to larger state spaces]\n        D --> D3[在部分可观测环境中有效/Effective in partially observable environments]"
    },
    {
      "title": "GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments",
      "authors": "Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj",
      "institution": "University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2601.00116",
      "code": "https://github.com/",
      "tags": [
        "reinforcement learning",
        "geometric reinforcement learning",
        "Hamiltonian optimization",
        "simultaneous navigation and mapping",
        "local sensory observations",
        "path differential"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp",
      "contributions": "1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks.",
      "summary": "This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks.",
      "mindmap": "graph TB\n        A[GRL-SNAM: Geometric RL for SNAM] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Simultaneous Navigation and Mapping in mapless environments]\n        C[主要方法/Method: Geometric RL with Path Differential Hamiltonians, local energy landscapes]\n        D[关键结果/Results: High-quality navigation with minimal exploration, generalizes to unseen layouts]"
    },
    {
      "title": "Reinforcement Learning with Function Approximation for Non-Markov Processes",
      "authors": "Ali Devran Kara",
      "institution": "Florida State University",
      "link": "https://arxiv.org/pdf/2601.00151",
      "code": null,
      "tags": [
        "reinforcement learning",
        "non-Markov processes",
        "linear function approximation",
        "policy evaluation",
        "Q-learning",
        "partially observed MDPs"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp",
      "contributions": "1. Proved convergence of policy evaluation with linear function approximation under ergodic non-Markov processes, linking the limit to a fixed point of a joint projection-Bellman operator. 2. Established convergence for a special case of Q-learning with linear approximation where basis functions are based on quantization maps under similar ergodicity conditions. 3. Applied the theoretical results to Partially Observed MDPs (POMDPs) using finite-memory state representations and derived explicit error bounds for the learning algorithm limits.",
      "summary": "This paper studies reinforcement learning with linear function approximation for non-Markov processes. It proves convergence for policy evaluation and a special case of Q-learning under ergodicity conditions, and applies the theory to POMDPs to derive error bounds.",
      "mindmap": "graph TB\n        Root[Reinforcement Learning with Function Approximation for Non-Markov Processes] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>RL with linear function approximation for non-Markov processes] --> P1[非马尔可夫过程/Non-Markov Processes]\n        Method[主要方法/Method<br>Theoretical analysis under ergodicity conditions] --> M1[策略评估/Policy Evaluation]\n        Method --> M2[Q学习/Q-learning]\n        M2 --> M2_1[特殊情况:基于量化的基函数/Special Case: Quantization-based basis]\n        Results[关键结果/Results] --> R1[收敛性证明/Convergence Proofs]\n        Results --> R2[应用于POMDPs/Application to POMDPs]\n        R2 --> R2_1[显式误差界/Explicit Error Bounds]"
    },
    {
      "title": "The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data",
      "authors": "Yann Bellec, Rohan Kaman, Siwen Cui, Aarav Agrawal, Calvin Chen",
      "institution": "University of California San Diego",
      "link": "https://arxiv.org/pdf/2601.00152",
      "code": null,
      "tags": [
        "predictive modeling",
        "XGBoost",
        "feature importance",
        "class imbalance"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27e52d086cf3d330117b8cda24994123a6a67bf935b7650263f6ca2e120a3ed2_w640_q70.webp",
      "contributions": "1. A large-scale empirical analysis using an XGBoost model to predict traffic accident severity from environmental, temporal, and spatial factors. 2. A feature importance analysis revealing that time of day, location, temperature, and wind speed are strong predictors, while precipitation and visibility show limited predictive power, suggesting driver behavioral adaptation. 3. Identification of dataset limitations (class imbalance, predominance of mid-level severity) and proposed future directions including alternative sampling and enhanced feature engineering.",
      "summary": "This study uses an XGBoost classifier on a dataset of 500,000 US traffic accidents to predict accident severity. The model achieves 78% accuracy, and feature analysis shows that while time, location, and some weather factors are predictive, precipitation and visibility are not, potentially due to driver adaptation. The findings highlight the limitations of current data for predicting extreme cases and suggest future research directions.",
      "mindmap": "graph TB\n        Root[The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1[Predict traffic accident severity/预测交通事故严重程度]\n        Method --> M1[XGBoost Classifier/XGBoost分类器]\n        Method --> M2[Randomized Search CV/随机搜索交叉验证]\n        Method --> M3[Class Weighting/类别加权]\n        Results --> R1[78% Accuracy/78%准确率]\n        Results --> R2[Precipitation low importance/降水预测力低]\n        Results --> R3[Dataset limitations identified/识别数据集局限]"
    },
    {
      "title": "Online Finetuning Decision Transformers with Pure RL Gradients",
      "authors": "Junkai Luo, Yinglun Zhu",
      "institution": "University of California, Riverside",
      "link": "https://arxiv.org/pdf/2601.00167",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Decision Transformer",
        "online finetuning",
        "GRPO",
        "hindsight return relabeling",
        "sub-trajectory optimization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp",
      "contributions": "1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs.",
      "summary": "This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks.",
      "mindmap": "graph TB\n    A[Online Finetuning Decision Transformers with Pure RL Gradients] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[在线微调DT时，纯RL梯度方法未被探索/Pure RL gradients for online DT finetuning unexplored]\n    B --> B2[后见之益回报重标注与RL算法不兼容/Hindsight return relabeling incompatible with RL]\n    C --> C1[适配GRPO至DT/Adapt GRPO to DTs]\n    C --> C2[引入关键修改: 子轨迹优化等/Introduce key modifications]\n    D --> D1[超越现有在线DT基线/Outperform online DT baselines]\n    D --> D2[实现SOTA性能/Achieve SOTA performance]"
    },
    {
      "title": "Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting",
      "authors": "Ata Akbari Asanjan, Filip Wudarski, Daniel O'Connor, Shaun Geaney, Elena Strbac, P. Aaron Lott, Davide Venturelli",
      "institution": "USRA Research Institute for Advanced Computer Science (RIACS), Standard Chartered Bank",
      "link": "https://arxiv.org/pdf/2601.00172",
      "code": null,
      "tags": [
        "others",
        "Reservoir Computing",
        "Sequential Architecture",
        "Spatiotemporal Forecasting",
        "High-dimensional Data",
        "Training Efficiency"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp",
      "contributions": "1. Introduces a Sequential Reservoir Computing architecture that decomposes a large reservoir into smaller, interconnected ones to reduce computational and memory costs. 2. Demonstrates superior performance with longer forecast horizons and lower error metrics on chaotic and high-dimensional physical systems compared to RNN/LSTM baselines. 3. Achieves up to three orders of magnitude lower training cost, maintaining RC's efficiency while improving scalability for high-dimensional forecasting.",
      "summary": "This paper proposes Sequential Reservoir Computing, a novel architecture that breaks a large reservoir into a sequence of smaller ones to efficiently forecast high-dimensional spatiotemporal systems. It outperforms traditional RNNs and LSTMs in forecast horizon and accuracy while drastically reducing training costs, offering a path to real-time, energy-efficient forecasting.",
      "mindmap": "graph TB\n        Root[”Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>RNN/LSTM训练成本高，传统RC扩展性差”]\n        Method[”主要方法/Method<br>顺序储层计算架构”]\n        Results[”关键结果/Results<br>预测更长，误差更低，训练成本大幅降低”]"
    },
    {
      "title": "Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings",
      "authors": "Moirangthem Tiken Singh, Adnan Arif",
      "institution": "Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University",
      "link": "https://arxiv.org/pdf/2601.00186",
      "code": null,
      "tags": [
        "semantic communication",
        "reinforcement learning",
        "unequal error protection",
        "adaptive repetition coding",
        "semantic distortion metric",
        "per-dimension protection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp",
      "contributions": "1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection",
      "summary": "This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity.",
      "mindmap": "graph TB\n        A[Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[带宽受限下保持语义/Bandwidth-constrained Semantic Preservation]\n        C --> C1[基于RL的自适应重复编码/RL-based Adaptive Repetition Coding]\n        C --> C2[复合语义失真度量/Composite Semantic Distortion Metric]\n        D --> D1[性能显著超越均匀保护/Significant Gains Over Uniform Protection]\n        D --> D2[挑战传统信道编码范式/Challenges Traditional Channel Coding]"
    },
    {
      "title": "Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score",
      "authors": "Zhuqi Miao, Sujan Ravi, Abdulaziz Ahmed",
      "institution": "Oklahoma State University, University of Alabama at Birmingham",
      "link": "https://arxiv.org/pdf/2601.00175",
      "code": null,
      "tags": [
        "clinical prediction",
        "XGBoost",
        "electronic health records (EHR)",
        "FIB-4 score",
        "liver cirrhosis",
        "early prediction"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10f123bbaaf0097372e87215cac6fce63e113f7c0a6ea463635fbc4532641048_w640_q70.webp",
      "contributions": "1. Developed and validated machine learning models for predicting liver cirrhosis 1, 2, and 3 years before diagnosis using routine EHR data. 2. Established a rigorous benchmark by comparing the ML models' performance directly against the traditional clinical FIB-4 score across multiple time horizons. 3. Demonstrated that ML models consistently outperform FIB-4, with performance gains increasing for longer-term (3-year) predictions, enabling earlier clinical risk stratification.",
      "summary": "This paper develops XGBoost models using electronic health record data to predict liver cirrhosis 1-3 years before diagnosis. The models consistently outperformed the standard FIB-4 score, with performance gains increasing for longer prediction horizons. The study concludes that these ML models can be integrated into clinical workflows as automated decision-support tools for earlier and more accurate risk stratification.",
      "mindmap": "graph TB\n    A[Early Prediction of Liver Cirrhosis Up to Three Years in Advance] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[早期预测肝硬化 / Early Prediction of Liver Cirrhosis]\n    B --> B2[超越FIB-4评分 / Benchmarking Against FIB-4]\n    C --> C1[使用EHR数据 / Use EHR Data]\n    C --> C2[构建预测场景 / Construct Prediction Scenarios]\n    C --> C3[训练XGBoost模型 / Train XGBoost Models]\n    D --> D1[ML模型性能更优 / ML Models Outperform FIB-4]\n    D --> D2[AUC: 0.81, 0.73, 0.69 / AUC: 0.81, 0.73, 0.69]\n    D --> D3[支持早期风险分层 / Supports Early Risk Stratification]"
    },
    {
      "title": "SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification",
      "authors": "Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti",
      "institution": "Deakin University, CSIRO Health and Biosecurity, Islamic Azad University, University of Southern Queensland",
      "link": "https://arxiv.org/pdf/2601.00189",
      "code": null,
      "tags": [
        "semi-supervised learning",
        "Generative Adversarial Network",
        "Swin Transformer",
        "spike classification",
        "semi-supervised learning",
        "Bayesian optimization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp",
      "contributions": "1. Proposed a novel semi-supervised GAN architecture (SSI-GAN) with a Swin-inspired, shifted-window discriminator for neuronal spike classification. 2. Introduced a transformer-based generator and a flat, window-based transformer discriminator with multi-head self-attention to capture sparse, high-frequency spike features. 3. Demonstrated state-of-the-art performance with 99.93% accuracy using only 1-3% labeled data, reducing manual labeling effort by 97-99% compared to supervised methods.",
      "summary": "The paper addresses the labor-intensive problem of classifying mosquito neuronal spikes for arboviral disease detection by proposing SSI-GAN, a semi-supervised GAN that combines a Swin-inspired discriminator with a transformer-based generator. Using only 1-3% labeled data from over 15 million spike samples, it achieved up to 99.93% accuracy in classifying Zika-infected, dengue-infected, or uninfected categories, significantly reducing labeling effort while outperforming baselines.",
      "mindmap": "graph TB\n        A[SSI-GAN: 半监督Swin启发的生成对抗网络用于神经元尖峰分类] --> B[核心问题/Problem: 蚊虫神经元尖峰模式手动分类劳动密集且昂贵，现有深度学习方法需要全标记数据和高度预处理]\n        A --> C[主要方法/Method: 提出SSI-GAN，使用Swin启发的移位窗口判别器和基于Transformer的生成器，仅需1-3%标记数据]\n        A --> D[关键结果/Results: 达到99.93%分类准确率，标记工作量减少97-99%，在所有感染阶段保持高精度]"
    },
    {
      "title": "Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework",
      "authors": "Moirangthem Tiken Singh, Manibhushan Yaikhom",
      "institution": "Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences",
      "link": "https://arxiv.org/pdf/2601.00192",
      "code": null,
      "tags": [
        "on-device ai",
        "hybrid feature engineering",
        "wavelet decomposition",
        "graph-theoretic descriptors",
        "linear separability",
        "model compression"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp",
      "contributions": "1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 µs latency) for real-time arrhythmia detection on edge devices.",
      "summary": "This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications.",
      "mindmap": "graph TB\n        A[”Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection<br>优化的混合特征工程用于资源高效的心律失常检测”] --> B\n        A --> C\n        A --> D\n        B[”Problem: Deep learning models are too heavy for edge devices.<br>核心问题: 深度学习模型在边缘设备上计算开销过大”]\n        C[”Method: Hybrid feature engineering (wavelet + graph) for linear separability.<br>主要方法: 混合特征工程（小波+图论）实现线性可分性”]\n        D[”Results: 98.44% accuracy, 8.54 KB model, 0.46 µs latency.<br>关键结果: 98.44% 准确率, 8.54 KB 模型, 0.46 µs 延迟”]"
    },
    {
      "title": "StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices",
      "authors": "Shaswat Mohanty",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2601.00197",
      "code": null,
      "tags": [
        "time series forecasting",
        "LSTM",
        "Transformer",
        "Stock Prediction",
        "Time Series Forecasting",
        "Attention"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp",
      "contributions": "1. Presents an enhanced StockBot architecture for systematic evaluation of modern time-series forecasting models (attention-based, convolutional, recurrent) in a unified setting. 2. Demonstrates empirically that a carefully constructed vanilla LSTM model consistently outperforms transformer-based models in stock price forecasting accuracy and decision-making stability under default hyperparameters. 3. Highlights the robustness, data efficiency, and importance of architectural inductive bias of recurrent models for financial forecasting, especially in data-limited scenarios.",
      "summary": "This paper presents StockBot 2.0, a framework for evaluating time-series models for stock prediction. It finds that a vanilla LSTM model, despite its simplicity, outperforms more complex transformer-based models in forecasting accuracy and trading decision stability when trained with default settings, emphasizing the value of recurrent inductive biases for financial data.",
      "mindmap": "graph TB\n        Root[”StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices”] --> Problem[”核心问题/Problem: Forecasting financial markets is challenging due to complexity and volatility.”]\n        Root --> Method[”主要方法/Method: Enhanced StockBot architecture for systematic evaluation of attention, CNN, and RNN models.”]\n        Root --> Results[”关键结果/Results: Vanilla LSTM achieves superior accuracy and stable decisions compared to transformers.”]"
    },
    {
      "title": "Unknown Aware AI-Generated Content Attribution",
      "authors": "Ellie Thieu, Jifan Zhang, Haoyue Bai",
      "institution": "University of Wisconsin–Madison (UW–Madison)",
      "link": "https://arxiv.org/pdf/2601.00218",
      "code": null,
      "tags": [
        "image attribution",
        "generative model attribution",
        "constrained optimization",
        "open world setting",
        "CLIP features",
        "unknown generators"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53bb1bd3ca2eb8682e06279d4402b6c4441fc76d44f3b86a930479ba697b4e06_w640_q70.webp",
      "contributions": "1. Establishes a strong baseline for target generator attribution using CLIP features and a linear classifier with limited labeled data. 2. Proposes a novel constrained optimization method that leverages unlabeled \"wild\" data from the internet to improve robustness to unseen generators. 3. Demonstrates that incorporating unlabeled wild data substantially improves attribution performance on challenging, unseen, and newly released generative models.",
      "summary": "This paper addresses the problem of attributing AI-generated images to their specific source model in an open-world setting where new, unseen generators constantly emerge. The authors propose a constrained optimization approach that uses unlabeled data collected from the internet to encourage the classifier to treat unknown samples as non-target, while maintaining performance on known labeled data. The method significantly improves attribution accuracy on challenging, unseen generative models compared to a baseline trained only on limited labeled data.",
      "mindmap": "graph TB\n        Root(”Unknown Aware AI-Generated Content Attribution”) --> Problem(”核心问题/Problem: Attribution in open world with unseen generators”)\n        Root --> Method(”主要方法/Method: Constrained optimization with unlabeled wild data”)\n        Root --> Results(”关键结果/Results: Improved performance on unseen generators”)"
    },
    {
      "title": "Robust Graph Fine-Tuning with Adversarial Graph Prompting",
      "authors": "Ziyan Zhang, Bo Jiang, Jin Tang",
      "institution": "Anhui University",
      "link": "https://arxiv.org/pdf/2601.00229",
      "code": null,
      "tags": [
        "graph neural networks",
        "Adversarial Graph Prompting",
        "Parameter-Efficient Fine-Tuning",
        "Graph Prompt Learning",
        "Adversarial Learning",
        "Robust Fine-Tuning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/337af6d04faa59592d198afa70192352bc013ab310b674dbdaed1185325d78c7_w640_q70.webp",
      "contributions": "1. Proposes a novel Adversarial Graph Prompting (AGP) framework that integrates adversarial learning into graph prompting for robust parameter-efficient fine-tuning of pre-trained GNNs. 2. Formulates AGP as a min-max optimization problem and develops an alternating optimization scheme, featuring a Joint Projected Gradient Descent (JointPGD) algorithm for generating adversarial noise and a module for learning optimal node prompts. 3. Provides theoretical analysis demonstrating that AGP can handle both graph topology and node feature noise, confirming its versatility and robustness.",
      "summary": "This paper addresses the vulnerability of Parameter-Efficient Fine-Tuning (PEFT) methods for Graph Neural Networks (GNNs) to noise and attacks. It proposes a novel Adversarial Graph Prompting (AGP) framework that formulates robust fine-tuning as a min-max optimization problem, using adversarial noise generation and prompt learning to counteract it. The method is theoretically sound and experimentally validated to be more robust than state-of-the-art approaches across various graph noises.",
      "mindmap": "graph TB\n        A[Robust Graph Fine-Tuning with Adversarial Graph Prompting] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有PEFT方法对图噪声和攻击脆弱/Existing PEFT methods are vulnerable to graph noise & attacks]\n        C --> C1[对抗图提示(AGP)框架/Adversarial Graph Prompting (AGP) Framework]\n        C1 --> C2[最小-最大优化/Min-Max Optimization]\n        C2 --> C3[内层: JointPGD生成对抗噪声/Inner: JointPGD for adversarial noise]\n        C2 --> C4[外层: 学习最优节点提示/Outer: Learn optimal node prompts]\n        D --> D1[理论证明处理拓扑和节点噪声/Theoretically handles topology & node noise]\n        D --> D2[实验验证鲁棒性和有效性/Experiments validate robustness & effectiveness]"
    },
    {
      "title": "GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation",
      "authors": "Pritish Saha, Chandrav Rajbangshi, Rudra Goyal, Mohit Goyal, Anurag Deo, Biswajit Roy, Ningthoujam Dhanachandra Singh, Raxit Goswami, Amitava Das",
      "institution": "RAAPID Lab, Pragya Lab (BITS Pilani, Goa)",
      "link": "https://arxiv.org/pdf/2601.00231",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "LoRA",
        "K-FAC",
        "Parameter-Efficient Fine-Tuning",
        "Fisher Information",
        "Dynamic Rank Adaptation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp",
      "contributions": "1. Introduces K-FAC-based gradient preconditioning in the low-rank subspace for more geometry-aware updates. 2. Proposes periodic Fisher-guided reprojection of the LoRA basis to suppress parameter drift. 3. Implements dynamic rank adaptation to concentrate capacity on high-signal directions, reducing the number of trainable parameters.",
      "summary": "The paper identifies that standard LoRA/QLoRA methods are geometry-agnostic, leading to inefficient updates and parameter drift. It proposes GRIT, a new LoRA procedure that uses K-FAC preconditioning, Fisher-guided reprojection, and dynamic rank adaptation to make updates more curvature-aware. This approach matches or surpasses baseline performance while reducing trainable parameters by an average of 46% and achieving lower drift.",
      "mindmap": "graph TB\n        A[GRIT: Geometry-Aware PEFT] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[标准LoRA/QLoRA忽略曲率/Standard LoRA/QLoRA ignores curvature]\n        B --> B2[导致低效更新与参数漂移/Causes inefficient updates & parameter drift]\n        C --> C1[K-FAC预条件梯度/K-FAC preconditioned gradients]\n        C --> C2[Fisher引导重投影/Fisher-guided reprojection]\n        C --> C3[动态秩适应/Dynamic rank adaptation]\n        D --> D1[性能相当或更好/Matches or surpasses baselines]\n        D --> D2[参数减少~46%/Reduces parameters by ~46%]\n        D --> D3[漂移更低/Lower drift]"
    },
    {
      "title": "Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection",
      "authors": "Chao Yang, Haoyuan Zheng, Yue Ma",
      "institution": "Xi’an Jiaotong Liverpool University",
      "link": "https://arxiv.org/pdf/2601.00237",
      "code": null,
      "tags": [
        "object detection",
        "CycleGAN",
        "YOLOv8",
        "infrared image generation",
        "data augmentation",
        "PCB defect detection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp",
      "contributions": "1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks.",
      "summary": "This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training.",
      "mindmap": "graph TB\n        Root[”Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection”] --> Problem[”核心问题/Problem: 红外数据稀缺 / IR Data Scarcity”]\n        Root --> Method[”主要方法/Method: CycleGAN跨模态生成 + YOLOv8检测 / CycleGAN Cross-modal Generation + YOLOv8 Detection”]\n        Root --> Results[”关键结果/Results: 性能显著提升，接近全监督基准 / Performance Significantly Improved, Approaches Fully Supervised Benchmark”]"
    },
    {
      "title": "Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing",
      "authors": "Osvaldo Simeone",
      "institution": "Northeastern University London (Intelligent Networked Systems Institute - INSI)",
      "link": "https://arxiv.org/pdf/2601.00245",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "neuromorphic computing",
        "state-space models",
        "sparse attention",
        "surrogate gradients",
        "local learning rules"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp",
      "contributions": "1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning.",
      "summary": "This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems.",
      "mindmap": "graph TB\n        A[Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[AI能耗增长 / Escalating AI Energy Requirements]\n        C --> C1[神经形态计算原则 / Neuromorphic Computing Principles]\n        C1 --> C2[离散稀疏激活 / Discrete & Sparse Activations]\n        C1 --> C3[循环动态 / Recurrent Dynamics]\n        C --> C4[处理框架: 令牌内与令牌间 / Processing Framework: Intra-Token vs. Inter-Token]\n        D --> D1[现代AI体现神经形态原则 / Modern AI Embodies Neuromorphic Principles]\n        D --> D2[连接SNN、状态空间模型、Transformer / Connects SNNs, State-Space Models, Transformers]"
    },
    {
      "title": "Rectifying Adversarial Examples Using Their Vulnerabilities",
      "authors": "Fumiya Morimoto, Ryuto Morita, Satoshi Ono",
      "institution": "Kagoshima University",
      "link": "https://arxiv.org/pdf/2601.00270",
      "code": null,
      "tags": [
        "adversarial defense",
        "adversarial examples",
        "label rectification",
        "re-attack",
        "white-box attack",
        "black-box attack"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial example rectification method based on \"re-attacking\" AEs to move them beyond the decision boundary for correct label estimation. 2. The method is designed to be straightforward, requiring only AEs as input without parameter adjustments or preliminary training, enabling it to address diverse attack types. 3. Demonstrates consistent performance and superior stability against various attacks, including targeted and black-box attacks, compared to conventional rectification and input transformation methods.",
      "summary": "This paper addresses the problem of rectifying adversarial examples (AEs) to recover the correct labels of the original inputs, which is crucial for applications like autonomous driving. The proposed method works by \"re-attacking\" the AEs to push them across the model's decision boundary. The results show that this method performs consistently across different attack types and is more stable than existing approaches.",
      "mindmap": "graph TB\n        A[Rectifying Adversarial Examples Using Their Vulnerabilities] --> B(核心问题/Problem: DNNs misclassify adversarial examples, needing correct label recovery)\n        A --> C(主要方法/Method: Re-attack AEs to move them beyond decision boundary)\n        A --> D(关键结果/Results: Consistent performance across attacks, outperforms conventional methods in stability)"
    },
    {
      "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations",
      "authors": "Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian Möller, Vera Schmitt",
      "institution": "Technische Universität Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen",
      "link": "https://arxiv.org/pdf/2601.00282",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "quantization",
        "self-explanations",
        "faithfulness",
        "natural language explanations",
        "counterfactual examples"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp",
      "contributions": "1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations.",
      "summary": "This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression.",
      "mindmap": "graph TB\n        Root[”Can Large Language Models Still Explain Themselves?<br/>大语言模型还能解释自己吗？”] --> Problem[”Quantization's effect on Self-Explanations is unknown.<br/>量化对自我解释的影响未知”]\n        Root --> Method[”Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>评估量化后LLM的自然语言解释和反事实示例”]\n        Root --> Results[”Moderate decline in quality/faithfulness; context-dependent impact.<br/>质量/忠实度适度下降；影响因上下文而异”]"
    },
    {
      "title": "Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering",
      "authors": "Hongxi Li, Chunlin Huang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2601.00276",
      "code": null,
      "tags": [
        "theoretical machine learning",
        "kernel evolution",
        "spectral truncation",
        "label rank compression",
        "Laplacian spectral filtering",
        "Neural Tangent Kernel"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03dc1352545d41fe155dce9a675ce3e4dfa7f83f33a1829acc745f5fa2954776_w640_q70.webp",
      "contributions": "1. Derives a kernel ODE revealing a \"water-filling\" spectral law for supervised learning, showing the kernel is compressed into a low-rank subspace bounded by the number of classes. 2. Proves that any stable steady state under L2-regularization and linear readout inherently exhibits label-driven rank compression, independent of the fast-readout approximation. 3. Demonstrates that SGD noise is confined to a low-rank subspace (O(C)), unifying deterministic and stochastic views of alignment, and contrasts this with expansive self-supervised representations.",
      "summary": "This paper develops a kernel-centric theory for feature learning in wide neural networks. It shows that supervised learning inherently compresses the kernel into a low-rank subspace bounded by the number of classes, and that SGD noise is similarly confined, contrasting this compressive behavior with the expansive representations of self-supervised learning.",
      "mindmap": "graph TB\n    A[Task-Driven Kernel Flows<br>任务驱动的核流] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Understanding kernel evolution in feature learning<br>理解特征学习中的核演化]\n    C --> C1[Kernel ODE & algebraic analysis<br>核ODE与代数分析]\n    D --> D1[Supervised learning is compressive (rank ≤ C)<br>监督学习是压缩性的 (秩 ≤ C)]\n    D --> D2[SGD noise is low-rank (O(C))<br>SGD噪声是低秩的 (O(C))]\n    D --> D3[Contrast with expansive self-supervision<br>与扩张的自监督学习对比]"
    },
    {
      "title": "Can Optimal Transport Improve Federated Inverse Reinforcement Learning?",
      "authors": "David Millard, Ali Baheri",
      "institution": "Rochester Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00309",
      "code": null,
      "tags": [
        "federated learning",
        "Inverse Reinforcement Learning",
        "Federated Learning",
        "Optimal Transport",
        "Wasserstein Barycenter",
        "Maximum Entropy IRL"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp",
      "contributions": "1. Introduces an optimal transport-based approach for federating learned reward functions in Inverse Reinforcement Learning (IRL). 2. Proposes using a Wasserstein barycenter for reward fusion, which accounts for the geometric structure of the reward landscape, as opposed to simple parameter averaging. 3. Provides a theoretical proof that the barycentric fusion yields a more faithful global reward estimate than conventional federated averaging methods.",
      "summary": "This paper addresses the challenge of learning a shared reward function across heterogeneous agents in privacy-sensitive, communication-limited settings. It proposes a federated IRL framework where agents perform local Maximum Entropy IRL and then fuse their reward functions via a Wasserstein barycenter. The authors prove this method provides a more accurate global reward estimate than standard parameter averaging, offering a principled and efficient solution for multi-agent systems.",
      "mindmap": "graph TB\n        Root[”Can Optimal Transport Improve Federated Inverse Reinforcement Learning?”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Heterogeneous agents need a shared reward, but data pooling is impractical due to privacy, dynamics differences, and limited bandwidth.”]\n        Method[”主要方法/Method<br>Local lightweight MaxEnt IRL followed by reward fusion via Wasserstein barycenter.”]\n        Results[”关键结果/Results<br>Barycentric fusion yields a more faithful global reward estimate than parameter averaging.”]"
    },
    {
      "title": "Quantum King-Ring Domination in Chess: A QAOA Approach",
      "authors": "Gerhard Stenzel, Michael Kölle, Tobias Rohe, Julian Hager, Leo Sünkel, Maximilian Zorn, Claudia Linnhoff-Popien",
      "institution": "LMU Munich",
      "link": "https://arxiv.org/pdf/2601.00318",
      "code": null,
      "tags": [
        "quantum optimization",
        "QAOA",
        "benchmark",
        "constraint-preserving mixers",
        "warm-start",
        "CVaR"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0b58913db95e3acf390bd659716f38782d77c5b0f4c58736c855c0b4940575b_w640_q70.webp",
      "contributions": "1. Introduction of the Quantum King-Ring Domination (QKRD) benchmark, a structured, NISQ-scale testbed derived from chess with 5,000 instances. 2. Systematic evaluation of QAOA design choices, showing the advantages of constraint-preserving mixers and warm-start strategies. 3. Demonstration that structured benchmarks reveal performance insights for problem-informed QAOA techniques that are obscured in random synthetic instances.",
      "summary": "The paper introduces a new structured benchmark called Quantum King-Ring Domination (QKRD) based on chess to evaluate the Quantum Approximate Optimization Algorithm (QAOA). Using this benchmark, the authors systematically test various QAOA design choices and find that constraint-preserving mixers and warm-start strategies significantly improve performance. The results show that structured benchmarks are crucial for revealing the advantages of problem-informed quantum optimization techniques.",
      "mindmap": "graph TB\n        A[Quantum King-Ring Domination in Chess: A QAOA Approach] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有基准缺乏语义结构/Existing benchmarks lack semantic structure]\n        C --> C1[提出基于国际象棋的QKRD基准/Propose chess-based QKRD benchmark]\n        C --> C2[系统评估QAOA设计选择/Systematically evaluate QAOA design choices]\n        D --> D1[约束保持混频器收敛更快/Constraint-preserving mixers converge faster]\n        D --> D2[热启动策略显著改进/Warm-start strategies yield significant improvement]\n        D --> D3[结构化基准揭示隐藏优势/Structured benchmarks reveal obscured advantages]"
    },
    {
      "title": "Smart Fault Detection in Nanosatellite Electrical Power System",
      "authors": "Alireza Rezaee, Niloofar Nobahari, Amin Asgarifar, Farshid Hajati",
      "institution": "University of Tehran, University of New England",
      "link": "https://arxiv.org/pdf/2601.00335",
      "code": null,
      "tags": [
        "fault diagnosis",
        "neural network",
        "PCA classification",
        "decision tree",
        "KNN"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab383b2f3d676c3a09faf3b55b3c7d4dd74c574ecc0a93d40c28f82897ffe1bf_w640_q70.webp",
      "contributions": "1. Proposes a new fault detection method for nanosatellite electrical power systems operating without an Attitude Determination Control Subsystem (ADCS) in LEO orbit. 2. Uses a neural network to simulate the fault-free system behavior using solar radiation and panel temperature as inputs to predict current and load. 3. Applies multiple machine learning classifiers (neural network, PCA, decision tree, KNN) to diagnose specific fault patterns and types in the power subsystem.",
      "summary": "This paper proposes a machine learning-based method for detecting faults in nanosatellite electrical power systems. It first simulates normal system behavior using a neural network and then employs various classifiers to identify specific fault types. The approach aims to diagnose common faults like line-to-line shorts and open circuits without relying on an ADCS.",
      "mindmap": "graph TB\n        A[Smart Fault Detection in Nanosatellite Electrical Power System] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LEO轨道无ADCS的纳卫星电源系统故障检测/Fault detection for nanosatellite EPS without ADCS in LEO]\n        C --> C1[使用神经网络模拟无故障系统/Simulate fault-free system with neural network]\n        C --> C2[应用多种机器学习分类器进行故障诊断/Apply multiple ML classifiers for fault diagnosis]\n        D --> D1[诊断光伏、转换器、电池等特定故障/Diagnose specific faults in PV, converter, battery]"
    },
    {
      "title": "BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics",
      "authors": "Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles",
      "institution": "Brown University",
      "link": "https://arxiv.org/pdf/2601.00366",
      "code": null,
      "tags": [
        "multilingual representation learning",
        "Joint Embedding Predictive Architecture (JEPA)",
        "BERT",
        "CLS token",
        "language-agnostic embedding",
        "multilingual benchmarks"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp",
      "contributions": "1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance.",
      "summary": "The paper addresses the problem that BERT's [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic \"thought space\". The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.",
      "mindmap": "graph TB\n        Root[”BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics”] --> Problem[”核心问题/Problem: CLS embeddings are not language-invariant and fail to capture true sentence semantics.”]\n        Root --> Method[”主要方法/Method: Add JEPA training objective to BERT to create a language-agnostic embedding space.”]\n        Root --> Results[”关键结果/Results: Improved multilingual benchmark performance; reorganized, semantic-first CLS space.”]"
    },
    {
      "title": "Deterministic Coreset for Lp Subspace",
      "authors": "Rachit Chhaya, Anirban Dasgupta, Dan Feldman, Supratim Shit",
      "institution": "Dhirubhai Ambani University, IIT Gandhinagar, University of Haifa, IIIT-Delhi",
      "link": "https://arxiv.org/pdf/2601.00361",
      "code": null,
      "tags": [
        "randomized algorithms",
        "numerical linear algebra",
        "data summarization",
        "coreset",
        "subspace embedding",
        "ℓp regression",
        "deterministic algorithm",
        "iterative algorithm"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp",
      "contributions": "1. Introduces the first iterative algorithm for constructing a deterministic ε-coreset for ℓp subspace embedding for any p in [1,∞). 2. Achieves an optimal coreset size of O(d^\\{max(1,p/2)\\}/ε²), removing long-standing logarithmic factors. 3. Provides a deterministic guarantee for the coreset, enabling its use for approximately solving ℓp regression deterministically.",
      "summary": "This paper presents a new iterative algorithm for constructing a deterministic coreset that provides an ℓp subspace embedding for any p≥1. The method ensures bounded loss in each iteration, leading to a coreset whose size is optimal and free of logarithmic factors. The result solves a long-standing open problem and enables deterministic approximate solutions to ℓp regression.",
      "mindmap": "graph TB\n        Root(”Deterministic Coreset for Lp Subspace”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要为 ℓp 子空间嵌入构建确定性核心集 / Need deterministic coreset for ℓp subspace embedding”)\n        Problem --> P2(”现有核心集存在对数因子 / Existing coresets have log factors”)\n        Method --> M1(”迭代算法 / Iterative algorithm”)\n        Method --> M2(”确保有界损失 / Ensures bounded loss”)\n        Results --> R1(”核心集大小: O(d^{max(1,p/2)}/ε²) / Coreset size: O(d^{max(1,p/2)}/ε²)”)\n        Results --> R2(”移除对数因子 / Removes log factors”)\n        Results --> R3(”确定性保证 / Deterministic guarantee”)"
    },
    {
      "title": "Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing",
      "authors": "Md Mahbub Hasan, Marcus Sternhagen, Krishna Chandra Roy",
      "institution": "New Mexico Institute of Mining and Technology",
      "link": "https://arxiv.org/pdf/2601.00384",
      "code": null,
      "tags": [
        "cyber-physical systems security",
        "intrusion detection system",
        "anomaly detection",
        "G-code manipulation",
        "transformer encoder",
        "self-attention autoencoder"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp",
      "contributions": "1. Investigation of stealthy Man-in-the-Middle (MitM) attack vectors targeting the CAD-to-machine interface in Fused Deposition Modeling (FDM) 3D printers. 2. Proposal of an unsupervised Intrusion Detection System (IDS) that uses a frozen Transformer-based encoder and contrastive learning to create anomaly-sensitive embeddings from machine logs. 3. Demonstration of effective anomaly classification using a combination of clustering and a self-attention autoencoder on real 3D printing systems.",
      "summary": "This paper investigates stealthy cyberattacks that manipulate G-code in additive manufacturing systems, leading to structurally defective parts. To detect these attacks, the authors propose an unsupervised intrusion detection system that uses a Transformer-based encoder and contrastive learning to analyze machine logs. Their method successfully distinguishes between normal and compromised printing executions.",
      "mindmap": "graph TB\n    A[Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[AM系统的新攻击面 / New Attack Surfaces in AM]\n    B --> B2[隐秘的中间人攻击 / Stealthy MitM Attacks]\n    C --> C1[基于日志的无监督IDS / Unsupervised IDS from Logs]\n    C --> C2[Transformer编码器 / Transformer Encoder]\n    C --> C3[对比学习与自注意力 / Contrastive Learning & Self-Attention]\n    D --> D1[有效区分正常与攻击 / Effectively Distinguishes Benign & Compromised]"
    },
    {
      "title": "Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models",
      "authors": "Nouar AlDahoul, Aznul Qalid Md Sabri, Ali Mohammed Mansoor",
      "institution": "University of Malaya",
      "link": "https://arxiv.org/pdf/2601.00391",
      "code": null,
      "tags": [
        "object detection",
        "aerial video",
        "optical flow",
        "convolutional neural network",
        "hierarchical extreme learning machine",
        "UCF-ARG dataset"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/104ed4332f4aebfb8e2617a89656868c3e4e5fae6201be63d4509d5b080a2f50_w640_q70.webp",
      "contributions": "1. Proposes a framework combining optical flow with three different deep learning models (S-CNN, pretrained CNN, H-ELM) for human detection in challenging aerial videos. 2. Conducts a comparative performance analysis of the models on the UCF-ARG dataset, evaluating accuracy and training speed across five human actions. 3. Demonstrates the effectiveness of automatic feature learning over handcrafted features for handling dynamic events like camera jitter and scale variation in aerial footage.",
      "summary": "This paper addresses the challenge of human detection in aerial videos with a non-static camera. It proposes using optical flow combined with three deep learning models (S-CNN, a pretrained CNN, and H-ELM) for automatic feature learning. The experiments on the UCF-ARG dataset show that the pretrained CNN achieves the highest accuracy (98.09%), successfully demonstrating the method's robustness to dynamic conditions.",
      "mindmap": "graph TB\n        A[Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[动态事件挑战<br/>Dynamic Event Challenges]\n        B1 --> B2[光照变化, 相机抖动, 目标尺寸变化<br/>Illumination Changes, Camera Jitter, Object Size Variation]\n        C --> C1[特征学习方法<br/>Feature Learning Methods]\n        C1 --> C2[结合光流与三种深度模型<br/>Combine Optical Flow with Three Deep Models]\n        C2 --> C3[监督CNN, 预训练CNN, 分层极限学习机<br/>S-CNN, Pretrained CNN, H-ELM]\n        D --> D1[预训练CNN准确率最高<br/>Pretrained CNN Highest Accuracy]\n        D1 --> D2[平均准确率98.09%<br/>Average Accuracy 98.09%]"
    },
    {
      "title": "NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion",
      "authors": "Muhammad Bilal, Omer Tariq, Hasan Ahmed",
      "institution": "Lancaster University, Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2601.00389",
      "code": null,
      "tags": [
        "network intrusion detection",
        "timing-controlled evasion",
        "weighted fair queueing (WFQ)",
        "network-optimised spiking (NOS)",
        "metadata-only detection",
        "streaming IDS"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16e859896027b80a597ad555df2beab42dc7cf683d8aef57af2a60ff1820126c_w640_q70.webp",
      "contributions": "1. Proposed NOS-Gate, a lightweight, streaming IDS for consumer gateways that uses a two-state unit derived from Network-Optimised Spiking dynamics per flow. 2. Introduced a queue-aware, reversible mitigation action that temporarily reduces a flagged flow's weight under Weighted Fair Queueing (WFQ). 3. Developed an executable 'worlds' benchmark for evaluating IDS under timing-controlled evasion, specifying benign processes, attacker budgets, and enabling packet-level WFQ replay.",
      "summary": "The paper addresses the challenge of detecting intrusions in encrypted traffic on resource-constrained consumer gateways, where attackers can evade detection by manipulating timing patterns. It proposes NOS-Gate, a lightweight streaming IDS that uses metadata features and a novel mitigation strategy integrated with queue management. The evaluation shows NOS-Gate achieves higher detection recall and reduces queueing delays compared to baselines, with low computational overhead.",
      "mindmap": "graph TB\n        Root[NOS-Gate: Queue-Aware Streaming IDS] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[加密流量中时序模式泄露/Timing patterns leak through encryption]\n        Problem --> P2[攻击者进行时序控制规避/Attacker uses timing-controlled evasion]\n        Problem --> P3[网关资源严格受限/Gateway has tight CPU & latency budget]\n        Method[主要方法/Method] --> M1[轻量级双态单元/Lightweight two-state NOS unit per flow]\n        Method --> M2[基于元数据窗口的评分/Score fixed-length metadata windows]\n        Method --> M3[可逆的WFQ权重缓解/Reversible WFQ weight mitigation]\n        Results[关键结果/Results] --> R1[高事件召回率/High incident recall (0.952)]\n        Results --> R2[降低排队延迟/Reduced p99.9 queueing delay]\n        Results --> R3[低计算开销/Low scoring cost (~2.09 µs)]"
    },
    {
      "title": "Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving",
      "authors": "Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov",
      "institution": "Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00397",
      "code": null,
      "tags": [
        "llm inference",
        "time-warp emulation",
        "CUDA interception",
        "virtual time coordination",
        "performance modeling",
        "discrete-event simulation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp",
      "contributions": "1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution.",
      "summary": "The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang.",
      "mindmap": "graph TB\n        A[Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GPU集群评估成本高且慢/Evaluating serving configs on GPU clusters is slow and expensive]\n        B --> B2[模拟器需要重写控制逻辑/Simulators require re-implementing complex control logic]\n        C --> C1[拦截CUDA API调用/Intercept CUDA API calls]\n        C --> C2[虚拟时间跳跃/Virtual time jumps based on kernel predictions]\n        C --> C3[分布式协调协议/Distributed coordination protocol]\n        D --> D1[<5%预测误差/<5% prediction error]\n        D --> D2[5-17倍加速/5-17x faster than real execution]"
    },
    {
      "title": "Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution",
      "authors": "Prajwal Panth, Sahaj Raj Malla",
      "institution": "KIIT University, Kathmandu University",
      "link": "https://arxiv.org/pdf/2601.00418",
      "code": null,
      "tags": [
        "Privacy-preserving data aggregation",
        "unanimous-release confidentiality",
        "consensus locking",
        "malicious deviation detection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp",
      "contributions": "1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (σ_S, σ_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework's properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines.",
      "summary": "The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE.",
      "mindmap": "graph TB\n        A[Secure, Verifiable, and Scalable Multi-Client Data Sharing<br>安全、可验证、可扩展的多客户端数据共享] --> B(核心问题/Problem: Secure multi-client data aggregation with privacy and verifiability<br>安全、可验证的多客户端隐私数据聚合)\n        A --> C(主要方法/Method: Consensus-Based Privacy-Preserving Data Distribution (CPPDD)<br>基于共识的隐私保护数据分发)\n        C --> C1(Affine Masking & Consensus Locking<br>仿射掩码与共识锁定)\n        C --> C2(Step/Data Checksums (σ_S, σ_D)<br>步骤/数据校验和)\n        A --> D(关键结果/Results: Linear scalability, 100% deviation detection, lower FLOPs vs MPC/HE<br>线性可扩展性，100%异常检测，相比MPC/HE更低的计算量)"
    },
    {
      "title": "RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers",
      "authors": "Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta",
      "institution": "Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2601.00426",
      "code": null,
      "tags": [
        "efficient transformers",
        "astrocyte-inspired computing",
        "long-term plasticity (LTP)",
        "short-term plasticity (STP)",
        "memory compression",
        "Long Range Arena (LRA)"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp",
      "contributions": "1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks.",
      "summary": "This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency.",
      "mindmap": "graph TB\n        Root[RMAAT: Astrocyte-Inspired Memory Compression and Replay] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Transformer自注意力二次复杂度/Quadratic Complexity of Self-Attention]\n        Method[主要方法/Method: 星形胶质细胞启发的循环记忆架构/Astrocyte-Inspired Recurrent Memory Architecture]\n        Results[关键结果/Results: 在LRA基准上具有竞争力的准确性和效率/Competitive Accuracy & Efficiency on LRA]"
    },
    {
      "title": "Deep Delta Learning",
      "authors": "Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu",
      "institution": "Princeton University, University of California, Los Angeles",
      "link": "https://arxiv.org/pdf/2601.00417",
      "code": "https://github.com/yifanzhang-pro/deep-delta-learning",
      "tags": [
        "neural network architecture",
        "residual networks",
        "geometric transformation",
        "spectral analysis",
        "rank-1 perturbation",
        "dynamic gating"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",
      "contributions": "1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.",
      "summary": "This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.",
      "mindmap": "graph TB\n        A[Deep Delta Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[残差网络限制/ResNet Limitation]\n        B1 --> B2[”刚性相加偏置/Rigid Additive Bias”]\n        B2 --> B3[”限制复杂状态转换/Limits Complex State Transitions”]\n        C --> C1[Delta 算子/Delta Operator]\n        C1 --> C2[”秩-1扰动/ Rank-1 Perturbation”]\n        C2 --> C3[”可学习几何变换/Learnable Geometric Transform”]\n        C3 --> C4[”动态门控/Dynamic Gating (β)”]\n        D --> D1[”谱分析/Spectral Analysis”]\n        D1 --> D2[”插值身份/投影/反射/Interpolates Identity/Projection/Reflection”]\n        D --> D3[”同步秩-1注入/Synchronous Rank-1 Injection”]\n        D3 --> D4[”控制转换谱/Controls Transition Spectrum”]\n        D4 --> D5[”保持稳定训练/Preserves Stable Training”]"
    },
    {
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "authors": "Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2601.00423",
      "code": "https://github.com/shengjun-zhang/VisualGRPO",
      "tags": [
        "reinforcement learning for human feedback",
        "reinforcement learning from human feedback (RLHF)",
        "flow matching",
        "stochastic differential equations (SDE)",
        "group relative policy optimization (GRPO)",
        "entropy-aware sampling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",
      "contributions": "1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.",
      "summary": "The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.",
      "mindmap": "graph TB\n        A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法在多个去噪步上优化，奖励信号稀疏模糊/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n        C --> C1[提出E-GRPO: 熵感知分组相对策略优化/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n        C1 --> C2[合并低熵步为高熵SDE采样步，其他步用ODE采样/Merge low-entropy steps for SDE, use ODE elsewhere]\n        C1 --> C3[引入多步分组归一化优势计算/Introduce multi-step group normalized advantage]\n        D --> D1[在不同奖励设置下验证了方法的有效性/Method effectiveness demonstrated across different reward settings]"
    },
    {
      "title": "A Comparative Analysis of Interpretable Machine Learning Methods",
      "authors": "Mattia Billa, Giovanni Orlandi, Veronica Guidetti, Federica Mandreoli",
      "institution": "University of Modena and Reggio Emilia",
      "link": "https://arxiv.org/pdf/2601.00428",
      "code": null,
      "tags": [
        "interpretable machine learning",
        "Explainable Boosting Machines (EBMs)",
        "Symbolic Regression (SR)",
        "Generalized Optimal Sparse Decision Trees (GOSDT)",
        "Interpretable Generalized Additive Neural Networks (IGANNs)",
        "tabular data"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b45f5348139a7a9be289787000b2099d616e153113ae1190dc4749f573df65bb_w640_q70.webp",
      "contributions": "1. Conducted a large-scale comparative evaluation of 16 inherently interpretable methods across 216 real-world tabular datasets. 2. Stratified performance analysis based on structural dataset characteristics (dimensionality, sample size, linearity, class imbalance) and assessed training time and robustness under distributional shifts. 3. Provided empirical findings on performance hierarchies and context-dependent model suitability, offering practical guidance for balancing interpretability and predictive performance.",
      "summary": "This paper addresses the scarcity of systematic evaluations for inherently interpretable machine learning models on tabular data by conducting a large-scale benchmark of 16 methods. The study analyzes performance across 216 datasets, considering structural characteristics and robustness. The results show that EBMs are strong for regression, while SR and IGANNs excel in non-linear settings, and GOSDT is sensitive to class imbalance, providing context-dependent guidance for practitioners.",
      "mindmap": "graph TB\n        A[A Comparative Analysis of Interpretable Machine Learning Methods<br/>可解释机器学习方法比较分析] --> B[Problem: Need for systematic evaluation of inherently interpretable models for tabular data<br/>问题: 缺乏对表格数据固有可解释模型的系统评估]\n        A --> C[Method: Large-scale benchmark of 16 interpretable methods across 216 datasets, stratified by data characteristics<br/>方法: 在216个数据集上对16种方法进行大规模基准测试，按数据特征分层]\n        A --> D[Results: EBMs strong for regression; SR/IGANNs good for non-linear data; GOSDT sensitive to imbalance<br/>结果: EBMs在回归中表现好；SR/IGANNs在非线性数据中表现好；GOSDT对不平衡敏感]"
    },
    {
      "title": "A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection",
      "authors": "Miseon Park, Kijung Yoon",
      "institution": "Hanyang University",
      "link": "https://arxiv.org/pdf/2601.00446",
      "code": null,
      "tags": [
        "time series anomaly detection",
        "time series foundation models",
        "parameter-efficient fine-tuning",
        "anomaly detection",
        "LoRA",
        "AUC-PR"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94e39b065d01be71b9920224b4a74b3db8c40a04646e3c3b4b83dce81d7aa0c3_w640_q70.webp",
      "contributions": "1. Systematically evaluated the use of Time Series Foundation Models (TSFMs) as universal backbones for anomaly detection, showing they outperform task-specific models. 2. Compared multiple adaptation strategies (zero-shot, full fine-tuning, PEFT) for TSFMs across benchmarks, highlighting their versatility. 3. Demonstrated that Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA can match or exceed full fine-tuning performance while being computationally cheaper.",
      "summary": "This paper investigates whether pretrained Time Series Foundation Models (TSFMs) can be effectively adapted for anomaly detection. It compares different adaptation strategies, including zero-shot inference and parameter-efficient fine-tuning (PEFT). The results show that TSFMs, especially when adapted with PEFT methods like LoRA, outperform traditional task-specific models, offering a scalable and efficient solution for time series anomaly detection.",
      "mindmap": "graph TB\n        Root[”A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection<br>时间序列基础模型在异常检测中适应策略的比较研究”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Most anomaly detection methods need extensive task-specific training.<br>大多数异常检测方法需要大量特定任务的训练。”]\n        Method[”主要方法/Method<br>Adapt TSFMs using zero-shot, full fine-tuning, and PEFT (e.g., LoRA).<br>使用零样本、全微调和PEFT（如LoRA）来适应TSFMs。”]\n        Results[”关键结果/Results<br>TSFMs outperform baselines; PEFT is efficient and effective.<br>TSFMs超越基线；PEFT高效且有效。”]"
    },
    {
      "title": "Controllable Concept Bottleneck Models",
      "authors": "Hongbin Lin, Chenyang Ren, Juangui Xu, Zhengyu Hu, Cheng-Long Wang, Yao Shu, Hui Xiong, Jingfeng Zhang, Di Wang, Lijie Hu",
      "institution": "Based on the author list and affiliations (Hui Xiong, Fellow, IEEE), the primary institution is likely Rutgers University. Other affiliations may be present but are not explicitly listed in the provided content.",
      "link": "https://arxiv.org/pdf/2601.00451",
      "code": null,
      "tags": [
        "explainable ai",
        "Concept Bottleneck Models",
        "Model Editing",
        "Influence Functions",
        "Machine Unlearning",
        "Incremental Learning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8610f217ce88f4c2d5513fec62b4fe3c5b8a65762f2a3fae9fcf60d3e539a686_w640_q70.webp",
      "contributions": "1. Proposes Controllable Concept Bottleneck Models (CCBMs) that support three granularities of model editing (concept-label, concept, and data-level) for dynamic maintenance. 2. Derives mathematically rigorous closed-form approximations for editing operations using influence functions, eliminating the need for retraining from scratch. 3. Demonstrates the efficiency and adaptability of CCBMs through experiments, validating their practical value for creating dynamic and trustworthy models.",
      "summary": "The paper addresses the challenge of efficiently editing Concept Bottleneck Models (CBMs) in dynamic real-world scenarios without retraining. It proposes Controllable Concept Bottleneck Models (CCBMs), which use influence functions to provide closed-form approximations for edits at concept-label, concept, and data levels. Experimental results show that CCBMs are efficient and adaptable, making them practical for maintaining trustworthy AI systems.",
      "mindmap": "graph TB\n        A[Controllable Concept Bottleneck Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[静态CBM难以动态编辑/Static CBMs are hard to edit dynamically]\n        C --> C1[CCBM与影响函数/CCBMs with Influence Functions]\n        D --> D1[高效且适应性强/Efficient and Adaptable]"
    },
    {
      "title": "Imitation from Observations with Trajectory-Level Generative Embeddings",
      "authors": "Yongtao Qu, Shangzhe Li, Weitong Zhang",
      "institution": "University of North Carolina at Chapel Hill",
      "link": "https://arxiv.org/pdf/2601.00452",
      "code": null,
      "tags": [
        "imitation learning",
        "imitation learning from observations",
        "offline learning",
        "diffusion models",
        "trajectory embedding",
        "surrogate reward"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8023941a359742c0280304bc5c6d6f3a9e7332f5f298e3867930992f450af6fb_w640_q70.webp",
      "contributions": "1. Proposes TGE, a trajectory-level generative embedding method for offline imitation learning from observations. 2. Introduces a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model. 3. Demonstrates that the method effectively bridges distributional gaps and outperforms prior methods on D4RL benchmarks.",
      "summary": "The paper addresses offline imitation learning from observations where expert data is scarce and offline data is suboptimal. It proposes TGE, a method that uses a temporal diffusion model to create a smooth trajectory embedding for robust reward estimation. The approach outperforms existing methods on standard locomotion and manipulation benchmarks.",
      "mindmap": "graph TB\n        A[Imitation from Observations with Trajectory-Level Generative Embeddings] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[离线模仿学习，专家数据稀缺，离线数据不理想/Offline LfO, scarce expert data, imperfect offline data]\n        C --> C1[提出TGE，利用扩散模型嵌入构建平滑的轨迹级奖励/Propose TGE, constructs smooth trajectory-level reward via diffusion embedding]\n        D --> D1[在D4RL基准上匹配或超越现有方法/Matches or outperforms prior methods on D4RL benchmarks]"
    },
    {
      "title": "Deep Networks Learn Deep Hierarchical Models",
      "authors": "Amit Daniely",
      "institution": "Hebrew University of Jerusalem, Google Research Tel Aviv",
      "link": "https://arxiv.org/pdf/2601.00455",
      "code": null,
      "tags": [
        "theoretical machine learning",
        "hierarchical models",
        "residual networks",
        "layerwise SGD",
        "efficient learnability",
        "teacher-student framework"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp",
      "contributions": "1. Proves that layerwise SGD on residual networks can efficiently learn a class of hierarchical models with polynomial depth, surpassing previous learnable models limited to log-depth. 2. Introduces a formal model where the existence of human teachers, providing granular labels, naturally reveals a hierarchical structure that facilitates learning. 3. Suggests that the learnability of deep hierarchical models could form a theoretical basis for understanding why deep learning works.",
      "summary": "The paper shows that layerwise stochastic gradient descent (SGD) on residual networks can efficiently learn a class of hierarchical models where labels are structured in increasingly complex levels. This class is more expressive, requiring polynomial depth, than previously known learnable models. The authors argue that this learnability, supported by a formal model of teaching, provides a potential theoretical foundation for understanding deep learning's success.",
      "mindmap": "graph TB\n    A[Deep Networks Learn Deep Hierarchical Models] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[理解深度网络为何有效/Understanding why deep networks work]\n    C --> C1[层级SGD与残差网络/Layerwise SGD on ResNets]\n    C --> C2[形式化教师-学生模型/Formal teacher-student model]\n    D --> D1[可高效学习多项式深度模型/Efficiently learn polynomial-depth models]\n    D --> D2[超越对数深度电路/Surpasses log-depth circuits]\n    D --> D3[为理解深度学习提供基础/Provides basis for understanding deep learning]"
    },
    {
      "title": "Laplacian Kernelized Bandit",
      "authors": "Shuang Wu, Arash A. Amini",
      "institution": "University of California, Los Angeles (UCLA)",
      "link": "https://arxiv.org/pdf/2601.00461",
      "code": null,
      "tags": [
        "multi-armed bandits",
        "graph Laplacian",
        "reproducing kernel Hilbert space (RKHS)",
        "Gaussian process",
        "regret bound",
        "multi-user contextual bandits"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4eb4360cdf936fab9711bd6722ad2b67b34b282bf409e4e900beb17c23a0b2_w640_q70.webp",
      "contributions": "1. Introduced a principled joint penalty combining graph smoothness and individual roughness, proving it is equivalent to the squared norm in a unified multi-user RKHS. 2. Explicitly derived the reproducing kernel for this RKHS, which fuses the graph Laplacian with a base arm kernel. 3. Designed two algorithms (LK-GP-UCB and LK-GP-TS) based on this kernel and provided theoretical regret bounds scaling with the kernel's effective dimension.",
      "summary": "This paper addresses multi-user contextual bandits where users are connected by a graph and have non-linear reward functions. It proposes a new kernel that combines graph structure with arm features, enabling the design of efficient Gaussian Process-based algorithms for exploration. The method provides strong theoretical regret guarantees and outperforms baselines in non-linear settings.",
      "mindmap": "graph TB\n        A[Laplacian Kernelized Bandit] --> B[核心问题/Problem: Multi-user contextual bandits with graph homophily and non-linear rewards]\n        A --> C[主要方法/Method: Unified multi-user RKHS with Laplacian-fused kernel; Algorithms LK-GP-UCB & LK-GP-TS]\n        A --> D[关键结果/Results: Regret bounds scale with effective kernel dimension; Outperforms baselines empirically]"
    },
    {
      "title": "Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations",
      "authors": "Hyunjun Kim",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2601.00457",
      "code": null,
      "tags": [
        "llm training",
        "Mixture-of-Experts",
        "Orthogonality Regularization",
        "Weight-Activation Gap",
        "Sparse Activation",
        "Expert Diversity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp",
      "contributions": "1. Showed that orthogonality regularization fails to reduce weight-space overlap and yields inconsistent effects on model performance across different datasets. 2. Identified a significant disconnect between weight-space and activation-space orthogonality, with no significant correlation between the two. 3. Demonstrated that weight-space regularization is an unreliable optimization target for improving expert diversity in MoE models.",
      "summary": "This paper investigates the effectiveness of applying orthogonality loss to enforce expert diversity in Mixture-of-Experts (MoE) models. The analysis reveals that this geometric regularization fails to reduce weight-space overlap, does not translate to activation-space orthogonality, and leads to inconsistent performance changes. The findings demonstrate that weight-space regularization is unsuitable for achieving MoE diversity.",
      "mindmap": "graph TB\n        A[Geometric Regularization in MoEs: The Disconnect Between Weights and Activations] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(专家多样性的几何正则化作用不明确/The role of geometric regularization in expert specialization is unclear)\n        C --> C1(应用正交性损失以强制专家多样性/Apply orthogonality loss to enforce expert diversity)\n        D --> D1(权重空间重叠未减少/Weight-space overlap not reduced)\n        D --> D2(激活空间重叠保持高位/Activation-space overlap remains high)\n        D --> D3(性能影响不一致/Inconsistent effects on performance)\n        D --> D4(权重与激活正交性无显著相关/No significant correlation between weight and activation orthogonality)"
    },
    {
      "title": "Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet",
      "authors": "Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown",
      "institution": "University of Virginia",
      "link": "https://arxiv.org/pdf/2601.00459",
      "code": null,
      "tags": [
        "biomedical signal processing",
        "EEG",
        "UNet",
        "data augmentation",
        "spike wave discharges",
        "seizure detection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/284cd89a49b52c6de51bc92974e7ebd74725aa53109914c27ce628c85db7b46c_w640_q70.webp",
      "contributions": "1. Comprehensive comparison of 14 machine learning classifiers on a large, manually annotated EEG dataset for SWD detection, identifying a 1D UNet as the best performer. 2. Enhancement of the 1D UNet model through data augmentation, with scaling identified as the most beneficial augmentation technique, resulting in the AugUNet1D model. 3. Public release of the AugUNet1D model (both pretrained and untrained) and demonstration of its superior performance against a state-of-the-art algorithmic method (\"Twin Peaks\").",
      "summary": "This paper addresses the time-consuming manual labeling of spike wave discharges (SWDs) in EEG recordings by proposing an automated detection method. The authors developed and evaluated a 1D UNet model enhanced with data augmentation (AugUNet1D) on a large mouse EEG dataset, finding it outperformed other classifiers and a recent algorithmic approach. The main conclusion is that AugUNet1D provides a superior, publicly available tool for accurate SWD detection.",
      "mindmap": "graph TB\n        A[Detecting SWD using 1D Residual UNet<br/>使用一维残差UNet检测SWD] --> B\n        A --> C\n        A --> D\n        B[Problem: Manual EEG labeling is time-consuming<br/>问题: 手动标记EEG耗时] --> B1[Target: Automate SWD detection<br/>目标: 自动化SWD检测]\n        C[Method: 1D UNet with data augmentation (AugUNet1D)<br/>方法: 使用数据增强的一维UNet] --> C1[Compared 14 classifiers<br/>比较了14种分类器]\n        D[Results: AugUNet1D outperforms other methods<br/>结果: AugUNet1D性能最优] --> D1[Model made public<br/>模型已公开]"
    },
    {
      "title": "Neural Chains and Discrete Dynamical Systems",
      "authors": "Sauro Succi, Abhisek Ganguly, Santosh Ansumali",
      "institution": "Italian Institute of Technology, Jawaharlal Nehru Centre for Advanced Scientific Research (JNCASR), University of Roma Tre, Harvard University, Cornell University",
      "link": "https://arxiv.org/pdf/2601.00473",
      "code": null,
      "tags": [
        "scientific machine learning",
        "neural chains",
        "physics-informed neural networks (PINNs)",
        "finite-difference methods",
        "Burgers equation",
        "Eikonal equation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp",
      "contributions": "1. Proposes and analyzes the analogy between transformer-based neural chains (without self-attention) and discrete dynamical systems from discretized neural integral/PDEs. 2. Conducts a comparative analysis between standard numerical discretization (finite-difference) and PINN learning for solving Burgers and Eikonal equations, showing they converge to similar dynamical knowledge. 3. Identifies that PINNs explore a vast space of random matrices, unlike the structured matrices of finite-difference methods, leading to more parameters, higher training costs, and reduced explainability for 1D problems.",
      "summary": "This paper investigates the connection between neural chains (transformers without self-attention) and discrete dynamical systems. It compares solving PDEs like Burgers and Eikonal equations using standard finite-difference methods versus Physics-Informed Neural Networks (PINNs), finding both methods yield similar solutions but PINNs use many more random, less interpretable parameters. The authors conclude that for these 1D problems, PINNs offer no efficiency advantage over traditional methods, though their potential for high-dimensional problems remains open.",
      "mindmap": "graph TB\n        A[Neural Chains and Discrete Dynamical Systems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[分析神经链与离散动力系统的类比/Analyze analogy between neural chains and discrete dynamical systems]\n        B --> B2[比较PINN与传统数值方法/Compare PINNs vs. traditional numerical methods]\n        C --> C1[将数值离散化表述为神经链/Cast numerical discretization as neural chains]\n        C --> C2[使用PINN求解方程/Use PINNs to solve equations]\n        C --> C3[比较矩阵结构与参数空间/Compare matrix structure and parameter space]\n        D --> D1[两种方法获得相同动力学知识/Both methods acquire same dynamical knowledge]\n        D --> D2[PINN使用更多随机参数/PINNs use more random parameters]\n        D --> D3[1D问题中PINN无效率优势/No PINN efficiency advantage for 1D problems]"
    },
    {
      "title": "Noise-Aware Named Entity Recognition for Historical VET Documents",
      "authors": "Alexander M. Esser, Jens Dörpinghaus",
      "institution": "Federal Institute for Vocational Education and Training (BIBB), University of Koblenz",
      "link": "https://arxiv.org/pdf/2601.00488",
      "code": null,
      "tags": [
        "named entity recognition",
        "Noise-Aware Training (NAT)",
        "OCR Noise",
        "Data Augmentation",
        "Transfer Learning",
        "Multi-stage Fine-tuning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp",
      "contributions": "1. Proposes a robust NER approach for historical VET documents using Noise-Aware Training with synthetic OCR errors. 2. Systematically compares three complementary training strategies (noisy, clean, and artificial data). 3. Demonstrates that domain-specific and noise-aware fine-tuning significantly improves robustness and accuracy under noisy conditions.",
      "summary": "This paper tackles Named Entity Recognition in noisy, historical Vocational Education and Training documents by proposing a method using Noise-Aware Training with synthetic OCR errors, transfer learning, and multi-stage fine-tuning. The approach, one of the first to recognize multiple entity types in this domain, shows that domain-specific and noise-aware fine-tuning substantially increases model robustness and accuracy. The method is applied to German but is designed to be transferable to other languages.",
      "mindmap": "graph TB\n        Root[Noise-Aware NER for Historical VET Documents] --> Problem(核心问题/Problem: NER in noisy historical VET documents)\n        Root --> Method(主要方法/Method: Noise-Aware Training with synthetic OCR errors, transfer learning, multi-stage fine-tuning)\n        Root --> Results(关键结果/Results: Increased robustness and accuracy under noisy conditions)"
    },
    {
      "title": "Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback",
      "authors": "Vidyut Sriram, Sawan Pandita, Achintya Lakshmanan, Aneesh Shamraj, Suman Saha",
      "institution": "Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2601.00509",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "Retrieval-Augmented Generation",
        "CodeQL",
        "KLEE",
        "Self-Repair",
        "Symbolic Execution"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdcdc91c311cf046454507445fa7f6baef39a437738ec8e32b2eb087f6fbfa91_w640_q70.webp",
      "contributions": "1. Proposes a retrieval-augmented, multi-tool repair workflow integrating compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution for iterative LLM self-repair. 2. Utilizes a lightweight embedding model for semantic retrieval of security-focused repair examples to guide code generation. 3. Demonstrates significant robustness improvements, reducing security vulnerabilities by up to 96% for DeepSeek-Coder and from 58.55% to 22.19% for CodeLlama.",
      "summary": "The paper addresses the problem of LLM-generated code containing security vulnerabilities and errors. It proposes a method where a code-generating LLM iteratively refines its output using feedback from multiple tools (compiler, CodeQL, KLEE) and retrieval of past successful repairs. The results show this approach significantly reduces security defects, even for larger, more stubborn models.",
      "mindmap": "graph TB\n        Root[”Improving LLM-Assisted Secure Code Generation<br>提升LLM辅助安全代码生成”] --> Problem[”LLM生成代码存在安全漏洞<br>LLM-generated code has security vulnerabilities”]\n        Root --> Method[”检索增强的多工具自修复工作流<br>Retrieval-Augmented Multi-Tool Self-Repair Workflow”]\n        Root --> Results[”安全漏洞显著减少<br>Security vulnerabilities significantly reduced”]\n        Problem --> P1[”逻辑不一致<br>Logical inconsistencies”]\n        Problem --> P2[”编译错误<br>Compilation errors”]\n        Method --> M1[”检索修复示例<br>Retrieve repair examples”]\n        Method --> M2[”工具反馈(编译器/CodeQL/KLEE)<br>Tool feedback (Compiler/CodeQL/KLEE)”]\n        Method --> M3[”迭代自修复<br>Iterative self-repair”]\n        Results --> R1[”DeepSeek漏洞减少96%<br>DeepSeek vulnerabilities reduced 96%”]\n        Results --> R2[”CodeLlama关键缺陷率22.19%<br>CodeLlama critical defect rate 22.19%”]"
    },
    {
      "title": "When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents",
      "authors": "Laksh Advani",
      "institution": "Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)",
      "link": "https://arxiv.org/pdf/2601.00513",
      "code": null,
      "tags": [
        "agent system",
        "reasoning integrity",
        "retrieval-augmented generation",
        "process verification",
        "small language models",
        "neural classifier"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b874fd2b8e4599623bc1f3efd5491de7179346cb166566b9907ab13137ae7a_w640_q70.webp",
      "contributions": "1. Introduces the \"Right-for-Wrong-Reasons\" (RWR) phenomenon and the Reasoning Integrity Score (RIS), a novel process-based metric for evaluating the trustworthiness of small language model agents, validated with high inter-rater agreement. 2. Empirically demonstrates that while retrieval-augmented generation (RAG) significantly improves reasoning integrity, meta-cognitive interventions like self-critique often degrade it in small models, providing mechanistic explanations for these effects. 3. Distills the verification capability into a lightweight neural classifier that achieves high performance (0.86 F1-score) and efficiency (100x speedup), enabling practical process-based verification for deployment.",
      "summary": "This paper identifies a critical reliability problem where small language models (7-9B parameters) often produce correct answers based on flawed reasoning, a phenomenon invisible to standard accuracy metrics. To address this, the authors introduce a process-based evaluation metric (RIS) and analyze the impact of interventions like RAG and self-critique, finding RAG improves reasoning while self-critique harms it in small models. They conclude that process verification is essential for trustworthy agents and demonstrate a fast neural classifier for this purpose.",
      "mindmap": "graph TB\n        A[When Small Models Are Right for Wrong Reasons<br>当小模型因错误原因而正确时] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Small models give correct answers with flawed reasoning<br>小模型基于错误推理给出正确答案]\n        C[主要方法/Method<br>Introduce Reasoning Integrity Score (RIS)<br>引入推理完整性评分]\n        D[关键结果/Results<br>RAG helps, self-critique hurts; Neural verifier is fast<br>RAG有效，自我批判有害；神经验证器速度快]"
    },
    {
      "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI",
      "authors": "Laksh Advani",
      "institution": "Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)",
      "link": "https://arxiv.org/pdf/2601.00516",
      "code": null,
      "tags": [
        "agent system",
        "Siamese Recurrent Autoencoder",
        "hybrid loss",
        "real-time anomaly detection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp",
      "contributions": "1. Demonstrated the ineffectiveness of standard anomaly detection methods for agent trajectory validation, establishing the need for specialized models. 2. Proposed a novel, sequence-aware Siamese Recurrent Autoencoder with a hybrid loss function for real-time trajectory anomaly detection. 3. Demonstrated that the approach is over 17x faster than LLM Judge baselines, making it suitable for real-time deployment.",
      "summary": "The paper addresses the problem of detecting anomalous action plans in autonomous LLM agents, where existing methods fail to capture sequential structure and context. It proposes Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss combining contrastive learning and reconstruction for unified anomaly detection. The method achieves high F1-scores (0.88-0.94) and significantly faster inference (32 ms) than LLM-based baselines, enabling real-time safety verification.",
      "mindmap": "graph TB\n        Root[Trajectory Guard] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[现有方法不适用/Existing methods ill-suited]\n        Problem --> P2[需要序列感知/Need for sequence-awareness]\n        Method --> M1[孪生循环自编码器/Siamese Recurrent Autoencoder]\n        Method --> M2[混合损失函数/Hybrid Loss Function]\n        M2 --> M2a[对比学习/Contrastive Learning]\n        M2 --> M2b[重建/Reconstruction]\n        Results --> R1[高F1分数/High F1-scores (0.88-0.94)]\n        Results --> R2[低延迟/32 ms latency]\n        Results --> R3[实时部署/Real-time deployment]"
    },
    {
      "title": "Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study",
      "authors": "Ravi Teja Pagidoju",
      "institution": "Campbellsville University",
      "link": "https://arxiv.org/pdf/2601.00525",
      "code": "https://github.com/RaviTeja444/sales-forecast-LSTM",
      "tags": [
        "model compression (quantization/pruning)",
        "LSTM compression",
        "model efficiency",
        "retail forecasting",
        "edge computing",
        "hidden units"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp",
      "contributions": "1. Systematic evaluation of LSTM network sizes from 16 to 128 hidden units on real retail data. 2. Discovery that moderate compression (to 64 units) actually improves forecast accuracy. 3. Practical guidelines for model selection based on the accuracy-efficiency trade-off.",
      "summary": "This paper studies LSTM model compression for resource-constrained retail sales forecasting by reducing the number of hidden units. The method involves systematically pruning the LSTM from 128 to 16 hidden units. The main conclusion is that reducing the model to 64 units not only makes it 73% smaller but also improves accuracy by 47%, showing larger models are not always better.",
      "mindmap": "graph TB\n        A[Optimizing LSTM for Resource-Constrained Retail Forecasting] --> B(核心问题/Problem: LSTM模型计算需求大，中小型零售商难以部署/LSTM models are computationally expensive for mid-to-small retailers)\n        A --> C(主要方法/Method: 通过逐步减少隐藏单元进行模型压缩/Model compression by reducing hidden units from 128 to 16)\n        A --> D(关键结果/Results: 64单元模型更小(76KB vs 280KB)且更准确(MAPE 12.4% vs 23.6%)/64-unit model is smaller and more accurate)"
    },
    {
      "title": "A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling",
      "authors": "Dristi Datta, Tanmoy Debnath, Minh Chau, Manoranjan Paul, Gourab Adhikary, Md Geaur Rahman",
      "institution": "Charles Sturt University",
      "link": "https://arxiv.org/pdf/2601.00519",
      "code": null,
      "tags": [
        "multimodal learning",
        "sparse-attention",
        "cross-attention",
        "class-balanced focal loss",
        "multimodal fusion",
        "explainable AI"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0104d1bebca16cde5f74be8dbe6bfbcfb976784343fbe1f1f8025e57eed10_w640_q70.webp",
      "contributions": "1. Proposed the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for multimodal Parkinson's disease profiling. 2. Introduced a sparsity-constrained attention-gating fusion layer to dynamically prioritize informative modalities from heterogeneous data (MRI, clinical, demographic). 3. Employed a Class-Balanced Focal Loss to effectively handle dataset imbalance without synthetic oversampling, achieving high performance.",
      "summary": "The paper proposes SAFN, an interpretable deep learning model that integrates MRI and clinical data using sparse-attention and cross-attention mechanisms for Parkinson's disease severity profiling. It addresses challenges in multimodal fusion and class imbalance. The model achieves high accuracy and interpretability, aligning clinical assessment importance with diagnostic principles.",
      "mindmap": "graph TB\n        Root[SAFN: Parkinson's Disease Severity Profiling<br/>SAFN: 帕金森病严重程度分析] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>Multimodal fusion, interpretability, class imbalance<br/>多模态融合、可解释性、类别不平衡] --> P1[挑战/Challenges<br/>Integrate imaging & clinical data<br/>整合影像与临床数据]\n        Problem --> P2[目标/Goal<br/>Robust & interpretable PD profiling<br/>鲁棒且可解释的PD分析]\n        Method[主要方法/Method<br/>Class-Weighted Sparse-Attention Fusion Network<br/>类别加权稀疏注意力融合网络] --> M1[技术/Techniques<br/>Modality-specific encoders, symmetric cross-attention<br/>模态特定编码器、对称交叉注意力]\n        Method --> M2[创新/Innovations<br/>Sparsity-constrained attention gating, Class-Balanced Focal Loss<br/>稀疏约束注意力门控、类别平衡焦点损失]\n        Results[关键结果/Results<br/>Evaluation on PPMI dataset<br/>在PPMI数据集上的评估] --> R1[性能/Performance<br/>Accuracy: 0.98±0.02, PR-AUC: 1.00±0.00<br/>准确率: 0.98±0.02, PR-AUC: 1.00±0.00]\n        Results --> R2[可解释性/Interpretability<br/>~60% weight to clinical assessments<br/>~60%权重分配给临床评估]"
    },
    {
      "title": "Federated Customization of Large Models: Approaches, Experiments, and Insights",
      "authors": "Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato",
      "institution": "Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2601.00526",
      "code": null,
      "tags": [
        "federated learning",
        "federated learning",
        "prefix-tuning",
        "large model customization",
        "efficient fine-tuning",
        "retrieval-augmented generation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp",
      "contributions": "1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods.",
      "summary": "This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges.",
      "mindmap": "graph TB\n        A[Federated Customization of Large Models: Approaches, Experiments, and Insights] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[在联邦学习框架下定制大模型的挑战/Challenges of customizing large models within FL]\n        C --> C1[回顾大模型定制技术/Review LM customization techniques]\n        C --> C2[讨论联邦学习实现/Discuss FL implementations]\n        C --> C3[实验联邦前缀调优/Experiment with federated prefix-tuning]\n        D --> D1[验证联邦前缀调优可行性/Validate feasibility of federated prefix-tuning]\n        D --> D2[性能接近集中式方法/Performance close to centralized]\n        D --> D3[展示竞争力与鲁棒性/Show competitive performance & robustness]"
    },
    {
      "title": "Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization",
      "authors": "Ravi Teja Pagidoju, Shriya Agarwal",
      "institution": "Campbellsville University, University of the Cumberlands",
      "link": "https://arxiv.org/pdf/2601.00527",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion models",
        "cloud-native architecture",
        "edge deployment",
        "constraint satisfaction",
        "planogram generation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b868081df5651946090e341fbb8526b8b99531ee775ea4b316dc9682b146e22e_w640_q70.webp",
      "contributions": "1. A novel cloud-native architecture for automated planogram synthesis using diffusion models, combining AWS for training and edge deployment for real-time inference. 2. A diffusion model that integrates retail-specific constraints through a modified loss function to generate store-specific layouts. 3. A comprehensive simulation-based and economic analysis demonstrating significant reductions in design time (98.3%) and cost (97.5%) with high constraint satisfaction (94.4%) and linear scalability.",
      "summary": "This paper proposes a cloud-native system using diffusion models to automate the generation of retail planograms. The method learns from successful shelf arrangements across multiple stores and incorporates business constraints into the model. The results show it drastically reduces design time and cost while maintaining high constraint satisfaction, proving the viability of generative AI for retail space optimization.",
      "mindmap": "graph TB\n        Root[Cloud-Native Generative AI for Planogram Synthesis] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Manual planogram creation is slow and expensive] --> P1[挑战/Challenge: 30 hours per layout]\n        Method[主要方法/Method: Cloud-native diffusion model] --> M1[训练/Training: Cloud-based (AWS)]\n        Method --> M2[推理/Inference: Edge deployment]\n        Method --> M3[模型/Model: Constraint-integrated diffusion]\n        Results[关键结果/Results] --> R1[效率/Efficiency: 98.3% time reduction]\n        Results --> R2[效果/Effectiveness: 94.4% constraint satisfaction]\n        Results --> R3[经济/Economic: 97.5% cost reduction]\n        Results --> R4[可扩展性/Scalability: Linear scaling to 10k stores]"
    },
    {
      "title": "Entropy Production in Machine Learning Under Fokker-Planck Probability Flow",
      "authors": "Lennon Shikhman",
      "institution": "Florida Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00554",
      "code": null,
      "tags": [
        "others",
        "Fokker-Planck equation",
        "Kullback-Leibler divergence",
        "entropy production",
        "data drift",
        "retraining trigger"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53bb0b8d5d5942ee17365e19e7b865b31c2701e9fcbcde1a5d6b9dc2ccc5b0e_w640_q70.webp",
      "contributions": "1. Proposes a novel entropy-based retraining framework for machine learning models grounded in nonequilibrium statistical physics, modeling data drift as probability flow governed by a Fokker-Planck equation. 2. Derives a dynamical interpretation of model-data mismatch by showing its time derivative decomposes into an entropy-balance equation featuring a nonnegative entropy production term. 3. Introduces and validates an entropy-triggered retraining strategy that maintains high predictive performance while significantly reducing retraining frequency compared to daily or label-based policies.",
      "summary": "This paper addresses performance degradation in ML models due to data drift in nonstationary environments. It proposes a principled retraining framework based on nonequilibrium dynamics, using a Fokker-Planck model of drift and an entropy-production trigger. The method achieves performance comparable to frequent retraining while drastically reducing the number of retraining events.",
      "mindmap": "graph TB\n        Root(”Entropy Production in Machine Learning Under Fokker-Planck Probability Flow”) --> Problem(”核心问题/Problem: Model performance degrades due to data drift in nonstationary environments.”)\n        Root --> Method(”主要方法/Method: Entropy-based retraining framework using Fokker-Planck dynamics and KL divergence.”)\n        Root --> Results(”关键结果/Results: Achieves high performance with order-of-magnitude fewer retraining events.”)"
    },
    {
      "title": "Adversarial Samples Are Not Created Equal",
      "authors": "Jennifer Crawford, Amol Khanna, Fred Lu, Amy R. Wagoner, Stella Biderman, Andre T. Nguyen, Edward Raff",
      "institution": "Scale AI, CrowdStrike, Booz Allen Hamilton, EleutherAI",
      "link": "https://arxiv.org/pdf/2601.00577",
      "code": null,
      "tags": [
        "adversarial robustness",
        "adversarial samples",
        "non-robust features",
        "adversarial bugs",
        "ensemble-based metric",
        "sharpness-aware minimization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e880259ebf04545c0ce64ed1e802b2caa6a3b401669986a52d0c3ccb0f063371_w640_q70.webp",
      "contributions": "1. Proposes an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations. 2. Introduces the concept of \"adversarial bugs\" to differentiate adversarial samples that do not rely on non-robust features. 3. Uses this new perspective to re-examine phenomena like the impact of sharpness-aware minimization and the robustness gap in adversarially trained models.",
      "summary": "The paper argues that not all adversarial samples are created equal, differentiating between those that exploit brittle \"non-robust\" features and those that do not (\"adversarial bugs\"). It proposes an ensemble-based metric to identify this distinction and uses it to analyze adversarial attacks, offering a new lens to re-examine existing robustness phenomena.",
      "mindmap": "graph TB\n        Root(”Adversarial Samples Are Not Created Equal<br>对抗性样本并非生而平等”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有理论无法解释所有对抗样本<br>Existing theory doesn't explain all adversarial samples”)\n        Method --> M1(”提出基于集成的度量<br>Propose ensemble-based metric”)\n        Method --> M2(”区分'对抗性漏洞'<br>Differentiate 'adversarial bugs'”)\n        Results --> R1(”重新审视鲁棒性现象<br>Re-examine robustness phenomena”)"
    },
    {
      "title": "Learning to be Reproducible: Custom Loss Design for Robust Neural Networks",
      "authors": "Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen",
      "institution": "Friedrich Schiller University Jena, University of Technology Chemnitz, Norwegian University of Science and Technology",
      "link": "https://arxiv.org/pdf/2601.00578",
      "code": null,
      "tags": [
        "training stability & reproducibility",
        "custom loss function",
        "training robustness",
        "reproducibility",
        "stochastic factors",
        "weight initialization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp",
      "contributions": "1. Identifies and empirically analyzes the critical gap in ensuring consistent performance across training runs due to stochastic factors like weight initialization and data shuffling. 2. Proposes a novel Custom Loss Function (CLF) designed to explicitly balance predictive accuracy with training stability, reducing sensitivity to these stochastic factors. 3. Demonstrates through extensive experiments on diverse architectures and tasks (image classification, time series forecasting) that CLF significantly improves training robustness without sacrificing predictive performance.",
      "summary": "The paper addresses the problem of inconsistent model performance across training runs due to stochastic factors. It proposes a Custom Loss Function (CLF) to explicitly balance accuracy and stability, which is shown to improve training robustness without harming predictive performance in experiments on image classification and time series forecasting.",
      "mindmap": "graph TB\n        Root(”Learning to be Reproducible: Custom Loss Design for Robust Neural Networks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”模型性能不一致/Inconsistent Model Performance”)\n        Problem --> P2(”对随机因素敏感/Sensitive to Stochastic Factors”)\n        Method --> M1(”提出自定义损失函数/Propose Custom Loss Function (CLF)”)\n        Results --> R1(”提高训练鲁棒性/Improves Training Robustness”)\n        Results --> R2(”保持预测性能/Maintains Predictive Performance”)"
    },
    {
      "title": "HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts",
      "authors": "Zihan Fang, Zheng Lin, Senkang Hu, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang",
      "institution": "City University of Hong Kong, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2601.00583",
      "code": null,
      "tags": [
        "federated learning",
        "Mixture-of-Experts",
        "federated fine-tuning",
        "resource-aware",
        "expert selection",
        "sparsity-aware aggregation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp",
      "contributions": "1. Introduces a method to identify expert importance based on contributions to fine-tuning performance, enabling informed expert selection. 2. Proposes an adaptive expert subset selection mechanism from an information bottleneck perspective to align with heterogeneous client computing budgets. 3. Designs a sparsity-aware model aggregation strategy that weights updates from actively fine-tuned experts and gating parameters to mitigate destructive interference during global aggregation.",
      "summary": "This paper proposes HFedMoE, a heterogeneous federated learning framework for fine-tuning large language models using Mixture-of-Experts. It addresses challenges in expert selection, resource heterogeneity, and aggregation interference by customizing expert subsets per client and using importance-weighted aggregation. Experiments show HFedMoE outperforms state-of-the-art methods in accuracy and convergence speed.",
      "mindmap": "graph TB\n        A[HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLM联邦微调中MoE面临的挑战/Challenges in MoE for FL fine-tuning]\n        B1 --> B2[专家选择困难/Difficulty in expert selection]\n        B1 --> B3[客户端资源异构性/Client resource heterogeneity]\n        B1 --> B4[聚合干扰/Aggregation interference]\n        C --> C1[定制化专家子集/Customized expert subset per client]\n        C1 --> C2[基于重要性的专家选择/Importance-based expert selection]\n        C1 --> C3[信息瓶颈视角的适配/Adaptation via information bottleneck]\n        C --> C4[稀疏感知的模型聚合/Sparsity-aware model aggregation]\n        D --> D1[更高的训练精度/Higher training accuracy]\n        D --> D2[更快的收敛速度/Faster convergence speed]"
    },
    {
      "title": "Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load",
      "authors": "Francisco Aguilera Moreno",
      "institution": "None specified (Inferred from author's email domain: gmail.com)",
      "link": "https://arxiv.org/pdf/2601.00604",
      "code": null,
      "tags": [
        "sports analytics",
        "Lasso regression",
        "training load (CTL/ATL)",
        "N-of-1 study",
        "feature engineering",
        "route topology"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9697eaafe078b4416e1e63fcfa2f99ad60e0031eff1e90380ad4c7d7042e046_w640_q70.webp",
      "contributions": "1. Proposes a personalized machine learning model for cycling time prediction that uses route topology and athlete fitness state, avoiding complex physics-based parameters. 2. Implements and validates the approach using an N-of-1 study design with rigorous feature engineering to prevent data leakage. 3. Demonstrates that integrating fitness metrics (CTL, ATL) reduces prediction error by 14% compared to using route features alone, highlighting the importance of physiological state.",
      "summary": "This paper presents a personalized machine learning approach to predict cycling race times by combining route topology features with the athlete's training load-derived fitness state. The method, evaluated on a single-athlete dataset, uses Lasso regression and achieves high accuracy (MAE=6.60 min, R²=0.922), showing that fitness metrics significantly improve predictions over topology alone.",
      "mindmap": "graph TB\n        Root[Cycling Race Time Prediction] --> Problem[核心问题/Problem<br>Physics-based models are impractical<br>物理模型不实用]\n        Root --> Method[主要方法/Method<br>Personalized ML with Route & Fitness<br>个性化ML结合路线与体能]\n        Root --> Results[关键结果/Results<br>Fitness reduces error by 14%<br>体能指标降低14%误差]"
    },
    {
      "title": "Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning",
      "authors": "Sonia Khetarpaul, P Y Sharan",
      "institution": "Shiv Nadar Institution of Eminence",
      "link": "https://arxiv.org/pdf/2601.00607",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Graph Neural Network",
        "Q-learning",
        "traffic-aware optimization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp",
      "contributions": "1. Proposes a novel traffic-aware, graph-based reinforcement learning framework for optimal taxi placement that integrates real-time traffic data (e.g., congestion scores) with historical demand patterns. 2. Employs Graph Neural Network (GNN) embeddings to encode spatial-temporal dependencies within the urban road network, enhancing the agent's understanding of network topology and dynamics. 3. Designs a multi-objective reward mechanism that jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance, leading to significant performance improvements over a baseline.",
      "summary": "This paper addresses the problem of inefficient taxi supply-demand matching in smart cities by proposing a framework that models the urban road network as a graph and uses Graph Neural Networks combined with Q-learning to recommend optimal taxi placement hotspots. The method integrates real-time traffic conditions and historical data to optimize for passenger waiting time and driver travel distance. Experiments on a simulated Delhi dataset show the model reduces passenger waiting time by 56% and travel distance by 38% compared to a stochastic baseline.",
      "mindmap": "graph TB\n        A[Traffic-Aware Optimal Taxi Placement<br>Using Graph Neural Network-Based Reinforcement Learning] --> B(核心问题/Problem: Conventional taxi hotspot models overlook dynamic traffic influences.)\n        A --> C(主要方法/Method: Graph-based RL with GNN embeddings for spatial-temporal dependencies.)\n        A --> D(关键结果/Results: Reduced passenger waiting time by 56% and travel distance by 38%.)"
    },
    {
      "title": "Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization",
      "authors": "Hareshkumar Jadav, Ranveer Singh, Vaneet Aggarwal",
      "institution": "IIT Indore, Purdue University",
      "link": "https://arxiv.org/pdf/2601.00611",
      "code": null,
      "tags": [
        "submodular optimization",
        "weakly DR-submodular",
        "continuous-greedy",
        "Frank-Wolfe",
        "approximation algorithm",
        "down-closed convex body"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp",
      "contributions": "1. A novel approximation algorithm for maximizing non-monotone γ-weakly DR-submodular functions over down-closed convex bodies. 2. A smooth approximation guarantee that recovers the 0.401 factor for DR-submodular (γ=1) and degrades gracefully for γ&lt;1, improving upon prior bounds. 3. A hybrid algorithmic framework combining Frank-Wolfe-guided continuous-greedy with a γ-aware double-greedy step to handle non-monotonicity.",
      "summary": "This paper addresses the problem of maximizing non-monotone, nonnegative γ-weakly DR-submodular functions over down-closed convex bodies. The authors propose a new algorithm that integrates a Frank-Wolfe-guided continuous-greedy approach with a γ-aware double-greedy step. This method achieves state-of-the-art approximation guarantees that depend smoothly on the parameter γ, improving upon previous results for this class of functions.",
      "mindmap": "graph TB\n        A[Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization<br>非单调γ-弱DR-子模最大化的更强近似保证] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Maximize non-monotone γ-weakly DR-submodular function<br>最大化非单调γ-弱DR-子模函数]\n        B --> B2[Over down-closed convex body<br>在下封闭凸体上]\n        C --> C1[Frank-Wolfe-guided continuous-greedy<br>Frank-Wolfe引导的连续贪心]\n        C --> C2[γ-aware double-greedy step<br>γ感知的双贪心步骤]\n        D --> D1[Smooth approximation guarantee based on γ<br>基于γ的平滑近似保证]\n        D --> D2[Recovers 0.401 for γ=1 (DR-submodular)<br>γ=1时恢复0.401因子]\n        D --> D3[Improves prior bounds for γ<1<br>改进了γ<1时的现有界限]"
    },
    {
      "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
      "authors": "Vadim Borisov, Michael Gröger, Mina Mikhael, Richard H. Schreiber",
      "institution": "tabularis.ai",
      "link": "https://arxiv.org/pdf/2601.00624",
      "code": "https://huggingface.co/datasets/tabularisai/yapbench",
      "tags": [
        "llm evaluation",
        "verbosity",
        "benchmark",
        "brevity",
        "over-generation",
        "evaluation metric"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b7bd8c8557d407f813af012f276e8f3d15707b3fbc649fe498992f3cbf4d01_w640_q70.webp",
      "contributions": "1. Introduces YapBench, a benchmark with over 300 prompts to quantify LLM over-generation in brevity-ideal scenarios. 2. Proposes YapScore, a tokenizer-agnostic metric based on character count to measure excess response length. 3. Establishes a live leaderboard and provides analysis revealing an order-of-magnitude spread in verbosity across 76 evaluated LLMs.",
      "summary": "The paper addresses the problem of LLMs generating unnecessarily long and verbose responses to simple prompts. It introduces the YapBench benchmark and YapScore metric to measure this over-generation. The evaluation of 76 models shows significant variation in verbosity, highlighting a common failure mode in current assistant LLMs.",
      "mindmap": "graph TB\n        A[Do Chatbot LLMs Talk Too Much? The YapBench Benchmark] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: LLMs often give unnecessarily long responses, increasing cognitive load and cost.]\n        C[主要方法/Method: Introduce YapBench benchmark and YapScore metric to measure excess verbosity.]\n        D[关键结果/Results: Large variation in verbosity found across 76 models; benchmark and leaderboard released.]"
    },
    {
      "title": "HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis",
      "authors": "Shuren Gabriel Yu, Sikang Ren, Yongji Tian",
      "institution": "Tsinghua University, Beijing Tiantan Hospital",
      "link": "https://arxiv.org/pdf/2601.00626",
      "code": null,
      "tags": [
        "medical image analysis",
        "Hypergraph Neural Network",
        "Learning Using Privileged Information",
        "Knowledge Distillation",
        "Severed Graph Strategy",
        "dual-stream distillation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp",
      "contributions": "1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference.",
      "summary": "The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis.",
      "mindmap": "graph TB\n        A[HyperPriv-EPN] --> B[核心问题/Problem: Pre-op prognosis lacks semantic insights from post-op reports]\n        A --> C[主要方法/Method: Hypergraph LUPI with Severed Graph Strategy & dual-stream distillation]\n        A --> D[关键结果/Results: SOTA accuracy & survival stratification on 311 patients]"
    },
    {
      "title": "Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability",
      "authors": "Kasra Fouladi, Hamta Rahmani",
      "institution": "Not explicitly stated; inferred from email domains as independent researchers.",
      "link": "https://arxiv.org/pdf/2601.00655",
      "code": null,
      "tags": [
        "interpretable machine learning",
        "Bi-objective Optimization",
        "Temporal Integrated Gradients",
        "Optimal Path Oracle",
        "Directed Acyclic Graph",
        "Structured Regularization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp",
      "contributions": "1. Proposes the IGBO framework that trains interpretable models by formalizing the task as a bi-objective optimization problem, jointly optimizing for accuracy and adherence to domain knowledge constraints. 2. Introduces an Optimal Path Oracle to generate data-manifold-aware integration paths, addressing the Out-of-Distribution problem in Temporal Integrated Gradients computation. 3. Provides theoretical analysis proving convergence properties and robustness to mini-batch noise, and demonstrates empirical effectiveness on time-series data with minimal accuracy loss.",
      "summary": "This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains models to be both accurate and interpretable by jointly optimizing a task loss and an interpretability loss derived from domain knowledge encoded as a DAG. It addresses a key challenge in gradient-based attribution (the OOD problem) by learning an Optimal Path Oracle. Empirical results show IGBO effectively enforces interpretability constraints with minimal impact on accuracy, outperforming standard regularization methods.",
      "mindmap": "graph TB\n        A[Interpretability-Guided Bi-objective Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[黑盒模型缺乏可解释性/Black-box models lack interpretability]\n        B --> B2[事后解释方法无法保证约束/Post-hoc methods don't guarantee constraints]\n        B --> B3[梯度归因存在OOD问题/Gradient attribution has OOD problem]\n        C --> C1[双目标优化框架/Bi-objective Optimization Framework]\n        C --> C2[使用DAG编码领域知识/Encode knowledge via DAG]\n        C --> C3[最优路径预言机/Optimal Path Oracle]\n        D --> D1[理论收敛性证明/Theoretical convergence proof]\n        D --> D2[实证效果优于基线/Empirically outperforms baselines]\n        D --> D3[最小精度损失/Minimal accuracy loss]"
    },
    {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "authors": "Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang",
      "institution": "KAIST, NTU Singapore, DeepAuto.ai",
      "link": "https://arxiv.org/pdf/2601.00664",
      "code": "https://taekyungki.github.io/AvatarForcing",
      "tags": [
        "talking head generation",
        "diffusion forcing",
        "direct preference optimization",
        "real-time interaction",
        "low latency",
        "multimodal inputs"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",
      "contributions": "1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.",
      "summary": "The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.",
      "mindmap": "graph TB\n        Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[缺乏真正互动/Lacks truly interactive communication]\n        Problem --> P2[单向反应缺乏情感/One-way responses lack emotional engagement]\n        Method[主要方法/Method] --> M1[扩散驱动框架/Diffusion forcing framework]\n        Method --> M2[无标签直接偏好优化/Label-free direct preference optimization]\n        Results[关键结果/Results] --> R1[低延迟实时交互/Low-latency real-time interaction (~500ms)]\n        Results --> R2[6.8倍加速/6.8x speedup]\n        Results --> R3[80%用户偏好/Over 80% user preference]"
    },
    {
      "title": "Three factor delay learning rules for spiking neural networks",
      "authors": "Luke Vassallo, Nima Taherinejad",
      "institution": "Heidelberg University",
      "link": "https://arxiv.org/pdf/2601.00668",
      "code": null,
      "tags": [
        "on-device ai",
        "spiking neural networks",
        "delay learning",
        "three-factor learning",
        "online learning",
        "neuromorphic processors"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp",
      "contributions": "1. Introduced learnable synaptic and axonal delays into LIF-based SNNs and proposed novel three-factor learning rules for online, simultaneous learning of both weights and delays. 2. Employed a smooth Gaussian surrogate gradient exclusively for eligibility trace calculation to enable gradient-equivalent delay parameter updates. 3. Demonstrated significant improvements in model efficiency, achieving up to 6.6x model size reduction and 67% lower inference latency with minimal accuracy loss, enabling on-device learning for resource-constrained environments.",
      "summary": "This paper addresses the limited temporal learning capability of Spiking Neural Networks (SNNs) by introducing learnable synaptic and axonal delays and proposing online three-factor learning rules to train them. The method uses a Gaussian surrogate gradient for eligibility traces and achieves competitive accuracy on temporal tasks like speech recognition while drastically reducing model size and latency. The findings facilitate efficient, on-device learning for power and area-constrained neuromorphic hardware.",
      "mindmap": "graph TB\n        A[Three factor delay learning rules for spiking neural networks] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[SNNs lack temporal parameters/SNNs缺乏时间参数]\n        B --> B2[Existing delay learning is offline & large/现有延迟学习是离线的且模型大]\n        C --> C1[Learnable synaptic & axonal delays/可学习的突触和轴突延迟]\n        C --> C2[Three-factor online learning rules/三因素在线学习规则]\n        C --> C3[Gaussian surrogate for eligibility trace/用于资格迹的高斯代理梯度]\n        D --> D1[Accuracy improved up to 20%/准确率提升高达20%]\n        D --> D2[Model size reduced 6.6x/模型大小减少6.6倍]\n        D --> D3[Latency reduced 67%/延迟降低67%]"
    },
    {
      "title": "Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs",
      "authors": "Seungchan Ko, Jiyeon Kim, Dongwook Shin",
      "institution": "Inha University, Ajou University",
      "link": "https://arxiv.org/pdf/2601.00672",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "operator learning",
        "finite element methods",
        "sparse networks",
        "computational efficiency",
        "stability"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ca7dbad16ad9ca12a77087fa33228fbe27facf787daefb7dcf7928c77caf2bd_w640_q70.webp",
      "contributions": "1. Proposes a novel sparse network architecture for FEONet, leveraging finite-element local sparsity to reduce computational cost and memory usage. 2. Provides theoretical analysis demonstrating the sparse architecture's approximation capability and stability for reliable training. 3. Validates the method through extensive numerical experiments, showing substantial efficiency gains while maintaining accuracy comparable to the original FEONet.",
      "summary": "This paper proposes Sparse FEONet, a memory-efficient variant of the Finite Element Operator Network, to address the scalability issues of the original model. By incorporating a sparse architecture inspired by finite element structures, it significantly reduces computational cost and memory footprint while preserving accuracy. Theoretical and experimental results confirm its effectiveness and stability for solving parametric PDEs.",
      "mindmap": "graph TB\n        Root[”Sparse FEONet<br>稀疏FEONet”] --> Problem[”核心问题/Problem<br>FEONet计算成本高，大规模问题有挑战<br>High computational cost of FEONet for large-scale problems”]\n        Root --> Method[”主要方法/Method<br>提出基于有限元局部稀疏性的新网络架构<br>Propose new sparse network architecture via finite-element local sparsity”]\n        Root --> Results[”关键结果/Results<br>计算成本效率显著提升，保持精度，理论保证<br>Substantial improvements in cost/efficiency, maintained accuracy, theoretical guarantees”]"
    },
    {
      "title": "IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning",
      "authors": "Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang",
      "institution": "HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University",
      "link": "https://arxiv.org/pdf/2601.00677",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Generative Reward Models",
        "Bradley-Terry Model",
        "Group Relative Policy Optimization",
        "Reinforcement Learning from Human Feedback",
        "Pointwise Scoring"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp",
      "contributions": "1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations.",
      "summary": "This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations.",
      "mindmap": "graph TB\n        A[IRPO: Scaling the Bradley-Terry Model via RL] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Pairwise GRMs create O(n²) bottleneck in RL/成对GRM在RL中造成O(n²)瓶颈]\n        C --> C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: 将Bradley-Terry融入GRPO实现逐点评分]\n        D --> D1[SOTA among pointwise GRMs/在逐点GRM中达到SOTA]\n        D --> D2[Outperforms pairwise GRMs in post-training/在训练后评估中优于成对GRM]"
    },
    {
      "title": "QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models",
      "authors": "Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique",
      "institution": "New York University (NYU) Abu Dhabi",
      "link": "https://arxiv.org/pdf/2601.00679",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "quantization",
        "spike-driven language models (SLMs)",
        "memory footprint",
        "tiered search",
        "embedded systems"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp",
      "contributions": "1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance.",
      "summary": "The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model.",
      "mindmap": "graph TB\n        A[QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: SLMs内存占用大，难以部署在资源受限的嵌入式设备/SLMs have large memory footprints, challenging for resource-constrained embedded deployment]\n        C[主要方法/Method: 自动化分层量化策略，结合网络层次、层敏感性和多目标权衡函数/Automated tiered quantization strategy using network hierarchy, layer sensitivity, and multi-objective trade-off]\n        D[关键结果/Results: 内存占用减少高达86.5%，功耗降低高达20%，性能接近原始模型/Memory footprint reduced by up to 86.5%, power by up to 20%, performance close to original model]"
    },
    {
      "title": "Cost Optimization in Production Line Using Genetic Algorithm",
      "authors": "Alireza Rezaee",
      "institution": "University of Tehran",
      "link": "https://arxiv.org/pdf/2601.00689",
      "code": null,
      "tags": [
        "combinatorial optimization",
        "genetic algorithm",
        "task scheduling",
        "production line",
        "chromosome encoding",
        "JGAP"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp",
      "contributions": "1. Proposes and compares two chromosome encoding strategies (station-based and task-based) for a GA applied to a production line scheduling problem. 2. Adapts standard GA operators (crossover, mutation, etc.) to preserve solution feasibility under precedence and capacity constraints. 3. Empirically demonstrates that the task-based encoding yields smoother convergence and more reliable cost minimization, especially for problems with a large number of valid schedules.",
      "summary": "This paper applies a genetic algorithm to optimize task scheduling in a production line to minimize cost. It investigates two different ways to represent the schedule (encoding) within the algorithm and finds that a task-based encoding performs better, converging more smoothly to lower-cost solutions. The study shows GAs are advantageous for this type of complex, constrained scheduling problem.",
      "mindmap": "graph TB\n        A[Cost Optimization in Production Line Using Genetic Algorithm] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[最小化生产线成本/Minimize Production Line Cost]\n        B --> B2[任务调度与约束/Task Scheduling with Constraints]\n        C --> C1[遗传算法/Genetic Algorithm]\n        C --> C2[两种编码策略/Two Encoding Strategies]\n        C2 --> C21[基于工位的编码/Station-based Encoding]\n        C2 --> C22[基于任务的编码/Task-based Encoding]\n        D --> D1[基于任务的编码性能更优/Task-based Encoding Performs Better]\n        D --> D2[更平滑的收敛/Smoother Convergence]\n        D --> D3[更可靠的优化/More Reliable Optimization]"
    },
    {
      "title": "TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications",
      "authors": "Mohamed Trabelsi, Huseyin Uzunalioglu",
      "institution": "Nokia Bell Labs",
      "link": "https://arxiv.org/pdf/2601.00691",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "ticket troubleshooting",
        "retrieval-augmented generation",
        "instruction-tuning",
        "domain-specific ranking",
        "large language models"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp",
      "contributions": "1. Proposes TeleDoCTR, an end-to-end system for telecom ticket troubleshooting integrating classification, retrieval, and generation tasks. 2. Introduces a domain-specific and contextual approach combining ranking and generative models tailored for the telecom domain. 3. Demonstrates superior performance over state-of-the-art methods on a real-world telecom dataset, enhancing troubleshooting accuracy and efficiency.",
      "summary": "The paper proposes TeleDoCTR, a system that automates telecom ticket troubleshooting by integrating domain-specific models for ticket classification, retrieval of similar historical tickets, and generation of fault analysis reports. It is evaluated on a real-world telecom dataset and shows improved performance over existing methods, making the troubleshooting process more accurate and efficient.",
      "mindmap": "graph TB\n        A[TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications] --> B[核心问题/Problem: Telecom ticket troubleshooting is complex, time-consuming, and human-intensive.]\n        A --> C[主要方法/Method: Integrates domain-specific ranking and generative models for classification, retrieval, and generation tasks.]\n        A --> D[关键结果/Results: Superior performance over SOTA methods on real-world data, enhancing accuracy and efficiency.]"
    },
    {
      "title": "ARISE: Adaptive Reinforcement Integrated with Swarm Exploration",
      "authors": "Rajiv Chaitanya M, D R Ramesh Babu",
      "institution": "Dayananda Sagar College of Engineering",
      "link": "https://arxiv.org/pdf/2601.00693",
      "code": null,
      "tags": [
        "reinforcement learning",
        "swarm intelligence",
        "policy gradient",
        "adaptive exploration",
        "non-stationary rewards",
        "particle swarm"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp",
      "contributions": "1. Introduces ARISE, a lightweight framework that augments standard policy-gradient RL methods with a swarm-based exploration layer., 2. Proposes an adaptive mechanism that modulates exploration intensity based on reward-variance cues., 3. Demonstrates significant performance improvements and robustness, particularly in challenging and non-stationary environments, without altering core algorithmic structures.",
      "summary": "The paper introduces ARISE, a framework that enhances reinforcement learning by integrating a swarm-based exploration layer with standard policy-gradient methods to improve exploration. It adaptively blends policy actions with particle-driven proposals and modulates exploration using reward variance. The method shows substantial performance gains on complex tasks and improved robustness in non-stationary environments.",
      "mindmap": "graph TB\n        Root[ARISE: Adaptive Reinforcement Integrated with Swarm Exploration] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1[RL探索挑战/RL Exploration Challenge]\n        Problem --> P2[非平稳奖励/Non-stationary Rewards]\n        Method --> M1[群体智能探索层/Swarm-based Exploration Layer]\n        Method --> M2[自适应调节/Adaptive Modulation]\n        Method --> M3[策略-粒子混合/Policy-Particle Blending]\n        Results --> R1[性能显著提升/Substantial Performance Gains]\n        Results --> R2[鲁棒性增强/Enhanced Robustness]\n        Results --> R3[架构无关/Architecture-agnostic]"
    },
    {
      "title": "BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting",
      "authors": "Maximilian Reinwardt, Michael Eichelbeck, Matthias Althoff",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2601.00698",
      "code": null,
      "tags": [
        "time series forecasting",
        "B-spline tokenization",
        "adaptive segmentation",
        "Rotary Positional Embedding (RoPE)",
        "long-term forecasting",
        "transformer efficiency"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6206fb30f6a9eddbc2ada96d43c221bc41515951535d8d579b2df705186641a9_w640_q70.webp",
      "contributions": "1. Introduces the B-Spline Adaptive Tokenizer (BSAT), a parameter-free method for adaptively segmenting time series by fitting B-splines and placing tokens in high-curvature regions. 2. Proposes a hybrid positional encoding strategy combining additive learnable encoding with a novel L-RoPE (layer-wise learnable base Rotary Positional Embedding). 3. Demonstrates that the model achieves competitive performance at high compression rates, making it suitable for memory-constrained use cases.",
      "summary": "This paper addresses the inefficiency of transformers for long-term time series forecasting by introducing BSAT, an adaptive tokenizer that uses B-splines to create variable-length tokens aligned with data semantics, and a novel hybrid positional encoding. The method achieves strong performance with high compression, making it effective under memory constraints.",
      "mindmap": "graph TB\n        Root[”BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting”] --> Problem[”核心问题/Problem<br>Quadratic self-attention complexity & rigid uniform patching in transformers for time series”]\n        Root --> Method[”主要方法/Method<br>B-Spline Adaptive Tokenizer (BSAT) & Hybrid Positional Encoding (L-RoPE)”]\n        Root --> Results[”关键结果/Results<br>Competitive performance at high compression rates, suitable for memory-constrained use”]"
    },
    {
      "title": "Bayesian Inverse Games with High-Dimensional Multi-Modal Observations",
      "authors": "Yash Jain, Xinjie Liu, Lasse Peters, David Fridovich-Keil, Ufuk Topcu",
      "institution": "The University of Texas at Austin, Delft University of Technology",
      "link": "https://arxiv.org/pdf/2601.00696",
      "code": null,
      "tags": [
        "inverse reinforcement learning",
        "Bayesian inference",
        "variational autoencoder",
        "Nash equilibrium",
        "inverse games",
        "multimodal observations"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp",
      "contributions": "1. Proposes a Bayesian inference framework for inverse games to quantify uncertainty in estimating agent objectives, addressing the overconfidence of point-estimate methods. 2. Introduces a structured variational autoencoder with an embedded differentiable Nash game solver, enabling posterior sampling without requiring labeled objective data. 3. Demonstrates that multimodal inference reduces uncertainty when trajectory data is insufficient, leading to safer downstream planning decisions.",
      "summary": "This paper addresses the problem of inferring agents' hidden objectives in multi-agent interactions, where existing maximum likelihood methods produce overconfident point estimates. The authors propose a Bayesian inverse game framework using a structured variational autoencoder with a differentiable Nash solver to generate posterior samples from multimodal observations. Experiments show the method improves inference quality, quantifies uncertainty, and enables safer autonomous decision-making compared to prior approaches.",
      "mindmap": "graph TB\n        Root(”Bayesian Inverse Games with High-Dimensional Multi-Modal Observations”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”MLE方法只提供点估计，导致不确定性被忽略/MLE methods provide only point estimates, ignoring uncertainty”)\n        Problem --> P2(”下游规划可能过度自信，导致不安全动作/Downstream planning can be overconfident, leading to unsafe actions”)\n        Method --> M1(”近似贝叶斯推理框架/Approximate Bayesian inference framework”)\n        Method --> M2(”结构化变分自编码器嵌入可微纳什求解器/Structured VAE with embedded differentiable Nash solver”)\n        Method --> M3(”利用多模态观测数据/Utilizes multi-modal observation data”)\n        Results --> R1(”成功学习先验和后验分布/Successfully learns prior and posterior distributions”)\n        Results --> R2(”推理质量优于MLE方法/Improves inference quality over MLE”)\n        Results --> R3(”多模态推理进一步减少不确定性/Multimodal inference further reduces uncertainty”)"
    },
    {
      "title": "Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL",
      "authors": "Erin Carson, Xinye Chen",
      "institution": "Charles University, Sorbonne Université, CNRS, LIP6",
      "link": "https://arxiv.org/pdf/2601.00728",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "reinforcement learning",
        "precision autotuning",
        "contextual bandit",
        "mixed-precision",
        "linear solvers"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp",
      "contributions": "1. Proposes a novel reinforcement learning framework formulated as a contextual bandit problem for adaptive precision tuning of numerical algorithms. 2. Applies the framework to iterative refinement for linear solvers, using a Q-table and epsilon-greedy strategy to dynamically select precision configurations based on system features. 3. Demonstrates the framework's effectiveness and generalization, reducing computational cost while maintaining accuracy comparable to double-precision baselines on unseen data.",
      "summary": "This paper proposes a reinforcement learning framework for adaptive precision tuning, formulated as a contextual bandit problem, to optimize the trade-off between computational cost and accuracy in linear solvers. The method dynamically selects precision configurations based on system features using a Q-learning approach. Empirical results show it reduces cost while maintaining accuracy, and it generalizes well to unseen data.",
      "mindmap": "graph TB\n        Root(Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(高精度计算能耗高/High-precision computing is energy-intensive)\n        Problem --> P2(低精度计算可能不稳定/Reduced-precision can be unstable)\n        Method --> M1(将问题建模为上下文赌博机/Formulate as Contextual Bandit)\n        Method --> M2(使用Q表和epsilon-greedy策略/Use Q-table & epsilon-greedy)\n        Method --> M3(应用于线性求解器的迭代精化/Apply to iterative refinement for linear solvers)\n        Results --> R1(有效降低计算成本/Effectively reduces computational cost)\n        Results --> R2(保持与双精度相当的精度/Maintains accuracy comparable to double-precision)\n        Results --> R3(泛化到未见数据/Generalizes to unseen data)"
    },
    {
      "title": "Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty",
      "authors": "Uğurcan Özalp",
      "institution": "Turkish Aerospace",
      "link": "https://arxiv.org/pdf/2601.00737",
      "code": null,
      "tags": [
        "reinforcement learning",
        "actor-critic",
        "overestimation",
        "aleatoric uncertainty",
        "distributional critic",
        "dropout"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp",
      "contributions": "1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency.",
      "summary": "The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability.",
      "mindmap": "graph TB\n        A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Critic网络系统性高估价值/Critic Networks Systematically Overestimate Value Estimates]\n        C --> C1[使用单分布评论家建模时序偶然不确定性/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]\n        C --> C2[在TD更新中应用基于不确定性的悲观偏差/Apply Uncertainty-Based Pessimistic Bias in TD Updates]\n        C --> C3[对评论家和行动者网络使用Dropout正则化/Use Dropout Regularization on Critic and Actor Networks]\n        D --> D1[缓解高估偏差/Mitigates Overestimation Bias]\n        D --> D2[在随机环境中产生风险规避行为/Leads to Risk-Averse Behavior in Stochastic Environments]\n        D --> D3[提高计算效率和训练稳定性/Improves Computational Efficiency and Training Stability]"
    },
    {
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "authors": "Max Ruiz Luyten, Mihaela van der Schaar",
      "institution": "University of Cambridge",
      "link": "https://arxiv.org/pdf/2601.00747",
      "code": null,
      "tags": [
        "reinforcement learning",
        "distributional creative reasoning",
        "diversity collapse",
        "gradient flow",
        "variational objective",
        "reasoning paths"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fbb639faaebe1f394144b6e1a9a7bbb2ea6b005c8194a324964adf46349c164_w640_q70.webp",
      "contributions": "1. Introduces Distributional Creative Reasoning (DCR), a unified variational framework that models training as gradient flow through probability measures on solution traces, encompassing methods like STaR, GRPO, and DPO as special cases. 2. Proves the diversity decay theorem, which explains how correctness-focused objectives lead to distinct modes of diversity collapse in different algorithms. 3. Provides principled designs and actionable recipes to ensure convergence to a stable and diverse policy, preventing creative collapse while maintaining correctness.",
      "summary": "The paper identifies that bootstrapped reasoning loops in LLMs, which optimize for correctness, lead to a collapse in the diversity of reasoning paths, harming creative problem-solving. To address this, it proposes Distributional Creative Reasoning (DCR), a unified theoretical framework that explains this collapse and offers designs to prevent it. The main conclusion is that DCR provides the first principled recipe for training LLMs that are both correct and creative.",
      "mindmap": "graph TB\n        Root[”The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving<br>推理-创造力权衡：迈向创造力驱动的问题解决”]\n        Root --> Problem[”核心问题/Problem<br>Correctness-focused training causes diversity collapse in reasoning paths.<br>以正确性为核心的训练导致推理路径的多样性崩溃。”]\n        Root --> Method[”主要方法/Method<br>Introduce Distributional Creative Reasoning (DCR), a unified variational framework.<br>提出分布创造性推理（DCR），一个统一的变分框架。”]\n        Root --> Results[”关键结果/Results<br>Diversity decay theorem, designs to prevent collapse, actionable recipes.<br>多样性衰减定理，防止崩溃的设计，可操作的方案。”]"
    },
    {
      "title": "A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football",
      "authors": "Sean Groom, Shuo Wang, Francisco Belo, Axl Rice, Liam Anderson",
      "institution": "The University of Birmingham, Nottingham Forest Football Club",
      "link": "https://arxiv.org/pdf/2601.00748",
      "code": null,
      "tags": [
        "sports analytics",
        "covariate-dependent Hidden Markov Model",
        "defensive credit attribution",
        "role-conditioned ghosting"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829cb6bc9dbc2a288f24bae908e436437a33ee4cddc07525ba2d5196a191500c_w640_q70.webp",
      "contributions": "1. Introduces a covariate-dependent Hidden Markov Model (CDHMM) to infer time-resolved defensive role assignments (man-marking and zonal) from player tracking data during corner kicks. 2. Proposes a novel framework for defensive credit attribution based on the inferred role assignments. 3. Develops a role-conditioned ghosting method for context-aware counterfactual analysis of off-ball defensive performance.",
      "summary": "This paper addresses the challenge of evaluating off-ball defensive performance in football, which is not captured by traditional metrics. The authors propose a covariate-dependent Hidden Markov Model to automatically infer defensive roles from tracking data and use this to create a new framework for credit attribution and counterfactual analysis. The method provides an interpretable, context-aware evaluation of defensive contributions.",
      "mindmap": "graph TB\n        Root[”A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football<br>足球中无球防守角色与性能评估的机器学习框架”]\n        Root --> Problem[”核心问题/Problem<br>传统指标无法评估无球防守表现<br>现有反事实方法缺乏战术背景”]\n        Root --> Method[”主要方法/Method<br>引入协变量依赖隐马尔可夫模型(CDHMM)<br>提出基于角色的防守贡献归因与幻影方法”]\n        Root --> Results[”关键结果/Results<br>提供可解释的、情境感知的防守贡献评估”]"
    },
    {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "authors": "Thomas Katraouras, Dimitrios Rafailidis",
      "institution": "University of Thessaly",
      "link": "https://arxiv.org/pdf/2601.00756",
      "code": "https://github.com/Thomkat/MBC",
      "tags": [
        "memory & caching",
        "memory bank compression",
        "codebook optimization",
        "online resetting mechanism",
        "Key-Value Low-Rank Adaptation (KV-LoRA)"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp",
      "contributions": "1. Proposed MBC, a model that compresses the memory bank for continual learning via a codebook optimization strategy. 2. Introduced an online resetting mechanism to prevent codebook collapse and ensure stable learning. 3. Employed Key-Value Low-Rank Adaptation (KV-LoRA) in the LLM's attention layers to efficiently utilize the compressed memory representations.",
      "summary": "This paper addresses the problem of memory bank growth in continual learning for LLMs by proposing MBC, which compresses the memory bank using codebook optimization and an online resetting mechanism. The method integrates KV-LoRA for efficient adaptation and achieves a 99.7% reduction in memory bank size while maintaining high accuracy on question-answering tasks.",
      "mindmap": "graph TB\n        A[Memory Bank Compression for Continual Adaptation of Large Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs知识过时 / LLMs' knowledge becomes outdated]\n        B --> B2[持续学习中的灾难性遗忘 / Catastrophic forgetting in continual learning]\n        B --> B3[内存库无限增长 / Memory bank grows unbounded]\n        C --> C1[内存库压缩 / Memory bank compression]\n        C --> C2[码本优化策略 / Codebook optimization strategy]\n        C --> C3[在线重置机制 / Online resetting mechanism]\n        C --> C4[KV-LoRA / Key-Value Low-Rank Adaptation]\n        D --> D1[内存库大小减少至0.3% / Memory bank size reduced to 0.3%]\n        D --> D2[保持高精度 / Maintains high retention accuracy]"
    },
    {
      "title": "Categorical Reparameterization with Denoising Diffusion models",
      "authors": "Samson Gourevitch, Alain Durmus, Eric Moulines, Jimmy Olsson, Yazid Janati",
      "institution": "CMAP, Ecole polytechnique; Mohamed Bin Zayed University of AI; KTH Royal Institute of Technology; Institute of Foundation Models, MBZUAI",
      "link": "https://arxiv.org/pdf/2601.00781",
      "code": null,
      "tags": [
        "diffusion models",
        "categorical reparameterization",
        "gradient estimation",
        "denoising diffusion",
        "continuous relaxation",
        "Gumbel-Softmax"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07551087da54975e4db6aaddfb0b58a659ed27c7a4b438249f27613f2153d75f_w640_q70.webp",
      "contributions": "1. Introduces a novel diffusion-based soft reparameterization for categorical distributions, extending the family of continuous relaxations. 2. Shows that the denoiser for categorical distributions under a Gaussian noising process has a closed-form, efficient solution, enabling a training-free diffusion sampler. 3. Demonstrates that the proposed method yields competitive or improved optimization performance on various benchmarks compared to existing gradient estimators.",
      "summary": "This paper addresses the challenge of gradient-based optimization with categorical variables by proposing a new diffusion-based reparameterization method. It leverages the closed-form denoiser for categorical distributions to create a differentiable, training-free sampler. Experimental results show this approach provides effective gradient estimates, outperforming or matching existing methods on several benchmarks.",
      "mindmap": "graph TB\n        Root[Categorical Reparameterization with Denoising Diffusion models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[梯度优化与分类变量/Gradient optimization with categorical variables]\n        P1 --> P2[现有估计器存在偏差-方差权衡/Existing estimators have bias-variance trade-off]\n        Method[主要方法/Method] --> M1[扩散基软重参数化/Diffusion-based soft reparameterization]\n        M1 --> M2[闭式去噪器/Closed-form denoiser for categorical distributions]\n        M2 --> M3[免训练可微分采样器/Training-free differentiable sampler]\n        Results[关键结果/Results] --> R1[竞争性或改进的优化性能/Competitive or improved optimization performance]"
    },
    {
      "title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning",
      "authors": "Valentin Noël",
      "institution": "Devoteam",
      "link": "https://arxiv.org/pdf/2601.00791",
      "code": null,
      "tags": [
        "reasoning verification",
        "spectral graph analysis",
        "attention patterns",
        "Fiedler value",
        "high-frequency energy ratio",
        "sliding window attention"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp",
      "contributions": "1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature.",
      "summary": "The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model's attention architecture.",
      "mindmap": "graph TB\n        Root(”Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning”) --> Problem(”核心问题/Problem: Detecting valid mathematical reasoning in LLMs”)\n        Root --> Method(”主要方法/Method: Spectral analysis of attention patterns as dynamic graphs”)\n        Root --> Results(”关键结果/Results: High classification accuracy, detects logical coherence, architectural dependency identified”)"
    },
    {
      "title": "FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing",
      "authors": "Sunny Gupta, Amit Sethi",
      "institution": "Indian Institute of Technology Bombay",
      "link": "https://arxiv.org/pdf/2601.00785",
      "code": "github.com/sunnyinAI/FedHypeVAE",
      "tags": [
        "federated learning",
        "hypernetwork",
        "conditional VAE",
        "differential privacy",
        "MMD alignment",
        "client heterogeneity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp",
      "contributions": "1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.",
      "summary": "The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.",
      "mindmap": "graph TB\n        A[FedHypeVAE] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[非IID数据与隐私挑战/non-IID & Privacy]\n        C --> C1[超网络生成条件VAE/Hypernetwork-Generated Conditional VAE]\n        C --> C2[差分隐私训练/Differentially Private Training]\n        C --> C3[MMD对齐与正则化/MMD Alignment & Regularization]\n        D --> D1[个性化与隐私统一/Unified Personalization & Privacy]\n        D --> D2[可控多域合成/Controllable Multi-Domain Synthesis]"
    },
    {
      "title": "Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI",
      "authors": "Wenhui Chu, Nikolaos V. Tsekos",
      "institution": "University of Houston",
      "link": "https://arxiv.org/pdf/2601.00794",
      "code": null,
      "tags": [
        "medical image segmentation",
        "U-Net",
        "layer normalization",
        "instance-batch normalization",
        "left ventricle segmentation",
        "cardiac MRI"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp",
      "contributions": "1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance.",
      "summary": "This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies—layer normalization and a combined instance-batch normalization—to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics.",
      "mindmap": "graph TB\n        A[Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[左心室分割对临床诊断至关重要/Left ventricle segmentation is critical for clinical diagnosis]\n        C --> C1[提出LNU-Net和IBU-Net/Propose LNU-Net and IBU-Net]\n        C1 --> C2[基于U-Net，采用不同归一化策略/Based on U-Net with different normalization strategies]\n        D --> D1[在805张MRI图像上评估/Evaluated on 805 MRI images]\n        D1 --> D2[性能优于现有方法/Outperforms state-of-the-art approaches]"
    },
    {
      "title": "Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference",
      "authors": "Shane A. McQuarrie, Mengwu Guo, Anirban Chaudhuri",
      "institution": "Brigham Young University, Lund University, The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2601.00038",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Bayesian operator inference",
        "active learning",
        "reduced-order models",
        "parametric systems",
        "adaptive sampling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c29bb4bdd9fe6e4385983b4dab88a4de7d95bf126d6c1b64568426082175c1b6_w640_q70.webp",
      "contributions": "1. Developed a probabilistic version of parametric operator inference by casting the learning problem as Bayesian linear regression. 2. Designed a sequential adaptive sampling scheme that uses prediction uncertainties from the probabilistic ROM to select new training parameters. 3. Demonstrated through numerical experiments that the active learning framework yields more stable and accurate ROMs than random sampling under the same computational budget.",
      "summary": "This paper proposes an active learning framework to improve data-driven reduced-order models (ROMs) for parametric dynamical systems. The method uses Bayesian operator inference to create probabilistic ROMs and leverages their prediction uncertainties to adaptively select the most informative training parameters. The results show this approach consistently produces more stable and accurate ROMs compared to training with random parameter samples.",
      "mindmap": "graph TB\n        Root(”Active learning for data-driven reduced models<br>数据驱动的降阶模型主动学习”) --> Problem(”ROM quality depends on training data<br>ROM质量依赖于训练数据”)\n        Root --> Method(”Bayesian OpInf + Adaptive Sampling<br>贝叶斯算子推断 + 自适应采样”)\n        Root --> Results(”More stable & accurate ROMs vs. random sampling<br>相比随机采样，获得更稳定准确的ROM”)"
    },
    {
      "title": "Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures",
      "authors": "Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak",
      "institution": "National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)",
      "link": "https://arxiv.org/pdf/2601.00067",
      "code": null,
      "tags": [
        "quantum computing",
        "quantum dot",
        "charge stability diagram",
        "automated characterization",
        "machine learning",
        "image processing"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp",
      "contributions": "1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method's effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization.",
      "summary": "This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays.",
      "mindmap": "graph TB\n        A[Automated electrostatic characterization of quantum dot devices<br>量子点器件自动静电表征] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Manual interpretation of CSDs is slow and error-prone<br>CSD手动解释慢且易错]\n        C --> C1[Integrates ML, image processing, object detection<br>集成ML、图像处理、目标检测]\n        D --> D1[Enables rapid extraction of device parameters (lever arms, couplings)<br>实现器件参数快速提取]"
    },
    {
      "title": "Group Cross-Correlations with Faintly Constrained Filters",
      "authors": "Benedikt Fluhr",
      "institution": "Cannot be inferred from the provided content.",
      "link": "https://arxiv.org/pdf/2601.00045",
      "code": null,
      "tags": [
        "group theory",
        "harmonic analysis",
        "topological dynamics",
        "group cross-correlations",
        "G-equivariant filters",
        "non-compact stabilizers",
        "non-transitive actions",
        "unimodularity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/140a9a6a7cb64898feb16f84d9ad4ac9bc745bc82682b538beb6dd66d1f871c8_w640_q70.webp",
      "contributions": "1. Introduces a new notion of group cross-correlations with less constrained filters, 2. Resolves incompatibilities for group actions with non-compact stabilizers, 3. Generalizes results to non-transitive group actions and weakens the unimodularity assumption.",
      "summary": "This paper proposes a generalized framework for group cross-correlations by relaxing the constraints on the filters used. This new approach resolves previous theoretical limitations for group actions with non-compact stabilizers and extends applicability to non-transitive actions without requiring unimodularity. The work provides a more flexible mathematical foundation for equivariant transformations on vector bundles.",
      "mindmap": "graph TB\n        A[Group Cross-Correlations with Faintly Constrained Filters] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[先前滤波器约束过紧 / Previous filter constraints too tight]\n        B --> B2[与非紧稳定子群不兼容 / Incompatible with non-compact stabilizers]\n        C --> C1[提出弱约束滤波器概念 / Propose faintly constrained filters]\n        C --> C2[推广到非传递群作用 / Generalize to non-transitive actions]\n        D --> D1[解决不兼容性问题 / Resolves incompatibility]\n        D --> D2[放宽单模性假设 / Weakens unimodularity assumption]"
    },
    {
      "title": "Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging",
      "authors": "Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani",
      "institution": "Zahedan University of Medical Sciences, Guilan University of Medical Sciences",
      "link": "https://arxiv.org/pdf/2601.00041",
      "code": null,
      "tags": [
        "medical image classification",
        "Convolutional Neural Network",
        "Transfer Learning",
        "Chest X-ray",
        "Pediatric Pneumonia",
        "RegNet"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp",
      "contributions": "1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task.",
      "summary": "This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise.",
      "mindmap": "graph TB\n        A[Deep Learning Approach for Pediatric Pneumonia Diagnosis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[儿童肺炎诊断困难 / Pediatric Pneumonia Diagnosis is Challenging]\n        C --> C1[使用预训练CNN进行迁移学习 / Use Pretrained CNNs with Transfer Learning]\n        C --> C2[评估ResNetRS, RegNet, EfficientNetV2 / Evaluate ResNetRS, RegNet, EfficientNetV2]\n        D --> D1[RegNet性能最佳 / RegNet Achieved Best Performance]\n        D --> D2[准确率92.4%, 敏感度90.1% / Accuracy 92.4%, Sensitivity 90.1%]"
    },
    {
      "title": "Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes",
      "authors": "Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani",
      "institution": "Ben Gurion University of the Negev, Tel Aviv University",
      "link": "https://arxiv.org/pdf/2601.00012",
      "code": "https://github.com/Shaharak88/neural-brain-fields",
      "tags": [
        "neural fields",
        "signal processing",
        "Neural Radiance Fields (NeRF)",
        "EEG",
        "brain-computer interfaces",
        "signal reconstruction",
        "continuous representation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp",
      "contributions": "1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.",
      "summary": "This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.",
      "mindmap": "graph TB\n        Root[”Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>EEG data challenges: low SNR, variability, limited datasets”] --> P1[”具体挑战/Challenges<br>Varying length, low SNR, participant differences”]\n        Method[”主要方法/Method<br>NeRF-inspired neural network for EEG”] --> M1[”核心类比/Core Analogy<br>Viewpoints (NeRF) ↔ Electrodes (EEG)”]\n        Method --> M2[”技术实现/Technique<br>Train on single sample to get fixed weight vector”]\n        Results[”关键结果/Results<br>Enables continuous visualization & reconstruction”] --> R1[”功能一/Capability 1<br>Render signal at unseen times/positions”]\n        Results --> R2[”功能二/Capability 2<br>Simulate non-existent electrodes”]\n        Results --> R3[”实证结果/Empirical Result<br>Improves standard EEG network performance”]"
    },
    {
      "title": "Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI",
      "authors": "Eran Zvuloni, Ronit Almog, Michael Glikson, Shany Brimer Biton, Ilan Green, Izhar Laufer, Offer Amir, Joachim A. Behar",
      "institution": "Technion - Israel Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00014",
      "code": null,
      "tags": [
        "health informatics",
        "deep learning",
        "Holter ECG",
        "explainable AI",
        "time series analysis",
        "risk prediction"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp",
      "contributions": "1. Developed DeepHHF, a deep learning model that uses full 24-hour single-lead ECG recordings for heart failure risk prediction, outperforming models using short segments and clinical scores. 2. Created and utilized the large-scale Technion-Leumit Holter ECG (TLHE) dataset, comprising 69,663 recordings from 47,729 patients collected over 20 years. 3. Provided explainability analysis showing the model focuses on arrhythmias and heart abnormalities, with key attention patterns during daytime hours (8 AM to 3 PM), linking model decisions to clinically relevant features.",
      "summary": "This paper proposes DeepHHF, a deep learning model that analyzes 24-hour single-lead ECG data to predict the 5-year risk of heart failure. The model achieved an AUC of 0.80, outperforming baseline methods, and identified high-risk individuals with a two-fold increased chance of hospitalization or death. The study demonstrates the feasibility of using long-term, continuous ECG data and explainable AI for non-invasive and accessible heart failure risk prediction.",
      "mindmap": "graph TB\n        A[Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[预测心力衰竭风险/Predict Heart Failure Risk]\n        B --> B2[使用长时程ECG数据/Using Long-term ECG Data]\n        C --> C1[深度学习模型DeepHHF/Deep Learning Model DeepHHF]\n        C --> C2[分析24小时单导联ECG/Analyze 24-hour Single-lead ECG]\n        C --> C3[可解释性分析/Explainability Analysis]\n        D --> D1[AUC达到0.80/AUC of 0.80]\n        D --> D2[识别高风险个体/Identify High-risk Individuals]\n        D --> D3[关注心律失常与日间模式/Focus on Arrhythmias & Daytime Patterns]"
    },
    {
      "title": "Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models",
      "authors": "Henry Crandall, Tyler Schuessler, Filip Bělík, Albert Fabregas, Barry M. Stults, Alexandra Boyadzhiev, Huanan Zhang, Jim S. Wu, Aylin R. Rodan, Stephen P. Juraschek, Ramakrishna Mukkamala, Alfred K. Cheung, Stavros G. Drakos, Christel Hohenegger, Braxton Osting, Benjamin Sanchez",
      "institution": "University of Utah, University of Illinois Chicago",
      "link": "https://arxiv.org/pdf/2601.00081",
      "code": null,
      "tags": [
        "biomedical sensing and instrumentation",
        "electrical bioimpedance",
        "physics-informed neural network",
        "cuffless blood pressure",
        "hemodynamic monitoring",
        "wearable device"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ba7d002ced9d6e8a70d39b806f9e6640ce2f6db3f9aa85c2884e1ce15b38a91_w640_q70.webp",
      "contributions": "1. Developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. 2. Established a multiscale biophysical modeling framework to elucidate the relationship between BioZ and blood pressure, identifying key influencing parameters. 3. Created a signal-tagged physics-informed neural network that incorporates fluid dynamics principles for calibration-free estimation of blood pressure and blood velocity.",
      "summary": "This paper develops a smartwatch-based system using electrical bioimpedance sensing and a physics-informed machine learning model to enable cuffless, calibration-free monitoring of blood pressure and blood velocity. The method addresses the theoretical limitations of existing wearable technologies by grounding the model in biophysical principles. The approach was successfully validated across diverse populations and settings, demonstrating its clinical feasibility.",
      "mindmap": "graph TB\n        Root[”Cuffless, Calibration-Free Hemodynamic Monitoring<br>无袖带、免校准血流动力学监测”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”现有无袖带设备依赖缺乏理论依据的方法，易受生理和实验混杂因素影响<br>Existing cuffless devices lack theoretical foundation and are vulnerable to confounders”]\n        Method[”开发具有实时生物电阻抗传感的智能手表，并使用结合流体动力学原理的信号标记物理信息神经网络<br>Developed smartwatch with real-time BioZ sensing and a signal-tagged physics-informed neural network”]\n        Results[”在健康个体和患者中成功测试，证明了该技术用于无袖带血压和血流速度监测的可行性<br>Successfully tested in healthy individuals and patients, demonstrating feasibility for cuffless BP and velocity monitoring”]"
    },
    {
      "title": "Detecting Unobserved Confounders: A Kernelized Regression Approach",
      "authors": "Yikai Chen, Yunxin Mao, Chunyuan Zheng, Hao Zou, Shanzhi Gu, Shixuan Liu, Yang Shi, Wenjing Yang, Kun Kuang, Haotian Wang",
      "institution": "National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2601.00200",
      "code": null,
      "tags": [
        "causal inference",
        "unobserved confounders",
        "kernel regression",
        "reproducing kernel hilbert space",
        "single-environment",
        "causal effect estimation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34f526abd65dea1ab2c42ba3db6b7f0a13399cb09990eded28483079693cf4f_w640_q70.webp",
      "contributions": "1. Proposed a novel method (KRCD) for detecting unobserved confounders in nonlinear observational data under single-environment conditions, bridging a key gap in existing methods. 2. Provided theoretical guarantees, proving that in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist, and that finite-sample differences converge to a zero-mean Gaussian distribution. 3. Demonstrated superior performance and computational efficiency over existing baselines through extensive experiments on synthetic benchmarks and the Twins dataset.",
      "summary": "This paper proposes Kernel Regression Confounder Detection (KRCD), a method to detect unobserved confounders in nonlinear, single-environment observational data by comparing standard and higher-order kernel regressions. Theoretically, it proves key properties about the test statistic's behavior. Experiments show KRCD outperforms existing baselines in both detection performance and computational efficiency.",
      "mindmap": "graph TB\n        Root(”Detecting Unobserved Confounders: A Kernelized Regression Approach”) --> Problem(”核心问题/Problem: Detecting unobserved confounders in nonlinear, single-environment data”)\n        Root --> Method(”主要方法/Method: Kernel Regression Confounder Detection (KRCD)”)\n        Root --> Results(”关键结果/Results: Outperforms baselines, proven theoretical guarantees, high computational efficiency”)"
    },
    {
      "title": "Neural Minimum Weight Perfect Matching for Quantum Error Codes",
      "authors": "Yotam Peled, David Zenati, Eliya Nachmani",
      "institution": "Ben-Gurion University of the Negev",
      "link": "https://arxiv.org/pdf/2601.00242",
      "code": null,
      "tags": [
        "others",
        "Quantum Error Correction",
        "Minimum Weight Perfect Matching",
        "Graph Neural Networks",
        "Transformer",
        "Hybrid Decoder"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp",
      "contributions": "1. Proposed a hybrid decoder (NMWPM) that integrates GNNs and Transformers to predict dynamic edge weights for the MWPM algorithm. 2. Formulated a novel proxy loss function to enable end-to-end training through the non-differentiable MWPM algorithm. 3. Demonstrated a significant reduction in Logical Error Rate (LER) compared to standard baselines.",
      "summary": "This paper proposes a neural-enhanced decoder for quantum error correction called Neural Minimum Weight Perfect Matching (NMWPM). It uses a hybrid architecture of Graph Neural Networks and Transformers to predict dynamic edge weights for the classical MWPM decoder, trained with a novel proxy loss. The method significantly reduces the Logical Error Rate, showing the advantage of combining neural networks with classical algorithms.",
      "mindmap": "graph TB\n        Root[”Neural Minimum Weight Perfect Matching for Quantum Error Codes”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Quantum error correction requires effective decoding.”]\n        Method[”主要方法/Method<br>Hybrid GNN+Transformer predicts weights for MWPM.”]\n        Results[”关键结果/Results<br>Significant reduction in Logical Error Rate.”]"
    },
    {
      "title": "Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction",
      "authors": "Vikram Seenivasan, Srinath Saikrishnan, Andrew Lizarraga, Jonathan Soriano, Bernie Boscoe, Tuan Do",
      "institution": "University of California, Los Angeles (UCLA), Southern Oregon University",
      "link": "https://arxiv.org/pdf/2601.00146",
      "code": null,
      "tags": [
        "astronomical image analysis",
        "Low-Rank Adaptation (LoRA)",
        "photometric redshift",
        "spectroscopic redshift",
        "CNN",
        "transfer learning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7c8dfd9d9ff319ca34f37730c3de7d103e6a39edc2131c3e4ca2711ce7fd7e3_w640_q70.webp",
      "contributions": "1. Applied Low-Rank Adaptation (LoRA), a technique from large language models, to fine-tune CNN-based regression models for photometric redshift prediction in astrophysics. 2. Demonstrated that LoRA effectively combines datasets with different ground truths (photometric and spectroscopic redshifts) to improve model performance, outperforming traditional transfer learning with significantly reduced bias and scatter. 3. Showed that LoRA provides a computationally efficient middle ground between full model retraining and no retraining, enabling the leveraging of pre-trained models for data-sparse tasks in astrophysics.",
      "summary": "This paper applies Low-Rank Adaptation (LoRA) to fine-tune a CNN model for predicting galaxy redshifts, combining a less accurate but broad photometric redshift dataset with a more accurate but limited spectroscopic redshift dataset. The LoRA-based method outperforms traditional transfer learning, achieving lower bias and scatter, and offers a computationally efficient compromise between full retraining and no adaptation. The work demonstrates LoRA's utility for improving regression models in astrophysics by efficiently integrating datasets with different quality ground truths.",
      "mindmap": "graph TB\n        A[Combining datasets with different ground truths using LoRA for photometric redshift prediction] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 如何结合不同精度的红移数据集以提升CNN模型泛化能力？<br>How to combine redshift datasets of different accuracy to improve CNN model generalization?]\n        C[主要方法/Method: 使用低秩适应(LoRA)在光谱红移数据集上微调基于测光红移训练的基模型<br>Using Low-Rank Adaptation (LoRA) to fine-tune a base model (trained on photometric redshifts) on a spectroscopic redshift dataset.]\n        D[关键结果/Results: LoRA模型比传统迁移学习偏差和散射更小；提供全重训练与不训练之间的高效折中方案<br>LoRA model has less bias/scatter than traditional transfer learning; offers an efficient middle ground between full retraining and no retraining.]"
    },
    {
      "title": "Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks",
      "authors": "Xuehui Qian, Hongkai Tao, Yongji Wang",
      "institution": "Washington University in St. Louis, University of Notre Dame, Central South University, Stanford University, New York University",
      "link": "https://arxiv.org/pdf/2601.00342",
      "code": null,
      "tags": [
        "computational fluid dynamics",
        "physics-informed neural networks",
        "multi-stage training",
        "unbounded domain",
        "coordinate transformation",
        "compressible potential equation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70afc10edc6a8a0b3606afe357472e00dfb9e25867b67948c402e19271e24608_w640_q70.webp",
      "contributions": "1. Proposes a novel PINN framework to solve the full nonlinear compressible potential equation in an infinite domain, overcoming a key limitation of traditional methods. 2. Introduces a coordinate transformation and embeds physical asymptotic constraints to address the unbounded-domain and convergence challenges inherent in standard PINNs. 3. Employs a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision.",
      "summary": "This paper proposes a novel framework using Physics-Informed Neural Networks (PINNs) to solve the full nonlinear subsonic compressible flow equation in an infinite domain. The method addresses domain and convergence challenges via coordinate transformation and a multi-stage training process. The results demonstrate high-fidelity solutions and quantify errors from traditional domain truncation and linearization, especially at higher Mach numbers.",
      "mindmap": "graph TB\n        Root[Solving Nonlinear Subsonic Flow in Infinite Domain via MS-PINNs<br>基于多阶段神经网络求解无限域非线性亚音速流动] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>传统方法在无限域求解非线性亚音速流动存在误差] --> P1[线性化或截断域引入误差<br>Linearization/Truncation Causes Error]\n        Problem --> P2[标准PINNs在无限域收敛困难<br>Standard PINNs Struggle in Unbounded Domain]\n        Method[主要方法/Method<br>提出新的PINN框架] --> M1[坐标变换处理无限域<br>Coordinate Transformation for Unbounded Domain]\n        Method --> M2[网络嵌入物理渐近约束<br>Embed Physical Asymptotic Constraints]\n        Method --> M3[多阶段训练提高精度<br>Multi-Stage Training for High Accuracy]\n        Results[关键结果/Results<br>验证框架并量化误差] --> R1[在圆和椭圆几何上验证<br>Validated on Circular/Elliptical Geometries]\n        Results --> R2[精度接近机器精度<br>Accuracy Approaches Machine Precision]\n        Results --> R3[量化截断和线性化误差<br>Quantifies Truncation/Linearization Errors]"
    },
    {
      "title": "Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials",
      "authors": "Li Chen, Leonardo Medrano Sandonas, Shirong Huang, Alexander Croy, Gianaurelio Cuniberti",
      "institution": "TUD Dresden University of Technology, Friedrich Schiller University Jena",
      "link": "https://arxiv.org/pdf/2601.00503",
      "code": null,
      "tags": [
        "interpretable machine learning",
        "quantum-mechanical properties",
        "CatBoost",
        "explainable AI",
        "binding features",
        "electronic descriptors"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19c57b3a84042598ddfad14eee8131675c83c4b83807b904281ff6859ca8850_w640_q70.webp",
      "contributions": "1. Development of the MORE-ML computational framework integrating quantum-mechanical data with machine learning for predicting sensing properties. 2. Expansion of the dataset to MORE-QX, providing extensive electronic binding features for body odor volatilome adsorption. 3. Demonstration that CatBoost models with explainable AI methods offer superior performance and mechanistic insights for rational receptor design.",
      "summary": "The paper addresses the challenge of designing sensing materials for complex body odor detection by developing MORE-ML, a framework that uses quantum-mechanical property data of molecular building blocks as inputs to train tree-based ML models (like CatBoost) to predict binding features. The approach, validated on an expanded dataset, shows that CatBoost models perform well, especially on unseen compounds, and explainable AI methods identify key quantum properties influencing predictions, providing a foundation for rational design of artificial sensing materials.",
      "mindmap": "graph TB\n        Root[”Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Challenges in extending e-noses to complex body odor volatilome (BOV)”] --> P1[”缺乏机理理解/Lack of mechanistic insight”]\n        Method[”主要方法/Method<br>MORE-ML framework”] --> M1[”数据集/Dataset<br>Expand MORE-Q to MORE-QX”]\n        Method --> M2[”模型/Model<br>Tree-based ML (CatBoost) with QM descriptors”]\n        Method --> M3[”分析/Analysis<br>Explainable AI methods”]\n        Results[”关键结果/Results<br>CatBoost outperforms, provides design principles”] --> R1[”高可转移性/High transferability”]\n        Results --> R2[”可解释的见解/Interpretable insights”]"
    },
    {
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "authors": "Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, Gianni De Fabritiis",
      "institution": "Acellera Labs, Universitat Pompeu Fabra, ICREA",
      "link": "https://arxiv.org/pdf/2601.00581",
      "code": "https://github.com/acellera/aceff-paper",
      "tags": [
        "machine learning interatomic potential",
        "TensorNet2",
        "force field",
        "drug-like compounds",
        "charged states",
        "DFT-level accuracy"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf03a6da5bff3c9952736adc7ba7fd161d48187c1623ade05d2174437fd823b_w640_q70.webp",
      "contributions": "1. Introduces AceFF, a pre-trained MLIP specifically optimized for small molecule drug discovery. 2. Employs a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds to improve generalizability. 3. Achieves a balance of high-throughput inference speed and DFT-level accuracy, explicitly supporting essential medicinal chemistry elements and charged states.",
      "summary": "This paper introduces AceFF, a machine learning interatomic potential designed for small molecule drug discovery. It uses a refined TensorNet2 architecture trained on drug-like compounds to achieve DFT-level accuracy with fast inference. The authors demonstrate its state-of-the-art performance on organic molecules through rigorous benchmarks.",
      "mindmap": "graph TB\n        A[AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules] --> B[核心问题/Problem: MLIPs lack generalizability across diverse chemical spaces for drug discovery.]\n        A --> C[主要方法/Method: Refined TensorNet2 architecture trained on comprehensive dataset of drug-like compounds.]\n        A --> D[关键结果/Results: Achieves DFT-level accuracy with high-throughput speed, establishing a new SOTA for organic molecules.]"
    },
    {
      "title": "Generative Conditional Missing Imputation Networks",
      "authors": "George Sun, Yi-Hui Zhou",
      "institution": "North Carolina State University",
      "link": "https://arxiv.org/pdf/2601.00517",
      "code": null,
      "tags": [
        "missing data imputation",
        "Generative Neural Network",
        "Missing Imputation",
        "Multiple Imputation by Chained Equations",
        "MCAR",
        "MAR"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c2fe27f0abc6f988f3b6e7f4d03378bacf1dab0ce55f725ea0b5d5b1991f7df_w640_q70.webp",
      "contributions": "1. Introduces the Generative Conditional Missing Imputation Networks (GCMI) with a theoretical foundation for MCAR and MAR mechanisms., 2. Enhances GCMI's robustness and accuracy by integrating a multiple imputation framework using chained equations., 3. Empirically demonstrates the superior performance of the proposed method compared to other leading imputation techniques.",
      "summary": "This paper proposes Generative Conditional Missing Imputation Networks (GCMI), a generative method for imputing missing data. The method is theoretically grounded for MCAR and MAR scenarios and is further improved by integrating a multiple imputation approach. Experiments show it outperforms other state-of-the-art imputation techniques.",
      "mindmap": "graph TB\n        Root[Generative Conditional Missing Imputation Networks] --> Problem(核心问题/Problem: Missing Data Imputation)\n        Root --> Method(主要方法/Method: GCMI with Multiple Imputation)\n        Root --> Results(关键结果/Results: Superior Performance)\n        Problem --> SubP1(缺失机制/Missing Mechanisms: MCAR, MAR)\n        Method --> SubM1(理论基础/Theoretical Foundation: GCMI)\n        Method --> SubM2(增强框架/Enhanced Framework: Multiple Imputation by Chained Equations)\n        Results --> SubR1(评估方式/Evaluation: Simulations & Benchmark Datasets)"
    },
    {
      "title": "Network Traffic Analysis with Process Mining: The UPSIDE Case Study",
      "authors": "Francesco Vitale, Paolo Palmiero, Massimiliano Rak, Nicola Mazzocca",
      "institution": "University of Naples Federico II, University of Campania Luigi Vanvitelli",
      "link": "https://arxiv.org/pdf/2512.23718",
      "code": null,
      "tags": [
        "network traffic analysis",
        "process mining",
        "Petri nets",
        "unsupervised characterization",
        "network traffic classification",
        "interpretability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aef2d73934ec0b931bbb0dbae5c9be217f30aa949e9142a6391ac35afe9d92a9_w640_q70.webp",
      "contributions": "1. Proposes a novel process mining-based method for the unsupervised characterization of states from gaming network traffic data. 2. Encodes the identified network states into interpretable process models (Petri nets) for explainable analysis. 3. Demonstrates the method's capability to classify network traffic to identify different video games being played, achieving good accuracy and model coherence.",
      "summary": "This paper proposes a process mining method to analyze gaming network traffic for explainable modeling and classification. The method performs unsupervised state characterization, encodes states into interpretable Petri nets, and classifies traffic to identify specific games. Results on the UPSIDE case study show the approach can effectively model network behavior with high coherence and specificity while maintaining good classification accuracy.",
      "mindmap": "graph TB\n        A[Network Traffic Analysis with Process Mining: The UPSIDE Case Study] --> B(核心问题/Problem: Need for explainable analysis of complex gaming network traffic)\n        A --> C(主要方法/Method: Process mining for unsupervised state characterization, encoding into Petri nets, and traffic classification)\n        A --> D(关键结果/Results: High model coherence (94.02%), specificity (174.99%), and good game classification accuracy (73.84% AUC))"
    },
    {
      "title": "A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation",
      "authors": "Steven Owen, Nathan Brown, Nikos Chrisochoides, Rao Garimella, Xianfeng Gu, Franck Ledoux, Na Lei, Roshan Quadros, Navamita Ray, Nicolas Winovich, Yongjie Jessica Zhang",
      "institution": "Sandia National Laboratories, Old Dominion University, Los Alamos National Laboratory, New York University / Stony Brook University, CEA, Dalian University of Technology, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.23719",
      "code": null,
      "tags": [
        "computational geometry",
        "mesh generation",
        "geometry preparation",
        "CAD-to-mesh",
        "machine learning",
        "large language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c3d8c73084ddd952c2e0c9fd20e1b7fe1d87bb02c884cf6327ca47e6ec442eb_w640_q70.webp",
      "contributions": "1. Surveys the application of AI/ML methods to automate and improve key steps in the CAD-to-mesh pipeline, such as part classification, mesh quality prediction, and defeaturing. 2. Reviews AI techniques for enhancing unstructured/block-structured meshing, volumetric parameterization, and parallel mesh generation. 3. Examines emerging tools like reinforcement learning and large language models for scripting automation in meshing workflows.",
      "summary": "This survey paper reviews how artificial intelligence and machine learning are being applied to address bottlenecks in geometry preparation and mesh generation for engineering simulation. It explores a range of methods, from quality prediction to automation with large language models, concluding that AI serves as an assistive technology to extend traditional tools and highlights key challenges for future data-driven workflows.",
      "mindmap": "graph TB\n        A[A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CAD-to-mesh流程瓶颈 / CAD-to-mesh Pipeline Bottlenecks]\n        C --> C1[AI辅助几何与网格生成 / AI-aided Geometry & Meshing]\n        C --> C2[机器学习方法 / Machine Learning Methods]\n        C --> C3[新兴自动化工具 / Emerging Automation Tools]\n        C1 --> C1a[部件分类 / Part Classification]\n        C1 --> C1b[网格质量预测 / Mesh Quality Prediction]\n        C1 --> C1c[去特征化 / Defeaturing]\n        C2 --> C2a[非结构化/块结构化网格 / Unstructured/Block-structured Meshing]\n        C2 --> C2b[体积参数化 / Volumetric Parameterizations]\n        C2 --> C2c[并行网格生成 / Parallel Mesh Generation]\n        C3 --> C3a[强化学习 / Reinforcement Learning]\n        C3 --> C3b[大语言模型 / Large Language Models]\n        D --> D1[AI作为辅助技术 / AI as Assistive Technology]\n        D --> D2[代表性方法与部署 / Representative Methods & Deployments]\n        D --> D3[关键研究挑战 / Key Research Challenges]"
    },
    {
      "title": "Governing Cloud Data Pipelines with Agentic AI",
      "authors": "Aswathnarayan Muthukrishnan Kirubakaran, Adithya Parthasarathy, Nitin Saksena, Ram Sekhar Bodala, Akshay Deshpande, Suhas Malempati, Shiva Carimireddy, Abhirup Mazumder",
      "institution": "IEEE, Independent Researcher, Albertsons, Amtrak, Cato",
      "link": "https://arxiv.org/pdf/2512.23737",
      "code": null,
      "tags": [
        "agent system",
        "policy-aware control",
        "bounded AI agents",
        "adaptive resource reconfiguration",
        "schema reconciliation",
        "automated failure recovery"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5297c02d67a11ab83e576eb218227051a192108c8ce2adf60d62e9fbdb7e355_w640_q70.webp",
      "contributions": "1. A production-oriented control plane architecture for policy-aware agentic management of cloud data pipelines. 2. Detailed agent workflows for monitoring, optimization, and schema management. 3. An evaluation demonstrating significant improvements in recovery time, operational cost, and reduction of manual intervention.",
      "summary": "This paper proposes Agentic Cloud Data Engineering, a control architecture that integrates bounded AI agents to autonomously govern cloud data pipelines by analyzing telemetry and enforcing declarative policies. The method enables adaptive actions like resource reconfiguration and automated recovery. Experimental results show it reduces recovery time by up to 45%, lowers costs by ~25%, and cuts manual intervention by over 70% compared to static orchestration.",
      "mindmap": "graph TB\n        Root[”Governing Cloud Data Pipelines with Agentic AI”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”静态配置与被动运维/Static Config & Reactive Ops”]\n        Problem --> P2[”恢复慢、成本高、手动多/Slow Recovery, High Cost, Manual Overhead”]\n        Method --> M1[”策略感知控制架构/Policy-aware Control Architecture”]\n        Method --> M2[”集成有界AI代理/Integrate Bounded AI Agents”]\n        Method --> M3[”自适应操作/Adaptive Actions”]\n        Results --> R1[”恢复时间降低45%/Recovery Time ↓45%”]\n        Results --> R2[”运营成本降低25%/Operational Cost ↓25%”]\n        Results --> R3[”手动干预减少70%/Manual Intervention ↓70%”]"
    },
    {
      "title": "A Comprehensive Study of Deep Learning Model Fixing Approaches",
      "authors": "Hanmo You, Zan Wang, Zishuo Dong, Luanqi Mo, Jianjun Zhao, Junjie Chen",
      "institution": "Tianjin University, Kyushu University",
      "link": "https://arxiv.org/pdf/2512.23745",
      "code": null,
      "tags": [
        "software testing and debugging",
        "deep learning model fixing",
        "empirical study",
        "robustness",
        "fairness",
        "backward compatibility"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2da70efdc2c67bda59610794db06383b27cdc3ff0d3c9ab3e06c8ec8a0fac3a1_w640_q70.webp",
      "contributions": "1. Conducted a large-scale empirical study evaluating 16 state-of-the-art DL model fixing approaches across model-level, layer-level, and neuron-level categories. 2. Comprehensively assessed the approaches not only on fixing effectiveness but also on their impact on critical properties like robustness, fairness, and backward compatibility. 3. Provided key findings and insights for industry and academia, such as model-level approaches having superior fixing effectiveness and the trade-off between fixing performance and maintaining other model properties.",
      "summary": "This paper conducts a comprehensive empirical study on 16 deep learning model fixing approaches. It evaluates their effectiveness and impact on properties like robustness and fairness, finding that model-level approaches are most effective but no single approach excels in all aspects. The study concludes that future research should focus on mitigating the side effects of model fixing.",
      "mindmap": "graph TB\n        A[A Comprehensive Study of Deep Learning Model Fixing Approaches] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[DL模型存在故障，可能导致严重后果/DL models have faults causing severe consequences]\n        C --> C1[对16种最新模型修复方法进行大规模实证研究/Large-scale empirical study on 16 SOTA model fixing approaches]\n        C --> C2[评估修复效果及对鲁棒性、公平性等属性的影响/Evaluate fixing effectiveness and impact on robustness, fairness, etc.]\n        D --> D1[模型级方法修复效果最佳/Model-level approaches have superior fixing effectiveness]\n        D --> D2[没有方法能在所有属性上表现最佳/No single approach performs best on all properties]"
    },
    {
      "title": "A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios",
      "authors": "Haley Rosso, Talea Mayo",
      "institution": "Emory University",
      "link": "https://arxiv.org/pdf/2512.23748",
      "code": null,
      "tags": [
        "simulation-based inference",
        "diffusion models",
        "score matching",
        "posterior sampling",
        "misspecification",
        "Schrodinger bridge"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7b2b583a1f5706dab9104c469ca95816c45b56fef4f982f0a27368fe696141d_w640_q70.webp",
      "contributions": "1. Provides a comprehensive review of diffusion models as a framework for simulation-based inference (SBI), connecting mathematical foundations to practical applications. 2. Analyzes the comparative advantages of diffusion-based SBI over methods like normalizing flows, particularly in addressing robustness under non-ideal data conditions. 3. Synthesizes specialized methods for handling challenges like model misspecification, unstructured observations, and missing data, highlighting inference-time prior adaptation.",
      "summary": "This review paper explores the use of diffusion models for simulation-based inference, a method for learning posterior distributions when likelihood functions are intractable. It examines how diffusion models, through conditional score matching, offer a flexible and robust framework, especially for scientific data with issues like model misspecification and missingness. The main conclusion is that diffusion-based SBI provides significant advantages in non-ideal data scenarios, though it introduces trade-offs such as iterative sampling costs.",
      "mindmap": "graph TB\n        Root[”A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios”] --> Problem[”核心问题/Problem: Intractable likelihoods in complex simulations hinder parameter inference.”]\n        Root --> Method[”主要方法/Method: Use diffusion models (score matching) for likelihood-free posterior sampling.”]\n        Root --> Results[”关键结果/Results: Diffusion-based SBI is robust to non-ideal data (misspecification, missingness).”]"
    },
    {
      "title": "Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents",
      "authors": "Amin Sadri, M Maruf Hossain",
      "institution": "Not explicitly stated (email domains are personal: gmail.com)",
      "link": "https://arxiv.org/pdf/2512.23749",
      "code": "GitHub",
      "tags": [
        "one-shot learning",
        "Coordinate Matrix Machine",
        "structural intelligence",
        "Green AI",
        "lazy learning",
        "glass-box model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acd9104b41dbc3c7f9d1597d31b940424a078e93beb2b2b21007c741209b006_w640_q70.webp",
      "contributions": "Proposes the Coordinate Matrix Machine (CM^2) for one-shot document classification, Introduces a structural coordinate-based approach as an alternative to semantic vectorization, Designs a \"Green AI\" model optimized for CPU use with inherent explainability",
      "summary": "This paper addresses the challenge of human-level concept learning, where machines require many examples to learn a concept. It proposes the Coordinate Matrix Machine (CM^2), a purpose-built model that learns document structures to classify very similar documents using only one sample per class. The method is presented as a \"Green AI\" solution that outperforms traditional models while being computationally efficient and explainable.",
      "mindmap": "graph TB\n        A[Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents] --> B[核心问题/Problem: Human-level Concept Learning Gap]\n        A --> C[主要方法/Method: Coordinate Matrix Machine (CM^2)]\n        A --> D[关键结果/Results: High Accuracy, One-shot Learning, Green AI]\n        B --> B1[人类单样本学习/Human one-shot learning]\n        B --> B2[机器需大量样本/Machine needs many samples]\n        C --> C1[学习文档结构/Learns document structure]\n        C --> C2[识别重要特征/Identifies important features]\n        D --> D1[高精度与低数据/High accuracy with minimal data]\n        D --> D2[CPU优化与可解释性/CPU-optimized & explainable]"
    },
    {
      "title": "Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation",
      "authors": "Deep Shankar Pandey, Hyomin Choi, Qi Yu",
      "institution": "Rochester Institute of Technology, InterDigital",
      "link": "https://arxiv.org/pdf/2512.23753",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "Evidential Deep Learning",
        "Subjective Logic",
        "Activation Functions",
        "Regularization",
        "Learning Dynamics"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fae8a70028f83294a153c8fd9b9083e99f87bf5f04f2fb8d64ce2fbab74beb82_w640_q70.webp",
      "contributions": "1. Theoretically characterizes the activation-dependent \"learning-freeze\" behavior in evidential deep learning models, where gradients vanish in low-evidence regions. 2. Designs a general family of activation functions and corresponding evidential regularizers to enable consistent evidence updates across different activation regimes. 3. Empirically validates the proposed theory and method through extensive experiments on multiple benchmark classification, few-shot classification, and blind face restoration tasks.",
      "summary": "This paper identifies and theoretically analyzes a \"learning-freeze\" problem in Evidential Deep Learning (EDL) models caused by specific activation functions. To solve this, the authors propose a generalized family of activation functions and regularizers. Extensive experiments show the proposed method improves learning dynamics and effectiveness across various tasks.",
      "mindmap": "graph TB\n        Root(”Generalized Regularized Evidential Deep Learning Models”) --> Problem(”核心问题/Problem: Activation functions in EDL cause learning-freeze in low-evidence regions”)\n        Root --> Method(”主要方法/Method: Design a general family of activation functions and evidential regularizers”)\n        Root --> Results(”关键结果/Results: Theory validated; method effective across multiple benchmarks”)"
    },
    {
      "title": "Geometric Scaling of Bayesian Inference in LLMs",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Dream Sports, Columbia University",
      "link": "https://arxiv.org/pdf/2512.23752",
      "code": null,
      "tags": [
        "interpretability",
        "Bayesian inference",
        "geometric scaling",
        "attention mechanism",
        "value manifolds",
        "predictive entropy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e11ae0519509ba1ae43d1087dfc56ed0f799df57a2ee4fe7c4a6a7f20eb11c3f_w640_q70.webp",
      "contributions": "1. Demonstrates that production-grade LLMs (Pythia, Phi-2, Llama-3, Mistral) preserve a geometric substrate (low-dimensional value manifolds) similar to that enabling exact Bayesian inference in small, controlled \"wind-tunnel\" models. 2. Shows that the dominant axis of last-layer value representations strongly correlates with predictive entropy, and domain-restricted prompts collapse the structure into the same low-dimensional manifolds. 3. Through targeted interventions on the entropy-aligned axis, reveals that this geometry is a privileged readout of uncertainty rather than a singular computational bottleneck for Bayesian-like behavior.",
      "summary": "This paper investigates whether the geometric structures that enable exact Bayesian inference in small, controlled transformer models persist in large-scale production language models. The authors find that models like Llama-3 and Mistral organize their value representations along an entropy-correlated axis, forming similar low-dimensional manifolds. They conclude that modern LLMs preserve this geometric substrate for approximate Bayesian updates, though it acts more as a readout mechanism than a sole computational bottleneck.",
      "mindmap": "graph TB\n        A[Geometric Scaling of Bayesian Inference in LLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Do geometric structures for Bayesian inference persist in production LLMs?]\n        C[主要方法/Method<br>Analyze value representations & perform targeted axis interventions]\n        D[关键结果/Results<br>Geometry persists as a privileged uncertainty readout]"
    },
    {
      "title": "HINTS: Extraction of Human Insights from Time-Series Without External Sources",
      "authors": "Sheo Yon Jhin, Noseong Park",
      "institution": "KAIST",
      "link": "https://arxiv.org/pdf/2512.23755",
      "code": null,
      "tags": [
        "time series forecasting",
        "self-supervised learning",
        "opinion dynamics",
        "attention mechanism",
        "latent factor extraction",
        "residual analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a97eb229421de0a13cd23a1f8c66b8e7b1ac081ecf63d3e2a393fa375ac5f65_w640_q70.webp",
      "contributions": "1. Proposes HINTS, a novel self-supervised framework that extracts latent human factors (e.g., sentiment, influence) endogenously from time series residuals without requiring external data sources like news or social media. 2. Introduces the use of the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns within the time series data. 3. Demonstrates that integrating the extracted human factors as an attention map into a state-of-the-art backbone model consistently improves forecasting accuracy across multiple datasets and provides interpretable insights aligned with real-world events.",
      "summary": "This paper addresses the high cost of using external data to model human factors in time series forecasting. It proposes HINTS, a self-supervised learning framework that extracts latent human insights directly from time series residuals using an opinion dynamics model as inductive bias. The method improves forecasting accuracy and provides interpretable factors aligned with real events, validated on nine real-world and benchmark datasets.",
      "mindmap": "graph TB\n        A[HINTS: Extraction of Human Insights from Time-Series] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[外部数据依赖成本高/High cost of external data dependency]\n        C --> C1[从残差中自监督提取人类因素/Self-supervised extraction from residuals]\n        C --> C2[使用意见动力学作为归纳偏置/Using opinion dynamics as inductive bias]\n        C --> C3[集成到注意力机制中/Integrated as attention map]\n        D --> D1[预测精度提升/Forecasting accuracy improved]\n        D --> D2[可解释性与现实事件对齐/Interpretability aligned with real events]"
    },
    {
      "title": "Drift-Based Dataset Stability Benchmark",
      "authors": "Dominik Soukup, Richard Plný, Daniel Vašata, Tomáš Čejka",
      "institution": "Czech Technical University in Prague, CESNET a.l.e.",
      "link": "https://arxiv.org/pdf/2512.23762",
      "code": null,
      "tags": [
        "communication & networking",
        "concept drift",
        "dataset stability",
        "traffic classification",
        "benchmark",
        "feature weights"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc4badd4590db607d7056063e44a30285afee3c4bd25f3f6b231fbc0932dd8de_w640_q70.webp",
      "contributions": "1. A novel methodology for evaluating dataset stability based on concept drift detection and ML feature weights. 2. A benchmark workflow for comparing datasets and identifying their weak points. 3. A demonstration and initial benchmark of the framework on the CESNET-TLS-Year22 dataset, showing its use for dataset optimization.",
      "summary": "This paper addresses the problem of model degradation in network traffic classification due to data/concept drift. It proposes a new framework that uses a concept drift detection method enhanced with ML feature weights to benchmark dataset stability. The method is demonstrated on a real-world TLS dataset, providing insights for dataset optimization.",
      "mindmap": "graph TB\n        A[Drift-Based Dataset Stability Benchmark] --> B[核心问题/Problem: Model degradation from data drift in network traffic classification]\n        A --> C[主要方法/Method: Concept drift detection boosted by ML feature weights for dataset benchmarking]\n        A --> D[关键结果/Results: Initial stability benchmark for CESNET-TLS-Year22 dataset, showing optimization impact]"
    },
    {
      "title": "Exploring Cumulative Effects in Survival Data Using Deep Learning Networks",
      "authors": "Kang-Chung Yang, Shinsheng Yuan",
      "institution": "Institute of Statistical Science, Academia Sinica",
      "link": "https://arxiv.org/pdf/2512.23764",
      "code": null,
      "tags": [
        "survival analysis",
        "cumulative exposure",
        "time-dependent data",
        "deep learning",
        "interpretability",
        "Cox proportional hazards"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c5684128b891f6cf0f0d31c2656cc3c3eda012e286475475646b128c847e73_w640_q70.webp",
      "contributions": "1. Introduces CENNSurv, a novel deep learning approach for modeling the cumulative effects of time-dependent exposures on survival outcomes., 2. Addresses the scalability limitations of conventional spline-based methods by offering a more efficient approach suitable for large datasets., 3. Provides interpretable insights into cumulative exposure patterns, a feature often overlooked by existing neural network-based survival analysis methods.",
      "summary": "The paper introduces CENNSurv, a deep learning method designed to model the complex cumulative effects of time-varying exposures in survival data. It overcomes scalability issues of traditional methods and provides interpretable patterns. Evaluations on real-world datasets demonstrate its ability to uncover both long-term lagged associations and short-term critical shifts prior to an event.",
      "mindmap": "graph TB\n        A[Exploring Cumulative Effects in Survival Data Using Deep Learning Networks] --> B[核心问题/Problem: Modeling cumulative effects of time-dependent exposures is challenging]\n        A --> C[主要方法/Method: Proposes CENNSurv, a novel deep learning approach]\n        A --> D[关键结果/Results: Reveals lagged associations & behavioral shifts; shows improved scalability & interpretability]"
    },
    {
      "title": "Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data",
      "authors": "Esha Saha, Hao Wang",
      "institution": "University of Alberta",
      "link": "https://arxiv.org/pdf/2512.23761",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "coupled systems",
        "sparsity regularization",
        "multitask learning",
        "partial differential equations",
        "mesh-free sampling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e3d79b9b98f61c6847aeaba2b3e63b7fa984963e6f3f20b8ab17794c98a542_w640_q70.webp",
      "contributions": "1. Proposes MUSIC, a novel sparsity-induced multitask neural network framework for learning coupled system dynamics when physics constraints and data are incomplete and mutually exclusive. 2. Introduces a method that integrates partial physical constraints with data-driven learning using mesh-free sampling and sparsity regularization for model compression and efficiency. 3. Demonstrates the framework's effectiveness on complex solutions (e.g., shock waves) under data-scarce and noisy conditions, outperforming non-sparse baselines.",
      "summary": "This paper addresses the challenge of modeling coupled dynamical systems where the governing equation is known for only one variable and data is available for another. It proposes MUSIC, a sparsity-regularized multitask neural network that integrates these partial constraints with data to recover full system solutions. The method shows improved accuracy and efficiency in learning complex solutions under scarce and noisy data conditions.",
      "mindmap": "graph TB\n        Root[”Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Mismatch between known physics (for one variable) and observed data (for another) in coupled systems.”]\n        Method[”主要方法/Method<br>MUSIC: Sparsity-induced multitask neural network with partial physics constraints and data-driven learning.”]\n        Results[”关键结果/Results<br>Accurately learns complex solutions (shock waves, patterns) under data-scarce, noisy conditions; outperforms non-sparse methods.”]"
    },
    {
      "title": "Neural Optimal Design of Experiment for Inverse Problems",
      "authors": "John E. Darges, Babak Maboudi Afkham, Matthias Chung",
      "institution": "Emory University, University of Oulu",
      "link": "https://arxiv.org/pdf/2512.23763",
      "code": null,
      "tags": [
        "inverse problems",
        "optimal experimental design",
        "neural reconstruction",
        "sparsity by design",
        "single-level optimization",
        "sensor placement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af595776f7b085ae6680908c7f2c444ddf88c3d25cdb5c27eba1e5fe6a41cbe_w640_q70.webp",
      "contributions": "1. Proposes a novel single-level optimization framework (NODE) for optimal experimental design, avoiding the computational complexity of classical bilevel formulations. 2. Introduces direct optimization of continuous design variables (e.g., sensor locations) to enforce sparsity inherently, eliminating the need for l1 regularization and tuning. 3. Demonstrates the framework's effectiveness on analytical, image-based (MNIST), and real-world (sparse-view CT) inverse problems, showing improved reconstruction accuracy over baselines.",
      "summary": "The paper introduces Neural Optimal Design of Experiments (NODE), a learning-based framework that jointly trains a neural reconstruction model and optimizes continuous experimental design variables (like sensor locations) in a single loop. This approach enforces sparsity directly, avoids complex bilevel optimization, and reduces computational cost. NODE is validated on multiple inverse problem benchmarks, consistently outperforming baseline methods in reconstruction accuracy.",
      "mindmap": "graph TB\n        Root(”Neural Optimal Design of Experiment for Inverse Problems<br>逆问题中的神经最优实验设计”) --> Problem(”核心问题/Problem: Optimal experimental design for inverse problems is traditionally complex (bilevel, requires sparsity tuning)<br>逆问题中的最优实验设计传统上很复杂（双层优化，需要稀疏性调参）”)\n        Root --> Method(”主要方法/Method: NODE - Jointly trains neural reconstruction model & continuous design variables in a single loop<br>NODE - 在单层循环中联合训练神经重建模型和连续设计变量”)\n        Root --> Results(”关键结果/Results: Enforces sparsity by design, reduces complexity, outperforms baselines on benchmarks (analytical, MNIST, CT)<br>通过设计实现稀疏性，降低复杂度，在多个基准测试上优于基线”)"
    },
    {
      "title": "A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit",
      "authors": "Karim Salta, Michael Kirby, Chris Peterson",
      "institution": "Colorado State University",
      "link": "https://arxiv.org/pdf/2512.23766",
      "code": null,
      "tags": [
        "subspace clustering",
        "Schubert Variety",
        "Grassmann Manifold",
        "Linde-Buzo-Grey (LBG)",
        "Subspace Clustering",
        "Geometric Learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a73e8fbe969f250567092a96ad55fca5bd7133b8ca34f30feedb918976b1faf5_w640_q70.webp",
      "contributions": "1. Introduces the concept of a trainable prototype called a Schubert Variety of Best Fit (SVBF) for representing clusters of subspaces. 2. Integrates the SVBF prototype into the Linde-Buzo-Grey (LBG) clustering pipeline to create the SVBF-LBG algorithm. 3. Demonstrates improved cluster purity on synthetic, image, spectral, and video action data compared to methods using subspace means.",
      "summary": "This paper proposes a new subspace clustering method that uses a geometric prototype called a Schubert Variety of Best Fit (SVBF) instead of a simple subspace mean. The SVBF is integrated into the Linde-Buzo-Grey algorithm, resulting in an SVBF-LBG framework that shows improved clustering performance on various data types while preserving mathematical structure for analysis.",
      "mindmap": "graph TB\n        Root[”A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Need for geometric representatives in subspace clustering”] --> Problem_Sub[”子问题/Sub-Problem<br>Subspace means on Grassmann manifold may not be optimal”]\n        Method[”主要方法/Method<br>Propose SVBF-LBG algorithm”] --> Method_Sub1[”方法组件/Component 1<br>Schubert Variety of Best Fit (SVBF) prototype”]\n        Method --> Method_Sub2[”方法组件/Component 2<br>Integration into Linde-Buzo-Grey (LBG) pipeline”]\n        Results[”关键结果/Results<br>Improved cluster purity”] --> Results_Sub1[”结果细节/Detail 1<br>Tested on synthetic, image, spectral, video data”]\n        Results --> Results_Sub2[”结果细节/Detail 2<br>Retains mathematical structure for downstream analysis”]"
    },
    {
      "title": "Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics",
      "authors": "Bin Xu, Ayan Banerjee, Sandeep Gupta",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.23767",
      "code": null,
      "tags": [
        "on-device ai",
        "FPGA acceleration",
        "model recovery",
        "hardware-software co-design",
        "GRU",
        "Neural ODE"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp",
      "contributions": "1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.",
      "summary": "This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.",
      "mindmap": "graph TB\n        A[Enabling Physical AI at the Edge<br>在边缘实现物理人工智能] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Model recovery methods (Neural ODEs) are inefficient for edge hardware<br>模型恢复方法在边缘硬件上效率低下]\n        C[主要方法/Method<br>MERINDA: FPGA-accelerated, hardware-friendly framework<br>MERINDA: FPGA加速的硬件友好框架]\n        D[关键结果/Results<br>114x lower energy, 28x smaller memory, 1.68x faster training<br>能耗降低114倍, 内存占用减少28倍, 训练速度提升1.68倍]"
    },
    {
      "title": "Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations",
      "authors": "Ranit Debnath Akash, Ashish Kumar, Verya Monjezi, Ashutosh Trivedi, Gang, Saeid Tizpaz-Niari",
      "institution": "University of Illinois Chicago, University of Colorado Boulder, Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.23769",
      "code": null,
      "tags": [
        "algorithmic fairness",
        "discrimination clustering",
        "individual fairness",
        "hybrid verification",
        "SMT solver",
        "MILP solver"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05da1223f6a1c9a93634f021d221ac5175d00dee272ded0b8782834941db9c55_w640_q70.webp",
      "contributions": "1. Introduced the concept of \"discrimination clustering\" as a generalization of individual fairness to uncover systematic bias patterns. 2. Proposed HyFair, a hybrid technique combining formal symbolic analysis (SMT/MILP) and randomized search for both certification and violation discovery. 3. Developed a novel explanation method to generate interpretable, decision-tree-style artifacts for inputs exhibiting high discrimination.",
      "summary": "The paper identifies a limitation in individual fairness, which only detects isolated unfairness, and proposes the concept of \"discrimination clustering\" to uncover systematic bias patterns. It introduces HyFair, a hybrid method combining formal verification and randomized search to detect these clusters and generate explanations. Experiments show HyFair outperforms existing fairness verification and explanation methods.",
      "mindmap": "graph TB\n        Root(”Uncovering Discrimination Clusters”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”个体公平性检查的局限性/Limitations of individual fairness checks”)\n        P1 --> P2(”无法捕捉系统性歧视模式/Fails to capture systematic bias patterns”)\n        Method --> M1(”提出歧视聚类概念/Propose discrimination clustering concept”)\n        Method --> M2(”开发HyFair混合技术/Develop HyFair hybrid technique”)\n        M2 --> M3(”结合形式分析与随机搜索/Combine formal analysis & randomized search”)\n        Results --> R1(”优于现有方法/Outperforms state-of-the-art methods”)\n        Results --> R2(”揭示系统性偏差/Reveals substantial discrimination clustering”)\n        Results --> R3(”提供可解释的说明/Provides intuitive explanations”)"
    },
    {
      "title": "Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions",
      "authors": "Ankit Kanwar, Dominik Wagner, Luke Ong",
      "institution": "Sony Corporation, Nanyang Technological University (NTU Singapore)",
      "link": "https://arxiv.org/pdf/2512.23770",
      "code": null,
      "tags": [
        "safe reinforcement learning",
        "constrained MDP",
        "trust region policy optimization",
        "natural policy gradient",
        "safety gymnasium",
        "hard constraints"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea6c2a84f6cc7b4f3144847fc78a84736f7247b99f773ed23dd1861f2ff0760_w640_q70.webp",
      "contributions": "1. Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new algorithm for hard-constrained RL that adaptively biases policy updates towards safety while seeking reward improvement. 2. Introduces a trust-region update using a convex combination of the natural policy gradients of cost and reward to ensure a fixed fraction of optimal cost reduction per step. 3. Provides a theoretical guarantee of local progress towards safety and demonstrates superior balance of safety and task performance on Safety Gymnasium benchmarks.",
      "summary": "The paper addresses the problem of reinforcement learning under hard safety constraints, where existing methods struggle to avoid violations without sacrificing reward. It proposes SB-TRPO, an algorithm that performs trust-region updates by combining reward and cost gradients to bias updates towards safety. Experiments show that SB-TRPO achieves a better balance of safety and task completion than state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[Safety-Biased Policy Optimisation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: RL in safety-critical domains requires strict constraint adherence without sacrificing reward performance.]\n        Method[主要方法/Method: SB-TRPO uses convex combination of natural policy gradients for cost and reward in trust-region updates.]\n        Results[关键结果/Results: Achieves best balance of safety and task completion on Safety Gymnasium benchmarks.]"
    },
    {
      "title": "A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms",
      "authors": "Kanishka Hewageegana, Janani Harischandra, Nipuna Senanayake, Gihan Danansuriya, Kavindu Hapuarachchi, Pooja Illangarathne",
      "institution": "Informatics Institute of Technology, Rajarata University, University of Sri Jayewardenepura",
      "link": "https://arxiv.org/pdf/2512.23777",
      "code": null,
      "tags": [
        "anomaly detection",
        "Graph Neural Networks (GNNs)",
        "Fraud Detection",
        "Class Imbalance",
        "Fraudulent Camouflage",
        "Ride-Hailing Platforms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65fc7f6ff0330c22a6dc35373a15256baac5eef984a9100cce967991d93a1e67_w640_q70.webp",
      "contributions": "1. Provides a structured overview and comparison of existing Graph Neural Network (GNN) architectures and methodologies for fraud detection in ride-hailing platforms. 2. Highlights and analyzes key challenges in the domain, specifically class imbalance and fraudulent camouflage, within the ride-hailing ecosystem. 3. Identifies significant methodological progress and research gaps, calling for further exploration into real-world applicability and technical improvements.",
      "summary": "This survey investigates the use of Graph Neural Networks (GNNs) for detecting fraud in ride-hailing platforms. It analyzes and compares various GNN models, focusing on their effectiveness in handling complex relational data and challenges like class imbalance. The paper concludes by identifying progress and gaps in the field, advocating for more research on real-world applications and technical enhancements.",
      "mindmap": "graph TB\n        Root[”A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms”] --> Problem[”核心问题/Problem: Fraud detection in ride-hailing platforms”]\n        Root --> Method[”主要方法/Method: Survey and analysis of Graph Neural Networks (GNNs)”]\n        Root --> Results[”关键结果/Results: Identifies progress, gaps, and calls for future work”]"
    },
    {
      "title": "Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark",
      "authors": "Manu, Yi Guo, Jo Plested, Tim Lynar, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jack Yang, Wangli Yang",
      "institution": "Western Sydney University, University of New South Wales Canberra, The University of Sydney, University of Wollongong",
      "link": "https://arxiv.org/pdf/2512.23779",
      "code": null,
      "tags": [
        "adversarial attacks on llms",
        "denial-of-service",
        "over-generation",
        "black-box attack",
        "evolutionary search",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3002b15ce3957d88befc241c59f1be73a9e49f4e8ad4c345e9d472f11883059e_w640_q70.webp",
      "contributions": "1. Introduces a black-box, query-only benchmark for evaluating prompt-induced denial-of-service attacks on LLMs. 2. Proposes two novel prompt-only attackers: an evolutionary search method (EOGen) and a goal-conditioned reinforcement learning method (RL-GOAL). 3. Defines the Over-Generation Factor (OGF) as a key metric to quantify attack success and characterize model vulnerability.",
      "summary": "This paper addresses the problem of denial-of-service attacks on large language models via prompt-induced over-generation. It proposes a standardized black-box benchmark and two automated attack methods, EOGen and RL-GOAL, to find adversarial prefixes that delay model termination. The results show that the RL-GOAL attacker is particularly effective at forcing models to generate excessively long outputs, highlighting a significant vulnerability.",
      "mindmap": "graph TB\n        A[Prompt-Induced Over-Generation as Denial-of-Service<br/>提示诱导过度生成作为拒绝服务攻击] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>LLM过度生成导致服务拒绝、延迟和成本增加]\n        C[主要方法/Method<br/>黑盒基准与两种攻击者: EOGen(进化搜索)和RL-GOAL(强化学习)]\n        D[关键结果/Results<br/>RL-GOAL攻击者实现更高的平均过度生成因子(OGF)]"
    },
    {
      "title": "FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading",
      "authors": "Molei Qin, Xinyu Cai, Yewen Li, Haochong Xia, Chuqiao Zong, Shuo Sun, Xinrun Wang, Bo An",
      "institution": "Nanyang Technological University, Singapore Management University, Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.23773",
      "code": null,
      "tags": [
        "reinforcement learning",
        "ensemble reinforcement learning",
        "selective update",
        "variational autoencoder",
        "high-frequency trading",
        "risk management"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c036ba975592c2c3be9068e742ccd28ed5b9722ff62085fbcc37e9f3627fe370_w640_q70.webp",
      "contributions": "1. A selective update mechanism for ensemble Q-learners using ensemble TD errors to stabilize training and improve convergence in high-leverage environments. 2. A risk-aware filtering and routing mechanism that uses VAEs to model market state dynamics and identify agent capability boundaries, enabling dynamic policy selection to mitigate risk. 3. A novel three-stage ensemble RL framework (FineFT) that integrates stable training and risk management, demonstrating superior profitability and over 40% risk reduction in crypto futures trading.",
      "summary": "The paper proposes FineFT, a three-stage ensemble reinforcement learning framework designed to address the challenges of high leverage and unseen market states in futures trading. The method uses selective updates for stable training and VAEs for risk-aware policy routing, achieving higher profitability and significantly lower risk compared to state-of-the-art baselines in high-frequency crypto futures experiments.",
      "mindmap": "graph TB\n        A[FineFT: Efficient and Risk-Aware Ensemble RL for Futures Trading] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[高杠杆放大波动/High leverage amplifies reward fluctuations]\n        B --> B2[缺乏能力边界意识/Lack of self-awareness of capability boundaries]\n        C --> C1[阶段I: 选择性更新/Stage I: Selective Update]\n        C --> C2[阶段II: 过滤与VAE训练/Stage II: Filtering & VAE Training]\n        C --> C3[阶段III: 动态路由/Stage III: Dynamic Routing]\n        D --> D1[超越12个SOTA基线/Outperforms 12 SOTA baselines]\n        D --> D2[风险降低超40%/Risk reduced by >40%]\n        D --> D3[实现更高盈利/Achieves superior profitability]"
    },
    {
      "title": "TabMixNN: A Unified Deep Learning Framework for Structural Mixed Effects Modeling on Tabular Data",
      "authors": "Deniz Akdemir",
      "institution": "Not specified in provided content (email domain is gmail)",
      "link": "https://arxiv.org/pdf/2512.23787",
      "code": null,
      "tags": [
        "statistical machine learning",
        "mixed-effects models",
        "deep learning",
        "tabular data",
        "interpretability",
        "structural equation models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e0b6c8d4704d8d650a7573d4312c5032944f0b87d311b6d8a7f5554e250498_w640_q70.webp",
      "contributions": "1. A modular three-stage deep learning framework (mixed-effects encoder, backbone architectures, outcome heads) that synthesizes classical mixed-effects modeling with neural networks for tabular data. 2. Key innovations including an R-style formula interface, support for DAG constraints for causal learning, SPDE kernels for spatial modeling, and comprehensive interpretability tools. 3. A unified interface that maintains the interpretability and theoretical grounding of classical models while leveraging the representational power of deep learning.",
      "summary": "This paper introduces TabMixNN, a PyTorch-based deep learning framework designed to handle hierarchical tabular data by combining classical mixed-effects models with neural network architectures. It supports diverse tasks like regression and classification through a modular design and emphasizes interpretability. The framework provides a unified tool for complex data analysis while preserving the strengths of traditional statistical models.",
      "mindmap": "graph TB\n        Root[TabMixNN: 统一深度学习框架 / Unified Deep Learning Framework] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[层次表格数据 / Hierarchical Tabular Data]\n        Problem --> P2[传统方法限制 / Limitations of Classical Methods]\n        Method --> M1[混合效应编码器 / Mixed-Effects Encoder]\n        Method --> M2[骨干架构 / Backbone Architectures]\n        Method --> M3[预测头 / Prediction Heads]\n        Results --> R1[灵活性应用 / Flexible Applications]\n        Results --> R2[可解释性工具 / Interpretability Tools]"
    },
    {
      "title": "Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems",
      "authors": "Samaresh Kumar Singh, Joyjit Roy, Martin So",
      "institution": "Independent Researchers (based on provided affiliations: IEEE Senior Member in Texas, IEEE Member in Texas, Independent Researcher in Canada)",
      "link": "https://arxiv.org/pdf/2512.23809",
      "code": null,
      "tags": [
        "federated learning",
        "Zero-Trust Architecture",
        "SHAP-weighted aggregation",
        "TPM-based attestation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baa785fc442fcbc6a80214c4fdc6361e67a8e34e5a9bb6f5dd8fb34baf21bb68_w640_q70.webp",
      "contributions": "1) Proposed a hierarchical edge-fog-cloud zero-trust federated learning architecture for trusted agent participation. 2) Introduced a novel SHAP-weighted aggregation algorithm for explainable Byzantine detection in non-IID environments. 3) Integrated TPM-based cryptographic attestation and on-device adversarial training into a defense-in-depth framework.",
      "summary": "The paper addresses security vulnerabilities in Federated Learning for Industrial IoT by proposing ZTA-FL, a framework combining zero-trust agent authentication, explainable Byzantine-resilient aggregation, and on-device adversarial training. It demonstrates high detection accuracy and robustness against attacks on intrusion detection benchmarks while reducing communication overhead.",
      "mindmap": "graph TB\n        Root[Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: IIoT安全漏洞与联邦学习攻击 / IIoT Security Gaps & FL Attacks]\n        Method[主要方法/Method: 零信任认证与可解释聚合 / Zero-Trust Attestation & Explainable Aggregation]\n        Results[关键结果/Results: 高检测精度与抗攻击鲁棒性 / High Detection Accuracy & Attack Robustness]"
    },
    {
      "title": "Improved Bounds for Private and Robust Alignment",
      "authors": "Wenqian Weng, Yi He, Xingyu Zhou",
      "institution": "Wayne State University",
      "link": "https://arxiv.org/pdf/2512.23816",
      "code": null,
      "tags": [
        "preference learning",
        "private alignment",
        "robust alignment",
        "uniform convergence",
        "log loss",
        "adversarial corruption"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07d766123762454b64f45852c98c882b263a3fe86efdee6b0c39b70b0d888215_w640_q70.webp",
      "contributions": "1. Showed that standard private MLE-type log loss can achieve near-optimal rates for private alignment, contrary to prior belief. 2. Demonstrated that existing offline algorithms for joint privacy-and-corruption provide stronger guarantees than previously known, leading to improved bounds for corruption-only settings. 3. Presented the first set of theoretical results for private and robust online alignment.",
      "summary": "This paper studies the theoretical alignment of language models under privacy constraints and adversarial corruption. It shows that a standard MLE-style log loss can achieve near-optimal rates for private alignment and provides improved bounds for joint private-and-robust settings, including the first online results, enabled by new uniform convergence guarantees.",
      "mindmap": "graph TB\n        A[Improved Bounds for Private and Robust Alignment] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[语言模型对齐/Language Model Alignment]\n        B --> B2[隐私与噪声标签/Private & Noisy Labels]\n        C --> C1[理论分析/Theoretical Analysis]\n        C --> C2[统一收敛/Uniform Convergence]\n        D --> D1[私有MLE达到最优/Private MLE Near-Optimal]\n        D --> D2[离线和在线改进界限/Improved Offline & Online Bounds]"
    },
    {
      "title": "MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling",
      "authors": "Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni",
      "institution": "Google Research, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.23824",
      "code": null,
      "tags": [
        "sequence modeling",
        "state-space models",
        "multi-scale dependencies",
        "linear recurrences",
        "long-range modeling",
        "hierarchical tasks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8a72c3eef1e96a4a3f749cd5d5056117d6d9f67369231cfaf5ce87b8f9bd943_w640_q70.webp",
      "contributions": "1. Introduced a multi-scale SSM framework that processes sequence dynamics across multiple resolutions with specialized state-space dynamics. 2. Proposed an input-dependent scale-mixer for dynamic information fusion across different resolutions. 3. Demonstrated consistent performance improvements over prior SSM-based models on benchmarks including Long Range Arena, hierarchical reasoning, time series classification, and image recognition.",
      "summary": "This paper proposes MS-SSM, a multi-scale state-space model that captures both fine-grained and coarse patterns in sequences to address the limited memory and multi-scale dependency issues of traditional SSMs. It introduces specialized dynamics per resolution and a dynamic scale-mixer, leading to enhanced memory efficiency and long-range modeling. Experiments show it outperforms prior SSM models across various tasks while maintaining computational efficiency.",
      "mindmap": "graph TB\n        A[MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统SSMs有效记忆有限/Traditional SSMs have limited effective memory]\n        B --> B2[现有SSMs难以捕捉多尺度依赖/Existing SSMs struggle to capture multi-scale dependencies]\n        C --> C1[多分辨率状态空间框架/Multi-resolution state-space framework]\n        C --> C2[输入依赖的尺度混合器/Input-dependent scale-mixer]\n        D --> D1[提升记忆效率和长程建模/Enhances memory efficiency and long-range modeling]\n        D --> D2[在多个基准测试中优于先前模型/Outperforms prior SSM-based models on benchmarks]"
    },
    {
      "title": "Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations",
      "authors": "Oluwatosin Akande, Gabriel P. Langlois, Akwum Onwunta",
      "institution": "Lehigh University, University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.23829",
      "code": null,
      "tags": [
        "inverse problems",
        "proximal operators",
        "Hamilton-Jacobi equations",
        "deep learning architectures",
        "prior learning",
        "nonconvex priors"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0a12124651668ce14c5fbfa24bee53d6d2566efb05706aeb5d6359b3d85bff6_w640_q70.webp",
      "contributions": "1. Proposes leveraging connections between proximal operators and Hamilton-Jacobi PDEs to develop novel deep learning architectures for learning priors in inverse problems. 2. Introduces a method to learn the prior directly without needing to invert it after training. 3. Demonstrates the efficiency of the proposed method through numerical results in high-dimensional settings.",
      "summary": "This paper addresses the challenge of solving ill-posed inverse problems by developing a novel deep learning approach that leverages the connection between proximal operators and Hamilton-Jacobi PDEs to directly learn the prior. The method avoids the need to invert the prior after training and is shown to be efficient in high-dimensional scenarios through numerical experiments.",
      "mindmap": "graph TB\n        A[Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations] --> B(核心问题/Problem: 从噪声数据中恢复模型参数，逆问题通常不适定，需要正则化或先验信息)\n        A --> C(主要方法/Method: 利用近端算子与Hamilton-Jacobi PDE之间的联系，开发新颖的深度学习架构来直接学习先验)\n        A --> D(关键结果/Results: 所提方法在无需训练后反演先验的情况下，在高维问题中展现出高效性)"
    },
    {
      "title": "Exploiting the Prior of Generative Time Series Imputation",
      "authors": "YuYang Miao, Chang Li, Zehua Chen",
      "institution": "Imperial College London, University of Science and Technology of China, Tsinghua University, Shengshu AI",
      "link": "https://arxiv.org/pdf/2512.23832",
      "code": null,
      "tags": [
        "time series imputation",
        "Schrodinger Bridge",
        "generative model",
        "diffusion model",
        "expert prior",
        "compositional priors"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e54cafc350b1a81442b6b33a9d58319eeae7b34b2cbcfba860bed3f8138ed8af_w640_q70.webp",
      "contributions": "1. Proposes Bridge-TS, a novel generative time series imputation method that builds a data-to-data generation process. 2. Introduces the concept of an \"expert prior\", using a pretrained transformer to provide a deterministic, informative starting point for the generation process. 3. Explores \"compositional priors\", combining estimations from multiple pretrained models to further enhance the imputation process.",
      "summary": "The paper addresses the problem of limited accuracy in generative time series imputation methods, which stems from using uninformative priors like Gaussian noise. It proposes Bridge-TS, a method that uses informative priors from pretrained models to guide a data-to-data generation process. Experiments show Bridge-TS achieves state-of-the-art imputation accuracy on benchmark datasets.",
      "mindmap": "graph TB\n        A[Bridge-TS: Exploiting the Prior of Generative Time Series Imputation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[生成式时间序列插值中先验信息不充分/Uninformative prior in generative time series imputation]\n        C --> C1[提出Bridge-TS方法/Propose Bridge-TS method]\n        C1 --> C2[专家先验/Expert Prior]\n        C1 --> C3[组合先验/Compositional Priors]\n        D --> D1[在ETT等数据集上取得SOTA精度/Achieves SOTA accuracy on ETT, etc.]"
    },
    {
      "title": "Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation",
      "authors": "Kaustubh Dhole",
      "institution": "Emory University",
      "link": "https://arxiv.org/pdf/2512.23837",
      "code": null,
      "tags": [
        "adversarial robustness",
        "mechanistic interpretability",
        "attention layers",
        "adversarial examples",
        "LLM evaluation",
        "token substitution"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial example generation method that exploits intermediate attention layer token distributions, contrasting with prompt-based or gradient-based attacks. 2. Introduces two specific attention-based generation techniques: attention-based token substitution and attention-based conditional generation. 3. Empirically demonstrates that such adversarial examples can degrade performance on an evaluation task (argument quality assessment) while maintaining semantic similarity, highlighting both the promise and limitations (e.g., grammatical degradation) of the approach.",
      "summary": "This paper proposes a new method to generate adversarial examples by extracting token predictions from the intermediate attention layers of LLMs, leveraging their iterative refinement property. The approach is used to stress-test LLM-based evaluation pipelines, showing it can cause performance drops on an argument quality task while preserving semantics, though grammatical issues can arise. The findings illustrate the potential and current constraints of using internal model representations for adversarial testing.",
      "mindmap": "graph TB\n        Root[Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Can intermediate attention layers be used to generate adversarial examples for LLM evaluation?]\n        Method[主要方法/Method: Leverage attention-layer token distributions for token substitution/conditional generation]\n        Results[关键结果/Results: Adversarial examples cause performance drop but may introduce grammatical issues]"
    },
    {
      "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?",
      "authors": "Dingmin Wang, Ji Ma, Shankar Kumar",
      "institution": "Google Research, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.23836",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "adaptive prompting",
        "context window",
        "open-domain QA",
        "retrieval-augmented generation",
        "LLM ignorance"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp",
      "contributions": "1. Proposes an adaptive prompting strategy for RAG that splits retrieved information into smaller chunks for sequential processing, mitigating the noise from irrelevant information in long contexts. 2. Demonstrates experimentally that this strategy matches or outperforms standard prompting on open-domain QA datasets while using fewer tokens. 3. Identifies and analyzes a key failure mode where LLMs generate incorrect answers instead of declining when information is insufficient, highlighting a critical area for future research.",
      "summary": "This paper addresses the problem that longer context windows in Retrieval-Augmented Generation (RAG) introduce irrelevant information, degrading LLM performance. It proposes an adaptive prompting strategy that processes retrieved text in smaller, sequential chunks, achieving comparable accuracy with lower token usage. The study concludes that a major source of error is the LLM's tendency to generate wrong answers rather than admit ignorance, pointing to the need for improved refusal capabilities.",
      "mindmap": "graph TB\n        A[Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[长上下文引入无关信息，降低LLM性能/Long contexts introduce irrelevant info, degrading LLM performance]\n        C --> C1[自适应提示策略：分块顺序处理/Adaptive prompting: sequential chunk processing]\n        D --> D1[性能相当，使用更少token/Matches performance, uses fewer tokens]\n        D --> D2[LLM常生成错误答案而非拒绝/LLM often generates wrong answers instead of declining]"
    },
    {
      "title": "Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs",
      "authors": "Yukun Zhang, Stefan Elbl Droguett, Samyak Jain",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.23848",
      "code": null,
      "tags": [
        "question answering",
        "Retrieval-Augmented Generation (RAG)",
        "SecBERT",
        "financial numerical reasoning",
        "multi-retriever",
        "few-shot learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de021f75daa09442db831a9d2d84064bf5381a47f4cb0fc792e2cfd3bfbd128b_w640_q70.webp",
      "contributions": "1. Proposed a multi-retriever RAG system that retrieves both external domain knowledge (e.g., financial definitions) and internal question contexts to improve financial QA. 2. Demonstrated that domain-specific training with the SecBERT encoder significantly boosts performance, allowing a neural symbolic model to surpass a strong baseline. 3. Showed that a prompt-based LLM generator achieves state-of-the-art performance with a &gt;7% improvement, highlighting the enhanced few-shot numerical reasoning of latest LLMs and the trade-off between hallucination and external knowledge gains.",
      "summary": "This paper addresses errors in financial numerical QA by proposing a multi-retriever RAG system that retrieves external financial knowledge and internal context. The best model, using domain-specific training and a prompt-based LLM, achieves state-of-the-art results, though still below human expert performance, and reveals a trade-off between hallucination and knowledge gains.",
      "mindmap": "graph TB\n        A[Integrating Domain Knowledge for Financial QA<br>金融QA领域知识集成] --> B[Problem: Errors in financial numerical QA due to lack of domain knowledge<br>核心问题: 金融数值QA因缺乏领域知识出错]\n        A --> C[Method: Multi-retriever RAG system with external/internal retrieval<br>主要方法: 多检索器RAG系统]\n        A --> D[Results: SOTA performance, domain-specific training effective, trade-off analyzed<br>关键结果: SOTA性能, 领域训练有效, 权衡分析]"
    },
    {
      "title": "Flow Matching Neural Processes",
      "authors": "Hussen Abu Hamad, Dan Rosenbaum",
      "institution": "University of Haifa",
      "link": "https://arxiv.org/pdf/2512.23853",
      "code": null,
      "tags": [
        "generative models",
        "neural processes",
        "flow matching",
        "ODE solver",
        "conditional sampling",
        "stochastic processes"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c876d5d150fc117d2b987aac4cc9a3bf9b34f3e670cf528e12b73266c4f2dab4_w640_q70.webp",
      "contributions": "1. Proposes a new Neural Process model based on the flow matching generative modeling paradigm. 2. Enables sampling from conditional distributions using an ODE solver without auxiliary conditioning methods. 3. Provides a controllable trade-off between accuracy and computational cost via the ODE solver steps.",
      "summary": "This paper introduces Flow Matching Neural Processes, a new model that integrates flow matching into the neural process framework for learning stochastic processes. The method allows for efficient and simple conditional sampling using an ODE solver. The authors demonstrate that their model outperforms previous state-of-the-art neural process methods on several benchmarks.",
      "mindmap": "graph TB\n        A[Flow Matching Neural Processes] --> B(核心问题/Problem: Learning stochastic processes for conditional predictions)\n        A --> C(主要方法/Method: Integrate flow matching with neural processes, use ODE solver for sampling)\n        A --> D(关键结果/Results: Outperforms SOTA on 1D GP, 2D images, weather data)"
    },
    {
      "title": "The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models",
      "authors": "Rahul Baxi",
      "institution": "Independent Researcher (affiliation inferred from email domain: alumni.cmu.edu, Carnegie Mellon University)",
      "link": "https://arxiv.org/pdf/2512.23850",
      "code": null,
      "tags": [
        "language model evaluation",
        "epistemic robustness",
        "semantic compression",
        "adversarial fabrication",
        "two-system cognitive model",
        "comprehension integrity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp",
      "contributions": "1. Introduces the Drill-Down and Fabricate Test (DDFT), a novel protocol for measuring epistemic robustness in language models under stress from semantic compression and adversarial fabrication. 2. Proposes a two-system cognitive model (Semantic System and Epistemic Verifier) to explain and analyze LLM behavior. 3. Provides empirical evidence that epistemic robustness is orthogonal to model scale and architecture, identifying error detection as the critical bottleneck.",
      "summary": "The paper identifies a gap in current language model evaluations, which fail to measure how robustly models maintain factual knowledge under stress. It introduces the Drill-Down and Fabricate Test (DDFT) to measure epistemic robustness by applying semantic compression and adversarial fabrication. The key finding is that epistemic robustness is not predicted by model size or architecture but by a model's internal verification mechanisms, challenging assumptions about scaling and reliability.",
      "mindmap": "graph TB\n        A[The Drill-Down and Fabricate Test (DDFT)] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有评估无法衡量知识鲁棒性/Current evaluations fail to measure knowledge robustness]\n        C --> C1[DDFT协议: 语义压缩与对抗伪造/DDFT Protocol: Semantic Compression & Adversarial Fabrication]\n        C --> C2[双系统认知模型/Two-System Cognitive Model]\n        D --> D1[鲁棒性与模型规模/架构无关/Robustness orthogonal to model size/architecture]\n        D --> D2[错误检测能力是关键瓶颈/Error detection is the critical bottleneck]"
    },
    {
      "title": "Trellis: Learning to Compress Key-Value Memory in Attention Models",
      "authors": "Mahdi Karami, Ali Behrouz, Praneeth Kacham, Vahab Mirrokni",
      "institution": "Google Research",
      "link": "https://arxiv.org/pdf/2512.23852",
      "code": null,
      "tags": [
        "llm inference",
        "KV cache compression",
        "bounded memory",
        "online gradient descent",
        "recurrent compression",
        "long-context"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/889c9150f49160406df5713209e4165753466bc69bac6ffa78b28c214caa5c7d_w640_q70.webp",
      "contributions": "1. Introduces Trellis, a novel Transformer architecture that replaces the standard unbounded KV cache with a fixed-size memory, enabling bounded memory usage. 2. Proposes a trainable two-pass recurrent compression mechanism that dynamically compresses new key-value pairs into the fixed memory at test time. 3. Leverages an online gradient descent procedure with a forget gate to recursively update the compressed memory, learning to retain important contextual information from long sequences.",
      "summary": "This paper addresses the quadratic complexity and unbounded memory growth of the KV cache in Transformers by proposing Trellis, an architecture with a fixed-size memory and a learned recurrent compression mechanism. The method uses online gradient descent with a forget gate to dynamically update the compressed memory during inference. Experiments show Trellis outperforms baselines, with increasing gains on longer sequences, demonstrating its potential for efficient long-context modeling.",
      "mindmap": "graph TB\n        Root[”Trellis: Learning to Compress Key-Value Memory”] --> Problem[”核心问题/Problem: Transformer KV cache leads to quadratic complexity and unbounded memory”]\n        Root --> Method[”主要方法/Method: Fixed-size memory + Two-pass recurrent compression with online gradient descent & forget gate”]\n        Root --> Results[”关键结果/Results: Outperforms baselines; Gains increase with sequence length for long-context”]"
    },
    {
      "title": "Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding",
      "authors": "Yue Guan, Changming Yu, Shihan Fang, Weiming Hu, Zaifeng Pan, Zheng Wang, Zihan Liu, Yangjie Zhou, Yufei Ding, Minyi Guo, Jingwen Leng",
      "institution": "Shanghai Jiao Tong University, Shanghai Qizhi Institute, University of California, San Diego",
      "link": "https://arxiv.org/pdf/2512.23858",
      "code": null,
      "tags": [
        "llm inference",
        "speculative decoding",
        "tree-based decoding",
        "latency optimization",
        "compiler-friendly execution",
        "static runtime"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13cf851f76b89c86dd0ecc981628839d1ca0ba1772a6b9b893ec9677720b6be0_w640_q70.webp",
      "contributions": "1. Introduces an equal-growth tree structure for speculative decoding that is compatible with static graph compilers. 2. Proposes a latency-aware optimization objective for draft selection, moving beyond simple average accepted length. 3. Designs a stage-based scheduling mechanism to reduce runtime overhead.",
      "summary": "The paper identifies a performance mismatch between dynamic speculative decoding algorithms and static runtime systems. It proposes Yggdrasil, a co-designed system that uses a context-aware tree drafting structure and compiler-friendly execution to achieve latency-optimal speculative decoding. The system supports unmodified LLMs and achieves up to 3.98x speedup over state-of-the-art baselines.",
      "mindmap": "graph TB\n        A[Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding] --> B[核心问题/Problem: Mismatch between dynamic speculation and static runtime assumptions leads to suboptimal performance]\n        A --> C[主要方法/Method: Co-designed system with context-aware tree drafting and compiler-friendly execution]\n        A --> D[关键结果/Results: Up to 3.98x speedup over SOTA baselines, supports unmodified LLMs]"
    },
    {
      "title": "Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense",
      "authors": "Samaresh Kumar Singh, Joyjit Roy",
      "institution": "IEEE (Inferred from author affiliations as IEEE members; specific institutional affiliation not provided in the excerpt)",
      "link": "https://arxiv.org/pdf/2512.23849",
      "code": null,
      "tags": [
        "IoT Security",
        "Economic Denial Security",
        "Stackelberg Game",
        "Cost Asymmetry",
        "Computational Puzzles"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53020dd5fb969c1980dd7f764afe5f97440c1ef68620bdcd3383e97bf39600fc_w640_q70.webp",
      "contributions": "1. Proposed the Economic Denial Security (EDS) framework, a detection-independent defense that exploits the defender's environmental control to impose economic infeasibility on attackers., 2. Formally modeled EDS as a Stackelberg game, deriving optimal parameters and proving that the composition of its four mechanisms yields superlinear (2.1x) cost amplification., 3. Demonstrated practical efficacy with a lightweight (&lt;12KB) implementation, validated on a 20-device IoT testbed and against IoT-23 malware, showing significant attack slowdown, cost asymmetry, and improved mitigation rates.",
      "summary": "The paper addresses the failure of detection-based security in resource-constrained IoT/edge environments. It proposes Economic Denial Security (EDS), a framework that uses mechanisms like computational puzzles and bandwidth taxation to make attacks economically infeasible by amplifying attacker costs. The method is proven to be lightweight, effective in significantly slowing attacks and reducing success rates, and provides a detection-independent layer of defense.",
      "mindmap": "graph TB\n        A[Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[检测安全在资源受限的IoT/边缘环境中失效/Detection-based security fails in resource-constrained IoT/edge]\n        C --> C1[经济拒绝安全框架 / Economic Denial Security (EDS) Framework]\n        C1 --> C2[四种机制组合 / Four Mechanism Composition]\n        C2 --> C3[计算谜题 / Computational Puzzles]\n        C2 --> C4[交互熵 / Interaction Entropy]\n        C2 --> C5[时间拉伸 / Temporal Stretching]\n        C2 --> C6[带宽征税 / Bandwidth Taxation]\n        C --> C7[斯塔克尔伯格博弈建模 / Stackelberg Game Modeling]\n        D --> D1[32-560倍攻击减速 / 32-560x Attack Slowdown]\n        D --> D2[85-520:1 成本不对称 / 85-520:1 Cost Asymmetry]\n        D --> D3[内存占用<12KB / <12KB Memory Footprint]\n        D --> D4[94% 恶意软件缓解 / 94% Malware Mitigation]"
    },
    {
      "title": "Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining",
      "authors": "Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu",
      "institution": "Huawei Technologies Canada Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.23862",
      "code": "https://github.com/RRaAy-H/nanotron-infini",
      "tags": [
        "llm training",
        "Infini-attention",
        "compressive memory",
        "small language models (SLMs)",
        "long-context extrapolation",
        "pretraining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp",
      "contributions": "1. Replaced standard attention in a 300M-parameter LLaMA model with Infini-attention to study compressive memory behavior under short-sequence pretraining. 2. Analyzed the training dynamics of SLMs with Infini-attention, revealing characteristics like loss fluctuations, gradient volatility, and early-layer memory concentration. 3. Demonstrated that Infini-attention improves long-context extrapolation over a baseline model, with supervised fine-tuning further boosting performance.",
      "summary": "This paper investigates whether the Infini-attention mechanism, which combines local attention with compressive memory, can enhance long-context capabilities in Small Language Models (SLMs) during small-scale pretraining. The authors empirically study a 300M-parameter LLaMA model equipped with Infini-attention and find it improves long-context retrieval accuracy over a baseline, despite some degradation over very long sequences. The conclusion is that architectural memory like Infini-attention is beneficial for achieving robust long-context performance in SLMs.",
      "mindmap": "graph TB\n        A[Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Enhancing long-context extrapolation for Small Language Models (SLMs)]\n        C[主要方法/Method: Using Infini-attention (compressive memory + local attention) in small-scale pretraining]\n        D[关键结果/Results: Improves long-context retrieval; Identifies balance factor importance; Shows performance degradation over very long sequences but still outperforms baseline]"
    },
    {
      "title": "Lifelong Domain Adaptive 3D Human Pose Estimation",
      "authors": "Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen",
      "institution": "University of Central Florida, University of North Carolina at Charlotte",
      "link": "https://arxiv.org/pdf/2512.23860",
      "code": null,
      "tags": [
        "human pose estimation",
        "lifelong domain adaptation",
        "catastrophic forgetting",
        "generative adversarial network",
        "pose-aware knowledge",
        "temporal-aware knowledge"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp",
      "contributions": "1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.",
      "summary": "This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.",
      "mindmap": "graph TB\n    A[Lifelong Domain Adaptive 3D Human Pose Estimation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[3D HPE泛化挑战/3D HPE Generalization Challenge]\n    B --> B2[非平稳目标域/Non-stationary Target Domains]\n    B --> B3[灾难性遗忘/Catastrophic Forgetting]\n    C --> C1[终身域适应任务/Lifelong DA Task]\n    C --> C2[GAN框架/GAN Framework]\n    C --> C3[3D姿态生成器/3D Pose Generator]\n    C2 --> C2a[3D姿态生成器/3D Pose Generators]\n    C2 --> C2b[2D姿态判别器/2D Pose Discriminator]\n    C2 --> C2c[3D姿态估计器/3D Pose Estimator]\n    C3 --> C3a[姿态感知/Pose-aware]\n    C3 --> C3b[时序感知/Temporal-aware]\n    C3 --> C3c[域感知/Domain-aware]\n    D --> D1[缓解域偏移/Mitigates Domain Shifts]\n    D --> D2[对齐姿态/Aligns Poses]\n    D --> D3[卓越性能/Superior Performance]"
    },
    {
      "title": "Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR",
      "authors": "Yuyang Zhang, Yang Hu, Bo Dai, Na Li",
      "institution": "Harvard University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23870",
      "code": null,
      "tags": [
        "reinforcement learning",
        "max-entropy reinforcement learning",
        "flow-based policy",
        "flow matching",
        "soft actor-critic",
        "linear quadratic regulator"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d43d8f718c30e1679c2274346eea229b95864ab6207cedce0e09dc784832028_w640_q70.webp",
      "contributions": "1. Proposes a variant of the Soft Actor-Critic (SAC) algorithm that uses flow-based models to parameterize the policy, enhancing expressiveness. 2. Introduces an online variant of flow matching called Importance Sampling Flow Matching (ISFM) for policy updates using samples from a user-specified distribution instead of the unknown target. 3. Provides a theoretical analysis of ISFM, characterizing how the choice of sampling distribution impacts learning efficiency, and validates the method with a case study on max-entropy Linear Quadratic Regulator (LQR) problems.",
      "summary": "This paper addresses the limitation of simple policy approximations in max-entropy reinforcement learning by proposing a new SAC variant that uses expressive flow-based policies. The method employs a novel online flow matching technique (ISFM) for efficient policy updates and demonstrates its effectiveness by learning the optimal action distribution in max-entropy LQR problems.",
      "mindmap": "graph TB\n        Root[Max-Entropy RL with Flow Matching and LQR Case Study] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[SAC使用简单策略类牺牲了表达性和鲁棒性/SAC's simple policy classes sacrifice expressiveness & robustness]\n        Method[主要方法/Method] --> M1[提出使用流模型参数化策略的SAC变体/Propose SAC variant with flow-based policy]\n        Method --> M2[开发在线流匹配变体ISFM进行策略更新/Develop online flow matching variant ISFM for policy update]\n        Results[关键结果/Results] --> R1[理论分析ISFM采样分布的影响/Theoretical analysis of ISFM sampling distribution impact]\n        Results --> R2[在最大熵LQR问题上验证算法有效性/Validate algorithm on max-entropy LQR problems]"
    },
    {
      "title": "Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City",
      "authors": "Tin Hoang",
      "institution": "University of Surrey",
      "link": "https://arxiv.org/pdf/2512.23898",
      "code": "github.com/Tin-Hoang/solar-timeseries-forecasting",
      "tags": [
        "time series forecasting",
        "Transformer",
        "Mamba",
        "Knowledge Distillation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8685dbd2386bb45fd799c168962048f6a243c0b8c836a5be5978ff64d0c2e184_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive benchmark of ten deep learning architectures for short-term solar irradiance forecasting, identifying the Transformer as the best-performing model. 2. Used SHAP analysis to reveal and contrast the distinct temporal reasoning patterns of different architectures (e.g., Transformer's recency bias vs. Mamba's periodic dependency). 3. Demonstrated that Knowledge Distillation can effectively compress the high-performance Transformer model, reducing its size by 23.5% while improving accuracy for edge deployment.",
      "summary": "This paper benchmarks ten deep learning models for 1-hour ahead solar irradiance forecasting in Ho Chi Minh City. The Transformer model achieved the highest accuracy, and the study used explainable AI to analyze model behavior and successfully compressed the model via Knowledge Distillation for efficient edge deployment.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Efficient Deep Learning for Short-Term Solar Irradiance Forecasting] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 预测全球水平辐照度(GHI)以缓解太阳能波动 / Forecasting GHI to mitigate solar energy variability]\n        C[主要方法 / Method: 对十种深度学习架构进行基准测试与可解释性分析 / Benchmarking 10 DL architectures with explainability analysis]\n        D[关键结果 / Results: Transformer性能最优；知识蒸馏实现高效压缩 / Transformer best; Knowledge Distillation enables efficient compression]"
    },
    {
      "title": "Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks",
      "authors": "Peter Farag",
      "institution": "SP Cloud & Technologies Inc.",
      "link": "https://arxiv.org/pdf/2512.23905",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "Stagewise Pairwise Mixers",
        "dense linear layers",
        "near-linear complexity",
        "compositional inductive bias",
        "drop-in replacement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6296322c9e11128e2315b979afc9e75bc26d003cffef0deb0d439140a648751a_w640_q70.webp",
      "contributions": "1. Proposes Stagewise Pairwise Mixers (SPM), a structured linear operator that replaces dense matrices with a composition of sparse pairwise-mixing stages, achieving near-linear time and parameter complexity. 2. Derives complete forward and backward expressions for two parameterizations: an orthogonal norm-preserving rotation-based variant and a fully general 2×2 mixing variant. 3. Demonstrates that SPM reduces wall-clock cost and improves accuracy on structured learning problems while maintaining competitive performance on real-world benchmarks.",
      "summary": "The paper addresses the high computational and parametric cost of dense linear layers in neural networks by introducing Stagewise Pairwise Mixers (SPM), a structured operator that composes sparse pairwise-mixing stages to achieve near-linear complexity. SPM serves as a drop-in replacement for dense layers, offering exact closed-form computations and an explicit compositional inductive bias. Proof-of-concept experiments show substantial reductions in training cost and improved generalization on structured tasks while retaining competitive performance on benchmarks.",
      "mindmap": "graph TB\n        A[Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[密集线性层计算和参数成本高/Dense linear layers are computationally and parametrically expensive]\n        B --> B2[与表示结构不匹配/Misaligned with compositional structure of representations]\n        C --> C1[引入SPM算子/Introduce Stagewise Pairwise Mixers (SPM)]\n        C --> C2[稀疏成对混合阶段组成/Composition of sparse pairwise-mixing stages]\n        C --> C3[近线性复杂度/Near-linear time and parameter complexity]\n        D --> D1[降低训练成本/Reduces wall-clock cost]\n        D --> D2[提升结构化任务准确率/Improves accuracy on structured problems]\n        D --> D3[保持基准性能/Retains competitive benchmark performance]"
    },
    {
      "title": "Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias",
      "authors": "Xia Chen",
      "institution": "Technische Universität München (Georg Nemetschek Institute, Munich Data Science Institute)",
      "link": "https://arxiv.org/pdf/2512.23916",
      "code": null,
      "tags": [
        "neuromorphic computing",
        "temporal inductive bias",
        "dissipative dynamics",
        "spiking neural networks",
        "generalization",
        "phase-space analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b28592acb015fa66de83fc46eda200fa0018a58a31fbfacdf9dd0fced988bac9_w640_q70.webp",
      "contributions": "1. Proposes that physical constraints (like metabolic budgets) act not as limitations but as a temporal inductive bias that promotes generalization in neural systems. 2. Reveals through phase-space analysis that proper dissipative dynamics compress the solution space and align with spectral bias to abstract invariant features, unlike expansive dynamics. 3. Empirically demonstrates across multiple tasks (classification, reconstruction, RL) that a critical \"transition\" regime of dynamical constraints maximizes generalization capability.",
      "summary": "The paper argues that physical constraints, often seen as limitations, can serve as a beneficial temporal inductive bias for generalization in neural networks. It analyzes signal propagation to show that dissipative dynamics compress phase space and abstract features, a principle implemented using Spiking Neural Networks. Experiments across various tasks confirm that properly constrained temporal dynamics maximize generalization, suggesting a new direction for robust AI development.",
      "mindmap": "graph TB\n        A[”Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias”] --> B[”核心问题/Problem: Conventional deep learning uses unconstrained optimization, unlike biologically constrained systems.”]\n        A --> C[”主要方法/Method: Propose temporal constraints as inductive bias; analyze phase-space dynamics; use Spiking Neural Networks (SNNs) for temporal integration.”]\n        A --> D[”关键结果/Results: Dissipative dynamics maximize generalization; a critical 'transition' regime is identified across tasks.”]"
    },
    {
      "title": "Interactive Machine Learning: From Theory to Scale",
      "authors": "Yinglun Zhu",
      "institution": "University of Wisconsin–Madison",
      "link": "https://arxiv.org/pdf/2512.23924",
      "code": null,
      "tags": [
        "interactive machine learning",
        "active learning",
        "contextual bandits",
        "model selection",
        "sequential decision making",
        "partial feedback"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d3c8fc2dd2145126e7321fa8de265f0c2652dbd2dc7df8c387585634498e335_w640_q70.webp",
      "contributions": "1. Developed computationally efficient active learning algorithms that achieve exponential label savings without requiring low-noise assumptions., 2. Introduced the first efficient, general-purpose contextual bandit algorithms whose performance guarantees are independent of the action space size., 3. Provided the first tight characterizations of the fundamental cost of model selection in sequential decision-making settings.",
      "summary": "This dissertation addresses the high cost of data labeling and trial-and-error in machine learning by developing new algorithms for interactive learning. It proposes statistically optimal and computationally efficient methods for active learning, contextual bandits with large action spaces, and model selection under partial feedback. The work advances the theoretical foundations of interactive learning and provides guidance for its deployment in large-scale, real-world applications.",
      "mindmap": "graph TB\n        Root(”Interactive Machine Learning: From Theory to Scale<br>交互式机器学习：从理论到规模”)\n        Root --> Problem(”Problem: High cost of labeled data & trial-and-error in ML<br>核心问题：机器学习中标注数据和试错的高成本”)\n        Root --> Method(”Method: Develop algorithms for interactive learning<br>主要方法：开发交互式学习算法”)\n        Root --> Results(”Results: Statistically optimal & computationally efficient algorithms<br>关键结果：统计最优且计算高效的算法”)\n        Problem --> P1(”Active learning with noisy data<br>含噪声数据的主动学习”)\n        Problem --> P2(”Sequential decision making with large action spaces<br>大动作空间的序列决策”)\n        Problem --> P3(”Model selection under partial feedback<br>部分反馈下的模型选择”)\n        Method --> M1(”New algorithmic principles<br>新算法原理”)\n        Method --> M2(”Establish fundamental limits<br>建立基本极限”)\n        Results --> R1(”Exponential label savings in active learning<br>主动学习中的指数级标签节省”)\n        Results --> R2(”Contextual bandit guarantees independent of action space size<br>与动作空间大小无关的上下文赌博机保证”)\n        Results --> R3(”Tight characterization of model selection cost<br>模型选择成本的紧致刻画”)"
    },
    {
      "title": "Statistical Guarantees in the Search for Less Discriminatory Algorithms",
      "authors": "Chris Hays, Ben Laufer, Solon Barocas, Manish Raghavan",
      "institution": "MIT, Cornell University, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.23943",
      "code": null,
      "tags": [
        "algorithmic fairness",
        "model multiplicity",
        "optimal stopping",
        "disparate impact",
        "statistical guarantees",
        "less discriminatory algorithms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/323b958070f176d29545b148d49cdc3b6141db385ed752500b67c6d9556a8c78_w640_q70.webp",
      "contributions": "1. Formalizes the search for less discriminatory algorithms (LDAs) as an optimal stopping problem, providing a statistical framework to define a \"good-faith effort\" in model development. 2. Proposes an adaptive stopping algorithm that yields a high-probability upper bound on the potential gains from continued search, allowing developers to certify the sufficiency of their exploration. 3. Provides a flexible framework where developers can incorporate stronger assumptions about the model distribution to obtain correspondingly stronger statistical bounds, validated on real-world datasets.",
      "summary": "The paper addresses the problem of how firms can demonstrate a good-faith effort to find less discriminatory algorithms. It proposes an adaptive stopping algorithm based on optimal stopping theory, which provides statistical guarantees on the potential benefits of further search. The method allows developers to certify that their search for fairer models was sufficient, as validated on credit, employment, and housing datasets.",
      "mindmap": "graph TB\n        Root[”Statistical Guarantees in the Search for Less Discriminatory Algorithms<br>寻找更少歧视性算法的统计保证”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>What constitutes a good-faith search for less discriminatory models?<br>什么是对更少歧视性模型的真诚搜索？”]\n        Method[”主要方法/Method<br>Formalize search as an optimal stopping problem; propose adaptive stopping algorithm.<br>将搜索形式化为最优停止问题；提出自适应停止算法。”]\n        Results[”关键结果/Results<br>High-probability bound on search gains; framework for certification.<br>搜索收益的高概率上界；用于认证的框架。”]"
    },
    {
      "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
      "authors": "Kacem Khaled, Felipe Gohring de Magalhães, Gabriela Nicolescu",
      "institution": "Polytechnique Montreal",
      "link": "https://arxiv.org/pdf/2512.23948",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "Quantization Aware Training",
        "Model Extraction Attack",
        "Quantized Convolutional Neural Networks",
        "Edge Device",
        "Robustness"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c08371f06267ec2fffb37ff15727bbbcfaf1e05de4f88d1541883445740a65d_w640_q70.webp",
      "contributions": "1. Proposes DivQAT, a novel algorithm to train quantized CNNs that integrates a defense mechanism directly into the Quantization Aware Training process to enhance robustness against model extraction attacks. 2. Demonstrates that the proposed technique effectively defends against model extraction attacks without compromising the model's accuracy, as validated on benchmark vision datasets. 3. Shows that combining the proposed quantization technique with other defense mechanisms improves their effectiveness compared to using traditional QAT.",
      "summary": "This paper addresses the vulnerability of quantized Convolutional Neural Networks to model extraction attacks. It proposes DivQAT, a novel training algorithm that modifies the quantization process to integrate a defense mechanism directly, enhancing model robustness. The method is shown to be effective against attacks without harming accuracy and can improve other defenses when combined.",
      "mindmap": "graph TB\n        Root[”DivQAT: Enhancing Robustness of Quantized CNNs against Model Extraction Attacks”] --> Problem[”核心问题/Problem: Quantized CNNs are vulnerable to model extraction attacks, posing IP theft risks.”]\n        Root --> Method[”主要方法/Method: Proposes DivQAT, a novel QAT-based algorithm integrating defense into the quantization training process.”]\n        Root --> Results[”关键结果/Results: Enhances robustness against attacks without compromising accuracy; improves other defenses.”]"
    },
    {
      "title": "Improved Balanced Classification with Theoretically Grounded Loss Functions",
      "authors": "Corinna Cortes, Mehryar Mohri, Yutao Zhong",
      "institution": "Google Research",
      "link": "https://arxiv.org/pdf/2512.23947",
      "code": null,
      "tags": [
        "imbalanced classification",
        "balanced loss",
        "surrogate loss",
        "H-consistency",
        "logit-adjusted",
        "class-weighted"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03edb44855299a7da70383f03cc9676cdeb3330caf1fa4f5212538cf5a33b248_w640_q70.webp",
      "contributions": "1. Introduces two new surrogate loss families for balanced classification: Generalized Logit-Adjusted (GLA) and Generalized Class-Aware weighted (GCA) losses. 2. Provides a comprehensive theoretical analysis showing GCA losses have stronger H-consistency guarantees (scaling as 1/√p_min) than GLA losses (scaling as 1/p_min) in imbalanced settings. 3. Empirically demonstrates that both GCA and GLA losses outperform standard class-weighted and Logit-Adjusted losses, with GLA slightly better on common benchmarks and GCA better in highly imbalanced scenarios.",
      "summary": "This paper addresses class imbalance in multi-class classification by proposing two theoretically grounded surrogate loss families: Generalized Logit-Adjusted (GLA) and Generalized Class-Aware weighted (GCA) losses. The theoretical analysis shows GCA offers stronger consistency guarantees, especially for imbalanced data. Experiments confirm both losses outperform existing methods, with each excelling in different imbalance settings.",
      "mindmap": "graph TB\n    A[Improved Balanced Classification with Theoretically Grounded Loss Functions] --> B(核心问题/Problem: Class imbalance leads to poor minority class performance)\n    A --> C(主要方法/Method: Proposes GLA and GCA surrogate loss families)\n    A --> D(关键结果/Results: GCA has stronger theoretical guarantees; both outperform baselines empirically)"
    },
    {
      "title": "Physics-informed Graph Neural Networks for Operational Flood Modeling",
      "authors": "Carlo Malapad Acosta, Herath Mudiyanselage Viraj Vidura Herath, Jia Yu Lim, Abhishek Saha, Sanka Rasnayaka, Lucy Marshall",
      "institution": "National University of Singapore, The University of Sydney, Delft University of Technology",
      "link": "https://arxiv.org/pdf/2512.23964",
      "code": "https://github.com/acostacos/dual_flood_gnn",
      "tags": [
        "graph neural networks",
        "physics-informed neural networks",
        "graph neural networks",
        "flood modeling",
        "curriculum learning",
        "message-passing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6bbddb2010c8550ce3ea4a09f24c04abc0993a7ee2a722f960fc0852c4f049_w640_q70.webp",
      "contributions": "1. Proposes DUALFloodGNN, a novel GNN architecture that embeds physical constraints at both global and local scales through explicit loss terms. 2. Introduces a model that jointly predicts water volume at nodes and flow along edges using a shared message-passing framework. 3. Enhances autoregressive inference performance via multi-step loss training with dynamic curriculum learning.",
      "summary": "This paper addresses the high computational cost of physics-based flood models by proposing DUALFloodGNN, a physics-informed graph neural network architecture. The model incorporates physical constraints into its loss function and uses a multi-step training strategy with curriculum learning. It achieves improved prediction accuracy for hydrologic variables while maintaining high computational efficiency compared to existing GNN models.",
      "mindmap": "graph TB\n        A[Physics-informed Graph Neural Networks for Operational Flood Modeling] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: High computational cost of physics-based flood models limits operational use]\n        C[主要方法/Method: DUALFloodGNN embeds physical constraints via loss terms and uses multi-step training with curriculum learning]\n        D[关键结果/Results: Achieves improved accuracy and maintains high computational efficiency]"
    },
    {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "authors": "Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu",
      "institution": "The Chinese University of Hong Kong, WeChat AI",
      "link": "https://arxiv.org/pdf/2512.23959",
      "code": "https://github.com/Encyclomen/HGMem",
      "tags": [
        "rag (retrieval-augmented generation)",
        "hypergraph memory",
        "multi-step reasoning",
        "global sense-making",
        "long-context modeling",
        "retrieval-augmented generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp",
      "contributions": "1. Proposes HGMem, a novel hypergraph-based memory mechanism that models memory as a dynamic structure with higher-order interactions, moving beyond passive storage. 2. Addresses the limitation of existing multi-step RAG memory in capturing complex relational structures and providing strong guidance for subsequent reasoning steps. 3. Demonstrates through extensive experiments that the method consistently improves multi-step RAG performance and substantially outperforms strong baselines on global sense-making tasks.",
      "summary": "This paper addresses the limitation of static, passive memory in multi-step RAG systems, which leads to fragmented reasoning in long-context tasks. It proposes HGMem, a dynamic hypergraph-based memory mechanism that captures high-order correlations among facts to form an integrated knowledge structure for stronger reasoning guidance. The method is shown to consistently and substantially outperform baseline systems across diverse global sense-making tasks.",
      "mindmap": "graph TB\n        A[Improving Multi-step RAG with Hypergraph-based Memory<br>改进多步RAG的超图记忆] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有记忆模块是被动的静态存储<br>Existing memory is passive static storage]\n        B1 --> B2[忽略了高阶关联，导致碎片化推理<br>Ignores high-order correlations, causing fragmented reasoning]\n        C --> C1[提出超图记忆机制 HGMem<br>Propose hypergraph memory mechanism HGMem]\n        C1 --> C2[将记忆表示为动态超图<br>Represent memory as a dynamic hypergraph]\n        C2 --> C3[超边形成高阶交互，构建集成知识结构<br>Hyperedges form high-order interactions, building integrated knowledge]\n        D --> D1[在多步RAG上取得一致改进<br>Achieves consistent improvement on multi-step RAG]\n        D1 --> D2[在全局理解任务上显著超越基线<br>Substantially outperforms baselines on global sense-making tasks]"
    },
    {
      "title": "Exploring the Potential of Spiking Neural Networks in UWB Channel Estimation",
      "authors": "Youdong Zhang, Xu He, Xiaolin Meng",
      "institution": "Southeast University (SEU)",
      "link": "https://arxiv.org/pdf/2512.23975",
      "code": null,
      "tags": [
        "on-device ai",
        "Spiking Neural Networks (SNNs)",
        "Ultra-Wide Band (UWB)",
        "Channel Estimation",
        "Liquid State Machine (LSM)",
        "Neuromorphic Computing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6223f5cf13e59469e26c765b0854ea31ff37f76d9f1c9137587f99a5a743dcc0_w640_q70.webp",
      "contributions": "1. Proposes a fully unsupervised Spiking Neural Network (SNN) solution for UWB channel estimation, addressing the resource constraints of edge devices. 2. Employs a Liquid State Machine (LSM) with fixed synaptic weights to extract spiking representations from UWB features, sidestepping typical SNN training difficulties. 3. Demonstrates that the proposed SNN approach achieves competitive accuracy (80%) compared to supervised deep learning methods while offering drastically reduced model complexity and suitability for neuromorphic deployment.",
      "summary": "This paper addresses the high computational cost of deep learning-based UWB channel estimation on resource-constrained edge devices by proposing a fully unsupervised Spiking Neural Network (SNN) solution. The method uses a Liquid State Machine to process encoded UWB features and achieves 80% test accuracy, comparable to supervised methods, while being inherently more efficient and suitable for neuromorphic hardware.",
      "mindmap": "graph TB\n        Root[”Exploring the Potential of Spiking Neural Networks in UWB Channel Estimation”] --> Problem[”核心问题/Problem: High computational cost of DL-based UWB channel estimation clashes with edge device constraints”]\n        Root --> Method[”主要方法/Method: Propose a fully unsupervised SNN solution using Liquid State Machine (LSM)”]\n        Root --> Results[”关键结果/Results: Achieves 80% accuracy (on par with supervised DL), drastic model complexity reduction, suited for neuromorphic deployment”]"
    },
    {
      "title": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing",
      "authors": "Giacinto Paolo Saggese, Paul Smith",
      "institution": "Not explicitly stated. Could be inferred from author names and arXiv submission, but no clear affiliation is provided in the given content.",
      "link": "https://arxiv.org/pdf/2512.23977",
      "code": null,
      "tags": [
        "others",
        "streaming machine learning",
        "directed acyclic graph (DAG)",
        "point-in-time idempotency",
        "temporal tiling",
        "causality enforcement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4105add65c1e7a9960d7e21b6bda977778c41365cfff18c04d16a1af3773b5c4_w640_q70.webp",
      "contributions": "1. A unified DAG-based execution model with point-in-time idempotency, ensuring identical model behavior in batch and streaming modes without code changes. 2. Automatic causality enforcement by tracking knowledge time across transformations, eliminating future-peeking bugs. 3. Flexible temporal and feature dimension tiling, allowing models to operate at different frequencies and memory profiles via configuration alone.",
      "summary": "The paper presents DataFlow, a framework for building high-performance ML systems on streaming time-series data. It uses a DAG-based model with point-in-time idempotency to bridge the gap between batch prototyping and streaming production, ensuring causality and reproducibility. The framework demonstrates effectiveness in domains like financial trading and IoT analytics.",
      "mindmap": "graph TB\n        Root[”Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing”] --> Problem[”核心问题/Problem: Gap between batch ML prototypes and streaming production systems causes causality violations and poor reproducibility.”]\n        Root --> Method[”主要方法/Method: Unified DAG execution model with point-in-time idempotency and automatic causality tracking.”]\n        Root --> Results[”关键结果/Results: Enables identical batch/stream execution, flexible tiling, and effective deployment in financial, IoT, and fraud detection domains.”]"
    },
    {
      "title": "Information-Theoretic Quality Metric of Low-Dimensional Embeddings",
      "authors": "Sebastián Gutiérrez-Bernal, Hector Medel Cobaxin, Abiel Galindo González",
      "institution": "Tecnologico de Monterrey",
      "link": "https://arxiv.org/pdf/2512.23981",
      "code": null,
      "tags": [
        "dimensionality reduction",
        "low-dimensional embeddings",
        "information-theoretic metric",
        "singular-value spectrum",
        "stable rank",
        "neighborhood preservation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30121111f7039384030734c0f1c119a147b1f9d2605df8260026b1ba34b6b2dd_w640_q70.webp",
      "contributions": "1. Introduces the Entropy Rank Preservation Measure (ERPM), a novel local quality metric for embeddings based on Shannon entropy and stable rank of neighborhood matrices. 2. Provides an information-theoretic perspective on embedding quality, directly assessing information preservation rather than just geometric or distance distortions. 3. Demonstrates that ERPM complements existing metrics by identifying neighborhoods with severe information loss, offering a more comprehensive assessment for information-sensitive applications.",
      "summary": "The paper proposes a new information-theoretic metric called ERPM to evaluate the quality of low-dimensional embeddings by measuring changes in uncertainty via the singular-value spectrum of neighborhood matrices. It shows that ERPM correlates strongly with geometric measures but identifies local discrepancies, complementing existing distance-based and geometric metrics. The conclusion is that ERPM enables a more thorough assessment of embeddings, especially for applications sensitive to information loss like early-warning indicators.",
      "mindmap": "graph TB\n        A[Information-Theoretic Quality Metric of Low-Dimensional Embeddings] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[经典指标如stress, MRRE, Local Procrustes不能直接评估信息保留/Classical metrics (stress, MRRE, Local Procrustes) do not directly assess information preservation]\n        C --> C1[提出基于香农熵和稳定秩的ERPM度量/Propose ERPM based on Shannon entropy and stable rank of neighborhood matrices]\n        D --> D1[距离指标与几何/谱指标相关性低/Distance-based criteria show low correlation with geometric/spectral measures]\n        D --> D2[ERPM与Local Procrustes强相关但局部有差异/ERPM and Local Procrustes show strong correlation but local discrepancies]\n        D --> D3[ERPM补充现有指标，识别信息严重丢失的邻域/ERPM complements existing metrics by identifying neighborhoods with severe information loss]"
    },
    {
      "title": "Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems",
      "authors": "Tinglong Dai, David Simchi-Levi, Michelle Xiao Wu, Yao Xie",
      "institution": "Johns Hopkins University, Massachusetts Institute of Technology, Purdue University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23978",
      "code": null,
      "tags": [
        "agent system",
        "flow-based generative models",
        "adversarial robustness",
        "optimal transport"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc5a435448048829b572c2ce3ff1418aa28bf4adcf98e17ff1ed80a690a1b51a_w640_q70.webp",
      "contributions": "1. Proposes a conceptual framework for \"assured autonomy\" to address the fragility of stochastic generative models in operational domains. 2. Identifies flow-based generative models as a key approach for enabling auditability and constraint-aware generation. 3. Formulates operational safety through an adversarial robustness lens to account for worst-case perturbations and unmodeled risks.",
      "summary": "The paper addresses the \"autonomy paradox\" where more autonomous GenAI systems require stronger formal constraints. It proposes a framework grounded in Operations Research, combining flow-based generative models for deterministic control and adversarial robustness for safety. This shifts OR's role from solver to system architect, defining a research agenda for assured autonomy in critical domains.",
      "mindmap": "graph TB\n        A[Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[自主性悖论: 更高的自主性需要更强的结构、约束和风险控制/Autonomy Paradox: Greater autonomy requires more structure, constraints, and risk discipline]\n        B --> B2[生成模型在操作领域可能很脆弱/Generative models can be fragile in operational domains]\n        C --> C1[基于流的生成模型: 确定性传输，可审计，约束感知/Flow-based generative models: Deterministic transport, auditability, constraint-aware]\n        C --> C2[操作安全: 对抗性鲁棒性视角，考虑最坏情况扰动/Operational safety: Adversarial robustness lens, worst-case perturbations]\n        D --> D1[框架将OR的角色从求解器转变为护栏和系统架构师/Framework shifts OR's role from solver to guardrail to system architect]\n        D --> D2[定义了安全关键领域确保自主性的研究议程/Defines a research agenda for assured autonomy in safety-critical domains]"
    },
    {
      "title": "MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning",
      "authors": "Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D",
      "institution": "National Institute of Technology Calicut",
      "link": "https://arxiv.org/pdf/2512.23987",
      "code": null,
      "tags": [
        "malware detection",
        "Model-Agnostic Meta-Learning (MAML)",
        "Chunk-wise Feature Selection (CFSGB)",
        "Gradient Boosting"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1509044c1c0bdfbb4a075c799e3c0cafd035f0b7c579548b445cf04caee2975d_w640_q70.webp",
      "contributions": "1. Proposed MeLeMaD, a novel malware detection framework leveraging Model-Agnostic Meta-Learning (MAML) for adaptability and generalization. 2. Introduced a novel Chunk-wise Feature Selection based on Gradient Boosting (CFSGB) technique to handle large-scale, high-dimensional datasets efficiently. 3. Demonstrated state-of-the-art performance on benchmark datasets (CIC-AndMal2020, BODMAS) and a custom dataset (EMBOD), achieving high accuracy and robustness.",
      "summary": "The paper proposes MeLeMaD, a novel malware detection framework that combines a new chunk-wise feature selection method (CFSGB) with meta-learning (MAML) to improve adaptability and efficiency on large-scale datasets. It achieves high accuracy on benchmark and custom datasets, outperforming existing state-of-the-art approaches and demonstrating robustness against evolving threats.",
      "mindmap": "graph TB\n        A[MeLeMaD: Adaptive Malware Detection] --> B[核心问题/Problem: Malware detection needs robustness & adaptability]\n        A --> C[主要方法/Method: Meta-Learning (MAML) + Chunk-wise Feature Selection (CFSGB)]\n        A --> D[关键结果/Results: High accuracy on benchmarks (98.04%, 99.97%) & custom dataset]"
    },
    {
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "authors": "Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang",
      "institution": "Google DeepMind, The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.23988",
      "code": null,
      "tags": [
        "mechanistic interpretability",
        "sparse auto-encoder",
        "reasoning vectors",
        "chain-of-thought"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp",
      "contributions": "1. Proposes RISE, an unsupervised framework using sparse auto-encoders (SAEs) to discover \"reasoning vectors\" that encode distinct reasoning behaviors from step-level LLM activations. 2. Demonstrates that these discovered vectors correspond to interpretable behaviors (e.g., reflection, backtracking) and can be used for targeted intervention to controllably steer the reasoning process without retraining. 3. Shows SAEs can uncover novel, human-undefined reasoning behaviors and structural properties, such as controlling response confidence, highlighting the potential of unsupervised latent discovery.",
      "summary": "This paper addresses the challenge of interpreting the internal reasoning process of large language models (LLMs). It proposes RISE, an unsupervised framework that uses sparse auto-encoders to discover disentangled \"reasoning vectors\" from chain-of-thought activations. The method enables the identification, visualization, and controllable intervention of specific reasoning behaviors, revealing novel insights beyond supervised analysis.",
      "mindmap": "graph TB\n        Root[”Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process”]\n        Root --> Problem[”核心问题/Problem<br>LLM推理内部机制不明确<br>Supervised methods are limited”]\n        Root --> Method[”主要方法/Method<br>RISE框架: 无监督稀疏自编码器<br>Unsupervised SAEs on step-level activations”]\n        Root --> Results[”关键结果/Results<br>发现可解释推理行为向量<br>可控干预推理轨迹<br>Discover novel behaviors”]"
    },
    {
      "title": "RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress",
      "authors": "Ruixuan Huang, Qingyue Wang, Hantao Huang, Yudong Gao, Dong Chen, Shuai Wang, Wei Wang",
      "institution": "HKUST, NTU",
      "link": "https://arxiv.org/pdf/2512.23995",
      "code": null,
      "tags": [
        "llm inference",
        "Mixture-of-Experts",
        "Router Imbalance",
        "Denial-of-Service",
        "Expert Parallelism",
        "Adversarial Prompt"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d78c253b115346789599383601182c2e6e0cd34050dd3dfd9f1541a370776561_w640_q70.webp",
      "contributions": "1. Identifies a novel DoS vulnerability in MoE LLMs where adversarial inputs can cause severe routing concentration and load imbalance during inference. 2. Proposes RepetitionCurse, a low-cost, black-box, and model-agnostic attack method that uses simple repetitive token patterns to exploit the router's universal flaw. 3. Empirically demonstrates significant performance degradation (e.g., 3.063x latency increase on Mixtral-8x7B), highlighting a critical risk to service-level agreements for real-world MoE deployments.",
      "summary": "This paper identifies a denial-of-service vulnerability in Mixture-of-Experts LLMs, where adversarial prompts can manipulate the router to concentrate all tokens on a few experts, creating severe load imbalance. The authors propose RepetitionCurse, a simple black-box attack using repetitive token patterns to exploit this flaw. Their method significantly increases inference latency, demonstrating a critical risk to the availability of MoE-based services.",
      "mindmap": "graph TB\n        A[RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[MoE推理负载不均/MoE Inference Load Imbalance]\n        B --> B2[路由集中导致DoS/Routing Concentration Leads to DoS]\n        C --> C1[重复令牌攻击/Repetitive Token Attack]\n        C --> C2[黑盒方法/Black-box Method]\n        D --> D1[延迟显著增加/Latency Significantly Increased]\n        D --> D2[服务可用性下降/Service Availability Degraded]"
    },
    {
      "title": "Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective",
      "authors": "Tan Pan, Yixuan Sun, Chen Jiang, Qiong Gao, Rui Sun, Xingmeng Zhang, Zhenqi Yang, Limei Han, Yixiu Liang, Yuan Cheng, Kaiyu Guo",
      "institution": "Fudan University, Shanghai Academy of Artificial Intelligence for Science",
      "link": "https://arxiv.org/pdf/2512.24002",
      "code": "https://github.com/Ashespt/CLEAR-HUG",
      "tags": [
        "self-supervised learning",
        "electrocardiogram (ECG)",
        "self-supervised learning (SSL)",
        "cardiac conduction",
        "sparse attention",
        "hierarchical diagnosis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4316ae9f56d525621d461087803f69e54c99676d3863c02d5df1d1ad32dab6a_w640_q70.webp",
      "contributions": "1. Identifies a key limitation in prior ECG self-supervised learning (eSSL) methods: they overlook inherent heartbeat differences rooted in cardiac conduction and neglect the sequential logic of clinical ECG diagnosis. 2. Proposes a novel two-stage framework (CLEAR-HUG), where the first stage (CLEAR) is an eSSL model that uses a sparse attention mechanism to reconstruct signals by treating each heartbeat as a distinct entity, capturing subtle conduction variations. 3. Introduces a Hierarchical lead-Unified Group head (HUG) for the downstream diagnosis stage, which mirrors the clinical workflow from heartbeats to leads to lead combinations, aligning model patterns with expert guidelines.",
      "summary": "This paper proposes CLEAR-HUG, a two-stage framework for ECG representation learning. The method first uses a self-supervised model (CLEAR) with sparse attention to learn from cardiac conduction variations, then applies a hierarchical diagnosis head (HUG) aligned with clinical guidelines. Experiments across six tasks show a 6.84% performance improvement, validating its effectiveness.",
      "mindmap": "graph TB\n        A[追踪心路：从心脏传导视角的ECG表征学习<br>Tracing the Heart's Pathways: ECG Representation Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有eSSL方法忽视心脏传导导致的细微差异<br>Prior eSSL overlooks conduction-based heartbeat differences]\n        B --> B2[模型未遵循从心跳到导联的临床诊断逻辑<br>Models neglect clinical diagnostic sequence]\n        C --> C1[两阶段框架CLEAR-HUG<br>Two-stage framework CLEAR-HUG]\n        C1 --> C2[阶段一: CLEAR (自监督学习)<br>Stage 1: CLEAR (eSSL)]\n        C2 --> C3[稀疏注意力重构信号<br>Sparse attention for reconstruction]\n        C1 --> C4[阶段二: HUG (分层诊断头)<br>Stage 2: HUG (hierarchical head)]\n        C4 --> C5[模仿临床工作流<br>Mirrors clinical workflow]\n        D --> D1[六项任务性能提升6.84%<br>6.84% improvement across six tasks]\n        D --> D2[验证了方法的有效性<br>Validates effectiveness]"
    },
    {
      "title": "Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models",
      "authors": "Rohit Kumar Salla, Manoj Saravanan, Shrikar Reddy Kota",
      "institution": "Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.24058",
      "code": "https://github.com/rohitsalla/CRS.git",
      "tags": [
        "llm evaluation",
        "reliability",
        "calibration",
        "robustness",
        "uncertainty quantification",
        "composite score"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp",
      "contributions": "1. A unified reliability metric (CRS) integrating calibration, robustness, and uncertainty. 2. A large-scale evaluation of ten open-source LLMs on five QA datasets. 3. The demonstration that CRS provides stable model rankings and uncovers hidden failure modes.",
      "summary": "This paper addresses the fragmented evaluation of Large Language Model (LLM) reliability by proposing the Composite Reliability Score (CRS), a unified metric that integrates calibration, robustness, and uncertainty quantification. Through experiments on ten open-source LLMs, the authors show that CRS provides consistent model rankings and reveals trade-offs between reliability dimensions. The main conclusion is that the most dependable LLM systems balance accuracy, robustness, and calibrated uncertainty.",
      "mindmap": "graph TB\n        A[Beyond Hallucinations: A Composite Score for Measuring Reliability] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM可靠性评估碎片化/Fragmented LLM Reliability Evaluation]\n        C --> C1[提出CRS复合分数/Propose Composite Reliability Score (CRS)]\n        D --> D1[CRS提供稳定模型排名/CRS Delivers Stable Model Rankings]\n        D --> D2[揭示隐藏的失败模式/Uncovers Hidden Failure Modes]"
    },
    {
      "title": "Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity",
      "authors": "Rui Chen, Junjun Guo, Hongbin Wang, Yan Xiang, Yantuan Xian, Zhengtao Yu",
      "institution": "Kunming University of Science and Technology, Yunnan Key Laboratory of Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.24062",
      "code": null,
      "tags": [
        "graph representation learning",
        "hyperspherical embedding",
        "neighbor-mean alignment",
        "sampling-free uniformity",
        "entropy-guided balancing",
        "graph neural networks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e0d10e35c66ca8482e3324110ac5fded6eefbaab8a774c81cba99d854efe694_w640_q70.webp",
      "contributions": "1. Proposes HyperGRL, a unified framework for hyperspherical graph representation learning using two adversarially coupled objectives: neighbor-mean alignment and sampling-free uniformity. 2. Introduces an entropy-guided adaptive balancing mechanism to dynamically regulate the interplay between alignment and uniformity without manual hyperparameter tuning. 3. Demonstrates superior performance on node classification, clustering, and link prediction tasks, achieving improvements over existing methods.",
      "summary": "This paper proposes HyperGRL, a novel framework for graph representation learning that embeds nodes on a hypersphere using neighbor-mean alignment and a sampling-free uniformity objective, stabilized by an adaptive balancing mechanism. The method avoids complex negative sampling and hyperparameter tuning, addressing issues like over-smoothing and training instability. Experiments show HyperGRL outperforms existing methods on standard graph learning tasks.",
      "mindmap": "graph TB\n        A[HyperGRL: 超球面图表示学习 / Hyperspherical Graph Representation Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 现有方法依赖复杂对比目标 / Existing methods rely on complex contrastive objectives]\n        C[主要方法 / Method: 邻域均值对齐与采样无关均匀性 / Neighbor-Mean Alignment & Sampling-Free Uniformity]\n        D[关键结果 / Results: 在节点分类等任务上性能提升 / Performance gains on node classification, etc.]"
    },
    {
      "title": "How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns",
      "authors": "Haoyue Bai, Yiyou Sun, Wenjie Hu, Shi Qiu, Maggie Ziyu Huan, Peiyang Song, Robert Nowak, Dawn Song",
      "institution": "University of Wisconsin, Madison; University of California, Berkeley; University of Pennsylvania; California Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24063",
      "code": null,
      "tags": [
        "large language models",
        "supervised fine-tuning",
        "reinforcement learning",
        "reasoning decomposition",
        "meta-probing",
        "generalization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e9fab646e78db9be62f127bfa42a9acf6be78552ea9847e5873b10c18802eaa_w640_q70.webp",
      "contributions": "1. Introduced a novel benchmark that decomposes reasoning into atomic core skills (e.g., calculation, simulation) for fine-grained analysis. 2. Proposed a meta-probing framework to track model behavioral profiles across different training stages. 3. Provided a combined analysis linking high-level cognitive skill changes to low-level statistical patterns, revealing that RL tuning preserves reasoning skills better than SFT.",
      "summary": "This paper investigates why LLMs generalize differently after SFT versus RL tuning. It proposes a new benchmark to decompose reasoning into core skills and a meta-probing framework to analyze model behavior, finding that RL-tuned models maintain more stable reasoning abilities while SFT models tend to overfit to surface patterns.",
      "mindmap": "graph TB\n        Root[”How and Why LLMs Generalize<br>LLM泛化能力研究”] --> Problem[”核心问题/Problem<br>Why SFT narrows generalization while RL preserves it?”]\n        Root --> Method[”主要方法/Method<br>Novel benchmark + Meta-probing framework”]\n        Root --> Results[”关键结果/Results<br>RL-tuned models resist skill collapse; SFT models overfit.”]"
    },
    {
      "title": "Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning",
      "authors": "Xusheng Zhang, Tuan Nguyen, Ting He",
      "institution": "University of Oxford, Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.24069",
      "code": null,
      "tags": [
        "federated learning",
        "decentralized federated learning",
        "mixing matrix",
        "energy consumption",
        "time-varying topology",
        "wireless networks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/090a96d0adae430f081f3c2325be19fc983de3cc1e88b07fcc83e4ab4e983d30_w640_q70.webp",
      "contributions": "1. A novel convergence theorem for DFL that allows for arbitrarily time-varying mixing matrices, providing theoretical justification for dynamic communication topologies. 2. A multi-phase design framework for mixing matrices that activates time-varying communication topologies to trade off per-iteration energy consumption and convergence rate. 3. An optimization approach that minimizes the maximum per-node energy consumption until convergence, explicitly considering the broadcast nature of wireless communications.",
      "summary": "This paper addresses the problem of high energy consumption in Decentralized Federated Learning (DFL) for wireless networks by designing time-varying mixing matrices. The proposed method introduces a multi-phase framework that dynamically adjusts communication topologies to balance energy use across nodes and trade off per-iteration cost with convergence speed. The evaluation shows the solution effectively combines the low energy of sparse topologies with the fast convergence of dense ones.",
      "mindmap": "graph TB\n        Root[”Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning<br>时变混合矩阵设计用于能效去中心化联邦学习”]\n        Root --> Problem[”Minimize max per-node energy consumption in DFL<br>最小化DFL中最大单节点能耗”]\n        Root --> Method[”Multi-phase framework with time-varying topologies<br>基于时变拓扑的多阶段框架”]\n        Root --> Results[”Validated efficacy: low energy + fast convergence<br>验证有效性：低能耗+快速收敛”]"
    },
    {
      "title": "Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework",
      "authors": "Jiazhao Shi, Ziyu Wang, Yichen Lin, Shoufeng Lu",
      "institution": "New York University, Wake Forest University, Nanjing Tech University",
      "link": "https://arxiv.org/pdf/2512.24075",
      "code": null,
      "tags": [
        "autonomous driving",
        "lane-change intention prediction",
        "physics-informed AI",
        "temporal embeddings",
        "class imbalance",
        "LightGBM"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b7843d7cd1396bf4f4df0cdc13d3a94ef4d76e2d2ca0c9948486895c4e482_w640_q70.webp",
      "contributions": "1. Proposed a hybrid Temporal Physics-Informed AI (TPI-AI) framework that fuses deep temporal embeddings from a Bi-LSTM encoder with physics-inspired interaction features for lane-change intention prediction. 2. Introduced imbalance-aware optimization techniques, including resampling/weighting and fold-wise threshold calibration, to improve minority-class reliability. 3. Demonstrated robust multi-scenario generalization by evaluating on two large-scale, heterogeneous highway datasets (highD and exiD) with location-based splits and outperforming standalone baselines.",
      "summary": "The paper proposes TPI-AI, a hybrid framework combining Bi-LSTM temporal embeddings with physics-informed interaction features for highway lane-change intention prediction. It addresses challenges like class imbalance and noisy data, achieving high macro-F1 scores on two drone-based datasets. The results show that integrating physics cues with learned representations yields robust performance across diverse highway scenarios.",
      "mindmap": "graph TB\n        A[Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[车道变换意图预测困难/Lane-change intention prediction is difficult]\n        B1 --> B2[噪声运动学, 类别不平衡, 泛化性差/Noisy kinematics, class imbalance, poor generalization]\n        C --> C1[提出TPI-AI框架/Propose TPI-AI framework]\n        C1 --> C2[融合时序嵌入与物理特征/Fuse temporal embeddings with physics features]\n        C2 --> C3[Bi-LSTM编码器 + LightGBM分类器/Bi-LSTM encoder + LightGBM classifier]\n        C3 --> C4[不平衡优化/Imbalance-aware optimization]\n        D --> D1[在两个数据集上评估/Evaluate on two datasets]\n        D1 --> D2[highD: 直道高速路/highD: straight highways]\n        D1 --> D3[exiD: 匝道丰富环境/exiD: ramp-rich environments]\n        D2 --> D4[宏F1: 0.9562, 0.9124, 0.8345/Macro-F1: 0.9562, 0.9124, 0.8345]\n        D3 --> D5[宏F1: 0.9247, 0.8197, 0.7605/Macro-F1: 0.9247, 0.8197, 0.7605]"
    },
    {
      "title": "Random Multiplexing",
      "authors": "Lei Liu, Yuhao Chi, Shunqi Huang, Zhaoyang Zhang",
      "institution": "Zhejiang University, Xidian University, Japan Advanced Institute of Science and Technology (JAIST)",
      "link": "https://arxiv.org/pdf/2512.24087",
      "code": null,
      "tags": [
        "wireless communication",
        "random multiplexing",
        "AMP detection",
        "power allocation",
        "replica optimality",
        "constrained capacity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2f6d7e30422bda561811ec881c0aa17f79a2f4e7bf13203955c43c9c8fab17_w640_q70.webp",
      "contributions": "1. Proposes a random multiplexing technique decoupled from physical channel structures, enabling application to arbitrary norm-bounded and spectrally convergent channel matrices. 2. Introduces a low-complexity cross-domain memory AMP (CD-MAMP) detector and derives optimal power allocations to minimize BER and maximize constrained capacity. 3. Investigates the optimal coding principle and proves the replica constrained-capacity optimality of the CD-MAMP detector for random multiplexing systems.",
      "summary": "This paper proposes a random multiplexing technique to overcome the limitations of traditional and emerging multiplexing schemes (like OFDM and OTFS) which rely on specific channel structures. The method decouples from the physical channel, uses a random transform to create an input-isotropic equivalent channel, and employs a low-complexity AMP-type detector to achieve near-optimal performance for arbitrary norm-bounded channels. The authors validate the approach with theoretical analysis and numerical results, demonstrating its robustness and versatility in dynamic wireless environments.",
      "mindmap": "graph TB\n        A[Random Multiplexing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统复用技术依赖特定信道结构/Traditional multiplexing relies on specific channel structures]\n        B --> B2[在动态真实环境中鲁棒性有限/Limited robustness in dynamic real-world environments]\n        C --> C1[随机复用技术/Random Multiplexing Technique]\n        C --> C2[构建输入各向同性等效信道/Construct input-isotropic equivalent channel]\n        C --> C3[CD-MAMP检测器/CD-MAMP Detector]\n        D --> D1[保证渐近最优BER/Guarantees asymptotic optimal BER]\n        D --> D2[推导最优功率分配/Derives optimal power allocation]\n        D --> D3[验证理论结果/Validates theoretical findings]"
    },
    {
      "title": "Training a Huggingface Model on AWS Sagemaker (Without Tears)",
      "authors": "Liling Tan",
      "institution": "(Institution not explicitly stated in provided content. Based on author name and context, likely independent researcher or affiliation not listed on first page.)",
      "link": "https://arxiv.org/pdf/2512.24098",
      "code": null,
      "tags": [
        "llm training",
        "AWS SageMaker",
        "Hugging Face",
        "MLOps",
        "cloud computing",
        "Jupyter as a Service"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9d3b4e38e89877a6b18e1df5bad45a785b69e26e421db2b8a658f22bdff539_w640_q70.webp",
      "contributions": "1. Provides a centralized, comprehensive guide to train a Hugging Face model on AWS SageMaker, addressing fragmented documentation. 2. Bridges the knowledge gap between local Jupyter Notebook development and cloud-based training on SageMaker. 3. Aims to democratize cloud adoption for researchers lacking on-premise computing resources by lowering the platform's learning curve.",
      "summary": "This demo paper addresses the steep learning curve and fragmented documentation that hinder researchers from using AWS SageMaker to train Hugging Face models. It proposes a centralized guide to bridge the gap between local and cloud-based development workflows. The main conclusion is that this approach can democratize cloud adoption, enabling more researchers to train models without extensive on-premise resources.",
      "mindmap": "graph TB\n        A[Training a Huggingface Model on AWS Sagemaker (Without Tears)] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Barriers to Cloud Adoption<br>资源壁垒与学习曲线]\n        C[主要方法/Method<br>Centralized Guide<br>提供集中化指南]\n        D[关键结果/Results<br>Democratized Cloud Training<br>实现云训练的民主化]"
    },
    {
      "title": "Enhancing LLM Planning Capabilities through Intrinsic Self-Critique",
      "authors": "Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, Noah Fiedel",
      "institution": "Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.24103",
      "code": null,
      "tags": [
        "planning",
        "self-critique",
        "few-shot learning",
        "many-shot learning",
        "iterative refinement",
        "planning benchmarks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1646fd60ae3c2dbada7b54640bd9b507a010326fc6e75dd48733e4e2612eeefd_w640_q70.webp",
      "contributions": "1. Proposes an intrinsic self-critique method for LLMs to improve their own planning outputs without external verifiers. 2. Demonstrates significant performance gains on established planning benchmarks (Blocksworld, Logistics, Mini-grid) over strong baselines. 3. Shows the method's applicability across different models and datasets, achieving new state-of-the-art results for the considered model class.",
      "summary": "This paper introduces an intrinsic self-critique approach where LLMs iteratively critique and refine their own plans. The method, building upon few-shot and many-shot learning, significantly improves planning performance on benchmarks like Blocksworld without needing external verification. The results set a new state-of-the-art, demonstrating that self-critique can effectively enhance LLM planning capabilities.",
      "mindmap": "graph TB\n        A[Enhancing LLM Planning Capabilities through Intrinsic Self-Critique] --> B[核心问题/Problem: LLM规划能力不足，传统自批判方法效果受质疑/LLM planning capability is limited, effectiveness of self-critique is questioned]\n        A --> C[主要方法/Method: 内在自批判与迭代精炼/Intrinsic Self-Critique and Iterative Refinement]\n        A --> D[关键结果/Results: 在规划基准测试中取得显著性能提升，达到新SOTA/Significant performance gains on planning benchmarks, achieving new SOTA]"
    },
    {
      "title": "Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study",
      "authors": "Yves Ruffenach",
      "institution": "Conservatoire National des Arts et Métiers",
      "link": "https://arxiv.org/pdf/2512.24102",
      "code": null,
      "tags": [
        "language modeling",
        "GP-VAE",
        "latent autoregression",
        "ablation study",
        "Gaussian process",
        "variational autoencoder"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bed10d31b6309110f5cec704462d295e7459d74c493da4d130689dc4d04af954_w640_q70.webp",
      "contributions": "1. Conducts a systematic ablation study to isolate and analyze the role of latent autoregression in a GP-VAE language model., 2. Demonstrates that latent autoregression leads to latent trajectories more aligned with the Gaussian-process prior and with greater long-horizon stability., 3. Provides empirical evidence that sequential structure can be effectively carried by latent dynamics even when using a non-autoregressive decoder.",
      "summary": "This paper analyzes the role of latent autoregression in a GP-VAE language model through an ablation study. It compares models with and without latent autoregression, finding that latent autoregression organizes long-range structure, leading to more stable and prior-aligned latent trajectories. The study shows latent dynamics can carry sequential structure even with a non-autoregressive decoder.",
      "mindmap": "graph TB\n        Root[”Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study<br/>GP-VAE语言模型潜在空间中的自回归性：一项实证消融研究”]\n        Root --> Problem[”核心问题/Problem<br/>What is the role of latent autoregression in organizing sequential structure?<br/>潜在自回归在组织序列结构中的作用是什么？”]\n        Root --> Method[”主要方法/Method<br/>Ablation study comparing GP-VAE with latent autoregression, without it, and a standard autoregressive Transformer.<br/>消融研究：比较带潜在自回归的GP-VAE、不带潜在自回归的版本和标准自回归Transformer。”]\n        Root --> Results[”关键结果/Results<br/>Latent autoregression yields more prior-aligned, stable latent trajectories, effectively organizing long-range structure.<br/>潜在自回归产生更符合先验、更稳定的潜在轨迹，有效组织长程结构。”]"
    },
    {
      "title": "OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization",
      "authors": "Advait Gadhikar, Riccardo Grazzi, James Hensman",
      "institution": "CISPA Helmholtz Center for Information Security, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.24124",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "post-training quantization",
        "weight outliers",
        "rotation",
        "GPTQ",
        "data-free"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp",
      "contributions": "1. Proposes OptRot, a data-free method that learns fusible rotations by minimizing a principled, cheap proxy objective (element-wise fourth power of weights) to reduce weight outliers for quantization. 2. Demonstrates that OptRot outperforms existing rotation methods (Hadamard, SpinQuant, OSTQuant) for weight quantization and improves W4A8 activation quantization. 3. Introduces OptRot+, a data-dependent variant that incorporates activation covariance information for further performance gains, while highlighting a trade-off between weight and activation quantization in the W4A4 setting.",
      "summary": "This paper addresses the challenge of quantizing Large Language Models (LLMs) by mitigating weight outliers. It proposes OptRot, a data-free method that learns efficient rotations to minimize a proxy for weight quantization error, and shows it outperforms existing techniques for weight and W4A8 activation quantization. The work also introduces an enhanced data-dependent variant and reveals a performance trade-off in more aggressive quantization settings.",
      "mindmap": "graph TB\n        A[OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization] --> B[核心问题/Problem: LLM权重和激活中的异常值使量化困难/Outliers in LLM weights & activations make quantization difficult]\n        A --> C[主要方法/Method: 通过最小化旋转后权重的四阶矩学习可融合的旋转/Learn fusible rotations by minimizing element-wise fourth power of rotated weights (OptRot)]\n        A --> D[关键结果/Results: OptRot在权重量化上优于现有方法，改进W4A8激活量化，W4A4下存在权衡/OptRot outperforms existing methods for weight quant., improves W4A8 activation quant., trade-off in W4A4 setting]"
    },
    {
      "title": "Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction",
      "authors": "Qianyi Chen, Bo Li",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24139",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "conformal prediction",
        "conditional coverage",
        "quantile regression",
        "density-weighted pinball loss",
        "three-headed network"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0a2f8de6a5fa6ef2df8d7f5895f2503ae4000f24fff02f3e88840fcb1d0f27f_w640_q70.webp",
      "contributions": "1. Derivation of a novel density-weighted pinball loss as a sharp surrogate objective for quantile regression to improve conditional coverage in conformal prediction. 2. Proposal of a three-headed quantile network architecture that estimates the necessary density weights via finite differences using auxiliary quantiles. 3. Provision of a theoretical analysis with exact non-asymptotic guarantees for the excess risk and demonstration of significant conditional coverage improvements on real-world datasets.",
      "summary": "This paper addresses the challenge of achieving reliable conditional coverage in conformal prediction. The authors propose a new method that refines quantile regression using a density-weighted pinball loss and a three-headed network to estimate the weights, leading to improved conditional coverage guarantees. Experiments show the method significantly enhances conditional coverage performance on diverse datasets.",
      "mindmap": "graph TB\n        Root(”Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”标准CP仅提供边际覆盖/Standard CP only provides marginal coverage”)\n        Problem --> P2(”难以实现可靠的输入条件覆盖/Hard to achieve reliable input-conditional coverage”)\n        Method --> M1(”推导密度加权分位数损失/Derive density-weighted quantile loss”)\n        Method --> M2(”提出三头分位数网络/Propose three-headed quantile network”)\n        Method --> M3(”通过有限差分估计权重/Estimate weights via finite differences”)\n        Results --> R1(”理论非渐近保证/Theoretical non-asymptotic guarantees”)\n        Results --> R2(”实验显示条件覆盖显著提升/Experiments show significant conditional coverage improvement”)"
    },
    {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "authors": "Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan",
      "institution": "Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.24138",
      "code": "https://tinnerhrhe.github.io/gardo_project",
      "tags": [
        "reinforcement learning",
        "reward hacking",
        "diffusion models",
        "regularization",
        "mode collapse",
        "online RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp",
      "contributions": "1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.",
      "summary": "This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.",
      "mindmap": "graph TB\n        A[GARDO: Reinforcing Diffusion Models without Reward Hacking] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Reward Hacking in RL for Diffusion Models/扩散模型RL中的奖励破解]\n        B --> B2[Proxy Reward Mismatch & Mode Collapse/代理奖励不匹配与模式崩溃]\n        C --> C1[Gated & Adaptive Regularization/门控自适应正则化]\n        C --> C2[Diversity-aware Reward Optimization/多样性感知奖励优化]\n        D --> D1[Mitigates Reward Hacking/缓解奖励破解]\n        D --> D2[Enhances Diversity & Maintains Efficiency/提升多样性并保持效率]"
    },
    {
      "title": "Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators",
      "authors": "Udit Sharma",
      "institution": "IIT Kharagpur",
      "link": "https://arxiv.org/pdf/2512.24145",
      "code": null,
      "tags": [
        "experimental methodology",
        "paired evaluation",
        "variance reduction",
        "statistical power",
        "random seeds",
        "learning-based simulators"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d4813c6d5407b2660a9657493b8a66ce92b409d971e7a9ac715b6c94922b60_w640_q70.webp",
      "contributions": "1. Formalizes a paired seed evaluation design for learning-based simulators, where competing systems are evaluated under identical random seeds to induce matched realizations of stochastic components. 2. Analyzes the statistical structure of comparative evaluation, showing that this design provides strict variance reduction and tighter confidence intervals when outcomes are positively correlated at the seed level. 3. Empirically demonstrates that seed-level correlations are typically large and positive, leading to order-of-magnitude efficiency gains in statistical power and effective sample size compared to independent evaluation.",
      "summary": "The paper addresses the high variance in evaluating learning-based simulators by proposing a paired seed evaluation design. This method evaluates competing systems under identical random seeds, reducing variance when outcomes are correlated. The main conclusion is that this approach is weakly dominant, improving statistical reliability with efficiency gains when correlation exists and reverting to standard evaluation without loss when it does not.",
      "mindmap": "graph TB\n    A[Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估结果高方差<br>High Variance in Evaluation]\n    B --> B2[独立评估设计低效<br>Inefficient Independent Evaluation]\n    C --> C1[配对种子评估<br>Paired Seed Evaluation]\n    C --> C2[共享随机源<br>Exploit Shared Randomness]\n    D --> D1[方差严格降低<br>Strict Variance Reduction]\n    D --> D2[统计效能提升<br>Higher Statistical Power]\n    D --> D3[效率显著增益<br>Order-of-Magnitude Gains]"
    },
    {
      "title": "Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges",
      "authors": "Yu-Tang Chang, Pin-Wei Chen, Shih-Fang Chen",
      "institution": "National Taiwan University",
      "link": "https://arxiv.org/pdf/2512.24172",
      "code": "https://github.com/b05611038/HSI_global_clustering",
      "tags": [
        "hyperspectral image segmentation",
        "deep global clustering",
        "memory-efficient segmentation",
        "unsupervised disease detection",
        "multi-objective loss balancing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5ca8fa4968e4f11184d8faa275b2f2edacdd6e6374072e519e722e16016a06_w640_q70.webp",
      "contributions": "1. Proposed Deep Global Clustering (DGC), a conceptual framework for memory-efficient hyperspectral image segmentation that learns global clustering from local patches without pre-training., 2. Demonstrated the framework's ability to achieve background-tissue separation and unsupervised disease detection on a leaf disease dataset with high efficiency (training &lt;30 min on consumer hardware)., 3. Identified and analyzed the key challenge of optimization instability due to multi-objective loss balancing, positioning the work as intellectual scaffolding for future principled solutions.",
      "summary": "This paper addresses the computational bottleneck in hyperspectral image (HSI) analysis by proposing Deep Global Clustering (DGC), a memory-efficient framework that learns global segmentation from local patches without pre-training. It successfully demonstrates background-tissue separation and unsupervised disease detection on agricultural data. However, the main conclusion is that while the design philosophy is promising, the framework suffers from optimization instability due to loss balancing, requiring more principled solutions for stable implementation.",
      "mindmap": "graph TB\n        A[Deep Global Clustering for Hyperspectral Image Segmentation<br>高光谱图像分割的深度全局聚类] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Computational bottlenecks in HSI analysis<br>Foundation models fail in domain-specific transfer]\n        C[主要方法/Method<br>Deep Global Clustering (DGC)<br>Memory-efficient, learns from local patches]\n        D[关键结果/Results<br>Achieves background-tissue separation<br>Shows unsupervised disease detection<br>Suffers from optimization instability]"
    },
    {
      "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
      "authors": "Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu",
      "institution": "University of Electronic Science and Technology of China, National University of Singapore, Sun Yat-sen University, North China Institute of Computer Systems Engineering",
      "link": "https://arxiv.org/pdf/2512.24176",
      "code": null,
      "tags": [
        "diffusion models",
        "Internal Guidance",
        "Diffusion Transformer",
        "Classifier-Free Guidance",
        "Sampling Guidance",
        "Denoising Diffusion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/605c83917fcbd68353ebfb53e0da152ab446f909b59cdfd81ee3bd6c6023a44f_w640_q70.webp",
      "contributions": "1. Proposes Internal Guidance (IG), a novel sampling guidance strategy that uses intermediate-layer outputs within a Diffusion Transformer to improve generation quality. 2. Introduces an auxiliary supervisory signal at an intermediate layer during training and extrapolates outputs during sampling, requiring no extra training, degradation strategies, or additional sampling steps. 3. Demonstrates state-of-the-art performance on ImageNet 256x256, achieving an FID of 1.19 when combined with CFG, and shows significant improvements in training efficiency and generation quality across various baselines.",
      "summary": "This paper addresses the issue of standard classifier-free guidance (CFG) causing over-simplified or distorted samples in diffusion models. It proposes Internal Guidance (IG), a simple method that adds auxiliary supervision to an intermediate layer during training and extrapolates outputs during sampling. The method significantly improves generation quality and efficiency, achieving state-of-the-art FID scores on ImageNet.",
      "mindmap": "graph TB\n    A[Guiding a Diffusion Transformer with the Internal Dynamics of Itself] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[标准CFG导致样本过简或失真/Standard CFG leads to over-simplified or distorted samples]\n    B --> B2[现有替代方法需要额外训练或步骤/Existing alternatives require extra training or steps]\n    C --> C1[提出内部引导/Propose Internal Guidance (IG)]\n    C --> C2[训练时中间层辅助监督/Auxiliary supervision on intermediate layer during training]\n    C --> C3[采样时输出外推/Extrapolate outputs during sampling]\n    D --> D1[显著提升训练效率和生成质量/Significant improvements in training efficiency and generation quality]\n    D --> D2[在ImageNet上达到SOTA FID=1.19/Achieves SOTA FID=1.19 on ImageNet]"
    },
    {
      "title": "Micro-Macro Tensor Neural Surrogates for Uncertainty Quantification in Collisional Plasma",
      "authors": "Wei Chen, Giacomo Dimarco, Lorenzo Pareschi",
      "institution": "Xiamen University, University of Ferrara",
      "link": "https://arxiv.org/pdf/2512.24205",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "tensor neural networks",
        "micro-macro decomposition",
        "asymptotic-preserving methods",
        "variance-reduced Monte Carlo",
        "physics-informed neural networks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b3b7038e3ab35211c6d097f5ecf8a93c74f1b92887ec66726a8e7618764cb3d_w640_q70.webp",
      "contributions": "1. A variance-reduced Monte Carlo framework for UQ in the Vlasov-Poisson-Landau system that uses neural network surrogates to replace costly collision term evaluations. 2. A generalization of separable physics-informed neural networks (SPINN) into a class of tensor neural networks based on an anisotropic micro-macro decomposition to reduce model complexity and the curse of dimensionality. 3. The calibration of a VPFP surrogate model and the design of an asymptotic-preserving SPINN to increase correlation with the high-fidelity VPL model, ensuring correct recovery of limiting systems.",
      "summary": "This paper addresses the challenge of efficient uncertainty quantification in collisional plasma simulations, which is hindered by high computational cost and dimensionality. The authors propose a method that couples a high-fidelity solver with inexpensive neural network surrogates based on tensor networks and a micro-macro decomposition. The results show their framework achieves substantial variance reduction, accurate statistics with fewer samples, and lower computational time compared to standard Monte Carlo.",
      "mindmap": "graph TB\n        A[Micro-Macro Tensor Neural Surrogates for UQ in Collisional Plasma] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[高维相空间与多尺度刚度/High-dimensional phase space & multiscale stiffness]\n        B --> B2[碰撞项计算成本高/High cost of collision term evaluation]\n        C --> C1[方差缩减蒙特卡洛框架/Variance-reduced Monte Carlo framework]\n        C --> C2[基于张量神经网络的代理模型/Tensor neural network surrogates]\n        C --> C3[各向异性微宏观分解/Anisotropic micro-macro decomposition]\n        D --> D1[方差大幅降低/Substantial variance reduction]\n        D --> D2[用更少样本获得准确统计/Accurate statistics with fewer samples]\n        D --> D3[更低的计算时间/Lower wall-clock time]"
    },
    {
      "title": "Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19",
      "authors": "Sina Jahromi, Farshid Hajati, Alireza Rezaee, Javaher Nourian",
      "institution": "University of Tehran, University of New England",
      "link": "https://arxiv.org/pdf/2512.24214",
      "code": null,
      "tags": [
        "medical image classification",
        "imbalanced data",
        "progressive generative adversarial network (ProGAN)",
        "slime mould algorithm (SMA)",
        "ResNet",
        "synthetic data generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95bbc93b1f6719a8c5ffdfe98638f4844e331a5a71715b6698a65f669e17f927_w640_q70.webp",
      "contributions": "1. Proposes a Progressive GAN (ProGAN) to generate synthetic medical images to address data imbalance. 2. Introduces a weighted approach for combining synthetic and real data before classification. 3. Employs the Slime Mould Algorithm (SMA), a multi-objective meta-heuristic, to optimize the hyper-parameters of the ResNet classifier.",
      "summary": "This paper tackles the problem of imbalanced medical image data, particularly for COVID-19 detection from chest X-rays. The proposed method uses a Progressive GAN to generate synthetic data and a weighted combination of real and synthetic data, with a ResNet classifier optimized by the Slime Mould Algorithm. The model achieved high accuracy (95.5% for 4-class, 98.5% for 2-class) on an imbalanced dataset, demonstrating its effectiveness for pandemic-related medical image classification.",
      "mindmap": "graph TB\n    A[Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19] --> B(核心问题/Problem: 医学图像数据不平衡/Imbalanced Medical Image Data)\n    A --> C(主要方法/Method: 使用ProGAN生成数据并用SMA优化ResNet/Use ProGAN for Data Generation and SMA to Optimize ResNet)\n    A --> D(关键结果/Results: 在COVID-19胸部X光数据集上取得高准确率/High Accuracy on COVID-19 Chest X-ray Dataset)\n    B --> B1(挑战: 疫情中数据集更不平衡/Challenge: Increased Imbalance During Pandemics)\n    C --> C1(步骤1: 用ProGAN生成合成数据/Step 1: Generate Synthetic Data with ProGAN)\n    C --> C2(步骤2: 加权结合真实与合成数据/Step 2: Weighted Combination of Real and Synthetic Data)\n    C --> C3(步骤3: 用SMA优化ResNet超参数/Step 3: Optimize ResNet Hyper-parameters with SMA)\n    D --> D1(4分类准确率: 95.5%/4-Class Accuracy: 95.5%)\n    D --> D2(2分类准确率: 98.5%/2-Class Accuracy: 98.5%)"
    },
    {
      "title": "MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model",
      "authors": "Rahul Medicharla, Alper Yilmaz",
      "institution": "The Ohio State University",
      "link": "https://arxiv.org/pdf/2512.24231",
      "code": "https://github.com/OSUPCVLab/EmotionFromFaceImages",
      "tags": [
        "facial expression recognition",
        "facial emotion recognition",
        "generalization",
        "foundation model",
        "masked autoencoder",
        "downstream task"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81d3ca85dac4626b7644912941beba01e188b0d41eefb51c00fdb16cd74177b8_w640_q70.webp",
      "contributions": "1. Proposes MotivNet, a novel FER model built on the Meta-Sapiens foundation model to achieve strong generalization without cross-domain training. 2. Defines and applies three criteria (benchmark performance, model similarity, data similarity) to validate a new downstream task for the Sapiens foundation model. 3. Demonstrates that the proposed approach achieves competitive performance across diverse datasets, making FER more viable for real-world, in-the-wild applications.",
      "summary": "This paper introduces MotivNet, a facial emotion recognition model that uses the Meta-Sapiens foundation model as a backbone to achieve strong generalization across datasets without requiring cross-domain training. The authors validate MotivNet as a suitable downstream task for Sapiens using specific criteria and show it achieves competitive performance. The work aims to make FER more robust and applicable in real-world scenarios.",
      "mindmap": "graph TB\n        Root[MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[FER模型泛化能力弱 / Poor generalization of FER models]\n        P1 --> P2[跨域训练不切实际 / Cross-domain training is impractical]\n        Method[主要方法/Method] --> M1[使用Sapiens作为主干 / Use Sapiens as backbone]\n        M1 --> M2[定义下游任务评估标准 / Define downstream task evaluation criteria]\n        Results[关键结果/Results] --> R1[跨数据集具有竞争力 / Competitive across datasets]\n        R1 --> R2[验证为有效的下游任务 / Validated as a viable downstream task]"
    },
    {
      "title": "Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm",
      "authors": "Alireza Rafiei, Farshid Hajati, Alireza Rezaee, Amirhossien Panahi, Shahadat Uddin",
      "institution": "University of Tehran, University of New England, The University of Sydney",
      "link": "https://arxiv.org/pdf/2512.24253",
      "code": null,
      "tags": [
        "on-device ai",
        "Genetic Algorithm",
        "LSTM",
        "Wearable Device",
        "Transfer Learning",
        "Heart Rate Signals"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1495386bf6867b81b1a66ce95759268f2ed495c711dc3b4668583fc8a9c26b95_w640_q70.webp",
      "contributions": "1. Developed and optimized four machine learning models (LGB, MLP, LSTM, LSTM-FCN) for early sepsis prediction specifically for deployment on wearable devices. 2. Used a genetic algorithm to refine the model architectures, optimizing for performance, computational complexity, and memory requirements suitable for wearables. 3. Extended the prediction window from one hour to four hours using transfer learning, demonstrating adaptability for longer-term forecasting.",
      "summary": "This paper proposes using genetic algorithm-optimized machine learning models to predict sepsis early by analyzing heart rate data from wearable devices. The models are designed for computational efficiency on wearables and were extended to a four-hour prediction window via transfer learning. The results show promise for enabling early sepsis detection outside of intensive care settings.",
      "mindmap": "graph TB\n        Root(”Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Early sepsis detection outside ICU settings”)\n        Method(”主要方法/Method: Genetic algorithm-optimized ML models for wearables”)\n        Results(”关键结果/Results: Promising potential for wearable-based early detection”)"
    },
    {
      "title": "Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem",
      "authors": "Pengfu Wan, Jiawei Chen, Gangyan Xu",
      "institution": "The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.24251",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Fleet Size and Mix Vehicle Routing Problem (FSMVRP)",
        "deep reinforcement learning (DRL)",
        "Markov Decision Process (MDP)",
        "fleet-and-route integrated policy network (FRIPN)",
        "remaining graph embedding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3969891b48accce35280355d106951820196973739958b782a307a2a3df23aa3_w640_q70.webp",
      "contributions": "1. Formulates the Fleet Size and Mix Vehicle Routing Problem (FSMVRP) as a Markov Decision Process (MDP) for a deep reinforcement learning approach. 2. Proposes a novel policy network (FRIPN) that integrates fleet composition and routing decisions into a single model. 3. Introduces specialized input embeddings, including a remaining graph embedding, to enhance decision-making for vehicle employment.",
      "summary": "This paper proposes a deep reinforcement learning method to solve the complex Fleet Size and Mix Vehicle Routing Problem (FSMVRP). The core innovation is a policy network called FRIPN that jointly decides on fleet composition and routing. Experiments show the method is computationally efficient and scalable, producing near-optimal solutions quickly, especially for large-scale problems.",
      "mindmap": "graph TB\n        A[Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem] --> B(核心问题/Problem: FSMVRP - simultaneous fleet composition & routing)\n        A --> C(主要方法/Method: DRL-based MDP formulation with FRIPN policy network & remaining graph embedding)\n        A --> D(关键结果/Results: Near-optimal solutions in seconds, high computational efficiency & scalability)"
    },
    {
      "title": "Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning",
      "authors": "Ziqing Fan, Yuqiao Xian, Yan Sun, Li Shen",
      "institution": "ByteDance Seed, Shanghai Jiao Tong University, University of Sydney, Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.24265",
      "code": "https://github.com/ByteDance-Seed/DATAMASK",
      "tags": [
        "llm training",
        "data selection",
        "policy gradient",
        "mask learning",
        "quality-diversity trade-off",
        "FineWeb"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fd504349b8c0bb1934304d821a648ed4ca490b2f41e136f9b7a6220f39d5d2_w640_q70.webp",
      "contributions": "1. Introduces DATAMASK, a novel joint learning framework for large-scale pre-training data selection that simultaneously optimizes quality and diversity metrics. 2. Formulates data selection as a mask learning problem and solves it efficiently using policy gradient-based optimization with acceleration enhancements, reducing selection time by 98.9% compared to greedy algorithms. 3. Creates and releases FineWeb-Mask, a high-quality and diverse 10% subset of the 15-trillion-token FineWeb dataset, which significantly improves model performance (e.g., +3.2% on a 1.5B model) across diverse tasks.",
      "summary": "The paper addresses the problem of efficiently selecting high-quality and diverse data for large-scale LLM pre-training, where traditional methods are costly and suboptimal. It proposes DATAMASK, a policy gradient-based framework that learns optimal data masks to jointly optimize quality and diversity, drastically speeding up selection. The resulting curated dataset, FineWeb-Mask, leads to significant performance gains in pre-trained models, demonstrating the framework's effectiveness.",
      "mindmap": "graph TB\n        A[Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 大规模预训练数据中，联合考虑质量与多样性指标进行样本选择计算成本过高]\n        C[主要方法/Method: 提出DATAMASK框架，将选择过程视为掩码学习问题，使用策略梯度进行优化]\n        D[关键结果/Results: 选择时间减少98.9%，从FineWeb中选出的子集显著提升多种模型性能]"
    },
    {
      "title": "Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction",
      "authors": "Haojin Li, Anbang Zhang, Chen Sun, Chenyuan Feng, Kaiqian Qu, Tony Q. S. Quek, Haijun Zhang",
      "institution": "University of Science and Technology Beijing, Sony China Research Laboratory, Shandong University, Southeast University, University of Exeter, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.24324",
      "code": null,
      "tags": [
        "multi-modal inference",
        "reliability-aware dynamic weighting",
        "cross-modal contrastive learning",
        "semantic-aware beam prediction",
        "low-altitude UAV",
        "multi-modal learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fa219c5db4bfd0eeac8ee93679e8eba9ceb6e2e5ce7336d45afc6686f1d8162_w640_q70.webp",
      "contributions": "1. Proposes a reliability-aware dynamic weighting scheme that adaptively allocates contributions across different modalities (e.g., visual, posture, geospatial) based on their instantaneous reliability, moving beyond fixed-weight approaches. 2. Introduces a semantic-aware multi-modal beam prediction framework (SaM²B) that uses cross-modal contrastive learning to align multi-source representations into a shared semantic space, enhancing robustness to noise and distribution shifts. 3. Validates the proposed SaM²B framework on real-world low-altitude UAV datasets, demonstrating superior performance over baseline methods.",
      "summary": "This paper addresses the problem of unreliable beam prediction in multi-modal UAV communications caused by static weighting and modal misalignment. It proposes SaM²B, a framework that uses reliability-aware dynamic weighting and cross-modal contrastive learning to adaptively fuse modalities and align their semantics. Experiments on real-world datasets show SaM²B outperforms existing baseline methods.",
      "mindmap": "graph TB\n        A[Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[静态权重与模态失配/Static Weighting & Modal Mismatch]\n        B --> B2[跨场景泛化弱/Weak Cross-Scenario Generalization]\n        C --> C1[可靠性感知动态加权/Reliability-Aware Dynamic Weighting]\n        C --> C2[跨模态对比学习/Cross-Modal Contrastive Learning]\n        C --> C3[语义感知多模态框架/Semantic-Aware Multi-Modal Framework (SaM²B)]\n        D --> D1[真实数据集验证/Validated on Real-World UAV Datasets]\n        D --> D2[优于基线方法/Superior to Baseline Methods]"
    },
    {
      "title": "MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems",
      "authors": "Wan Jiang, Xinyi Zang, Yudong Zhao, Yusi Zou, Yunfei Lu, Junbo Tong, Yang Liu, Ming Li, Jiani Shi, Xin Yang",
      "institution": "JD.com, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24325",
      "code": null,
      "tags": [
        "agent system",
        "Multi-Agent Reinforcement Learning",
        "Centralized Training with Decentralized Execution (CTDE)",
        "Model Predictive Control (MPC)",
        "Dynamic Computation Allocation",
        "Recommender Systems"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5648d9986b292c2db26fc1ddb4325c3fc0f14c6516ddecf792ceaf524e365c4_w640_q70.webp",
      "contributions": "1. Proposes MaRCA, a multi-agent reinforcement learning framework that models recommender system stages as cooperative agents for end-to-end computation resource allocation. 2. Introduces an AutoBucket TestBench for accurate computation cost estimation in large-scale systems. 3. Designs a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off.",
      "summary": "This paper addresses the challenge of dynamic computation allocation in large-scale, multi-stage recommender systems under resource constraints. It proposes MaRCA, a multi-agent reinforcement learning framework that uses Centralized Training with Decentralized Execution (CTDE) and integrates a Model Predictive Control-based balancer to optimize revenue. The system was deployed on a major e-commerce platform, handling hundreds of billions of daily requests and achieving a 16.67% revenue uplift using existing resources.",
      "mindmap": "graph TB\n        A[MaRCA: Multi-Agent RL for Dynamic Computation Allocation] --> B[核心问题/Problem: 大规模推荐系统中，模型复杂度和流量规模增长带来的计算挑战，现有方法忽略阶段间依赖，限制全局最优性。]\n        A --> C[主要方法/Method: 提出MaRCA框架，将推荐系统阶段建模为合作智能体，使用CTDE进行训练，并引入AutoBucket TestBench和基于MPC的收益-成本平衡器。]\n        A --> D[关键结果/Results: 在领先的全球电商平台广告管线中端到端部署，每日处理数千亿广告请求，使用现有计算资源实现16.67%的收益提升。]"
    },
    {
      "title": "Tubular Riemannian Laplace Approximations for Bayesian Neural Networks",
      "authors": "Rodrigo Pereira David",
      "institution": "National Institute of Metrology, Technology and Quality (Inmetro)",
      "link": "https://arxiv.org/pdf/2512.24381",
      "code": null,
      "tags": [
        "bayesian deep learning",
        "Laplace approximation",
        "Riemannian geometry",
        "uncertainty quantification",
        "Bayesian neural networks",
        "model calibration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbef449cd138ae804891e277de76405797dfc3e78511bcb03bf13e56823c9746_w640_q70.webp",
      "contributions": "1. Introduces the Tubular Riemannian Laplace (TRL) approximation, a novel method that models the posterior as a probabilistic tube following low-loss valleys induced by functional symmetries. 2. Proposes using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty, adapting to the anisotropic, curved loss surfaces of deep models. 3. Demonstrates empirically that TRL achieves calibration comparable to Deep Ensembles on ResNet-18 (CIFAR-10/100) at a fraction (1/5) of the training cost, bridging single-model efficiency with ensemble-grade reliability.",
      "summary": "This paper addresses the poor calibration of traditional Euclidean Laplace approximations in Bayesian Neural Networks. It proposes the Tubular Riemannian Laplace (TRL) approximation, which models the posterior as a tube using a Riemannian metric to better capture parameter space geometry. The method achieves excellent uncertainty calibration on image classification tasks, matching Deep Ensembles' reliability with significantly lower computational cost.",
      "mindmap": "graph TB\n        A[Tubular Riemannian Laplace Approximations<br>管状黎曼拉普拉斯近似] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统拉普拉斯近似在深度模型中校准不佳<br>Traditional Laplace approximations struggle with calibration in deep models]\n        C --> C1[提出管状黎曼拉普拉斯(TRL)近似<br>Propose Tubular Riemannian Laplace (TRL) approximation]\n        C1 --> C2[使用Fisher/Gauss-Newton度量建模概率管<br>Model probabilistic tube using Fisher/Gauss-Newton metric]\n        D --> D1[在ResNet-18上实现优秀校准<br>Achieves excellent calibration on ResNet-18]\n        D1 --> D2[匹配集成方法可靠性，成本仅1/5<br>Matches ensemble reliability at 1/5 training cost]"
    },
    {
      "title": "Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning",
      "authors": "Soham Pahari, M. Srinivas",
      "institution": "UPES (School of Computer Science), NIT Warangal (Department of CS&E)",
      "link": "https://arxiv.org/pdf/2512.24404",
      "code": null,
      "tags": [
        "cross-view geo-localization",
        "visual reasoning",
        "reinforcement learning",
        "contrastive learning",
        "cross-view alignment",
        "visual planning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4164fa0c19f567cf79df4a59a8129e5b520a00f0d6456c7d4f5649d4a580da_w640_q70.webp",
      "contributions": "1. Proposes a novel visual reasoning paradigm called Geo-Consistent Visual Planning and a framework named ViReLoc for planning and localization using only visual representations. 2. Introduces a method that learns spatial and geometric dependencies through step-by-step visual inference optimized with reinforcement learning objectives. 3. Integrates contrastive learning and adaptive feature interaction to align ground and aerial perspectives and reduce viewpoint differences.",
      "summary": "This paper addresses the limitation of text-based reasoning in spatial tasks by proposing ViReLoc, a visual reasoning framework for ground-to-aerial localization and route planning. The method uses reinforcement learning and contrastive learning to perform inference directly in the visual domain without relying on GPS. Experiments show improved spatial reasoning and cross-view retrieval, establishing visual reasoning as a secure complementary approach for navigation.",
      "mindmap": "graph TB\n        A[Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[文本推理在空间任务中的局限性<br/>Limitations of Text-Based Reasoning in Spatial Tasks]\n        C --> C1[视觉推理框架ViReLoc<br/>Visual Reasoning Framework ViReLoc]\n        C1 --> C2[Geo-Consistent Visual Planning<br/>Geo-Consistent Visual Planning]\n        C1 --> C3[强化学习与对比学习<br/>Reinforcement & Contrastive Learning]\n        D --> D1[空间推理与跨视图检索性能提升<br/>Improved Spatial Reasoning & Cross-View Retrieval]\n        D --> D2[无需GPS的安全导航方案<br/>Secure Navigation Without GPS]"
    },
    {
      "title": "Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models",
      "authors": "Lars van der Laan, Aurelien Bibaut, Nathan Kallus",
      "institution": "University of Washington, Netflix Research, Cornell University",
      "link": "https://arxiv.org/pdf/2512.24407",
      "code": null,
      "tags": [
        "reinforcement learning",
        "inverse reinforcement learning",
        "dynamic discrete choice",
        "semiparametric inference",
        "debiased machine learning",
        "efficient influence function"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f071bfe838a84ec68caefb3e8f32ddce9a94446707278cc78a1f8f23d2b1cdcf_w640_q70.webp",
      "contributions": "1. Introduced a semiparametric framework for debiased inverse reinforcement learning that enables statistically efficient inference for reward-dependent functionals. 2. Showed that the log-behavior policy acts as a pseudo-reward that identifies policy value differences and, with normalization, the reward itself, formalizing these as smooth functionals. 3. Constructed automatic debiased machine-learning estimators that allow flexible nonparametric nuisance estimation while achieving √n-consistency, asymptotic normality, and semiparametric efficiency.",
      "summary": "This paper develops a unified semiparametric framework for inference in inverse reinforcement learning and dynamic discrete choice models. The method leverages the log-behavior policy as a pseudo-reward and constructs debiased machine learning estimators, enabling flexible nonparametric estimation while providing statistical guarantees like asymptotic normality and efficiency. The framework bridges classical econometric inference with modern machine learning tools for sequential decision-making problems.",
      "mindmap": "graph TB\n        A[Efficient Inference for IRL and DDC Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Flexible IRL lacks inference guarantees;<br>Classical DDC is restrictive & computationally heavy]\n        C[主要方法/Method<br>Semiparametric debiased IRL framework;<br>Log-behavior policy as pseudo-reward;<br>Automatic debiased ML estimators]\n        D[关键结果/Results<br>√n-consistent, asymptotically normal,<br>semiparametrically efficient inference;<br>Unified, tractable approach]"
    },
    {
      "title": "Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics",
      "authors": "Akash Samanta, Sheldon Williamson",
      "institution": "Ontario Tech University",
      "link": "https://arxiv.org/pdf/2512.24445",
      "code": null,
      "tags": [
        "adaptive learning",
        "bias-noise-alignment",
        "diagnostic-driven adaptation",
        "temporal-difference error",
        "stabilized optimizer",
        "actor-critic"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fffef5a11b6873e7029659f1544c8ea507323fdf48462b3c7caeee1ffd63e30_w640_q70.webp",
      "contributions": "1. Proposes a novel diagnostic-driven adaptive learning framework that decomposes error evolution into bias, noise, and alignment components. 2. Derives and instantiates the framework across multiple learning paradigms, including supervised optimization, actor-critic RL, and learned optimizers. 3. Establishes theoretical stability guarantees and bounded updates for the proposed diagnostic-driven methods under standard assumptions.",
      "summary": "This paper addresses the problem of unstable and slow learning in nonstationary environments by proposing a new framework that models error evolution through bias, noise, and alignment diagnostics. The method uses these online-computed diagnostics to guide and stabilize learning in optimization, reinforcement learning, and meta-learning. The work provides a unifying, interpretable foundation for reliable adaptation in dynamic settings.",
      "mindmap": "graph TB\n        A[Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics] --> B(核心问题/Problem: Learning instability in nonstationary environments)\n        A --> C(主要方法/Method: Diagnostic-driven framework decomposing error into bias, noise, alignment)\n        A --> D(关键结果/Results: Unifying control backbone for optimization/RL/learned optimizers with stability guarantees)"
    },
    {
      "title": "Sparse classification with positive-confidence data in high dimensions",
      "authors": "Tien Mai, Mai Anh Nguyen, Trung Nghia Nguyen",
      "institution": "Norwegian Institute of Public Health, Seoul National University, Rutgers University",
      "link": "https://arxiv.org/pdf/2512.24443",
      "code": null,
      "tags": [
        "weakly-supervised learning",
        "Pconf classification",
        "sparse regularization",
        "Lasso",
        "SCAD",
        "MCP"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d4d37b85ea25099bdc6c46c973123d1cadddc0454b10411668cb1a47bc23596_w640_q70.webp",
      "contributions": "1. Proposes a novel sparse-penalization framework for high-dimensional Positive-Confidence (Pconf) classification, bridging weak supervision and high-dimensional statistics. 2. Introduces estimators using both convex (Lasso) and non-convex (SCAD, MCP) penalties to address shrinkage bias and improve feature recovery. 3. Establishes theoretical error bounds for the L1-regularized estimator and develops an efficient proximal gradient algorithm to solve the composite objective.",
      "summary": "This paper addresses the challenge of performing sparse classification in high-dimensional settings using only positive samples with confidence scores (Pconf data). It proposes a new framework using Lasso, SCAD, and MCP penalties for variable selection and develops an efficient algorithm. The method achieves performance comparable to fully supervised approaches, effectively bridging weak supervision and high-dimensional learning.",
      "mindmap": "graph TB\n        A[Sparse classification with positive-confidence data in high dimensions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[High-dim weak-supervised learning<br>高维弱监督学习]\n        B --> B2[Existing Pconf methods ill-suited<br>现有Pconf方法不适用]\n        C --> C1[Sparse-penalization framework<br>稀疏惩罚框架]\n        C --> C2[Convex & non-convex penalties<br>凸与非凸惩罚项]\n        C --> C3[Proximal gradient algorithm<br>近端梯度算法]\n        D --> D1[Near minimax-optimal recovery<br>接近极小极大最优恢复]\n        D --> D2[Comparable to supervised methods<br>性能媲美全监督方法]"
    },
    {
      "title": "Generative forecasting with joint probability models",
      "authors": "Patrick Wyrod, Ashesh Chattopadhyay, Daniele Venturi",
      "institution": "University of California Santa Cruz",
      "link": "https://arxiv.org/pdf/2512.24446",
      "code": null,
      "tags": [
        "probabilistic forecasting",
        "joint probability distribution",
        "generative model",
        "uncertainty quantification",
        "chaotic dynamical systems",
        "Wasserstein drift"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95523ae23ff02d01f3cc01323f495f300ee24988a99409698e6d43daf9c09a82_w640_q70.webp",
      "contributions": "1. Reframes forecasting as a fully generative problem by learning the joint probability distribution of system states over temporal windows and obtaining forecasts via marginalization. 2. Introduces a general, model-agnostic training and inference framework for joint generative forecasting. 3. Proposes three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, cumulative Wasserstein drift) to assess forecast robustness without ground truth.",
      "summary": "This paper proposes a new generative forecasting method that learns the joint probability distribution of system states over time windows, rather than just next-step predictions. This approach better captures temporal dependencies and dynamics, leading to improved short-term predictions and more accurate long-term statistical behavior for chaotic systems like Lorenz-63 and Kuramoto-Sivashinsky, as demonstrated by several uncertainty metrics.",
      "mindmap": "graph TB\n        A[Generative forecasting with joint probability models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[确定性预测有限/Deterministic forecasting limited for chaotic systems]\n        B --> B2[现有方法关注下一步预测/Existing methods focus on next-step prediction]\n        C --> C1[学习联合概率分布/Learn joint probability distribution of lagged states]\n        C --> C2[通过边缘化进行预测/Forecast via marginalization]\n        C --> C3[模型无关框架/Model-agnostic framework]\n        D --> D1[改进短期预测技能/Improved short-term predictive skill]\n        D --> D2[保持吸引子几何/Preserve attractor geometry]\n        D --> D3[更准确的长程统计行为/More accurate long-range statistical behavior]"
    },
    {
      "title": "Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations",
      "authors": "Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus",
      "institution": "Nexcepta, The Ohio State University, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.24452",
      "code": null,
      "tags": [
        "semantic communications",
        "min-max optimization",
        "adversarial perturbations",
        "multi-task learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e9c394c6a473f54b07d7337b43fb693f3ebef1de80e7b275ea3c91df03de11_w640_q70.webp",
      "contributions": "1. A deep learning-based semantic communication framework that jointly supports multiple receiver tasks (e.g., inference and reconstruction) while explicitly limiting semantic information leakage to an eavesdropper. 2. Formulation of the privacy problem as an iterative min-max optimization, where the legitimate transmitter-receiver pair is trained to degrade an adaptive eavesdropper's semantic inference performance. 3. Introduction of an auxiliary adversarial perturbation layer that superimposes a crafted signal on the transmitted waveform to degrade eavesdropper performance, even when the legitimate link is not co-trained against it.",
      "summary": "This paper addresses privacy leakage in semantic communications, where task-optimized representations can be exploited by eavesdroppers. The proposed method uses a min-max adversarial training framework and an auxiliary perturbation layer to protect semantic information. Evaluations on image datasets show the approach significantly reduces eavesdropper inference accuracy without harming legitimate receiver performance.",
      "mindmap": "graph TB\n        Root[Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Semantic representations leak sensitive information to eavesdroppers] --> Problem_Detail[语义泄露/Semantic Leakage]\n        Method[主要方法/Method: Deep learning framework with min-max optimization and adversarial perturbations] --> Method_Detail1[对抗训练/Min-Max Optimization]\n        Method --> Method_Detail2[扰动层/Perturbation Layer]\n        Results[关键结果/Results: Reduces eavesdropper accuracy, maintains legitimate performance] --> Results_Detail[有效隐私保护/Effective Privacy Preservation]"
    },
    {
      "title": "Spectral and Spatial Graph Learning for Multispectral Solar Image Compression",
      "authors": "Prasiddha Siwakoti, Atefeh Khoshkhahtinat, Piyush M. Mehta, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva",
      "institution": "West Virginia University, NASA Goddard Space Flight Center",
      "link": "https://arxiv.org/pdf/2512.24463",
      "code": "https://github.com/agyat4/sgraph",
      "tags": [
        "image compression",
        "graph neural network",
        "multispectral image compression",
        "spectral graph embedding",
        "spatial graph attention",
        "learned image compression"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22141561baf730c3986a8299115f064f9a3f82996264d04ebeefffad89a18be1_w640_q70.webp",
      "contributions": "1. Proposed an Inter-Spectral Windowed Graph Embedding (iSWGE) module to model inter-band relationships by representing spectral channels as graph nodes with learned edges. 2. Introduced a Windowed Spatial Graph Attention and Convolutional Block Attention (WSGA-C) module to reduce spatial redundancy and emphasize fine-scale structures. 3. Developed a learned image compression framework tailored for multispectral solar imagery, achieving improved spectral fidelity and reconstruction quality on the SDOML dataset.",
      "summary": "This paper addresses the challenge of compressing high-volume multispectral solar imagery for space missions. It proposes a learned compression framework that uses two novel graph-based modules to model spectral and spatial dependencies. The method demonstrates improved performance in preserving spectral information and reconstruction quality compared to strong baselines.",
      "mindmap": "graph TB\n        A[论文标题: Spectral and Spatial Graph Learning for Multispectral Solar Image Compression] --> B(核心问题/Problem: High-fidelity compression of multispectral solar imagery with limited bandwidth)\n        A --> C(主要方法/Method: Two complementary graph learning modules: iSWGE for spectral, WSGA-C for spatial dependencies)\n        A --> D(关键结果/Results: 20.15% MSID reduction, up to 1.09% PSNR gain, 1.62% MS-SSIM gain)"
    },
    {
      "title": "HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors",
      "authors": "Hyunjun Kim",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), École Polytechnique Fédérale de Lausanne (EPFL)",
      "link": "https://arxiv.org/pdf/2512.24478",
      "code": "https://github.com/hyunjun1121/holograph",
      "tags": [
        "causal discovery",
        "sheaf theory",
        "large language models",
        "natural gradient descent",
        "algebraic latent projection",
        "presheaf"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eaf3f0002711b7e42d725e2a453cb23c5db4c9ca0b3a10f0290cfce9779f2e_w640_q70.webp",
      "contributions": "1. A sheaf-theoretic framework formalizing LLM-guided causal discovery as a presheaf satisfaction problem. 2. A natural gradient descent algorithm on the belief manifold for principled optimization. 3. The introduction of Algebraic Latent Projection to handle hidden confounders.",
      "summary": "The paper introduces HOLOGRAPH, a framework that uses sheaf theory to formally integrate Large Language Model priors for causal discovery, addressing issues of coherence and hidden confounders. It proposes novel methods like Algebraic Latent Projection and natural gradient optimization. The approach provides a rigorous mathematical foundation and shows competitive performance, while analysis reveals a failure of the Locality axiom in larger graphs.",
      "mindmap": "graph TB\n        A[HOLOGRAPH] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[因果发现受可识别性限制/Causal discovery limited by identifiability]\n        B --> B2[现有LLM方法缺乏理论基础/Existing LLM methods lack theory]\n        C --> C1[层理论框架/Sheaf-theoretic framework]\n        C --> C2[代数潜在投影/Algebraic Latent Projection]\n        C --> C3[自然梯度下降/Natural Gradient Descent]\n        D --> D1[提供数学基础/Provides mathematical foundation]\n        D --> D2[性能有竞争力/Achieves competitive performance]\n        D --> D3[局部性公理失效/Locality axiom fails for large graphs]"
    },
    {
      "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
      "authors": "Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun",
      "institution": "Meta FAIR, INRIA Paris, Ecole normale supérieure/PSL, New York University",
      "link": "https://arxiv.org/pdf/2512.24497",
      "code": "https://github.com/facebookresearch/jepa-wms",
      "tags": [
        "world models",
        "joint-embedding predictive architecture",
        "representation space planning",
        "model-based reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp",
      "contributions": "1. Proposes a comprehensive characterization and study of Joint-Embedding Predictive Architecture World Models (JEPA-WMs) for physical planning. 2. Systematically investigates the impact of model architecture, training objective, and planning algorithm on planning success in simulated and real-world robotic tasks. 3. Combines the findings to propose a new model that outperforms established baselines (DINO-WM and V-JEPA-2-AC) in navigation and manipulation tasks.",
      "summary": "This paper investigates the key factors for successful physical planning using Joint-Embedding Predictive World Models (JEPA-WMs). It conducts a systematic study of architectural and algorithmic choices within this family of methods and proposes a new model that achieves superior performance on navigation and manipulation tasks compared to existing baselines.",
      "mindmap": "graph TB\n        Root[What Drives Success in Physical Planning with JEPA-WMs?] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>How to build agents that generalize to new physical tasks?]\n        Method[主要方法/Method<br>Study JEPA-WMs: architecture, objective, planning algorithm]\n        Results[关键结果/Results<br>Proposed model outperforms baselines (DINO-WM, V-JEPA-2-AC)]"
    },
    {
      "title": "Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice",
      "authors": "Jiachen T. Wang, Tong Wu, Kaifeng Lyu, James Zou, Dawn Song, Ruoxi Jia, Prateek Mittal",
      "institution": "Princeton University, Tsinghua University, Stanford University, UC Berkeley, Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.24503",
      "code": null,
      "tags": [
        "llm training",
        "proxy models",
        "data curation",
        "hyperparameter tuning",
        "learning rate",
        "pretraining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0253b877f5ec90fd3f09b9d1a8f7d0967a88407fbbd25eb1e8a8bfcdf23081c4_w640_q70.webp",
      "contributions": "1. Identifies a critical flaw in the standard proxy-model evaluation protocol, showing that using a fixed training configuration for all data recipes leads to unreliable conclusions that can flip with minor hyperparameter changes. 2. Proposes a simple and effective patch to the protocol: training proxy models with reduced learning rates, which preserves the relative performance ranking of data recipes and correlates strongly with fully-tuned large-scale training. 3. Provides theoretical justification for the proposed method by proving it preserves dataset ordering for random-feature models, and validates it empirically across 23 data recipes.",
      "summary": "The paper identifies that the standard practice of using small proxy models with identical hyperparameters to evaluate data recipes is unreliable because optimal training configurations are data-dependent. To fix this, the authors propose training proxy models with reduced learning rates, a simple change that makes small-scale experiment rankings strongly correlate with those from fully-tuned large-scale LLM pretraining. This method is theoretically justified and empirically validated, dramatically improving the reliability of data curation guidance from small training runs.",
      "mindmap": "graph TB\n        Root[Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[固定配置评估不可靠/Fixed-config evaluation unreliable]\n        Problem --> P2[结论随超参翻转/Conclusions flip with hyperparams]\n        Method[主要方法/Method] --> M1[降低学习率训练代理模型/Train proxy models with reduced LR]\n        Method --> M2[数据特定调优目标/Data-specific tuning objective]\n        Results[关键结果/Results] --> R1[与大规模训练强相关/Strong correlation with large-scale training]\n        Results --> R2[理论证明与实证验证/Theoretical proof & empirical validation]"
    },
    {
      "title": "Generalising E-prop to Deep Networks",
      "authors": "Beren Millidge",
      "institution": "Zyphra",
      "link": "https://arxiv.org/pdf/2512.24506",
      "code": null,
      "tags": [
        "biologically plausible learning algorithms",
        "E-prop",
        "eligibility traces",
        "credit assignment",
        "recurrent neural networks",
        "backpropagation through time"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6eda8a1b73fb9e52a5baf437a42dfe8578438c0dde5dfa6d166ba856f037f47_w640_q70.webp",
      "contributions": "1. Extends the E-prop framework to handle arbitrarily deep networks, enabling credit assignment across both time and depth. 2. Derives a novel recursion relationship across depth that generalizes eligibility traces to deeper layers. 3. Demonstrates an online learning algorithm capable of training deep recurrent networks without backpropagation through time.",
      "summary": "This paper addresses the biological implausibility of Backpropagation Through Time (BPTT) for training recurrent neural networks. It proposes an extension of the E-prop algorithm to deep networks, enabling online credit assignment across both time and depth. The main conclusion is that this method allows for the training of deep recurrent networks without BPTT.",
      "mindmap": "graph TB\n        Root(”Generalising E-prop to Deep Networks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”BPTT生物不可信/BPTT biologically implausible”)\n        Problem --> P2(”RTRL计算复杂/RTRL computationally expensive”)\n        Problem --> P3(”现有方法局限于单层/Existing methods limited to single layer”)\n        Method --> M1(”扩展E-prop框架/Extend E-prop framework”)\n        Method --> M2(”推导跨深度的递归关系/Derive depth-wise recursion”)\n        Method --> M3(”推广资格迹到深层/Generalize eligibility traces to deep layers”)\n        Results --> R1(”实现跨时空的在线信用分配/Online credit assignment across time and depth”)\n        Results --> R2(”无需BPTT训练深度循环网络/Train deep recurrent networks without BPTT”)"
    },
    {
      "title": "A Graph Neural Network with Auxiliary Task Learning for Missing PMU Data Reconstruction",
      "authors": "Bo Li, Zijun Chen, Haiwang Zhong, Di Cao, Guangchun Ruan",
      "institution": "Tsinghua University (inferred from IEEE affiliations and common institutional patterns for authors Bo Li, Haiwang Zhong, Di Cao, Guangchun Ruan)",
      "link": "https://arxiv.org/pdf/2512.24542",
      "code": null,
      "tags": [
        "graph neural networks",
        "graph neural network",
        "auxiliary task learning",
        "missing data reconstruction",
        "spatial-temporal dependencies",
        "low-rank property"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bb99f00deaaada2673c4fa2deba197e3de14c9c3164c0d055378a9290049033_w640_q70.webp",
      "contributions": "1. Proposes a K-hop GNN that operates directly on the subgraph of observable PMU nodes, enabling learning under incomplete system observability. 2. Designs an auxiliary learning framework with two complementary GNNs: a spatial-temporal GNN for reconstruction and an auxiliary GNN for unsupervised online learning using the low-rank property of data. 3. Demonstrates that the method dynamically leverages low-rank properties across the architecture to achieve robustness and self-adaptation, showing superior performance under high missing rates and incomplete observability.",
      "summary": "This paper addresses the problem of reconstructing missing data from Phasor Measurement Units (PMUs) in power systems, which is critical for grid monitoring. The proposed method uses a Graph Neural Network with Auxiliary Task Learning, combining a spatial-temporal GNN for reconstruction with an auxiliary GNN for online adaptation using data's low-rank properties. The results show the method is robust under high missing rates and incomplete system observability.",
      "mindmap": "graph TB\n        Root[”A Graph Neural Network with Auxiliary Task Learning for Missing PMU Data Reconstruction<br>基于辅助任务学习的图神经网络用于PMU缺失数据重构”]\n        Root --> Problem[”核心问题/Problem<br>PMU数据因故障、攻击等缺失，现有方法对概念漂移适应性差、高缺失率下鲁棒性弱、依赖全系统可观性假设”]\n        Root --> Method[”主要方法/Method<br>提出K跳GNN在PMU子图上学习；设计辅助学习框架，包含时空GNN和利用数据低秩特性的辅助GNN”]\n        Root --> Results[”关键结果/Results<br>在高缺失率和不完全可观性下，方法展现出优越的离线和在线性能”]"
    },
    {
      "title": "More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization",
      "authors": "Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.24545",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "extreme quantization",
        "double binary factorization",
        "low-bit LLM",
        "post-training quantization",
        "binary matrix multiplication"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp",
      "contributions": "1. Proposed Multi-Envelope Double Binary Factorization (MDBF), which replaces the single magnitude envelope in DBF with a rank-l envelope to enhance magnitude expressiveness while maintaining a shared binary sign carrier. 2. Introduced a closed-form initialization and an alternating refinement method to effectively optimize the MDBF parameters. 3. Demonstrated that MDBF improves perplexity and zero-shot accuracy over prior binary formats on LLaMA and Qwen models at matched bit budgets while preserving the same efficient inference primitive.",
      "summary": "The paper addresses the performance saturation of Double Binary Factorization (DBF) in extreme low-bit quantization of LLMs, where a single magnitude envelope limits expressiveness. It proposes Multi-Envelope DBF (MDBF), which uses multiple envelope components to allocate more expressivity to magnitudes while keeping binary sign matrices shared. Experiments on LLaMA and Qwen families show MDBF outperforms previous binary formats in accuracy and perplexity at the same bit rate without changing the inference primitive.",
      "mindmap": "graph TB\n        Root[”More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DBF scaling too restrictive,<br>single envelope causes<br>performance saturation”]\n        Method[”主要方法/Method<br>Propose MDBF: shared 1-bit sign bases,<br>replace single envelope with<br>rank-l envelope”]\n        Results[”关键结果/Results<br>Better perplexity & accuracy<br>over previous binary formats,<br>same inference primitive”]"
    },
    {
      "title": "From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme",
      "authors": "Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu",
      "institution": "Shanghai Artificial Intelligence Laboratory, Xi'an Jiaotong University, Columbia University, The Chinese University of Hong Kong MMLab",
      "link": "https://arxiv.org/pdf/2512.24555",
      "code": null,
      "tags": [
        "multimodal generation",
        "vision-language models",
        "chain-of-thought",
        "reinforcement learning from human feedback"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51d7b92e3cbe88fcb46458e3ce7a303a30ccee8230eb6865946f2c654b707c34_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical, multi-path Chain-of-Thought (CoT) method to enhance reasoning diversity for meme generation. 2. Introduces a group-wise pairwise reward model trained on memes sharing the same template to robustly capture subjective human humor preferences. 3. Develops a group-wise reinforcement learning optimization framework with a theoretical guarantee for monotonic improvement, enabling better alignment with human preferences.",
      "summary": "This paper introduces HUMOR, a framework for generating humorous memes. It uses a hierarchical multi-path Chain-of-Thought to guide reasoning and a group-wise reward model with RL for preference alignment. Experiments show it improves reasoning diversity, alignment, and overall meme quality in VLMs.",
      "mindmap": "graph TB\n        A[From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 超越监督的幽默梗图生成/Humorous meme generation beyond direct supervision]\n        C[主要方法/Method: HUMOR框架/HUMOR Framework]\n        D[关键结果/Results: 提升多样性、对齐性和质量/Improved diversity, alignment, and quality]\n        C --> C1[分层多路径思维链/Hierarchical Multi-path CoT]\n        C --> C2[基于分组的奖励模型与强化学习/Group-wise Reward Model & RL]"
    },
    {
      "title": "CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts",
      "authors": "Shunbo Jia, Caizhi Liao",
      "institution": "Macau University of Science and Technology, Shenzhen University of Advanced Technology",
      "link": "https://arxiv.org/pdf/2512.24564",
      "code": null,
      "tags": [
        "robust machine learning",
        "causal representation learning",
        "adversarial robustness",
        "electrocardiogram (ECG)",
        "structural causal model (SCM)",
        "smooth adversarial perturbations (SAP)"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ba48c31785d4bc725e8bd6077843ed01150da361f64874d3d85d9a541b0d692_w640_q70.webp",
      "contributions": "1. Proposes Causal Physiological Representation Learning (CPR), a novel framework that integrates a Physiological Structural Prior into a causal disentanglement model for ECG analysis. 2. Models ECG generation via a Structural Causal Model (SCM) to enforce a structural intervention that strictly separates invariant pathological features from non-causal artifacts. 3. Demonstrates that CPR achieves certified robustness comparable to Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off for real-time clinical applications.",
      "summary": "This paper addresses the vulnerability of deep learning ECG diagnostic models to smooth adversarial perturbations. The authors propose Causal Physiological Representation Learning (CPR), a method that uses a causal disentanglement framework with a physiological prior to separate robust pathological features from artifacts. The results show that CPR provides strong adversarial robustness with efficient inference, outperforming existing defense methods.",
      "mindmap": "graph TB\n        A[CPR: Causal Physiological Representation Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[ECG模型对对抗性扰动脆弱/Fragile to adversarial perturbations (SAP)]\n        B --> B2[现有防御方法存在效率与鲁棒性权衡/Existing defenses (AT, RS) have efficiency-robustness trade-off]\n        C --> C1[提出因果生理表征学习/Propose Causal Physiological Representation Learning (CPR)]\n        C --> C2[结合生理结构先验与因果解耦/Integrate Physiological Structural Prior & causal disentanglement]\n        C --> C3[使用结构因果模型分离病理特征/Use SCM to separate pathological features from artifacts]\n        D --> D1[在SAP攻击下F1分数0.632/F1 score 0.632 under SAP attacks]\n        D --> D2[超越中值平滑9.1%/Surpass Median Smoothing by 9.1%]\n        D --> D3[匹配随机平滑的鲁棒性且保持单次推理效率/Matches RS robustness with single-pass inference]"
    },
    {
      "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
      "authors": "Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun",
      "institution": "University of Texas at Austin, Together AI, University of Sydney",
      "link": "https://arxiv.org/pdf/2512.24574",
      "code": "https://github.com/togethercomputer/CREST",
      "tags": [
        "llm inference",
        "chain-of-thought reasoning",
        "attention heads",
        "test-time intervention",
        "computational efficiency",
        "reasoning steering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp",
      "contributions": "1. Identified specialized attention heads in LLMs that correlate with distinct cognitive reasoning behaviors (e.g., verification, backtracking). 2. Proposed CREST, a training-free method for Cognitive REasoning Steering at Test-time, which involves offline calibration to find steering vectors and inference-time rotation to suppress unproductive reasoning. 3. Demonstrated that CREST improves reasoning accuracy and reduces token usage across diverse benchmarks, offering a pathway to faster and more reliable LLM inference.",
      "summary": "This paper addresses the inefficiency and instability of long chain-of-thought reasoning in LLMs, which leads to high latency and alternating underthinking/overthinking. The authors propose CREST, a training-free method that identifies and steers specific attention heads at test-time to suppress unproductive cognitive behaviors. The method improves accuracy by up to 17.5% and reduces token usage by 37.6%, enabling faster and more reliable reasoning.",
      "mindmap": "graph TB\n        A[Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM推理轨迹低效且不稳定/Inefficient & Unstable LLM Reasoning Trajectories]\n        B1 --> B2[过度思考与思考不足/Overthinking & Underthinking]\n        B1 --> B3[高延迟与高令牌消耗/High Latency & Token Usage]\n        C --> C1[识别与认知行为相关的注意力头/Identify Cognitive Attention Heads]\n        C --> C2[提出CREST方法: 测试时认知推理引导/Propose CREST: Test-time Cognitive REasoning Steering]\n        C2 --> C3[离线校准获取引导向量/Offline Calibration for Steering Vectors]\n        C2 --> C4[推理时旋转隐藏表示/Inference-time Representation Rotation]\n        D --> D1[准确率显著提升/Accuracy Improved Up to 17.5%]\n        D --> D2[令牌使用大幅减少/Token Usage Reduced by 37.6%]\n        D --> D3[实现更快更可靠的推理/Enables Faster, More Reliable Reasoning]"
    },
    {
      "title": "3D Semantic Segmentation for Post-Disaster Assessment",
      "authors": "Nhut Le, Maryam Rahnemoonfar",
      "institution": "Lehigh University",
      "link": "https://arxiv.org/pdf/2512.24593",
      "code": null,
      "tags": [
        "3D semantic segmentation",
        "3D point clouds",
        "Structure-from-Motion (SfM)",
        "Multi-View Stereo (MVS)",
        "Fast Point Transformer (FPT)",
        "Point Transformer v3 (PTv3)"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7f40d3c2e21cef6917feeb765394fae515f763b5c2f2f5b374f921e3a8ac1ba_w640_q70.webp",
      "contributions": "1. Constructed a specialized 3D dataset for post-disaster assessment using UAV footage and 3D reconstruction techniques (SfM/MVS). 2. Evaluated state-of-the-art 3D semantic segmentation models (FPT, PTv3, OA-CNNs) on this new dataset. 3. Identified significant limitations of existing models in disaster-stricken environments, highlighting the need for new techniques and benchmarks.",
      "summary": "This paper addresses the lack of specialized datasets for 3D semantic segmentation in post-disaster scenarios by constructing a new 3D point cloud dataset from UAV footage of Hurricane Ian. The authors evaluated several state-of-the-art models on this dataset and found their performance to be significantly limited, demonstrating an urgent need for improved methods and benchmarks tailored to disaster environments.",
      "mindmap": "graph TB\n        A[3D Semantic Segmentation for Post-Disaster Assessment] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏灾后3D数据集 / Lack of post-disaster 3D datasets]\n        C --> C1[使用无人机与SfM/MVS构建3D数据集 / Construct 3D dataset using UAV & SfM/MVS]\n        C --> C2[评估SOTA 3D分割模型 / Evaluate SOTA 3D segmentation models]\n        D --> D1[现有模型存在显著局限 / Existing models have significant limitations]\n        D --> D2[需要新技术与基准 / Need for new techniques & benchmarks]"
    },
    {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "authors": "Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang",
      "institution": "ByteDance Seed, University of Manchester, Mila - Quebec AI Institute, Tsinghua University, M-A-P",
      "link": "https://arxiv.org/pdf/2512.24617",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical compression",
        "compression-aware scaling law",
        "decoupled µP parametrization",
        "concept space",
        "adaptive semantic boundaries"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dff9c61b225b5a86d535cfc752b13f390ae301473e649284a6815c3eaf80b24_w640_q70.webp",
      "contributions": "1. Proposed Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns variable-length semantic concepts end-to-end and shifts computation from tokens to a compressed concept space for more efficient reasoning. 2. Introduced the first compression-aware scaling law that disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. 3. Developed a decoupled µP parametrization for stable training of the heterogeneous architecture, supporting zero-shot hyperparameter transfer across model widths and compression regimes.",
      "summary": "The paper addresses the inefficiency of uniform token-level computation in LLMs by proposing Dynamic Large Concept Models (DLCM), which learns adaptive semantic concepts and reallocates compute to a higher-capacity reasoning backbone in a compressed concept space. This approach achieves a +2.69% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.",
      "mindmap": "graph TB\n        A[Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space] --> B[核心问题/Problem: LLMs apply uniform computation to tokens, wasting capacity on predictable spans and under-allocating to critical transitions]\n        A --> C[主要方法/Method: Hierarchical framework learns semantic boundaries, shifts computation to compressed concept space, introduces compression-aware scaling law and decoupled µP parametrization]\n        A --> D[关键结果/Results: +2.69% average improvement on 12 zero-shot benchmarks under matched inference FLOPs with R=4 compression]"
    },
    {
      "title": "AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt",
      "authors": "Zijian Zhao, Yitong Shang, Sen Li",
      "institution": "The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24625",
      "code": "https://github.com/RS2002/AutoFed",
      "tags": [
        "federated learning",
        "personalized federated learning",
        "prompt learning",
        "traffic prediction",
        "non-IID data",
        "hyper-parameter tuning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e572784e0dff554ff14fce989febaa7869bdfb488d60b6fd86ebd7e98cdf0bf7_w640_q70.webp",
      "contributions": "1. Proposes AutoFed, a novel Personalized Federated Learning (PFL) framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. 2. Introduces a federated representor with a client-aligned adapter to distill local data into a compact, globally shared prompt matrix, inspired by prompt learning. 3. Demonstrates through extensive experiments that AutoFed consistently achieves superior performance across diverse real-world traffic prediction scenarios.",
      "summary": "This paper proposes AutoFed, a manual-free Personalized Federated Learning framework for traffic prediction that uses a client-aligned adapter to generate a shared prompt matrix, enabling knowledge sharing while preserving local specificity. Experiments on real-world datasets show that AutoFed achieves superior performance without requiring manual hyper-parameter tuning.",
      "mindmap": "graph TB\n        A[AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据孤岛与隐私问题 / Data Silos & Privacy]\n        B --> B2[非独立同分布数据 / Non-IID Data]\n        B --> B3[手动超参数调优 / Manual Hyper-parameter Tuning]\n        C --> C1[个性化联邦学习 / Personalized Federated Learning (PFL)]\n        C --> C2[提示学习 / Prompt Learning]\n        C --> C3[联邦表征器与客户端对齐适配器 / Federated Representor & Client-Aligned Adapter]\n        D --> D1[性能优越 / Superior Performance]\n        D --> D2[无需手动调参 / No Manual Tuning]\n        D --> D3[真实数据集验证 / Validated on Real-world Datasets]"
    },
    {
      "title": "AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels",
      "authors": "Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang",
      "institution": "Weill Cornell Medicine",
      "link": "https://arxiv.org/pdf/2512.24628",
      "code": null,
      "tags": [
        "medical audio classification",
        "hierarchical classification",
        "acoustic biomarkers",
        "mel-spectrograms",
        "voice disorders",
        "sustained vowels"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp",
      "contributions": "1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.",
      "summary": "This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework's potential as a scalable tool for early voice disorder screening and diagnostic triage.",
      "mindmap": "graph TB\n        A[AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[良性喉部嗓音疾病分类/Benign Laryngeal Voice Disorder Classification]\n        C --> C1[三级分层机器学习框架/Three-Stage Hierarchical ML Framework]\n        C1 --> C1_1[阶段1: 病理筛查/Stage 1: Pathological Screening]\n        C1 --> C1_2[阶段2: 粗粒度分层/Stage 2: Coarse Stratification]\n        C1 --> C1_3[阶段3: 细粒度分类/Stage 3: Fine-Grained Classification]\n        C1_1 --> C1_1a[融合CNN梅尔谱特征与21种声学生物标志物/Integrates CNN Mel-Spectrogram & 21 Acoustic Biomarkers]\n        D --> D1[性能优于平面多类分类器与预训练模型/Outperforms Flat Classifiers & Pre-trained Models (HuBERT, HeAR)]\n        D --> D2[结合深度表征与可解释特征，增强临床可操作性/Enhances Transparency & Clinical Alignment via Deep & Interpretable Features]"
    },
    {
      "title": "A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling",
      "authors": "Malikussaid, Septian Caesar Floresko, Ade Romadhony, Isman Kurniawan, Warih Maharani, Hilal Hudan Nuha",
      "institution": "Telkom University",
      "link": "https://arxiv.org/pdf/2512.24643",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "byte-offset indexing",
        "ensemble modeling",
        "SHAP analysis",
        "heteroskedasticity",
        "stratified modeling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad416775f59ba17a28f8c4aa4f177b114e9e8b7b5d9ab0210d586ba595ee51e6_w640_q70.webp",
      "contributions": "1. Developed a novel computational infrastructure for terabyte-scale data integration, achieving a 740-fold speedup in processing time using a byte-offset indexing architecture. 2. Conducted a comprehensive analysis revealing the multivariate nature of lipophilicity, identifying molecular weight as the most important global predictor via SHAP analysis, despite its weak bivariate correlation. 3. Proposed and validated a stratified modeling strategy (specialized models for drug-like vs. extreme molecules) that achieved optimal predictive performance, demonstrating the competitiveness of well-curated descriptor-based ensemble models with graph neural networks.",
      "summary": "This paper presents a scalable framework for predicting molecular lipophilicity (logP). It introduces a high-performance data integration infrastructure and employs tree-based ensemble models with a stratified strategy, achieving robust prediction accuracy and providing insights into key molecular descriptors. The work shows that carefully engineered traditional machine learning models can remain competitive with advanced neural architectures for this task.",
      "mindmap": "graph TB\n        Root[”A Scalable Framework for logP Prediction<br>可扩展的logP预测框架”] --> Problem[”核心问题/Problem<br>Accurate, scalable logP prediction for drug discovery<br>药物发现中准确、可扩展的logP预测”]\n        Root --> Method[”主要方法/Method<br>Byte-offset indexing & stratified ensemble modeling<br>字节偏移索引与分层集成建模”]\n        Root --> Results[”关键结果/Results<br>740x speedup, robust models competitive with GNNs<br>740倍加速，模型性能与图神经网络相当”]"
    },
    {
      "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation",
      "authors": "Yury Kolomeytsev, Dmitry Golembiovsky",
      "institution": "Lomonosov Moscow State University",
      "link": "https://arxiv.org/pdf/2512.24651",
      "code": null,
      "tags": [
        "robot navigation",
        "hybrid motion planning",
        "deep reinforcement learning",
        "entity-aware reward",
        "graph-based global planner",
        "collision avoidance"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp",
      "contributions": "1. Proposes HMP-DRL, a hybrid framework integrating a graph-based global planner with a local DRL policy via checkpoints. 2. Introduces an entity-aware reward structure for the local planner to ensure social compliance by adjusting safety based on agent type. 3. Validates the method in a realistic simulation, showing superior performance in success rate, collision rate, and time to goal.",
      "summary": "The paper proposes HMP-DRL, a hybrid motion planning framework that combines a graph-based global planner for long-range pathfinding with a local Deep Reinforcement Learning policy for reactive, socially-compliant navigation. The method uses checkpoints to integrate the global path and an entity-aware reward function to dynamically adjust to different moving agents. Experiments in realistic simulation show it outperforms other methods in key navigation metrics, enhancing safety and reliability in complex environments.",
      "mindmap": "graph TB\n        A[Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统图规划器缺乏反应性/Traditional graph planners lack reactivity]\n        B --> B2[深度强化学习方法缺乏全局上下文/DRL methods lack global context]\n        C --> C1[混合框架HMP-DRL/Hybrid framework HMP-DRL]\n        C1 --> C2[图规划器生成路径/Graph planner generates path]\n        C1 --> C3[局部DRL策略使用检查点和实体感知奖励/Local DRL policy uses checkpoints & entity-aware reward]\n        D --> D1[更高的成功率/Higher success rate]\n        D --> D2[更低的碰撞率/Lower collision rate]\n        D --> D3[更短的到达时间/Shorter time to goal]"
    },
    {
      "title": "HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
      "authors": "Honglin Gao, Lan Zhao, Junhao Ren, Xiang Li, Gaoxi Xiao",
      "institution": "Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.24665",
      "code": null,
      "tags": [
        "adversarial machine learning",
        "heterogeneous graph neural networks",
        "backdoor attack",
        "adaptive instance normalization",
        "maximum mean discrepancy",
        "node classification"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ba54f6485be7540924a3b9c9f4089fbeb975b730fbabfbde211ba1bfb4ba09c_w640_q70.webp",
      "contributions": "1. A generative backdoor attack framework (HeteroHBA) for heterogeneous graphs that selects influential trigger attachment points and synthesizes diverse trigger features/connections. 2. A stealthiness enhancement method combining Adaptive Instance Normalization (AdaIN) and Maximum Mean Discrepancy (MMD) loss to align trigger features with benign statistics. 3. A bilevel optimization objective that jointly maximizes attack success rate and preserves clean model accuracy.",
      "summary": "This paper proposes HeteroHBA, a generative backdoor attack method for heterogeneous graph neural networks (HGNNs) that manipulates graph structure and node features to stealthily poison the model. The method uses saliency-based screening and distribution alignment techniques to improve attack effectiveness and stealth. Experiments show HeteroHBA achieves higher attack success than baselines while maintaining clean accuracy, demonstrating significant security risks in heterogeneous graph learning.",
      "mindmap": "graph TB\n        A[HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs] --> B[核心问题/Problem: Backdoor attacks on heterogeneous graphs are understudied.]\n        A --> C[主要方法/Method: Generative trigger synthesis with saliency screening, AdaIN+MMD for stealth, bilevel optimization.]\n        A --> D[关键结果/Results: Higher attack success than baselines, maintains clean accuracy, evades a structural defense.]"
    },
    {
      "title": "Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach",
      "authors": "Reza Jahani, Md Farhamdur Reza, Richeng Jin, Huaiyu Dai",
      "institution": "North Carolina State University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.24694",
      "code": null,
      "tags": [
        "federated learning",
        "decentralized federated learning",
        "user mobility",
        "convergence analysis",
        "data heterogeneity",
        "wireless networks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcaaf30fa2b4fe4f882f45ad096166cc0d22979181161bedc0b1504a8bf7549b_w640_q70.webp",
      "contributions": "1. Established the convergence of Decentralized Federated Learning (DFL) in sparse networks under user mobility, theoretically showing that even random movement can boost performance. 2. Proposed a novel DFL framework that utilizes mobile users with data-distribution-aware induced mobility patterns to enhance information propagation. 3. Provided extensive empirical validation and a comprehensive analysis of how network parameters influence DFL performance in mobile settings.",
      "summary": "This paper investigates the role of user mobility in improving Decentralized Federated Learning (DFL) performance in sparse, heterogeneous networks. It proposes a data-driven DFL framework where mobile users follow induced trajectories to enhance information flow. Theoretical and experimental results show that mobility, even when random, significantly boosts DFL convergence and performance.",
      "mindmap": "graph TB\n        Root[”Mobility-Assisted Decentralized Federated Learning”] --> Problem[”核心问题/Problem: DFL性能受限于稀疏连接和数据异构性”]\n        Root --> Method[”主要方法/Method: 利用数据分布知识引导用户移动性以增强信息传播”]\n        Root --> Results[”关键结果/Results: 理论证明移动性提升收敛性; 实验验证方法优越性”]"
    },
    {
      "title": "Causal Discovery with Mixed Latent Confounding via Precision Decomposition",
      "authors": "Amir Asiaee, Samhita Pal, James O'quinn, James P. Long",
      "institution": "Vanderbilt University Medical Center, Johns Hopkins University, MD Anderson Cancer Center",
      "link": "https://arxiv.org/pdf/2512.24696",
      "code": null,
      "tags": [
        "causal discovery",
        "latent confounding",
        "precision matrix decomposition",
        "DAG learning",
        "identifiability",
        "deconfounding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0fdc733393b17ba1fd45e5626a557d39d8072f5494f4f2325aa8b588ae2f0ee_w640_q70.webp",
      "contributions": "1. Proposed DCL-DECOR, a modular pipeline using precision matrix decomposition to separate pervasive from local latent confounding. 2. Provided identifiability results characterizing recoverable causal structure under mixed confounding. 3. Demonstrated improved directed edge recovery in synthetic experiments over baseline methods.",
      "summary": "The paper addresses causal discovery in linear Gaussian systems with mixed latent confounding, where some confounders affect many variables and others only a few. It introduces DCL-DECOR, a method that decomposes the precision matrix to isolate pervasive confounders and then applies a correlated-noise DAG learner to recover the causal graph. Experiments show it consistently improves edge recovery compared to applying DAG learning directly to confounded data.",
      "mindmap": "graph TB\n        Root[Causal Discovery with Mixed Latent Confounding via Precision Decomposition] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Mixed latent confounding in causal discovery] --> P1[挑战/Challenge: Global confounders mimic causal edges]\n        Problem --> P2[现状/Current Methods: DAG learners or undirected models fail]\n        Method[主要方法/Method: DCL-DECOR pipeline] --> M1[步骤1/Step 1: Precision matrix decomposition]\n        Method --> M2[步骤2/Step 2: Correlated-noise DAG learning]\n        Method --> M3[步骤3/Step 3: Bow-freeness reconciliation]\n        Results[关键结果/Results] --> R1[理论/Theoretical: Identifiability guarantees]\n        Results --> R2[实验/Experimental: Improved edge recovery]"
    },
    {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "authors": "Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni",
      "institution": "Google Research (inferred from authors Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni, who are affiliated with Google)",
      "link": "https://arxiv.org/pdf/2512.24695",
      "code": null,
      "tags": [
        "learning theory",
        "nested learning",
        "in-context learning",
        "continual learning",
        "associative memory",
        "self-modifying model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac4e338b76a10773a96b12a072d809a81c99a79e63d8b40927cb078da7b7fdb_w640_q70.webp",
      "contributions": "1. Expressive Optimizers: Shows gradient-based optimizers are associative memory modules and proposes more expressive variants with deeper memory and learning rules. 2. Self-Modifying Learning Module: Presents a sequence model that learns to modify itself by learning its own update algorithm. 3. Continuum Memory System: Introduces a new memory formulation generalizing long/short-term memory, which is combined with the self-modifying model to create \"Hope\", a continual learning module.",
      "summary": "The paper proposes a new learning paradigm called Nested Learning (NL), which frames machine learning models as nested optimization problems. This view explains the emergence of in-context learning and is used to design more expressive optimizers, a self-modifying model, and a new memory system, culminating in a continual learning module named \"Hope\" that shows promising results on various tasks.",
      "mindmap": "graph TB\n        A[Nested Learning: The Illusion of Deep Learning Architecture] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何实现持续学习与自我改进/How to achieve continual learning and self-improvement]\n        C --> C1[嵌套学习范式/Nested Learning Paradigm]\n        C --> C2[设计表达性优化器/Design Expressive Optimizers]\n        C --> C3[自修改学习模块/Self-Modifying Learning Module]\n        C --> C4[连续体记忆系统/Continuum Memory System]\n        D --> D1[提出持续学习模块Hope/Propose continual learning module Hope]\n        D --> D2[在多个任务上展示潜力/Show potential on multiple tasks]"
    },
    {
      "title": "BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework",
      "authors": "András Millinghoffer, András Formanek, András Antos, Péter Antal",
      "institution": "Budapest University of Technology and Economics, E-Group ICT Software Zrt., KU Leuven",
      "link": "https://arxiv.org/pdf/2512.24708",
      "code": null,
      "tags": [
        "multi-task learning",
        "multi-armed bandit",
        "negative transfer",
        "auxiliary task selection",
        "multi-bandit framework",
        "drug-target interaction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e49fedc6c9b07cd38435ef8daff0ba16207d6088f86e049c77cd192822bd2cf_w640_q70.webp",
      "contributions": "1. A three-stage method (BandiK) for efficient auxiliary task subset selection in multi-task learning, 2. Reduction of candidate auxiliary sets from exponential to linear complexity using pairwise transfer estimations, 3. A novel multi-bandit framework that exploits semi-overlapping arms across tasks to improve computational efficiency.",
      "summary": "The paper introduces BandiK, a three-stage method using multi-armed bandits to efficiently select beneficial auxiliary task subsets in multi-task learning, reducing computational cost by estimating pairwise transfers and leveraging a multi-bandit structure. It is validated on a drug-target interaction benchmark, showing scalable performance for complex multi-task scenarios.",
      "mindmap": "graph TB\n        A[BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework] --> B[核心问题/Problem: 多任务学习中负迁移和辅助任务选择的高计算成本与复杂性]\n        A --> C[主要方法/Method: 三阶段多臂老虎机框架，估计任务间转移、构建线性候选集、利用半重叠臂的多老虎机结构]\n        A --> D[关键结果/Results: 在药物-靶点相互作用基准上验证，实现高效可扩展的任务分解]"
    },
    {
      "title": "FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference",
      "authors": "Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu",
      "institution": "Institute of Information Science, Academia Sinica",
      "link": "https://arxiv.org/pdf/2512.24713",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "N:M structured pruning",
        "4-bit quantization",
        "systolic array",
        "FPGA accelerator",
        "hardware-software co-design"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a594f5eb4f19e15f5f182ee786cc270613c6a3d07553a78731a54b9a3ae90ea_w640_q70.webp",
      "contributions": "1. Proposes an automation framework and unified pipeline for applying N:M structured pruning and 4-bit integer quantization to compress LLMs. 2. Presents a hardware-software co-design method that generates a custom systolic-array-based FPGA accelerator for efficient inference. 3. Demonstrates the synergy of fine-grained sparsity and quantization, achieving significant reductions in storage and latency while offering flexibility beyond fixed hardware sparsity patterns.",
      "summary": "This paper addresses the high computational and memory demands of LLMs by proposing a hardware-software co-design framework. The method combines N:M structured pruning and 4-bit quantization to compress models, and implements a custom FPGA accelerator for efficient inference. The results show significant reductions in storage and latency, demonstrating the effectiveness of the approach for deployable LLM inference.",
      "mindmap": "graph TB\n        Root(”FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LLM部署困难/LLM Deployment Challenge”)\n        P1 --> P2(”高计算与内存需求/High Computation & Memory Requirements”)\n        Method --> M1(”模型压缩/Model Compression”)\n        M1 --> M2(”N:M结构化剪枝与4-bit量化/N:M Structured Pruning & 4-bit Quantization”)\n        Method --> M3(”软硬件协同设计/Hardware-Software Co-Design”)\n        M3 --> M4(”生成基于脉动阵列的FPGA加速器/Generating Systolic-Array-based FPGA Accelerator”)\n        Results --> R1(”存储减少4倍/4x Weight Storage Reduction”)\n        Results --> R2(”矩阵乘法加速1.71倍/1.71x Matrix Multiplication Speedup”)\n        Results --> R3(”端到端延迟降低1.29倍/1.29x End-to-End Latency Reduction”)"
    },
    {
      "title": "From Trial to Deployment: A SEM Analysis of Traveler Adoptions to Fully Operational Autonomous Taxis",
      "authors": "Yutong Cai, Hua Wang",
      "institution": "Singapore University of Technology and Design, Hefei University of Technology",
      "link": "https://arxiv.org/pdf/2512.24767",
      "code": null,
      "tags": [
        "human-computer interaction",
        "transportation systems",
        "Structural Equation Modeling",
        "autonomous taxis",
        "user adoption",
        "survey",
        "latent constructs"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8981785dc74c2d0e8bd1e9be7c18041b4ff904b4eac18913982db535cbd97bd4_w640_q70.webp",
      "contributions": "1. Conducts a study on actual user behavior of a fully operational autonomous taxi service (Baidu Apollo Robotaxi in Wuhan), moving beyond hypothetical scenarios. 2. Identifies and validates six key latent psychological constructs (Trust & Policy Support, Cost Sensitivity, Performance, Behavioral Intention, Lifestyle, Education) influencing adoption using Structural Equation Modeling. 3. Provides empirical evidence that Cost Sensitivity and Behavioral Intention are the strongest positive predictors of adoption frequency, offering insights for real-world policy and service deployment.",
      "summary": "This study investigates the factors influencing traveler adoption of fully operational autonomous taxis by analyzing survey data from actual users of Baidu's Apollo Robotaxi service in Wuhan, China. Using Structural Equation Modeling on 336 valid responses, the research identifies key psychological constructs and finds that Cost Sensitivity and Behavioral Intention are the strongest predictors of adoption frequency. The findings offer empirical support for policymaking and service design to scale autonomous taxi deployments in urban settings.",
      "mindmap": "graph TB\n        A[FROM TRIAL TO DEPLOYMENT: A SEM ANALYSIS OF TRAVELER ADOPTIONS TO FULLY OPERATIONAL AUTONOMOUS TAXIS] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Gap in studying actual user behavior of operational AV services]\n        C[主要方法/Method: Survey & Structural Equation Modeling on real Apollo Robotaxi users]\n        D[关键结果/Results: Cost Sensitivity & Behavioral Intention are strongest adoption predictors]"
    },
    {
      "title": "Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation",
      "authors": "Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono",
      "institution": "Kagoshima University",
      "link": "https://arxiv.org/pdf/2512.24792",
      "code": null,
      "tags": [
        "monocular depth estimation",
        "adversarial attack",
        "physics-in-the-loop optimization",
        "sep-CMA-ES"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp",
      "contributions": "1. Proposes a projection-based adversarial attack method for monocular depth estimation models, using projected light as the perturbation. 2. Employs physics-in-the-loop (PITL) optimization to design perturbations in real-world environments, accounting for device specifications and disturbances. 3. Utilizes a distributed covariance matrix adaptation evolution strategy (sep-CMA-ES) for effective black-box optimization to generate adversarial examples.",
      "summary": "This paper proposes a physical adversarial attack method for monocular depth estimation models. The method projects perturbation light onto a target object and uses physics-in-the-loop optimization with a distributed evolution strategy to create adversarial examples. Experiments confirmed the attack's success, causing depth misestimations that made parts of objects disappear from the scene.",
      "mindmap": "graph TB\n        Root[Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: DNN-based monocular depth estimation models are vulnerable to adversarial attacks, threatening reliability in applications like autonomous systems.]\n        Method[主要方法/Method: Proposes a projection-based attack using physics-in-the-loop optimization and sep-CMA-ES to generate adversarial light perturbations.]\n        Results[关键结果/Results: Successfully created adversarial examples causing depth misestimation, making parts of objects disappear from the scene.]"
    },
    {
      "title": "Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks",
      "authors": "Shota Suzuki, Satoshi Ono",
      "institution": "Kagoshima University",
      "link": "https://arxiv.org/pdf/2512.24793",
      "code": null,
      "tags": [
        "multi-modal training",
        "neural architecture search",
        "self-supervised learning",
        "multimodal fusion",
        "contrastive learning",
        "gradient-based search"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a6c1a495cc476572529c11ecd2d19b6e7849693fc7a5c6941d8e99e91599cc5_w640_q70.webp",
      "contributions": "1. Proposes a self-supervised learning (SSL) method for neural architecture search (NAS) specifically for multimodal deep neural networks. 2. Applies SSL comprehensively to both the architecture search and model pretraining processes, eliminating the need for labeled data during search. 3. Demonstrates that the method can successfully design network architectures from unlabeled training data, achieving performance comparable to supervised NAS methods.",
      "summary": "This paper addresses the problem that neural architecture search (NAS) for multimodal deep neural networks typically requires large amounts of labeled data. The authors propose a self-supervised learning method that uses contrastive learning to perform NAS without labeled data. Experimental results show the method can successfully design effective multimodal network architectures using only unlabeled data.",
      "mindmap": "graph TB\n        Root[Self-supervised Neural Architecture Search for Multimodal Deep Neural Networks] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Multimodal NAS requires substantial labeled data] --> P1[问题细化/Sub-problem: High labeling cost for multimodal data]\n        Method[主要方法/Method: Self-supervised NAS] --> M1[方法基础/Foundation: Gradient-based NAS (BM-NAS)] --> M1_1[技术/Technique: Differential Architecture Search]\n        Method --> M2[自监督机制/SSL Mechanism: Contrastive Learning (SimCLR)] --> M2_1[目标/Objective: Learn from unlabeled data]\n        Results[关键结果/Results: Successfully designed architectures from unlabeled data] --> R1[评估/Evaluation: Comparable to supervised methods]"
    },
    {
      "title": "Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training",
      "authors": "Andrew Tinits, Stephen Mann",
      "institution": "University of Waterloo",
      "link": "https://arxiv.org/pdf/2512.24794",
      "code": null,
      "tags": [
        "image denoising",
        "Noise2Noise",
        "Monte Carlo denoising",
        "high dynamic range",
        "tone mapping",
        "Jensen gap"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7ef23248abb9edf960ef0ea8dac0c52ee12d9db03ab8dd602dfd8c83c62645_w640_q70.webp",
      "contributions": "1. Identified that certain nonlinear functions can be applied to noisy targets in Noise2Noise training without introducing significant bias. 2. Developed a theoretical framework to analyze the effects of nonlinearities and described a class of functions with minimal bias. 3. Demonstrated the method's effectiveness for training Monte Carlo denoisers on HDR images using only noisy data, achieving results comparable to models trained with clean references.",
      "summary": "This paper addresses the limitation of Noise2Noise training where applying nonlinear functions to noisy targets introduces bias. The authors propose a theoretical framework to identify low-bias nonlinearities and apply this to denoise high dynamic range Monte Carlo renderings using tone mapping. Their method, trained only on noisy data, achieves performance close to models trained with clean reference images.",
      "mindmap": "graph TB\n        Root(”Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Noise2Noise训练中非线性函数导致偏差/Bias from nonlinearities in Noise2Noise”)\n        Problem --> P2(”HDR图像训练被异常值干扰/HDR training overwhelmed by outliers”)\n        Method --> M1(”理论分析非线性影响/Theoretical analysis of nonlinear effects”)\n        Method --> M2(”识别低偏差非线性函数类/Identify low-bias nonlinear function class”)\n        Method --> M3(”特定损失与色调映射组合/Specific loss & tone mapping combination”)\n        Results --> R1(”仅用噪声数据训练/Train with only noisy data”)\n        Results --> R2(”性能接近干净数据训练模型/Performance approaches clean-data-trained model”)"
    },
    {
      "title": "Gradient Descent as Implicit EM in Distance-Based Neural Models",
      "authors": "Alan Oursland",
      "institution": "Unknown (Inferred from arXiv identifier only; no explicit affiliation provided)",
      "link": "https://arxiv.org/pdf/2512.24780",
      "code": null,
      "tags": [
        "machine learning theory",
        "gradient descent",
        "expectation-maximization",
        "log-sum-exp",
        "distance-based models",
        "probabilistic inference"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dedc1f2ebe4cae6339387562e130f21772d7b403da5771f698e488286b8cffce_w640_q70.webp",
      "contributions": "1. Provides a direct derivation showing that for objectives with a log-sum-exp structure, the gradient with respect to a distance is exactly the negative posterior responsibility, an algebraic identity. 2. Demonstrates that gradient descent on such objectives implicitly performs expectation-maximization, embedding inference within the optimization process. 3. Unifies learning in unsupervised mixture modeling, attention mechanisms, and supervised classification under a single mechanism, explaining observed Bayesian behaviors as a necessary consequence of objective geometry.",
      "summary": "The paper shows that gradient descent on objectives with a log-sum-exp structure (common in neural networks) is algebraically equivalent to performing expectation-maximization, where the gradient directly corresponds to posterior responsibilities. This finding unifies learning across unsupervised, attention-based, and supervised regimes, explaining probabilistic behaviors like soft clustering as a fundamental property of the objective, not an emergent phenomenon.",
      "mindmap": "graph TB\n        A[Gradient Descent as Implicit EM in Distance-Based Neural Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[神经网络表现出概率推断行为/Neural nets show probabilistic inference behaviors]\n        B --> B2[现有解释不充分/Existing explanations are unsatisfactory]\n        C --> C1[推导梯度与后验责任的恒等式/Derive gradient-posterior identity]\n        C --> C2[分析目标函数的几何结构/Analyze objective function geometry]\n        D --> D1[梯度下降即隐式EM/Gradient descent is implicit EM]\n        D --> D2[统一三种学习机制/Unifies three learning regimes]\n        D --> D3[贝叶斯结构是必然结果/Bayesian structure is necessary]"
    },
    {
      "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
      "authors": "Rongge Xu, Hui Dai, Yiming Fu, Jiedong Jiang, Tianjiao Nie, Hongwei Wang, Junkai Wang, Holiverse Yang, Jiatong Yang, Zhi-Hao Zhang",
      "institution": "Tsinghua University, Southern University of Science and Technology, Westlake University, Xi'an Jiaotong-Liverpool University, The Chinese University of Hong Kong, Yanqi Lake Beijing Institute of Mathematical Sciences and Applications (BIMSA)",
      "link": "https://arxiv.org/pdf/2512.24796",
      "code": "https://github.com/sciencraft/LeanCat",
      "tags": [
        "theorem proving",
        "formal verification",
        "category theory",
        "benchmark",
        "Lean",
        "large language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bb17bb33fa46697baca7f3b3a6262916453dd0a4bf5f92ff26cebdd7d681ffe_w640_q70.webp",
      "contributions": "1. Introduces LeanCat, a benchmark for formal category theory in Lean, designed to stress-test abstraction and library-mediated reasoning. 2. Presents a curated dataset of 100 tasks with topic families and difficulty tiers, created via an LLM-assisted human grading process. 3. Demonstrates the benchmark's utility by evaluating models and the LeanBridge method, showing current AI capabilities and providing a checkpoint for tracking progress.",
      "summary": "The paper introduces LeanCat, a benchmark for formalizing category theory in Lean to better evaluate AI's ability for abstract, library-based reasoning in mathematics. It presents a curated set of 100 tasks and evaluates models, finding low success rates, especially on harder problems, while showing that retrieval-augmented methods like LeanBridge can improve performance. The benchmark serves as a compact checkpoint for tracking progress in research-level formal theorem proving.",
      "mindmap": "graph TB\n        A[LeanCat: A Benchmark Suite for Formal Category Theory in Lean] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>现有基准未能充分衡量抽象和基于库的推理/Current benchmarks under-measure abstraction and library-mediated reasoning]\n        C[主要方法/Method<br>为Lean创建形式化范畴论基准，包含100个分级任务/Create a Lean benchmark for formal category theory with 100 graded tasks]\n        D[关键结果/Results<br>最佳模型pass@1为8.25%，检索增强方法有提升/Best model pass@1 is 8.25%, retrieval-augmented methods show gains]"
    },
    {
      "title": "DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes",
      "authors": "Bence Bolgár, András Millinghoffer, Péter Antal",
      "institution": "None",
      "link": "https://arxiv.org/pdf/2512.24810",
      "code": null,
      "tags": [
        "drug-target interaction prediction",
        "Gaussian processes",
        "deep kernel learning",
        "Bayesian inference",
        "drug-target interaction",
        "Bayesian precedence matrix"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5db80a2ae688e29179cef03704df858a81feee85a09984daebbb301bc3ea93a0_w640_q70.webp",
      "contributions": "1. Proposed DTI-GP, a deep kernel Gaussian process architecture for drug-target interaction prediction that combines neural embeddings with Bayesian inference. 2. Introduced novel Bayesian operations, including classification with rejection, top-K selection, and ranking, enabled by sampling from the predictive distribution. 3. Demonstrated superior performance over state-of-the-art methods and enabled new evaluation metrics like a Bayesian accuracy-confidence enrichment score.",
      "summary": "This paper addresses the need for precise probabilistic predictions in drug-target interaction (DTI) tasks. It proposes DTI-GP, a method that integrates deep neural embeddings for drugs and proteins with a Gaussian process module to enable scalable Bayesian inference. The approach outperforms existing methods and facilitates novel operations like confidence-aware rejection and ranking.",
      "mindmap": "graph TB\n        Root[DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Need for precise probabilistic DTI predictions] --> P1[理解限制与提升性能/Understanding limitations & boosting performance]\n        Method[主要方法/Method: Deep Kernel Gaussian Process] --> M1[神经嵌入模块/Neural embedding module for compounds & proteins]\n        Method --> M2[高斯过程模块/GP module for Bayesian inference]\n        Method --> M3[采样预测分布/Sampling predictive distribution for Bayesian precedence matrix]\n        Results[关键结果/Results] --> R1[性能超越SOTA/Outperforms state-of-the-art]\n        Results --> R2[支持新操作/Enables novel operations: rejection, top-K, ranking]\n        Results --> R3[构建贝叶斯富集分数/Constructs Bayesian accuracy-confidence enrichment score]"
    },
    {
      "title": "Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback",
      "authors": "Shulun Chen, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du",
      "institution": "Tsinghua University, University of Washington, HKUST",
      "link": "https://arxiv.org/pdf/2512.24818",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Nash equilibrium",
        "Optimistic Multiplicative Weights Update",
        "duality gap",
        "last-iterate convergence",
        "non-transitive preferences"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad0c58b07c97ac061247ceb0efd9cdb7a7663982811f67d347d7d126d158bebc_w640_q70.webp",
      "contributions": "1. Provides the first convergence guarantee for Optimistic Multiplicative Weights Update (OMWU) in the Nash Learning from Human Feedback (NLHF) setting, showing it achieves last-iterate linear convergence to the original Nash equilibrium after a burn-in phase when a full-support NE exists. 2. Removes the prior assumption of Nash equilibrium uniqueness required by related work. 3. Identifies a novel marginal convergence behavior where the probability of rarely played actions grows exponentially from small values, leading to an exponentially better dependence on instance-dependent constants.",
      "summary": "This paper addresses the challenge of aligning LLMs with non-transitive human preferences by framing it as a zero-sum game in the NLHF framework. It proposes using the unregularized Optimistic Multiplicative Weights Update (OMWU) algorithm and proves it achieves last-iterate linear convergence to the original Nash equilibrium without requiring uniqueness assumptions. The theoretical findings are supported by experiments on tabular and neural policy classes, demonstrating the method's potential for LLM alignment.",
      "mindmap": "graph TB\n        A[Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Standard preference models assume transitivity, overlooking non-transitive human preferences in LLM alignment.]\n        C[主要方法/Method: Use unregularized Optimistic Multiplicative Weights Update (OMWU) to find Nash equilibrium in the NLHF zero-sum game.]\n        D[关键结果/Results: OMWU achieves last-iterate linear convergence to the original NE without uniqueness assumption, with a novel marginal convergence behavior.]"
    },
    {
      "title": "Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics",
      "authors": "Raul D. Steleac, Mohan Sridharan, David Abel",
      "institution": "University of Edinburgh, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.24827",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "option discovery",
        "graph Laplacian",
        "state abstraction",
        "coordination",
        "Fermat state"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fbcbdbf3ff73a3709f40a0baeb20e2f256323634f984656081dbac35380c30_w640_q70.webp",
      "contributions": "1. Proposes a novel joint-state abstraction method that compresses the state space while preserving information needed for discovering strongly coordinated behaviors. 2. Introduces the concept of a \"Fermat state\" (a fictitious state of maximal team alignment) and a \"spreadness\" measure to quantify team-level misalignment. 3. Employs a neural graph Laplacian estimator on this representation to derive options that capture state synchronization patterns between agents.",
      "summary": "This paper addresses the challenge of discovering coordinated, temporally extended actions (options) in multi-agent systems, where the joint state space grows exponentially. The proposed method introduces a joint-state abstraction based on agent state synchronization, using a \"Fermat state\" and \"spreadness\" measure, and then applies a neural graph Laplacian estimator to discover options. The resulting options demonstrate stronger downstream coordination capabilities compared to existing methods in multi-agent domains.",
      "mindmap": "graph TB\n        A[Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics<br/>通过智能体间相对动力学发现协调的联合选项] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[多智能体选项发现挑战<br/>Multi-agent Option Discovery Challenge]\n        B1 --> B2[状态空间指数增长<br/>Exponential State Space Growth]\n        B1 --> B3[现有方法缺乏协调<br/>Existing Methods Lack Coordination]\n        C --> C1[联合状态抽象<br/>Joint-State Abstraction]\n        C1 --> C2[费马状态与扩散度<br/>Fermat State & Spreadness]\n        C2 --> C3[神经图拉普拉斯估计器<br/>Neural Graph Laplacian Estimator]\n        D --> D1[更强的协调能力<br/>Stronger Coordination Capabilities]\n        D1 --> D2[优于现有方法<br/>Outperforms Alternative Methods]"
    },
    {
      "title": "AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference",
      "authors": "Linhao Fan, Hongqiang Fang, Jingyang Dai, Yong Jiang, Qixing Zhang",
      "institution": "University of Science and Technology of China, National Institute of Standards and Technology",
      "link": "https://arxiv.org/pdf/2512.24847",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion-based Bayesian inference",
        "corruption-aware training",
        "decoupled annealing posterior sampling",
        "uncertainty quantification",
        "spatiotemporal prior"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c1c9e0ed0baf5b503de97c6a15b2d7601232aba8c5c2f231a32ee44d046afcf_w640_q70.webp",
      "contributions": "1. Proposes AODDiff, a probabilistic reconstruction framework using diffusion-based Bayesian inference to learn a spatiotemporal prior for AOD fields. 2. Introduces a corruption-aware training strategy to learn the prior from naturally incomplete data, eliminating the need for complete training data. 3. Employs a decoupled annealing posterior sampling strategy to effectively integrate heterogeneous observations as constraints for flexible task adaptation without retraining.",
      "summary": "The paper proposes AODDiff, a diffusion-based Bayesian inference framework for probabilistically reconstructing Aerosol Optical Depth fields. It learns a spatiotemporal prior from incomplete data and uses a novel sampling strategy to integrate observations for tasks like downscaling and inpainting. Experiments show it maintains high fidelity and enables uncertainty quantification.",
      "mindmap": "graph TB\n        A[AODDiff: Probabilistic Reconstruction of AOD] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据稀缺与不确定性/Data Scarcity & Uncertainty]\n        C --> C1[扩散贝叶斯推断/Diffusion-based Bayesian Inference]\n        C1 --> C2[损坏感知训练/Corruption-aware Training]\n        C1 --> C3[解耦退火采样/Decoupled Annealing Sampling]\n        D --> D1[高保真度/High Fidelity]\n        D --> D2[不确定性量化/Uncertainty Quantification]"
    },
    {
      "title": "Characterization of Transfer Using Multi-task Learning Curves",
      "authors": "András Millinghoffer, Bence Bolgár, Péter Antal",
      "institution": "Budapest University of Technology and Economics",
      "link": "https://arxiv.org/pdf/2512.24866",
      "code": null,
      "tags": [
        "multi-task learning",
        "multi-task learning curves",
        "task affinity grouping",
        "transfer effects",
        "inductive inference",
        "foundation models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7a18c81d007e0d8e8e143a6ef66be8aaaafd4edddc20571b598040702aaa801_w640_q70.webp",
      "contributions": "1. Proposes using multi-task learning curves (approximating performance over varying sample sizes) as a fundamental method to characterize transfer effects, complementing gradient-based training analysis. 2. Describes an efficient method to approximate these multi-task learning curves, analogous to the Task Affinity Grouping method. 3. Compares statistical (learning curve) and computational (training-based) approaches to transfer, finding the former has lower compute cost while the latter has better power and broader applicability, and demonstrates its utility on a drug-target interaction dataset and for analyzing foundation models.",
      "summary": "This paper proposes characterizing transfer effects in machine learning by analyzing multi-task learning curves, which model inductive performance as sample size varies, rather than solely through gradient updates during training. It introduces an efficient method to approximate these curves and compares this statistical approach to traditional computational methods. The results show that learning curves effectively capture multi-task learning effects and can delineate transfer in foundation models, offering a complementary and more computationally efficient perspective.",
      "mindmap": "graph TB\n        A[Characterization of Transfer Using Multi-task Learning Curves] --> B(核心问题/Problem: How to fundamentally characterize transfer effects beyond gradient-based training analysis?)\n        A --> C(主要方法/Method: Use multi-task learning curves over varying sample sizes; efficient approximation method analogous to Task Affinity Grouping)\n        A --> D(关键结果/Results: Learning curves better capture MTL effects; method delineates transfer in foundation models; statistical approach has lower compute cost)"
    },
    {
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "authors": "Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang",
      "institution": "DeepSeek-AI",
      "link": "https://arxiv.org/pdf/2512.24880",
      "code": null,
      "tags": [
        "llm training",
        "Hyper-Connections",
        "residual connection",
        "identity mapping",
        "manifold constraint",
        "training stability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp",
      "contributions": "1. Proposes Manifold-Constrained Hyper-Connections (mHC), a framework that projects the residual connection space onto a specific manifold to restore the identity mapping property compromised by Hyper-Connections (HC). 2. Incorporates rigorous infrastructure optimization to address the memory access overhead and ensure training efficiency. 3. Demonstrates that mHC enables effective large-scale training with tangible performance improvements and superior scalability, offering a flexible and practical extension of HC.",
      "summary": "The paper identifies that Hyper-Connections (HC), while improving performance, lose the identity mapping property of standard residual connections, leading to training instability and memory overhead. To solve this, the authors propose Manifold-Constrained Hyper-Connections (mHC), which projects HC's connection space onto a manifold to restore identity mapping and includes infrastructure optimizations. Empirical results show mHC is effective for scalable training, offering better performance and stability.",
      "mindmap": "graph TB\n        A[mHC: Manifold-Constrained Hyper-Connections] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”HC 破坏了恒等映射，导致训练不稳定/HC compromises identity mapping, causing instability”]\n        B --> B2[”HC 带来内存开销/HC incurs memory overhead”]\n        C --> C1[”将残差连接空间投影到特定流形/Project residual space onto a manifold”]\n        C --> C2[”恢复恒等映射属性/Restore identity mapping property”]\n        C --> C3[”结合基础设施优化/Incorporate infrastructure optimization”]\n        D --> D1[”实现可扩展的有效训练/Enables effective training at scale”]\n        D --> D2[”提供性能改进和可扩展性/Offers performance improvements & scalability”]"
    },
    {
      "title": "PRISM: A hierarchical multiscale approach for time series forecasting",
      "authors": "Zihao Chen, Alexandre Andre, Wenrui Ma, Ian Knight, Sergey Shuvaev, Eva Dyer",
      "institution": "University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.24898",
      "code": "https://github.com/nerdslab/prism",
      "tags": [
        "time series forecasting",
        "hierarchical modeling",
        "multiscale decomposition",
        "wavelet transform",
        "tree-based partitioning",
        "multivariate forecasting"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a39698d43f8debac5a07c7c8b7b2c886add93fdb385fed95a191b3257708859_w640_q70.webp",
      "contributions": "1. Proposes PRISM, a novel hierarchical forecasting method using a learnable tree-based partitioning of time series signals. 2. Introduces a joint time-frequency decomposition (e.g., wavelets) at each tree level to extract and aggregate scale-specific features. 3. Demonstrates a lightweight and flexible framework that outperforms state-of-the-art methods on benchmark datasets.",
      "summary": "The paper addresses the challenge of forecasting time series with multiscale features by proposing PRISM, a method that uses a learnable tree structure to hierarchically partition the signal and apply time-frequency transforms at each level. This approach jointly captures global trends and local dynamics, leading to improved forecasting accuracy. Experiments show that PRISM outperforms existing state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[PRISM: 时间序列预测 / Time Series Forecasting]\n        Root --> Problem[核心问题: 多尺度特征预测 / Problem: Forecasting Multiscale Features]\n        Root --> Method[主要方法: 可学习树状分层 / Method: Learnable Tree-based Hierarchy]\n        Root --> Results[关键结果: 性能超越SOTA / Results: Outperforms SOTA]\n        Problem --> P1[全局趋势与局部结构 / Global Trends & Local Structure]\n        Method --> M1[时间-频率分解 / Time-Frequency Decomposition]\n        Method --> M2[特征跨层次聚合 / Feature Aggregation Across Hierarchy]\n        Results --> R1[轻量灵活框架 / Lightweight & Flexible Framework]"
    },
    {
      "title": "Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes",
      "authors": "Debasis Maji, Arghya Banerjee, Debaditya Barman",
      "institution": "Visva-Bharati, Institute of Engineering & Management",
      "link": "https://arxiv.org/pdf/2512.24901",
      "code": "https://github.com/gnnplayground/SpectralBrainGNN",
      "tags": [
        "neuroimaging analysis",
        "Graph Neural Network",
        "Spectral Convolution",
        "Graph Fourier Transform",
        "fMRI Connectome",
        "Cognitive Classification"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef6720522974da991b45f55d20e9895bdadedeae152a749a83e04947607b62bd_w640_q70.webp",
      "contributions": "1. Proposed SpectralBrainGNN, a novel spectral convolution framework for brain network analysis. 2. Applied Graph Fourier Transform (GFT) computed via normalized Laplacian eigendecomposition to model functional connectivity. 3. Demonstrated state-of-the-art cognitive task classification performance (96.25% accuracy) on the HCP-Task dataset.",
      "summary": "The paper addresses cognitive task classification from fMRI brain networks. It proposes SpectralBrainGNN, a spectral graph neural network model based on Graph Fourier Transforms, to capture complex connectivity patterns. The method achieves high classification accuracy on a standard dataset, showing its effectiveness for decoding brain states.",
      "mindmap": "graph TB\n        A[”Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes”] --> B[”核心问题/Problem: Decoding brain states from neuroimaging data”]\n        A --> C[”主要方法/Method: SpectralBrainGNN using Graph Fourier Transform (GFT)”]\n        A --> D[”关键结果/Results: 96.25% classification accuracy on HCP-Task dataset”]"
    },
    {
      "title": "Frequent subgraph-based persistent homology for graph classification",
      "authors": "Xinyang Chen, Amaël Broustet, Guoting Chen",
      "institution": "Harbin Institute of Technology, Shenzhen; Université de Lille, Laboratoire Painlevé (CNRS UMR 8524); Great Bay University",
      "link": "https://arxiv.org/pdf/2512.24917",
      "code": null,
      "tags": [
        "graph representation learning",
        "persistent homology",
        "frequent subgraph mining",
        "graph neural networks",
        "graph filtration",
        "topological data analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027326f2cb71708e2df7a0cf125f5d22c8db2742b7d53533a738ac9ae412b01d_w640_q70.webp",
      "contributions": "1. Proposed a novel Frequent Subgraph Filtration (FSF) method to generate frequency-based persistent homology features for graphs. 2. Developed two graph classification frameworks: an FPH-based machine learning model (FPH-ML) and a hybrid framework integrating FPH with GNNs (FPH-GNNs). 3. Provided theoretical analysis and experimental validation showing performance improvements over baseline methods.",
      "summary": "This paper addresses the limitation of existing persistent homology methods on graphs, which rely on simple filtrations and miss recurring structural patterns. The authors propose a Frequent Subgraph Filtration (FSF) to extract richer topological features and integrate them into both traditional ML and GNN models for graph classification. Experiments show that the proposed methods achieve competitive or superior accuracy, with the hybrid FPH-GNN framework yielding significant performance gains over standard GNN backbones.",
      "mindmap": "graph TB\n        A[Frequent subgraph-based persistent homology for graph classification] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有持久同源方法过滤有限，忽略数据集中重复出现的子图特征/Existing PH methods use limited filtrations, overlooking recurring subgraph features.]\n        C --> C1[提出基于频繁子图的过滤方法(FSF)，生成频率持久同源(FPH)特征/Propose Frequent Subgraph Filtration (FSF) to generate Frequency-based Persistent Homology (FPH) features.]\n        C --> C2[构建FPH-ML和FPH-GNN混合框架进行图分类/Build FPH-ML and hybrid FPH-GNN frameworks for graph classification.]\n        D --> D1[FPH-ML达到竞争性或更优的准确率/FPH-ML achieves competitive or superior accuracy.]\n        D --> D2[FPH-GNN相比GCN/GIN基线获得显著性能提升/FPH-GNN yields significant performance gains over GCN/GIN backbones.]"
    },
    {
      "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline",
      "authors": "Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi",
      "institution": "Huawei Poisson Lab",
      "link": "https://arxiv.org/pdf/2512.24933",
      "code": null,
      "tags": [
        "agent system",
        "prompt optimization",
        "multi-step LLM pipeline",
        "Shapley value",
        "text-gradient estimation",
        "dependency modeling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0120693b7eaf05e640c60779fef913238cda72212cee4971942ac26a248c12d_w640_q70.webp",
      "contributions": "1. Proposes ADOPT, a framework that explicitly models the dependency between each LLM step and the final task outcome for precise text-gradient estimation. 2. Decouples textual gradient estimation from gradient updates, reducing complex multi-prompt optimization to flexible single-prompt optimization steps. 3. Employs a Shapley-based mechanism to adaptively allocate optimization resources across different pipeline steps.",
      "summary": "The paper addresses the challenge of jointly optimizing prompts in multi-step LLM pipelines, where missing step-level supervision and inter-step dependencies make optimization difficult. It proposes ADOPT, an Adaptive Dependency-aware Prompt Optimization framework that models step dependencies for precise gradient estimation and uses a Shapley-based resource allocation mechanism. Experiments show ADOPT is effective and robust, consistently outperforming existing prompt optimization methods.",
      "mindmap": "graph TB\n        Root[ADOPT: Adaptive Dependency-aware Prompt Optimization Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Multi-step LLM pipeline prompt optimization is difficult due to missing supervision and dependencies] --> P1[子问题/Sub-problem: Missing step-level supervision]\n        Problem --> P2[子问题/Sub-problem: Inter-step dependencies]\n        Method[主要方法/Method: ADOPT Framework] --> M1[关键技术/Key Technique: Explicit dependency modeling for text-gradient estimation]\n        Method --> M2[关键技术/Key Technique: Decouples gradient estimation from updates]\n        Method --> M3[关键技术/Key Technique: Shapley-based adaptive resource allocation]\n        Results[关键结果/Results: Effective and robust, outperforms SOTA baselines] --> R1[实验/Experiments: Real-world datasets]\n        Results --> R2[实验/Experiments: Diverse pipeline structures]"
    },
    {
      "title": "Iterative Deployment Improves Planning Skills in LLMs",
      "authors": "Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal",
      "institution": "University of Oxford, AI Sequrity Company, UFRGS",
      "link": "https://arxiv.org/pdf/2512.24940",
      "code": null,
      "tags": [
        "reinforcement learning",
        "iterative deployment",
        "implicit reward",
        "data curation",
        "planning",
        "fine-tuning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp",
      "contributions": "1. Demonstrates that iterative deployment and fine-tuning on curated user data significantly improves LLM planning skills, including emergent generalization to longer plans. 2. Provides a theoretical analysis showing iterative deployment effectively implements an outer-loop reinforcement learning process with an implicit reward function. 3. Highlights the AI safety implications of this implicit training regime and positions it as an alternative to explicit RL training.",
      "summary": "The paper shows that repeatedly deploying LLMs and fine-tuning them on curated data from previous deployments significantly improves their planning capabilities. This process is analyzed as an implicit form of reinforcement learning, which raises safety concerns due to the undefined reward function and offers an alternative training paradigm based on data curation.",
      "mindmap": "graph TB\n        A[Iterative Deployment Improves Planning Skills in LLMs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLM规划能力/LLM Planning Skills]\n        C --> C1[迭代部署与微调/Iterative Deployment & Fine-tuning]\n        C1 --> C2[用户数据筛选/User Data Curation]\n        D --> D1[规划能力提升/Improved Planning Skills]\n        D --> D2[发现隐式RL/Discovering Implicit RL]\n        D2 --> D3[AI安全影响/AI Safety Implications]"
    },
    {
      "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
      "authors": "Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng",
      "institution": "Taobao & Tmall Group of Alibaba",
      "link": "https://arxiv.org/pdf/2512.24943",
      "code": null,
      "tags": [
        "information retrieval",
        "relevance assessment",
        "benchmark",
        "long-tail",
        "visual salience",
        "e-commerce"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp",
      "contributions": "1. Proposes RAIR, a comprehensive Chinese benchmark for e-commerce relevance assessment derived from real-world scenarios. 2. Establishes a standardized evaluation framework with universal rules to address the lack of standardized metrics. 3. Introduces a dataset with three specialized subsets (general, long-tail hard, visual salience) to evaluate fundamental, challenging, and multimodal capabilities.",
      "summary": "The paper proposes RAIR, a rule-aware benchmark for e-commerce search relevance assessment, to address the lack of complex and standardized evaluation datasets. It introduces a comprehensive dataset with three subsets to test different model capabilities. Experiments on 14 models show RAIR is challenging, with GPT-5 performing best, and it serves as a new industry benchmark.",
      "mindmap": "graph TB\n        A[RAIR: 一个用于电子商务相关性评估的规则感知基准 / RAIR: A Rule-Aware Benchmark for E-commerce Relevance Assessment]\n        A --> B[核心问题/Problem: 现有基准缺乏复杂性，缺少标准化评估 / Existing benchmarks lack complexity and standardized evaluation]\n        A --> C[主要方法/Method: 提出包含通用、长尾、视觉显著性子集的基准和规则框架 / Propose benchmark with general, long-tail, visual-salience subsets and rule framework]\n        A --> D[关键结果/Results: 对14个模型构成挑战，GPT-5表现最佳，可作为行业基准 / Presents challenge to 14 models, GPT-5 performs best, serves as industry benchmark]"
    },
    {
      "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control",
      "authors": "Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She",
      "institution": "Beihang University",
      "link": "https://arxiv.org/pdf/2512.24955",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Lyapunov certificates",
        "exponential stability",
        "multi-step learning",
        "actor-critic",
        "maximum entropy RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp",
      "contributions": "1. Proposes a novel framework (MSACL) that integrates exponential stability theory with maximum entropy RL via multi-step Lyapunov certificate learning, using off-policy data to learn certificates that satisfy theoretical stability conditions. 2. Introduces Exponential Stability Labels (ESL) and a λ-weighted aggregation mechanism to effectively balance the bias-variance trade-off in multi-step learning. 3. Guides policy optimization with a stability-aware advantage function to ensure the learned policy promotes rapid Lyapunov descent, achieving provable stability and robustness under simple rewards.",
      "summary": "This paper proposes MSACL, a model-free reinforcement learning framework that ensures provable exponential stability by learning Lyapunov certificates from multi-step data and guiding policy optimization with a stability-aware advantage. It demonstrates superior performance over baseline and state-of-the-art Lyapunov-based RL methods across six benchmarks, achieving rapid convergence and robustness with simple rewards. The work establishes a link between Lyapunov theory and actor-critic frameworks for verifiably safe control.",
      "mindmap": "graph TB\n        A[MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Provable Stability in Model-Free RL / 模型无关RL的可证明稳定性]\n        C --> C1[Multi-Step Lyapunov Certificate Learning / 多步李雅普诺夫证书学习]\n        C --> C2[Stability-Aware Advantage Function / 稳定性感知优势函数]\n        D --> D1[Superiority over SOTA / 优于现有最优方法]\n        D --> D2[Exponential Stability & Robustness / 指数稳定性与鲁棒性]"
    },
    {
      "title": "ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT",
      "authors": "Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou",
      "institution": "Northwestern University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24948",
      "code": null,
      "tags": [
        "medical image analysis",
        "diffusion models",
        "motion correction",
        "synthetic data",
        "coronary artery calcium",
        "non-gated CT"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd53987de700ad7024fad5ea5d53da57ecb375707cda1bff594c7264ec4ca118_w640_q70.webp",
      "contributions": "1. A CAC motion simulation data engine that synthesizes realistic non-gated CT acquisitions from gated CTs for supervised training without paired data. 2. A property-aware learning strategy that incorporates calcium-specific priors via a differentiable consistency loss to preserve lesion integrity. 3. A progressive correction scheme that gradually reduces motion artifacts across diffusion steps to enhance stability and calcium fidelity.",
      "summary": "The paper proposes ProDM, a diffusion model framework to correct motion artifacts in coronary calcium lesions from non-gated chest CT scans. The method uses a synthetic data engine for training, incorporates calcium-specific priors, and applies progressive correction. Experiments show it improves scoring accuracy, lesion fidelity, and risk stratification compared to baselines.",
      "mindmap": "graph TB\n        A[ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[非门控CT中冠脉钙化运动伪影影响评分/Non-gated CT CAC scoring affected by motion artifacts]\n        C --> C1[合成数据引擎/Synthetic Data Engine]\n        C --> C2[属性感知学习/Property-aware Learning]\n        C --> C3[渐进式校正/Progressive Correction]\n        D --> D1[提高评分准确性和病灶保真度/Improved scoring accuracy & lesion fidelity]\n        D --> D2[提升风险分层性能/Enhanced risk stratification]\n        D --> D3[抑制伪影，提升临床可用性/Suppresses artifacts & improves clinical usability]"
    },
    {
      "title": "Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning",
      "authors": "András Antos, András Millinghoffer, Péter Antal",
      "institution": "Budapest University of Technology and Economics, E-Group ICT Software Zrt.",
      "link": "https://arxiv.org/pdf/2512.24959",
      "code": null,
      "tags": [
        "multi-armed bandits",
        "semi-overlapping multi-bandit",
        "best arm identification",
        "sequential support network learning",
        "GapE algorithm",
        "sample complexity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1ccf2e1312507d052da8a3c6c2b6fb042432d3f13e5efa2572b5d2cd1dff292_w640_q70.webp",
      "contributions": "1. Proposes a new pure-exploration model called the semi-overlapping multi-bandit (SOMMAB) for Sequential Support Network Learning (SSNL)., 2. Develops a generalized GapE algorithm for the SOMMAB setting., 3. Derives new exponential error bounds that improve the best-known constant in the exponent and scale linearly with the degree of overlap, showing sample complexity gains from shared evaluations.",
      "summary": "This paper introduces a new framework called Sequential Support Network Learning (SSNL) and models it as a semi-overlapping multi-bandit (SOMMAB) problem, where a single evaluation provides feedback to multiple bandits. The authors develop a generalized GapE algorithm for SOMMABs and prove new, improved error bounds that demonstrate significant sample-complexity reductions due to structural overlap.",
      "mindmap": "graph TB\n        Root[”Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Selecting beneficial partners via shared, asymmetric evaluations”] --> P1[”问题领域/Application Domains: MTL, ATL, FL, MAS”]\n        Method[”主要方法/Method: SOMMAB model & generalized GapE algorithm”] --> M1[”模型/Model: Semi-overlapping multi-bandit”]\n        Results[”关键结果/Results: Improved error bounds & sample complexity gains”] --> R1[”理论保证/Theoretical: Exponential bounds scale with overlap”]"
    },
    {
      "title": "Efficiently Estimating Data Efficiency for Language Model Fine-tuning",
      "authors": "Gyung Hyun Je, Colin Raffel",
      "institution": "University of Toronto",
      "link": "https://arxiv.org/pdf/2512.24991",
      "code": "https://github.com/r-three/dataefficiency",
      "tags": [
        "fine-tuning",
        "data efficiency",
        "gradient cosine similarity",
        "low-confidence examples",
        "fine-tuning scaling",
        "annotation cost"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd5fa15acf63fdcf1cd6c628775da72e936b364cb45a18adddcde2e329ce8c48_w640_q70.webp",
      "contributions": "1. Introduced a concrete metric to quantify a task's data efficiency for LLM fine-tuning. 2. Proposed a method using gradient cosine similarity of low-confidence examples to predict data efficiency from a small number of samples. 3. Validated the approach on diverse tasks, achieving 8.6% prediction error and significantly reducing unnecessary annotations.",
      "summary": "The paper addresses the problem of unknown data efficiency for fine-tuning LLMs, which leads to costly annotation cycles. It proposes predicting data efficiency using gradient cosine similarity from low-confidence examples, based on a small labeled set. The method achieves low prediction error and can eliminate hundreds of unnecessary annotations per task.",
      "mindmap": "graph TB\n        A[Efficiently Estimating Data Efficiency for Language Model Fine-tuning] --> B[核心问题/Problem: Unknown data efficiency for fine-tuning leads to costly annotation cycles]\n        A --> C[主要方法/Method: Predict using gradient cosine similarity of low-confidence examples]\n        A --> D[关键结果/Results: 8.6% prediction error, eliminates hundreds of annotations]"
    },
    {
      "title": "Attribution-Guided Distillation of Matryoshka Sparse Autoencoders",
      "authors": "Cristina P. Martin-Linares, Jonathan P. Ling",
      "institution": "Johns Hopkins University",
      "link": "https://arxiv.org/pdf/2512.24975",
      "code": null,
      "tags": [
        "llm training",
        "sparse autoencoders",
        "mechanistic interpretability",
        "gradient attribution",
        "feature distillation",
        "matryoshka"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a691c55852370e5366257008c9914761d773cede53c49ece8c170d5653fcbac4_w640_q70.webp",
      "contributions": "1. Introduces Distilled Matryoshka Sparse Autoencoders (DMSAEs), a novel training pipeline for distilling a compact, reusable core of consistent features from sparse autoencoders. 2. Proposes an iterative, attribution-guided distillation cycle that uses gradient × activation to select the most useful features based on their contribution to next-token loss. 3. Demonstrates empirically that the distilled core improves SAEBench metrics and enables the transfer of consistent latent features across different sparsity levels.",
      "summary": "This paper addresses the problem of feature redundancy and inconsistency in sparse autoencoders (SAEs) used for mechanistic interpretability. It proposes Distilled Matryoshka Sparse Autoencoders (DMSAEs), a method that iteratively distills a core set of useful features using gradient-based attribution. The results show that this approach yields a stable, compact feature set that improves performance and can be transferred across training runs and sparsity levels.",
      "mindmap": "graph TB\n        A[Attribution-Guided Distillation of Matryoshka Sparse Autoencoders] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: SAE特征冗余且不一致/SAE features are redundant and inconsistent]\n        C[主要方法/Method: 基于归因的迭代蒸馏循环/Attribution-guided iterative distillation cycle]\n        D[关键结果/Results: 获得紧凑、可迁移的特征核心/Obtains a compact, transferable feature core]"
    },
    {
      "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
      "authors": "Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)",
      "link": "https://arxiv.org/pdf/2512.24985",
      "code": null,
      "tags": [
        "embodied vision-language reasoning",
        "low-light vision",
        "embodied question answering",
        "vision-language models",
        "image enhancement",
        "benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp",
      "contributions": "1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.",
      "summary": "This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.",
      "mindmap": "graph TB\n        A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --> B[核心问题/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]\n        A --> C[主要方法/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]\n        A --> D[关键结果/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]"
    },
    {
      "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
      "authors": "Haozhe Jiang, Nika Haghtalab, Lijie Chen",
      "institution": "University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.25014",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion language models",
        "parallel sampling",
        "chain-of-thought",
        "remasking",
        "revision"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp",
      "contributions": "1. Formalized a model of parallel sampling and proved that DLMs with CoT can simulate any parallel sampling algorithm with an optimal number of sequential steps. 2. Showed that enabling remasking or revision with CoT allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. 3. Established a strict expressivity gap, proving DLMs with revision or remasking are strictly more expressive than those without.",
      "summary": "This paper provides a theoretical foundation for the efficiency of Diffusion Language Models (DLMs) as parallel samplers. It proves that DLMs augmented with chain-of-thought reasoning can simulate any parallel sampling algorithm with optimal sequential steps and, when further equipped with token remasking or revision, also achieve optimal space complexity. The results theoretically justify DLMs as highly efficient parallel samplers and advocate for enabling revision capabilities in such models.",
      "mindmap": "graph TB\n        Root[”Diffusion Language Models are Provably Optimal Parallel Samplers<br>扩散语言模型是可证明最优的并行采样器”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DLMs的理论优势与效率极限未明<br>Theoretical advantages and efficiency limits of DLMs are unclear”] --> P1[”并行采样效率/Parallel Sampling Efficiency”]\n        Problem --> P2[”空间复杂度/Space Complexity”]\n        Method[”主要方法/Method<br>形式化并行采样模型与电路复杂度<br>Formalize parallel sampling model & circuit complexity”] --> M1[”增强CoT/Augment with CoT”]\n        Method --> M2[”引入重掩码或修订/Introduce Remasking or Revision”]\n        Results[”关键结果/Results”] --> R1[”最优顺序步骤/Optimal Sequential Steps”]\n        Results --> R2[”最优空间复杂度/Optimal Space Complexity”]\n        Results --> R3[”严格表达能力差距/Strict Expressivity Gap”]"
    },
    {
      "title": "Convergence of the generalization error for deep gradient flow methods for PDEs",
      "authors": "Chenguang Liu, Antonis Papapantoleon, Jasper Rou",
      "institution": "(Inferred from author names and typical affiliations in the field; specific institutions not listed on the first page. Could be academic institutions like universities.)",
      "link": "https://arxiv.org/pdf/2512.25017",
      "code": null,
      "tags": [
        "scientific machine learning",
        "deep gradient flow methods",
        "generalization error",
        "partial differential equations",
        "neural networks",
        "approximation error"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ade90edaf9660be616e42f9a752fe8dcdd594d87db2454f913bca0a983f2d8bc_w640_q70.webp",
      "contributions": "1. Provides a rigorous mathematical framework for analyzing the generalization error of Deep Gradient Flow Methods (DGFMs) for solving PDEs. 2. Proves that the approximation error for PDE solutions using neural networks converges to zero as the network width increases. 3. Derives and analyzes the gradient flow dynamics in the wide network limit, showing the training error converges as training time increases.",
      "summary": "This paper establishes a mathematical foundation for Deep Gradient Flow Methods (DGFMs) used to solve high-dimensional PDEs. It decomposes the generalization error into approximation and training components, proving both converge to zero as the number of neurons and training time go to infinity. The main conclusion is that DGFMs are theoretically sound, with provable convergence of the total error.",
      "mindmap": "graph TB\n        A[Convergence of the generalization error for deep gradient flow methods for PDEs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[为DGFM提供数学基础/Provide mathematical foundation for DGFMs]\n        C --> C1[分解泛化误差/Decompose generalization error]\n        C1 --> C2[近似误差分析/Analyze approximation error]\n        C1 --> C3[训练误差分析/Analyze training error]\n        D --> D1[近似误差趋于零/Approximation error tends to zero]\n        D --> D2[训练误差在极限下收敛/Training error converges in limit]\n        D --> D3[泛化误差趋于零/Generalization error tends to zero]"
    },
    {
      "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
      "authors": "Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier",
      "institution": "LMU Munich, University of Konstanz",
      "link": "https://arxiv.org/pdf/2512.25023",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (RLHF)",
        "preference strength",
        "reward modeling",
        "sample efficiency",
        "utility difference",
        "Pearson Distance Correlation (PDC)"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/89532a897b8fa7db270c20a989bfbc8848f6809665ba966305a08daa55266fce_w640_q70.webp",
      "contributions": "1. ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals. 2. Empirical evidence of improved sample efficiency and robustness across diverse tasks. 3. The Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.",
      "summary": "The paper addresses the limitation of standard RLHF, which only captures the direction of a preference but not its strength. It proposes ResponseRank, a method that learns preference strength by ranking responses using relative differences in noisy proxy signals (like response times) within local strata. The method demonstrates improved sample efficiency and robustness across synthetic, language modeling, and RL control tasks.",
      "mindmap": "graph TB\n        A[ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[标准RLHF仅提供偏好方向/Standard RLHF only provides preference direction]\n        B --> B2[偏好强度难以可靠测量/Preference strength is hard to measure reliably]\n        C --> C1[利用代理信号(如响应时间)/Leverage proxy signals (e.g., response time)]\n        C --> C2[局部层内相对比较/Local within-strata relative comparison]\n        C --> C3[排序推断强度/Rank to infer strength]\n        D --> D1[提升样本效率与鲁棒性/Improved sample efficiency & robustness]\n        D --> D2[提出新评估指标PDC/Proposed new metric PDC]"
    },
    {
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "authors": "Alexander C. Li, Ananya Kumar, Deepak Pathak",
      "institution": "Carnegie Mellon University, Stanford University",
      "link": "https://arxiv.org/pdf/2512.25034",
      "code": "https://github.com/alexlioralexli/generative-classifiers",
      "tags": [
        "generative models",
        "generative classifiers",
        "spurious correlations",
        "distribution shift",
        "diffusion models",
        "autoregressive models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp",
      "contributions": "1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.",
      "summary": "The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.",
      "mindmap": "graph TB\n        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]\n        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]\n        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]\n        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]\n        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]"
    },
    {
      "title": "Reliable and Resilient Collective Communication Library for LLM Training and Serving",
      "authors": "Wei Wang, Nengneng Yu, Sixian Xiong, Zaoxing Liu",
      "institution": "University of Maryland, College Park",
      "link": "https://arxiv.org/pdf/2512.25059",
      "code": "https://github.com/r2cc-project/R-2CCL",
      "tags": [
        "communication & networking",
        "fault-tolerant collective communication",
        "multi-NIC failover",
        "connection migration",
        "bandwidth-aware load redistribution",
        "resilient collective algorithms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd037366831b8135233f491feff11095f79c30662f36cb517794f6588542c452_w640_q70.webp",
      "contributions": "1. A fault-tolerant communication library (R²CCL) that provides lossless, low-overhead failover by exploiting multi-NIC hardware. 2. Techniques including rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under network failures. 3. Demonstrated high robustness to NIC failures with minimal overhead (&lt;1% for training, &lt;3% for inference) and significant performance improvements over baselines.",
      "summary": "The paper presents R²CCL, a fault-tolerant collective communication library designed to handle network faults in large-scale LLM training and serving. It achieves low-overhead recovery through techniques like rapid connection migration and bandwidth-aware load redistribution. Evaluation shows it incurs minimal performance overhead and significantly outperforms existing solutions.",
      "mindmap": "graph TB\n        Root[Reliable and Resilient Collective Communication Library for LLM Training and Serving] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Network faults waste GPU hours and cause job failures in large-scale ML clusters]\n        Method[主要方法/Method: R²CCL library with rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms]\n        Results[关键结果/Results: <1% training overhead, <3% inference overhead, 12.18x-47x faster than baselines]"
    },
    {
      "title": "On the geometry and topology of representations: the manifolds of modular addition",
      "authors": "Gabriela Moisescu-Pareja, Gavin McCracken, Harley Wiltzer, Vincent Létourneau, Colin Daniels, Doina Precup, Jonathan Love",
      "institution": "McGill University, Mila, Université de Montréal, Leiden University, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.25060",
      "code": null,
      "tags": [
        "mechanistic interpretability",
        "modular addition",
        "manifold hypothesis",
        "topological analysis",
        "circuit universality",
        "attention mechanisms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17fc5efae344878642fe3a2ab7985eeb71b49efef00cf839b1bb65714a390090_w640_q70.webp",
      "contributions": "1. Demonstrates that different neural network architectures (uniform vs. learnable attention) learn topologically and geometrically equivalent representations for modular addition, refuting prior claims of disparate circuits. 2. Introduces a methodology that studies learned representations as collective manifolds rather than interpreting individual neurons, applying tools from topology. 3. Provides statistical analysis across hundreds of circuits to show the similarity of learned modular addition algorithms under common deep learning paradigms, supporting the universality hypothesis.",
      "summary": "This paper challenges the claim that different neural network architectures learn fundamentally different circuits for modular addition. By analyzing learned representations as manifolds using topological methods, the authors show that both uniform and trainable attention architectures implement the same underlying algorithm with equivalent representations. The work provides statistical evidence supporting the universality hypothesis in mechanistic interpretability.",
      "mindmap": "graph TB\n        A[On the geometry and topology of representations: the manifolds of modular addition] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Do different architectures learn distinct circuits for modular addition?]\n        C --> C1[Analyze representations as manifolds using topology/将表示作为流形进行拓扑分析]\n        D --> D1[Uniform and learnable attention learn the same algorithm/均匀和可学习注意力学习相同算法]\n        D --> D2[Representations are topologically & geometrically equivalent/表示在拓扑和几何上等价]"
    },
    {
      "title": "Many Minds from One Model: Bayesian Transformers for Population Intelligence",
      "authors": "Diji Yang, Yi Zhang",
      "institution": "University of California Santa Cruz",
      "link": "https://arxiv.org/pdf/2512.25063",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Bayesian Transformers",
        "Variational Inference",
        "Population Diversity",
        "Normalization Layers",
        "Wisdom of Crowds"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0113a2c263c7e9a33e3b5ce49ac7afb88b3a3baeb7fd88c121fac5ef4b745b_w640_q70.webp",
      "contributions": "1. Proposes Population Bayesian Transformers (B-Trans), a method to convert a standard LLM into a Bayesian model by treating normalization layer biases as stochastic variables with a Gaussian variational approximation, enabling diverse model sampling from a single weight set. 2. Introduces sequence-level noise freezing to maintain temporal coherence within each sampled model instance's generation, ensuring consistent behavior across tokens. 3. Demonstrates that aggregating predictions from a population of sampled B-Trans instances enhances exploration and decision-making, leading to superior semantic diversity and task performance in zero-shot generation, RLVR, and RL without labels.",
      "summary": "The paper addresses the lack of diversity and exploration in deterministic LLMs by proposing B-Trans, which transforms a standard LLM into a Bayesian model by making normalization biases stochastic. This allows sampling diverse \"minds\" from one model, and aggregating their predictions improves performance and semantic variety in reasoning tasks.",
      "mindmap": "graph TB\n        A[Many Minds from One Model: Bayesian Transformers for Population Intelligence] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现代Transformer是单一思维的/Modern Transformers are single-minded]\n        B --> B2[缺乏多样性阻碍探索/Lack of diversity hinders exploration]\n        C --> C1[提出B-Trans: 贝叶斯Transformer/Propose B-Trans: Bayesian Transformer]\n        C --> C2[归一化层偏置作为随机变量/Normalization biases as stochastic variables]\n        C --> C3[序列级噪声冻结/Sequence-level noise freezing]\n        D --> D1[增强语义多样性/Improved semantic diversity]\n        D --> D2[提升任务性能/Better task performance]\n        D --> D3[实现群体智慧/Achieves wisdom of crowds]"
    },
    {
      "title": "Scaling Open-Ended Reasoning to Predict the Future",
      "authors": "Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping",
      "institution": "Max Planck Institute for Intelligent Systems, ELLIS Institute Tübingen, Tübingen AI Center, University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.25070",
      "code": "/github",
      "tags": [
        "language model forecasting",
        "open-ended forecasting",
        "reinforcement learning",
        "retrieval-augmented generation",
        "calibration",
        "Qwen3"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7433331ffccb9fb0f52db33a75b12ae808ae87c5410037274fcfdc5f22b3505_w640_q70.webp",
      "contributions": "1. A fully automated pipeline to synthesize a large-scale dataset (OpenForesight) for training language models on open-ended forecasting questions from news events. 2. A specialized forecasting system integrating retrieval and an improved RL reward function, trained on Qwen3, which prevents future information leakage. 3. The OpenForecaster 8B model, which demonstrates that specialized training improves accuracy, calibration, and consistency, matching larger proprietary models, with calibration benefits generalizing to other benchmarks.",
      "summary": "This paper addresses the challenge of training language models for open-ended future prediction. The authors propose an automated method to generate a large forecasting dataset from news and train a specialized model (OpenForecaster 8B) using retrieval and an improved RL reward. Their final model matches the performance of much larger proprietary models, showing improved prediction accuracy, calibration, and consistency.",
      "mindmap": "graph TB\n        A[Scaling Open-Ended Reasoning to Predict the Future<br>预测未来的开放式推理扩展] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[训练语言模型进行开放式未来预测<br>Train LLMs for open-ended future forecasting]\n        C --> C1[自动从新闻生成数据集<br>Automated dataset generation from news]\n        C --> C2[使用检索和改进的RL进行训练<br>Training with retrieval & improved RL]\n        C --> C3[防止未来信息泄露<br>Prevent future info leakage]\n        D --> D1[OpenForecaster 8B 匹配更大模型<br>Matches larger proprietary models]\n        D --> D2[提升准确性、校准和一致性<br>Improves accuracy, calibration, consistency]"
    },
    {
      "title": "Coordinated Humanoid Manipulation with Choice Policies",
      "authors": "Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik",
      "institution": "UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.25072",
      "code": "https://choice-policy.github.io",
      "tags": [
        "imitation learning",
        "humanoid robot",
        "teleoperation",
        "choice policy",
        "multimodal behavior",
        "whole-body coordination"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp",
      "contributions": "1. A modular teleoperation interface that decomposes humanoid control into intuitive submodules (e.g., hand-eye coordination, locomotion) for efficient, high-quality data collection. 2. The Choice Policy, a novel imitation learning architecture that generates multiple candidate actions and learns to score them, enabling fast inference and effective modeling of multimodal behaviors. 3. Empirical validation on real-world tasks (dishwasher loading, whiteboard wiping) showing superior performance over diffusion policies and behavior cloning, and highlighting the critical role of hand-eye coordination.",
      "summary": "This paper tackles the challenge of achieving robust whole-body coordination for humanoid robots in unstructured environments. It proposes a system combining a modular teleoperation interface for data collection with a novel \"Choice Policy\" for imitation learning, which scores multiple candidate actions. Experiments on real-world tasks demonstrate that this approach outperforms baseline methods and that hand-eye coordination is crucial for success in long-horizon manipulation.",
      "mindmap": "graph TB\n        A[Coordinated Humanoid Manipulation with Choice Policies] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”实现人形机器人头、手、腿的鲁棒全身协调/Robust whole-body coordination for humanoids”]\n        C --> C1[”模块化遥操作接口/Modular teleoperation interface”]\n        C --> C2[”选择策略：生成并评估候选动作/Choice Policy: generate & score candidate actions”]\n        D --> D1[”在洗碗机装载、白板擦拭任务上超越基线/Outperforms baselines on dishwasher loading & whiteboard wiping”]\n        D --> D2[”手眼协调对长时域任务至关重要/Hand-eye coordination is critical for long-horizon tasks”]"
    },
    {
      "title": "Spike-Timing-Dependent Plasticity for Bernoulli Message Passing",
      "authors": "Sepideh Adamiat, Wouter M. Kouw, Bert de Vries",
      "institution": "TU Eindhoven, Lazy Dynamics B.V.",
      "link": "https://arxiv.org/pdf/2512.23728",
      "code": null,
      "tags": [
        "computational neuroscience",
        "spike-timing-dependent plasticity",
        "Bayesian inference",
        "message passing",
        "factor graphs",
        "spiking neural networks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a449542e05c85b78a1c2c36d2343549e26e8c17a9edfccf34c692fd2512f710c_w640_q70.webp",
      "contributions": "1. Bridging Bayesian inference and spike-based neural computation by designing spiking neural networks for Bernoulli message passing. 2. Employing spike-timing-dependent plasticity (STDP), a biologically plausible Hebbian learning rule, to train these networks. 3. Demonstrating the approach's versatility by applying it to a factor graph example from coding theory for signal transmission over an unreliable channel.",
      "summary": "This paper bridges Bayesian inference and spike-based neural activity by designing spiking neural networks that perform message passing for Bernoulli variables. The networks are trained using the biologically plausible spike-timing-dependent plasticity rule. The results show the network's performance matches the true numerical solution, and the method is demonstrated on a coding theory problem.",
      "mindmap": "graph TB\n        A[Spike-Timing-Dependent Plasticity for Bernoulli Message Passing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[连接贝叶斯推理与脉冲神经活动/Bridging Bayesian inference and spike-based neural activity]\n        C --> C1[设计用于伯努利消息传递的脉冲神经网络/Designing SNNs for Bernoulli message passing]\n        C --> C2[使用脉冲时序依赖可塑性进行训练/Using STDP for training]\n        D --> D1[网络性能匹配真实数值解/Network performance matches true numerical solution]\n        D --> D2[方法应用于编码理论因子图/Method applied to coding theory factor graph]"
    },
    {
      "title": "Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting",
      "authors": "Lars van der Laan, Nathan Kallus",
      "institution": "University of Washington, Netflix, Cornell University",
      "link": "https://arxiv.org/pdf/2512.23805",
      "code": null,
      "tags": [
        "reinforcement learning",
        "off-policy evaluation",
        "fitted Q-evaluation",
        "Bellman completeness",
        "stationary distribution",
        "density ratio"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0334a3bea1166b5e2cbda1d446e2561bf91e6e45af4da23da7bb2759aa9a5043_w640_q70.webp",
      "contributions": "1. Identified the fundamental norm mismatch causing FQE's reliance on Bellman completeness, 2. Proposed a simple fix by reweighting regression steps with an estimated stationary density ratio, 3. Provided strong evaluation guarantees without requiring realizability or Bellman completeness, avoiding geometric error blow-up.",
      "summary": "This paper addresses the fragility of Fitted Q-evaluation (FQE) in off-policy reinforcement learning, which traditionally requires the strong assumption of Bellman completeness. The authors propose a simple modification to FQE by reweighting each regression step using an estimate of the stationary density ratio, aligning the optimization with the contractive norm of the Bellman operator. This enables robust policy evaluation guarantees even when the function class is not Bellman complete, maintaining the practicality of regression-based methods.",
      "mindmap": "graph TB\n        A[Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[标准FQE需要贝尔曼完备性假设 / Standard FQE requires Bellman completeness]\n        B --> B2[假设过强且难以满足 / Assumption is strong and hard to satisfy]\n        C --> C1[识别根本的范数不匹配 / Identify fundamental norm mismatch]\n        C --> C2[使用平稳密度比重新加权回归步骤 / Reweight regression steps using stationary density ratio]\n        D --> D1[无需贝尔曼完备性的强保证 / Strong guarantees without Bellman completeness]\n        D --> D2[避免几何误差爆炸 / Avoid geometric error blow-up]"
    },
    {
      "title": "Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware",
      "authors": "Seyed Mohamad Ali Tousi, Adib Bazgir, Yuwen Zhang, G. N. DeSouza",
      "institution": "University of Missouri",
      "link": "https://arxiv.org/pdf/2512.23817",
      "code": null,
      "tags": [
        "others",
        "quantum error mitigation",
        "attention graph neural network",
        "NISQ hardware",
        "Burgers equation",
        "zero-noise extrapolation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57bb32110f5a354755f846f1b5a7ab41ac3fc4a701e97b9b25486f2bab0c72eb_w640_q70.webp",
      "contributions": "1. A hybrid quantum-classical framework for solving the viscous Burgers equation on NISQ hardware, using the Cole-Hopf transformation and Trotterized quantum circuits. 2. The creation of a large parametric dataset of noisy, ZNE-corrected, hardware, and classical solutions with circuit metadata for data-driven error mitigation. 3. A novel attention-based graph neural network model that ingests circuit features and noisy outputs to predict error-mitigated solutions, outperforming ZNE alone.",
      "summary": "This paper proposes a hybrid quantum-classical framework enhanced with a learned error mitigation model to solve the Burgers equation on noisy quantum hardware. The method uses an attention graph neural network trained on a dataset of noisy quantum simulations to predict corrected solutions. The results show the learned model consistently reduces errors beyond standard zero-noise extrapolation techniques.",
      "mindmap": "graph TB\n        A[Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[在含噪声量子硬件上求解Burgers方程/Solving Burgers Equation on Noisy Quantum Hardware]\n        C --> C1[混合量子-经典框架与注意力图神经网络/Hybrid Quantum-Classical Framework with Attention GNN]\n        D --> D1[学习模型超越ZNE，减少量子-经典解差异/Learned Model Outperforms ZNE, Reduces Quantum-Classical Discrepancy]"
    },
    {
      "title": "A Test of Lookahead Bias in LLM Forecasts",
      "authors": "Zhenyu Gao, Wenxi Jiang, Yutong Yan",
      "institution": "The Chinese University of Hong Kong (CUHK Business School, Department of Finance)",
      "link": "https://arxiv.org/pdf/2512.23847",
      "code": null,
      "tags": [
        "machine learning for finance",
        "lookahead bias",
        "pre-training data detection",
        "forecast accuracy",
        "Lookahead Propensity (LAP)",
        "statistical test"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3862b50bacdf9663a9e884b32c3a91e6601a609ccd21f8f622e4d85c8d4b3dee_w640_q70.webp",
      "contributions": "1. Proposes a novel statistical test to detect lookahead bias in LLM-generated economic forecasts. 2. Introduces the concept of Lookahead Propensity (LAP), a metric estimating the likelihood a prompt was in the model's training data. 3. Demonstrates the test's application on real-world forecasting tasks (stock returns and capital expenditures) to assess forecast validity.",
      "summary": "This paper develops a statistical test to detect lookahead bias in LLM forecasts by correlating forecast accuracy with a new metric called Lookahead Propensity (LAP), which estimates if a prompt was in the training data. The method is applied to financial forecasting tasks, providing a cost-efficient tool to assess the reliability of LLM-generated predictions.",
      "mindmap": "graph TB\n        A[A Test of Lookahead Bias in LLM Forecasts] --> B[核心问题/Problem: Detecting lookahead bias in LLM economic forecasts]\n        A --> C[主要方法/Method: Use pre-training data detection to compute Lookahead Propensity (LAP)]\n        A --> D[关键结果/Results: Positive correlation between LAP and accuracy indicates lookahead bias; Test applied to financial tasks]"
    },
    {
      "title": "Energy-Tweedie: Score meets Score, Energy meets Energy",
      "authors": "Andrej Leban",
      "institution": "University of Michigan",
      "link": "https://arxiv.org/pdf/2512.23818",
      "code": null,
      "tags": [
        "diffusion models",
        "Tweedie's formula",
        "energy score",
        "elliptical distributions",
        "score estimation",
        "denoising"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ff6ceee3e2856f05799a33cb8c1de1d291f58a0c5ebc90275070d02dddc961d_w640_q70.webp",
      "contributions": "1. Extends Tweedie's identity beyond the exponential family to a broad class of noising distributions (energy models/elliptical distributions). 2. Derives a fundamental identity connecting the Stein score of the noisy marginal to the (path-) derivative of the energy score. 3. Proposes a practical score estimation method based on this identity, using samples from the denoising posterior.",
      "summary": "This paper connects the concepts of denoising and score estimation by extending Tweedie's formula to elliptical distributions and deriving a new identity linking the energy score's derivative to the Stein score. This allows for new applications in score estimation, noise parameter estimation, and enables the use of energy score models with a wider variety of noising distributions in diffusion model samplers.",
      "mindmap": "graph TB\n        A[Energy-Tweedie: Score meets Score, Energy meets Energy] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[连接去噪与评分规则/Linking Denoising and Scoring Rules]\n        C --> C1[扩展Tweedie公式至椭圆分布/Extend Tweedie's Formula to Elliptical Distributions]\n        C --> C2[推导能量评分与Stein评分的关系/Derive Identity Between Energy Score and Stein Score]\n        D --> D1[提出基于新恒等式的评分估计方法/Propose Score Estimation Method]\n        D --> D2[支持更广泛的噪声分布/Enable Wider Array of Noising Distributions]"
    },
    {
      "title": "Tensor Computing Interface: An Application-Oriented, Lightweight Interface for Portable High-Performance Tensor Network Applications",
      "authors": "Rong-Yang Sun, Tomonori Shirakawa, Hidehiko Kohshiro, D. N. Sheng, Seiji Yunoki",
      "institution": "California State University Northridge, RIKEN",
      "link": "https://arxiv.org/pdf/2512.23917",
      "code": null,
      "tags": [
        "compiler & ir",
        "tensor networks",
        "high-performance computing",
        "portable interface",
        "tensor linear-algebra",
        "Cytnx"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa77bed89109740ba89e21e2beb0128aa40959ecc783f216c3104fb3a4a59038_w640_q70.webp",
      "contributions": "1. Introduces the Tensor Computing Interface (TCI), a lightweight, application-oriented API for framework-independent tensor network applications. 2. Provides a well-defined type system and a minimal, expressive set of core functions abstracting tensor objects and operations. 3. Demonstrates through numerical experiments that TCI enables seamless code migration across heterogeneous platforms while maintaining performance comparable to native implementations.",
      "summary": "The paper addresses the lack of portability in tensor network applications due to framework-specific dependencies. It proposes the Tensor Computing Interface (TCI), a lightweight API that abstracts tensor operations, enabling developers to write portable, high-performance code. The authors demonstrate that TCI-based applications can be migrated across different hardware and software backends without sacrificing performance.",
      "mindmap": "graph TB\n        Root(”Tensor Computing Interface (TCI)”) --> Problem(”核心问题/Problem: Lack of unified interface limits tensor network application portability”)\n        Root --> Method(”主要方法/Method: Lightweight, application-oriented API with abstracted tensor type system and core functions”)\n        Root --> Results(”关键结果/Results: Enables seamless cross-platform migration with native-level performance”)"
    },
    {
      "title": "A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe",
      "authors": "Wendong Yao, Binhua Huang, Soumyabrata Dev",
      "institution": "ADAPT SFI Research Centre, University College Dublin",
      "link": "https://arxiv.org/pdf/2512.23906",
      "code": null,
      "tags": [
        "remote sensing image analysis",
        "InSAR",
        "Transformer",
        "ground deformation forecasting",
        "cross-site generalization",
        "multimodal learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp",
      "contributions": "1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.",
      "summary": "This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.",
      "mindmap": "graph TB\n        Root[论文标题: A multimodal Transformer for InSAR-based ground deformation forecasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 如何利用历史InSAR数据预测未来的地表形变?] --> P1[挑战/Challenges: 长期趋势、季节周期、突变事件的叠加]\n        Method[主要方法/Method: 多模态Transformer] --> M1[输入/Inputs: 近期形变图、静态运动学指标、时间编码]\n        Method --> M2[任务/Task: 单步、固定间隔的下一时期临近预报]\n        Results[关键结果/Results] --> R1[性能/Performance: RMSE=0.90mm, R²=0.97 (爱尔兰测试集)]\n        Results --> R2[泛化/Generalization: 跨欧洲5个未见区域，R²≥0.93]"
    },
    {
      "title": "Assessing generative modeling approaches for free energy estimates in condensed matter",
      "authors": "Maximilian Schebek, Jiajun He, Emil Hoffmann, Yuanqi Du, Frank Noé, Jutta Rogal",
      "institution": "Freie Universität Berlin, University of Cambridge, Cornell University, Microsoft Research AI for Science, Rice University, Flatiron Institute",
      "link": "https://arxiv.org/pdf/2512.23930",
      "code": null,
      "tags": [
        "generative models",
        "free energy estimation",
        "normalizing flows",
        "generative modeling",
        "Jarzynski equality",
        "molecular simulation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e09c407083c9249376083548ea23d622f173cbe2b1df29b4bfcf42731dca47e3_w640_q70.webp",
      "contributions": "1. Systematic review and benchmarking of generative-model-based methods for free energy estimation in condensed matter systems., 2. Evaluation of discrete and continuous normalizing flows, FEAT, and the escorted Jarzynski equality on coarse-grained ice and Lennard-Jones solids., 3. Provides a quantitative framework comparing accuracy, data efficiency, cost, and scalability to guide method selection.",
      "summary": "This paper addresses the computationally expensive challenge of estimating free energy differences in molecular simulations by systematically benchmarking generative modeling approaches like normalizing flows. It evaluates these methods on condensed-matter systems to assess their trade-offs in efficiency, accuracy, and scalability. The main conclusion provides a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.",
      "mindmap": "graph TB\n        A[Assessing generative modeling approaches for free energy estimates in condensed matter] --> B[核心问题/Problem: Accurate free energy estimation is computationally expensive]\n        A --> C[主要方法/Method: Benchmark generative models (normalizing flows, FEAT, escorted Jarzynski)]\n        A --> D[关键结果/Results: Provides a quantitative framework for method selection]"
    },
    {
      "title": "Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration",
      "authors": "Lars van der Laan, Nathan Kallus",
      "institution": "University of Washington, Netflix, Cornell University",
      "link": "https://arxiv.org/pdf/2512.23927",
      "code": null,
      "tags": [
        "reinforcement learning",
        "fitted Q-iteration",
        "entropy regularization",
        "stationary distribution",
        "Bellman operator",
        "offline RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a133d7f0a68e796e5601b9dd17517ab3deea2ceb5f9dce008c265e4c615a1b43_w640_q70.webp",
      "contributions": "1. Identified a geometric mismatch causing instability in soft FQI, showing the soft Bellman operator is contractive in the stationary norm of the soft-optimal policy, not the behavior norm. 2. Proposed stationary-reweighted soft FQI, a method that reweights regression updates using the current policy's stationary distribution to restore contraction. 3. Provided a theoretical analysis proving local linear convergence under function approximation with damped weight-estimation errors and suggested a continuation approach for global convergence via temperature annealing.",
      "summary": "This paper addresses the instability of entropy-regularized fitted Q-iteration (soft FQI) under function approximation and distribution shift in offline reinforcement learning. The authors propose a new method, stationary-reweighted soft FQI, which reweights updates using the policy's stationary distribution to restore local contraction. They prove local linear convergence and suggest that global convergence can be achieved by gradually reducing the softmax temperature.",
      "mindmap": "graph TB\n        A[Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration] --> B[核心问题/Problem: Soft FQI instability under function approximation & distribution shift]\n        A --> C[主要方法/Method: Stationary-reweighted soft FQI (reweights updates using policy's stationary distribution)]\n        A --> D[关键结果/Results: Local linear convergence proven; global convergence via temperature annealing suggested]"
    },
    {
      "title": "Fundamental limits for weighted empirical approximations of tilted distributions",
      "authors": "Sarvesh Ravichandran Iyer, Himadri Mandal, Dhruman Gupta, Rushil Gupta, Agniv Bandhyopadhyay, Achal Bassamboo, Varun Gupta, Sandeep Juneja",
      "institution": "Ashoka University, Indian Statistical Institute, Kolkata, Tata Institute of Fundamental Research, Northwestern University, The University of Utah",
      "link": "https://arxiv.org/pdf/2512.23979",
      "code": null,
      "tags": [
        "Monte Carlo simulation",
        "rare event estimation",
        "exponential tilting",
        "self-normalized importance sampling",
        "rare event simulation",
        "scaling limits",
        "weighted empirical approximations"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2b2078a5f12e67b3138f44c52f21fc16fdfbf4166041f6980ae84c0a56a3c52_w640_q70.webp",
      "contributions": "1. Provides a sharp asymptotic characterization of the accuracy of a self-normalized importance sampler for tilted distributions. 2. Establishes a fundamental dichotomy in sample complexity: polynomial scaling for bounded random vectors vs. super-polynomial scaling for unbounded ones. 3. Analyzes the efficiency of data-driven tilting when the underlying distribution is unknown but samples are available.",
      "summary": "This paper analyzes the efficiency of generating samples from a tilted distribution when only samples from the original, unknown distribution are available, using a self-normalized importance sampling method. It provides a precise characterization of the estimator's accuracy based on sample size and tilt degree. The key finding is a dichotomy in sample complexity: it grows polynomially for bounded random vectors but super-polynomially for unbounded ones.",
      "mindmap": "graph TB\n        Root(”Fundamental limits for weighted empirical approximations of tilted distributions<br>倾斜分布加权经验近似的根本极限”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”从倾斜分布生成样本<br>Generating samples from tilted distribution”)\n        Problem --> P2(”基础分布未知<br>Underlying distribution unknown”)\n        Method --> M1(”自归一化重要性采样<br>Self-normalized importance sampler”)\n        Results --> R1(”有界变量: 样本量多项式增长<br>Bounded: Polynomial sample growth”)\n        Results --> R2(”无界变量: 样本量超多项式增长<br>Unbounded: Super-polynomial sample growth”)"
    },
    {
      "title": "Implicit geometric regularization in flow matching via density weighted Stein operators",
      "authors": "Shinto Eguchi",
      "institution": "The Institute of Statistical Mathematics",
      "link": "https://arxiv.org/pdf/2512.23956",
      "code": null,
      "tags": [
        "generative models",
        "flow matching",
        "density weighting",
        "Stein metric",
        "Sobolev regularization",
        "continuous normalizing flows"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51edeed5e04fe354ae4150f7b6f2ac284b3b71da9bc820ece3883dd4198be9ce_w640_q70.webp",
      "contributions": "1. Proposes γ-Flow Matching (γ-FM), a density-weighted variant of Flow Matching that aligns regression geometry with the underlying probability flow to address inefficiencies in high-dimensional void regions., 2. Introduces a Dynamic Density-Weighting strategy that estimates the target density from training particles, enabling dynamic downweighting of the loss in void regions without requiring explicit density evaluation., 3. Theoretically establishes that γ-FM minimizes transport cost on a statistical manifold with the γ-Stein metric and shows it induces implicit Sobolev regularization, leading to smoother vector fields and improved sampling efficiency.",
      "summary": "This paper identifies that standard Flow Matching is inefficient in high dimensions due to unweighted regression over low-density \"void\" regions. To solve this, it proposes γ-Flow Matching, a method that dynamically weights the regression loss based on estimated target density, aligning the geometry with the probability flow. The approach theoretically connects to the γ-Stein metric, induces implicit regularization, and empirically improves vector field smoothness and sampling efficiency.",
      "mindmap": "graph TB\n        Root[Implicit geometric regularization in flow matching via density weighted Stein operators] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[标准FM在低密度区域效率低下/Standard FM inefficient in low-density 'void' regions]\n        Method[主要方法/Method] --> M1[提出γ-Flow Matching (γ-FM)/Propose γ-Flow Matching (γ-FM)]\n        Method --> M2[动态密度加权策略/Dynamic Density-Weighting strategy]\n        Results[关键结果/Results] --> R1[理论: 最小化γ-Stein度量下的传输成本/Theory: Minimizes transport cost under γ-Stein metric]\n        Results --> R2[谱分析: 诱导隐式Sobolev正则化/Spectral analysis: Induces implicit Sobolev regularization]\n        Results --> R3[实证: 改善向量场平滑度与采样效率/Empirical: Improves vector field smoothness & sampling efficiency]"
    },
    {
      "title": "Constructive Approximation of Random Process via Stochastic Interpolation Neural Network Operators",
      "authors": "Sachin Saini, Uaday Singh",
      "institution": "Indian Institute of Technology Roorkee",
      "link": "https://arxiv.org/pdf/2512.24106",
      "code": null,
      "tags": [
        "function approximation",
        "stochastic interpolation",
        "neural network operators",
        "sigmoidal functions",
        "mean square approximation",
        "modulus of continuity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f705010d1796a2048a2e675bb4dc8e1bca921170079aa88325df4b4644ffd64a_w640_q70.webp",
      "contributions": "1. Constructed a novel class of Stochastic Interpolation Neural Network Operators (SINNOs) with random coefficients., 2. Established theoretical properties of SINNOs, including boundedness, interpolation accuracy, and approximation capabilities in mean square, probability, and path-wise senses., 3. Provided quantitative error estimates for the approximation using the modulus of continuity of the stochastic processes.",
      "summary": "This paper proposes Stochastic Interpolation Neural Network Operators (SINNOs) to approximate random processes. The method constructs operators with random coefficients and sigmoidal activations, proving their theoretical approximation properties and providing error bounds. The results demonstrate SINNOs' effectiveness for approximating stochastic processes, with potential applications like COVID-19 case prediction.",
      "mindmap": "graph TB\n        A[Constructive Approximation of Random Process via Stochastic Interpolation Neural Network Operators] --> B(核心问题/Problem: Approximating stochastic processes)\n        A --> C(主要方法/Method: Stochastic Interpolation Neural Network Operators (SINNOs) with random coefficients)\n        A --> D(关键结果/Results: Proven boundedness, interpolation, and approximation capabilities with error estimates)"
    },
    {
      "title": "Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data",
      "authors": "Wenye Li, Hongxu Chen, Jiacai Liu, Ke Wei",
      "institution": "Fudan University",
      "link": "https://arxiv.org/pdf/2512.24056",
      "code": null,
      "tags": [
        "reinforcement learning",
        "policy mirror descent",
        "temporal difference learning",
        "sample complexity",
        "Markov decision process",
        "policy optimization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e127c9df69a866d0fceeb04827980cde5bc823d8ff492633bf924e6720d9ce1_w640_q70.webp",
      "contributions": "1. Proposes two novel algorithms (Expected TD-PMD and Approximate TD-PMD) that combine policy mirror descent with TD learning under online Markovian sampling. 2. Establishes an $\\tilde\\{O\\}(\\varepsilon^\\{-2\\})$ sample complexity for achieving average-time $\\varepsilon$-optimality with a constant step size. 3. Improves sample complexity to $O(\\varepsilon^\\{-2\\})$ for last-iterate $\\varepsilon$-optimality using adaptive policy update step sizes.",
      "summary": "This paper studies the sample complexity of policy mirror descent combined with temporal difference learning under online Markovian data. It introduces two algorithms, Expected TD-PMD and Approximate TD-PMD, and proves they achieve $\\tilde\\{O\\}(\\varepsilon^\\{-2\\})$ sample complexity for average-time optimality, which is further refined to $O(\\varepsilon^\\{-2\\})$ for last-iterate optimality with adaptive step sizes.",
      "mindmap": "graph TB\n    A[Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data] --> B[核心问题/Problem: Existing PMD analysis limited to generative or Markovian sampling with pre-approximated action values]\n    A --> C[主要方法/Method: Propose Expected TD-PMD (off-policy) and Approximate TD-PMD (mixed policy) algorithms]\n    A --> D[关键结果/Results: Achieve ˜O(ε⁻²) sample complexity for average-time ε-optimality; improved to O(ε⁻²) for last-iterate ε-optimality with adaptive steps]"
    },
    {
      "title": "Quantitative Understanding of PDF Fits and their Uncertainties",
      "authors": "Amedeo Chiefa, Luigi Del Debbio, Richard Kenway",
      "institution": "The University of Edinburgh, Higgs Centre for Theoretical Physics",
      "link": "https://arxiv.org/pdf/2512.24116",
      "code": null,
      "tags": [
        "neural tangent kernel",
        "Parton Distribution Functions",
        "Neural Tangent Kernel",
        "training dynamics",
        "uncertainty quantification",
        "stochastic gradient descent"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283b144a92809766c93247cb89d39262cf21156f386f26a3c176b21ddd101f6d_w640_q70.webp",
      "contributions": "1. Developed a theoretical framework based on the Neural Tangent Kernel (NTK) to analyze the training dynamics of neural networks in PDF fits. 2. Derived an analytical description of neural network evolution during training, clarifying the role of NN architecture and the impact of experimental data. 3. Provided a quantitative description of how uncertainties propagate from data to the fitted function by describing the evolution of the covariance of the NN output.",
      "summary": "This paper develops a theoretical framework using the Neural Tangent Kernel to analytically model the training dynamics of neural networks used for fitting Parton Distribution Functions. The method provides a quantitative understanding of how the network learns and how uncertainties propagate from data, serving as a diagnostic tool to assess the robustness of PDF fitting methodologies. The findings also offer a testbed for applying machine learning theory, showing deviations from the simple \"lazy training\" regime.",
      "mindmap": "graph TB\n        Root(”Quantitative Understanding of PDF Fits and their Uncertainties”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”高精度LHC测量需要稳健的PDF和不确定性量化/Robust PDF & uncertainty quantification needed for high-precision LHC”)\n        Problem --> P2(”需要理解NN训练动态和不确定性传播/Need to understand NN training dynamics & uncertainty propagation”)\n        Method --> M1(”基于神经正切核的理论框架/Theoretical framework based on Neural Tangent Kernel”)\n        Method --> M2(”分析神经网络训练动态/Analyze neural network training dynamics”)\n        Results --> R1(”对训练过程和架构作用的定量理解/Quantitative understanding of training process & architecture role”)\n        Results --> R2(”不确定性从数据到函数的传播描述/Description of uncertainty propagation from data to function”)\n        Results --> R3(”评估当前拟合方法的诊断工具/Diagnostic tool to assess current fitting methodologies”)"
    },
    {
      "title": "Variational Quantum Brushes",
      "authors": "Jui-Ting Lu, Henrique Ennes, Chih-Kang Huang, Ali Abbassi",
      "institution": "Université de Lorraine, CNRS, Inria, Université de Technologie de Troyes, Orange Research",
      "link": "https://arxiv.org/pdf/2512.24173",
      "code": "https://github.com/moth-quantum/QuantumBrush",
      "tags": [
        "quantum computing for art",
        "variational quantum algorithms",
        "quantum geometric control",
        "variational eigensolver",
        "computational art",
        "quantum brushes"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d8a1ae2b1cfdee3a098d127a86fa773993bb3eab697781cc52c1e25f3f449b7_w640_q70.webp",
      "contributions": "1. Introduces a mathematical framework for quantum brushes based on variational quantum algorithms. 2. Implements the \"Steerable\" brush, which uses quantum geometric control theory to merge two images. 3. Implements the \"Chemical\" brush, which mimics variational eigensolvers to evolve colors on a canvas.",
      "summary": "This paper introduces two new quantum brushes for computational art, built on variational quantum algorithms. The \"Steerable\" brush merges artworks using quantum geometric control, while the \"Chemical\" brush evolves colors by mimicking molecular energy estimation. The implementations are open-source and compatible with existing quantum brush software.",
      "mindmap": "graph TB\n        A[Variational Quantum Brushes] --> B[核心问题/Problem: Leveraging quantum behavior for novel artistic effects]\n        A --> C[主要方法/Method: Two variational quantum algorithm-based brushes: Steerable and Chemical]\n        A --> D[关键结果/Results: Open-source implementation, compatible with original quantum brushes]"
    },
    {
      "title": "Score-based sampling without diffusions: Guidance from a simple and modular scheme",
      "authors": "M. J. Wainwright",
      "institution": "Massachusetts Institute of Technology (MIT)",
      "link": "https://arxiv.org/pdf/2512.24152",
      "code": null,
      "tags": [
        "diffusion models",
        "score-based sampling",
        "strongly log concave",
        "modular scheme"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18db1a89bccabdb56e6dc374b810b4ddafad4b8325763f60f894ada6c6f8926c_w640_q70.webp",
      "contributions": "1. Proposes a modular scheme that reduces score-based sampling to a sequence of \"nice\" sampling problems, specifically those defined by strongly log concave (SLC) distributions. 2. Shows how to design forward trajectories such that both the terminal and backward conditional distributions are SLC, enabling the use of any high-accuracy SLC sampler. 3. Establishes novel theoretical guarantees for both uni-modal and multi-modal densities, achieving ε-accuracy with polynomial dependence on log(1/ε) and √d dimension dependence.",
      "summary": "The paper addresses the challenge of score-based sampling, which typically relies on approximate score functions. It introduces a modular method that transforms the sampling problem into a short sequence of simpler, strongly log-concave sampling sub-problems. This approach allows leveraging existing high-accuracy samplers and provides theoretical guarantees for efficient and accurate sampling from complex densities.",
      "mindmap": "graph TB\n        A[Score-based sampling without diffusions: Guidance from a simple and modular scheme] --> B[核心问题/Problem: Sampling from complex densities using score-based methods]\n        A --> C[主要方法/Method: Modular reduction to a sequence of strongly log concave (SLC) sampling problems]\n        A --> D[关键结果/Results: Novel guarantees for uni/multi-modal densities, ε-accuracy with poly-log and √d dependence]"
    },
    {
      "title": "Topological Spatial Graph Coarsening",
      "authors": "Anna Calissano, Etienne Lasalle",
      "institution": "University College London, Nantes Université, École Centrale Nantes, CNRS",
      "link": "https://arxiv.org/pdf/2512.24327",
      "code": null,
      "tags": [
        "topological data analysis",
        "spatial graph coarsening",
        "persistent diagrams",
        "triangle-aware graph filtration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06067063c056f1d3d0e7b1438bde8daf5524598eca6b311651201d86b9e87d88_w640_q70.webp",
      "contributions": "1. Proposes a topological spatial graph coarsening method that balances graph reduction with topological feature preservation. 2. Introduces a triangle-aware graph filtration to adapt persistent diagrams (a topological descriptor) for spatial graphs. 3. Demonstrates that the method is parameter-free and equivariant under rotations, translations, and scaling.",
      "summary": "The paper addresses the problem of reducing spatial graphs while preserving their topological structure. It proposes a method that collapses short edges and uses a novel triangle-aware graph filtration to guide the coarsening process. The approach is shown to significantly reduce graph size while maintaining key topological information.",
      "mindmap": "graph TB\n        A[Topological Spatial Graph Coarsening] --> B(核心问题/Problem: Spatial graph reduction while preserving topology)\n        A --> C(主要方法/Method: Collapse short edges using triangle-aware graph filtration & persistent diagrams)\n        A --> D(关键结果/Results: Parameter-free, equivariant method that reduces size and preserves topology)"
    },
    {
      "title": "Fast reconstruction-based ROI triggering via anomaly detection in the CYGNO optical TPC",
      "authors": "F. D. Amaro, R. Antonietti, E. Baracchini, L. Benussi, C. Capoccia, M. Caponero, L. G. M. de Carvalho, G. Cavoto, I. A. Costa, A. Croce, M. D'Astolfo, G. D'Imperio, G. Dho, E. Di Marco, J. M. F. dos Santos, D. Fiorina, F. Iacoangeli, Z. Islam, E. Kemp, H. P. Lima Jr., G. Maccarrone, R. D. P. Mano, D. J. G. Marques, G. Mazzitelli, P. Meloni, A. Messina, V. Monno, C. M. B. Monteiro, R. A. Nobrega, G. M. Oppedisano, I. F. Pains, E. Paoletti, F. Petrucci, S. Piacentini, D. Pierluigi, D. Pinci, F. Renga, A. Russo, G. Saviano, P. A. O. C. Silva, N. J. Spooner, R. Tesauro, S. Tomassini, D. Tozzi",
      "institution": "University of Coimbra, Gran Sasso Science Institute, INFN (Istituto Nazionale di Fisica Nucleare), Sapienza University of Rome",
      "link": "https://arxiv.org/pdf/2512.24290",
      "code": null,
      "tags": [
        "others",
        "anomaly detection",
        "autoencoder",
        "online data reduction",
        "optical TPC",
        "ROI triggering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07a2394c8601a3636fcb96b0e78aa474c0a406733284a194c5cbd05434e20b37_w640_q70.webp",
      "contributions": "1. Proposed an unsupervised, reconstruction-based anomaly detection method using a convolutional autoencoder trained exclusively on pedestal (noise) images for fast ROI extraction in optical TPCs. 2. Demonstrated the critical impact of the training objective design through a controlled comparison of two autoencoder configurations on real data. 3. Achieved high performance, retaining 93.0% of signal intensity while discarding 97.8% of image area with an inference time of ~25 ms per frame, establishing a transparent, detector-agnostic baseline for online data reduction.",
      "summary": "The paper addresses the challenge of real-time data selection from large images in optical Time Projection Chambers (TPCs). It proposes an unsupervised anomaly detection method using a pedestal-trained convolutional autoencoder to quickly identify and extract Regions of Interest (ROIs) from particle tracks. The results show the method is highly effective for online data reduction, with performance heavily dependent on the careful design of the training objective.",
      "mindmap": "graph TB\n        A[Fast reconstruction-based ROI triggering via anomaly detection in the CYGNO optical TPC] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Optical TPCs produce large images challenging real-time data selection for rare-event searches.]\n        C[主要方法/Method: Unsupervised anomaly detection using a pedestal-trained convolutional autoencoder for fast ROI extraction.]\n        D[关键结果/Results: Retains 93% signal, discards 98% area, ~25 ms inference, training objective is critical.]"
    },
    {
      "title": "Deep Learning in Geotechnical Engineering: A Critical Assessment of PINNs and Operator Learning",
      "authors": "Krishna Kumar",
      "institution": "None (Institution not provided in the given content. Author is Krishna Kumar, but no affiliation or email domain is shown.)",
      "link": "https://arxiv.org/pdf/2512.24365",
      "code": null,
      "tags": [
        "scientific machine learning",
        "physics-informed neural networks",
        "deep operator networks",
        "graph network simulators",
        "automatic differentiation",
        "geotechnical engineering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4f4b161baea59d1dc122276a25b7891aac2248e8367a3b0e01bacce08a12baa_w640_q70.webp",
      "contributions": "1. A critical empirical comparison of PINNs, DeepONet, and GNS against traditional solvers for canonical geotechnical problems, revealing severe performance and accuracy limitations. 2. Demonstrates the efficacy of automatic differentiation through traditional solvers for rapid and accurate inverse parameter estimation in geotechnical engineering. 3. Provides practical recommendations for the application of deep learning in geotechnical contexts, emphasizing the importance of site-based validation and the limited envelope where neural networks are viable.",
      "summary": "This paper critically evaluates deep learning methods like PINNs, DeepONet, and GNS for geotechnical engineering simulations. It finds these methods are often orders of magnitude slower and less accurate than traditional solvers, and fail when extrapolating. The authors recommend using automatic differentiation for inverse problems and reserving neural networks only for specific, well-bounded cases where traditional solvers are prohibitively expensive.",
      "mindmap": "graph TB\n        A[Deep Learning in Geotechnical Engineering: A Critical Assessment<br/>岩土工程中的深度学习：关键评估] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>评估深度学习在岩土工程中的实用性<br/>Assessing the practicality of deep learning in geotechnical engineering]\n        C[主要方法/Method<br/>对比PINNs, DeepONet, GNS与传统求解器<br/>Compare PINNs, DeepONet, GNS vs. traditional solvers]\n        D[关键结果/Results<br/>PINNs慢90,000倍；DeepONet需大量训练；神经网络外推失败；推荐自动微分<br/>PINNs 90,000x slower; DeepONet needs massive training; NNs fail at extrapolation; Recommends automatic differentiation]"
    },
    {
      "title": "OptiVote: Non-Coherent FSO Over-the-Air Majority Vote for Communication-Efficient Distributed Federated Learning in Space Data Centers",
      "authors": "Anbang Zhang, Chenyuan Feng, Wai Ho Mow, Jia Ye, Shuaishuai Guo, Geyong Min, Tony Q. S. Quek",
      "institution": "Shandong University, University of Exeter, The Hong Kong University of Science and Technology, Chongqing University, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.24334",
      "code": null,
      "tags": [
        "federated learning",
        "over-the-air computation",
        "free-space optical communication",
        "signSGD",
        "majority vote",
        "pulse-position modulation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72d34a686b6aa623eca2a7d8853b5d062ec21148534d26149a8be732a2526af4_w640_q70.webp",
      "contributions": "1. Proposes OptiVote, a non-coherent FSO AirComp framework that integrates signSGD with majority-vote aggregation and PPM to eliminate the need for precise phase synchronization in space environments. 2. Develops an importance-aware, CSI-free dynamic power control scheme to mitigate aggregation bias caused by heterogeneous FSO channels without extra signaling. 3. Provides theoretical analysis of aggregate error probability and convergence guarantees, and demonstrates superior communication efficiency and learning accuracy through experiments.",
      "summary": "This paper addresses the challenge of communication-efficient federated learning in space data centers by proposing OptiVote, a robust non-coherent free-space optical over-the-air computation framework. It combines signSGD with majority-vote aggregation and pulse-position modulation to perform aggregation without precise phase synchronization, and includes a power control scheme to handle channel heterogeneity. The method is shown to outperform baselines in both communication efficiency and learning accuracy for distributed intelligence in space.",
      "mindmap": "graph TB\n        A[OptiVote: Non-Coherent FSO Over-the-Air Majority Vote] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[卫星数据中心联邦学习通信效率低/Federated Learning in SDCs is communication-inefficient]\n        B --> B2[相干AirComp需要严格相位同步/Coherent AirComp requires precise phase sync]\n        C --> C1[非相干FSO AirComp框架/Non-coherent FSO AirComp framework]\n        C --> C2[集成signSGD与多数投票/PPM/Integrates signSGD with MV & PPM]\n        C --> C3[CSI-free动态功率控制/CSI-free dynamic power control]\n        D --> D1[消除相位同步需求/Eliminates need for phase sync]\n        D --> D2[提升通信效率与学习精度/Improves comm. efficiency & learning accuracy]\n        D --> D3[理论分析与实验验证/Theoretical analysis & experimental validation]"
    },
    {
      "title": "Implicit score matching meets denoising score matching: improved rates of convergence and log-density Hessian estimation",
      "authors": "Konstantin Yakovlev, Anna Markovich, Nikita Puchkin",
      "institution": "HSE University",
      "link": "https://arxiv.org/pdf/2512.24378",
      "code": null,
      "tags": [
        "diffusion models",
        "score matching",
        "implicit score matching",
        "denoising score matching",
        "Fisher divergence",
        "Gagliardo-Nirenberg inequality"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09412dadfda9474a45ce50ae36a846be163fb7cca6813c986c3a6c13f5ba9670_w640_q70.webp",
      "contributions": "1. Proved that implicit score matching can adapt to the intrinsic dimension of low-dimensional data distributions and achieve the same convergence rates as denoising score matching. 2. Demonstrated that both implicit and denoising score matching enable estimation of log-density Hessians without the curse of dimensionality via simple differentiation. 3. Provided theoretical justification for the convergence of ODE-based samplers used in generative diffusion models.",
      "summary": "This paper studies the estimation of the score function using implicit and denoising score matching. It proves that implicit score matching matches the convergence rates of denoising score matching for low-dimensional data and that both methods can efficiently estimate log-density Hessians, thereby justifying the convergence of ODE-based samplers in diffusion models.",
      "mindmap": "graph TB\n        A[Implicit score matching meets denoising score matching] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[估计得分函数 / Estimate Score Function]\n        C --> C1[隐式得分匹配与去噪得分匹配 / Implicit & Denoising Score Matching]\n        C --> C2[基于Gagliardo-Nirenberg不等式 / Gagliardo-Nirenberg Inequalities]\n        D --> D1[收敛率相同 / Same Convergence Rates]\n        D --> D2[无维度灾难估计Hessian / Hessian Estimation without Curse of Dimensionality]\n        D --> D3[证明ODE采样器收敛 / Justifies ODE Sampler Convergence]"
    },
    {
      "title": "Virasoro Symmetry in Neural Network Field Theories",
      "authors": "Brandon Robinson",
      "institution": "University of Amsterdam",
      "link": "https://arxiv.org/pdf/2512.24420",
      "code": null,
      "tags": [
        "theoretical machine learning",
        "Neural Network Field Theories",
        "Virasoro symmetry",
        "conformal field theory",
        "stress-energy tensor",
        "super-Virasoro"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e2e1fe141fac45bc31d96a59d9378b6ce343fa02fab3e24378bc7554b1789c_w640_q70.webp",
      "contributions": "1. First construction of a Neural Network Field Theory (NN-FT) that encodes the full Virasoro symmetry of a 2d CFT, achieved via a specific \"Log-Kernel Network\" architecture and prior distribution. 2. Extension of the framework to include super-Virasoro symmetry by constructing neural realizations of a Majorana Fermion and an N=(1,1) scalar multiplet. 3. Development of boundary NN-FTs that preserve (super-)conformal symmetry using the method of images, demonstrating the robustness of the framework.",
      "summary": "The paper addresses the lack of local, infinite-dimensional conformal symmetry (Virasoro symmetry) in typical Neural Network Field Theories (NN-FTs). It proposes a specific neural architecture, the Log-Kernel Network, to construct a 2d free boson theory with a local stress-energy tensor, enabling the emergence of Virasoro algebra. The work is validated numerically, extended to super-Virasoro symmetry, and further generalized to boundary theories preserving conformal invariance.",
      "mindmap": "graph TB\n        A[Virasoro Symmetry in Neural Network Field Theories] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[NN-FTs lack local stress tensor & Virasoro symmetry]\n        C --> C1[Design Log-Kernel Network architecture]\n        C --> C2[Proper prior for parameters]\n        D --> D1[Verified central charge & scaling dimensions]\n        D --> D2[Extended to super-Virasoro & boundary NN-FTs]"
    },
    {
      "title": "Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features",
      "authors": "Theodore MacMillan, Nicholas T. Ouellette",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.24440",
      "code": null,
      "tags": [
        "mechanistic interpretability",
        "sparse autoencoders",
        "feature discovery",
        "model intervention",
        "GraphCast",
        "interpretability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0540d3e6e8dd4531270c37251313fe5f6b7b7ef8f4792bfb500db7d9f5d7ba6_w640_q70.webp",
      "contributions": "1. Adapted interpretability tools from Large Language Models (specifically sparse autoencoders) to analyze a large-scale data-driven weather model (GraphCast). 2. Discovered that the model's internal activations correspond to interpretable physical features like tropical cyclones and atmospheric rivers. 3. Demonstrated causal probing via feature interventions, showing physically consistent modifications to model predictions (e.g., altering hurricane evolution).",
      "summary": "This paper investigates the interpretability of data-driven physics models like GraphCast by applying mechanistic interpretability techniques from LLMs. Using sparse autoencoders, the authors discover that the model's internal features correspond to meaningful physical phenomena. They further show that targeted interventions on these features lead to interpretable and physically consistent changes in the model's forecasts, advancing the trustworthiness of such models.",
      "mindmap": "graph TB\n        A[Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>How do data-driven physics models work internally? Are they interpretable and physically consistent?]\n        C[主要方法/Method<br>Adapt LLM interpretability tools (sparse autoencoders) to analyze GraphCast's activations.]\n        D[关键结果/Results<br>Discovered interpretable physical features (cyclones, rivers). Interventions yield physically consistent predictions.]"
    },
    {
      "title": "Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling",
      "authors": "Jiani Wei, Xiaocheng Shang",
      "institution": "University of Birmingham",
      "link": "https://arxiv.org/pdf/2512.24515",
      "code": null,
      "tags": [
        "Bayesian sampling",
        "stochastic gradient Langevin dynamics",
        "covariance-controlled adaptive Langevin",
        "numerical stability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9577aa049234db392488eb89c746ecf45ccd1d5a0555bdc8f9a7fefd2c23a7_w640_q70.webp",
      "contributions": "1. Proposed a modified CCAdL (mCCAdL) thermostat using the scaling part of the scaling and squaring method and a truncated Taylor series to approximate the exact solution to a key subsystem. 2. Introduced a symmetric splitting method for discretizing the mCCAdL thermostat, replacing the Euler-type discretization used in the original CCAdL. 3. Demonstrated that the mCCAdL thermostat achieves substantial improvement in numerical stability over the original CCAdL and outperforms alternative stochastic gradient methods in accuracy for large-scale applications.",
      "summary": "This paper addresses the numerical stability issue in the covariance-controlled adaptive Langevin (CCAdL) thermostat, a method for large-scale Bayesian sampling. The authors propose a modified version (mCCAdL) that uses a more stable numerical approximation scheme and a symmetric splitting integrator. Their experiments show that mCCAdL significantly improves stability and accuracy compared to the original CCAdL and other stochastic gradient methods.",
      "mindmap": "graph TB\n        A[Improving the stability of the covariance-controlled adaptive Langevin thermostat] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[CCAdL数值稳定性差，限制步长/CCAdL poor numerical stability limits stepsize]\n        C --> C1[提出mCCAdL，使用缩放和平方及泰勒近似/Propose mCCAdL using scaling & squaring and Taylor approximation]\n        C --> C2[提出对称分裂方法/Propose symmetric splitting method]\n        D --> D1[稳定性显著提升/Stability substantially improved]\n        D --> D2[精度优于其他方法/Accuracy outperforms alternatives]"
    },
    {
      "title": "Probabilistic Computers for Neural Quantum States",
      "authors": "Shuvro Chowdhury, Jasper Pieterse, Navid Anjum Aadit, Johan H. Mentink, Kerem Y. Camsari",
      "institution": "University of California, Santa Barbara, Radboud University",
      "link": "https://arxiv.org/pdf/2512.24558",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "probabilistic computing",
        "FPGA",
        "Boltzmann machine",
        "neural quantum states",
        "Monte Carlo sampling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ef4c4bc51129bbcf4c7afb6bc33c07bd787081538aa3dbb925d18f3bf724567_w640_q70.webp",
      "contributions": "1. Implementation of a probabilistic computer on a custom multi-FPGA cluster to serve as a fast sampler for neural quantum states. 2. Introduction of a dual-sampling algorithm to train deep Boltzmann machines by replacing intractable marginalization with conditional sampling. 3. Demonstration of scaling variational quantum simulations to large system sizes (up to 6400 spins) and deeper architectures, overcoming the MCMC sampling bottleneck.",
      "summary": "This paper addresses the Monte Carlo sampling bottleneck in scaling neural quantum states for quantum many-body simulations. It proposes combining sparse Boltzmann machine architectures with probabilistic computing hardware implemented on FPGAs and introduces a dual-sampling training algorithm. The results show that this approach enables accurate simulations of large spin systems and deeper models, overcoming a key scaling limitation.",
      "mindmap": "graph TB\n        Root[”Probabilistic Computers for Neural Quantum States”] --> Problem[”核心问题/Problem: Monte Carlo sampling limits scaling of neural quantum states”]\n        Root --> Method[”主要方法/Method: Combine sparse Boltzmann machines with FPGA-based probabilistic computer & dual-sampling algorithm”]\n        Root --> Results[”关键结果/Results: Accurate ground-state energies for 6400 spins; trained deep models for 1225 spins”]"
    },
    {
      "title": "MultiRisk: Multiple Risk Control via Iterative Score Thresholding",
      "authors": "Sunay Joshi, Yan Sun, Hamed Hassani, Edgar Dobriban",
      "institution": "University of Pennsylvania, New Jersey Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24587",
      "code": null,
      "tags": [
        "risk control",
        "test-time filtering",
        "dynamic programming",
        "risk constraints",
        "score thresholding",
        "exchangeability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51cedb2b9055aea411d20349fafef70fbd282669fb33f5fc821167eb1304019e_w640_q70.webp",
      "contributions": "1. Formalizes the problem of enforcing multiple risk constraints with user-defined priorities for generative AI systems. 2. Introduces two efficient dynamic programming algorithms (MULTIRISK-BASE and MULTIRISK) for selecting thresholds to control risks. 3. Provides a theoretical analysis showing that MULTIRISK achieves nearly tight simultaneous control of all constraint risks under mild assumptions.",
      "summary": "This paper proposes MultiRisk, a framework for controlling multiple risks in generative AI outputs via test-time filtering. It introduces dynamic programming algorithms to set score thresholds, guaranteeing simultaneous risk control by leveraging data exchangeability. The method is validated on an LLM alignment task, showing it can control individual risks close to their target levels.",
      "mindmap": "graph TB\n    A[MultiRisk: Multiple Risk Control via Iterative Score Thresholding] --> B[核心问题/Problem: Regulating multiple dimensions of generative AI model behavior at test-time]\n    A --> C[主要方法/Method: Dynamic programming algorithms (MULTIRISK-BASE, MULTIRISK) for iterative score thresholding]\n    A --> D[关键结果/Results: Achieves nearly tight simultaneous control of multiple constraint risks on LLM alignment task]"
    },
    {
      "title": "Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning",
      "authors": "Shanyu Han, Yangbo He, Yang Liu",
      "institution": "Peking University, The Chinese University of Hong Kong, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.24580",
      "code": null,
      "tags": [
        "reinforcement learning",
        "risk-sensitive reinforcement learning",
        "Bayesian dynamic programming",
        "coherent risk measures",
        "robust Markov decision process",
        "convex optimization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b966fa69f9b8aafe26700ce5afe83099b3257d5b90314e5d3f668e4079ecdda_w640_q70.webp",
      "contributions": "1. Proposes a novel unified framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty by defining inner and outer coherent risk measures. 2. Develops a Bayesian Dynamic Programming algorithm that alternates posterior updates with value iteration, using a Monte Carlo and convex optimization estimator with strong consistency guarantees. 3. Provides theoretical analysis including convergence, sample complexity, and computational complexity under Dirichlet posterior and CVaR, validated through numerical experiments and an option hedging application.",
      "summary": "The paper introduces a robust Bayesian framework for on-policy risk-sensitive reinforcement learning that addresses transition uncertainty through coupled inner and outer risk measures. It develops a Bayesian Dynamic Programming algorithm with theoretical guarantees and demonstrates its effectiveness in convergence and robustness via numerical experiments and an option hedging application.",
      "mindmap": "graph TB\n        Root[”Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning”] --> Problem[”核心问题/Problem: Need for risk-sensitive RL robust to transition uncertainty”]\n        Root --> Method[”主要方法/Method: Unified framework with inner/outer risk measures, Bayesian DP algorithm with posterior updates and value iteration”]\n        Root --> Results[”关键结果/Results: Theoretical guarantees, convergence, validated via experiments and option hedging application”]"
    },
    {
      "title": "Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers",
      "authors": "Chandler Haight, Svetlana Roudenko, Zhongming Wang",
      "institution": "Wayne State University",
      "link": "https://arxiv.org/pdf/2512.24634",
      "code": null,
      "tags": [
        "scientific computing",
        "Physics-Informed Neural Networks",
        "Deep Operator Network",
        "Petviashvili method",
        "solitary waves",
        "numerical solvers"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b9a088c1ebb97139a0da346b37d398fe825a096d76a313979e6fe0f8544c65_w640_q70.webp",
      "contributions": "1. A comprehensive comparative study of classical numerical schemes (e.g., Petviashvili's method) and neural network-based solvers (PINNs, operator-learning) for computing soliton profiles in 1D dispersive PDEs. 2. An empirical confirmation that classical methods retain superior accuracy and computational efficiency for single-instance problems in one-dimensional settings. 3. An analysis highlighting the trade-offs of neural network methods, where operator-learning approaches, despite costly training, offer rapid inference and reusability for multi-parameter or real-time applications.",
      "summary": "This paper compares classical numerical solvers and neural network-based methods for finding solitary-wave solutions to nonlinear PDEs like the NLS and KdV equations. It finds that classical methods are more accurate and efficient for single problems, while neural operators are better suited for repeated simulations due to fast inference after training. The core conclusion is that the choice of solver depends on the application context: precision for single instances vs. speed for parameterized families.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers] --> B(核心问题 / Problem: 如何计算一维色散PDE的孤立波解？ / How to compute solitary-wave solutions for 1D dispersive PDEs?)\n        A --> C(主要方法 / Method: 比较经典数值方法与神经网络求解器 / Compare classical numerical methods and neural network-based solvers)\n        A --> D(关键结果 / Results: 经典方法在单实例上更优，算子学习方法适合重复模拟 / Classical methods better for single instances, operator-learning suitable for repeated simulations)"
    },
    {
      "title": "Sparse Offline Reinforcement Learning with Corruption Robustness",
      "authors": "Nam Phuong Tran, Andi Nika, Goran Radanovic, Long Tran-Thanh, Debmalya Mandal",
      "institution": "University of Warwick, Max Planck Institute for Software Systems (MPI-SWS)",
      "link": "https://arxiv.org/pdf/2512.24768",
      "code": null,
      "tags": [
        "reinforcement learning",
        "offline reinforcement learning",
        "sparsity",
        "corruption robustness",
        "single-policy concentrability",
        "actor-critic"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ce133c71cdf619bf6a1597c047c8adc5b10b7d0584bd6af1944718635e12340_w640_q70.webp",
      "contributions": "1. Identifies limitations of integrating sparsity into standard robust offline RL methods like LSVI, showing they can fail due to overly pessimistic bonuses. 2. Proposes novel actor-critic methods with sparse robust estimator oracles that avoid pointwise pessimistic bonuses. 3. Provides the first non-vacuous theoretical guarantees for learning in high-dimensional sparse MDPs under weak (single-policy concentrability) coverage and strong data corruption.",
      "summary": "This paper addresses the challenge of robust offline reinforcement learning in high-dimensional, sparse Markov Decision Processes where data may be corrupted. The authors propose new actor-critic methods that use sparse robust estimator oracles, avoiding the pitfalls of traditional approaches. Their work provides the first theoretical guarantees showing that learning a near-optimal policy is possible under weak data coverage and strong corruption, where previous methods fail.",
      "mindmap": "graph TB\n        Root[”Sparse Offline RL with Corruption Robustness<br>稀疏离线强化学习与抗污染鲁棒性”] --> Problem[”Problem: High-dim sparse MDPs with corrupted data & weak coverage<br>问题: 具有污染数据和弱覆盖的高维稀疏MDP”]\n        Root --> Method[”Method: Actor-critic with sparse robust estimator oracles<br>方法: 使用稀疏鲁棒估计器oracle的Actor-critic”]\n        Root --> Results[”Results: First non-vacuous guarantees under single-policy concentrability & corruption<br>结果: 在单策略集中性和污染下的首个非平凡保证”]"
    },
    {
      "title": "A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing",
      "authors": "Kuangyu Ding, Marie Maros, Gesualdo Scutari",
      "institution": "Purdue University, Texas A&M University",
      "link": "https://arxiv.org/pdf/2512.24676",
      "code": null,
      "tags": [
        "communication & networking",
        "Message Passing",
        "Jacobi Block Updates",
        "Graph Decomposition",
        "Hypergraph Optimization",
        "Decentralized Optimization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b545e505c1cccda1a2f472afecb206a4b95c552dd11319ac3f007bab097156a8_w640_q70.webp",
      "contributions": "1. Introduces MP-Jacobi, a novel decentralized framework that combines min-sum message passing with Jacobi block updates for graph-structured nonlinear programs, enabling single-hop communication and convergence on loopy graphs. 2. Establishes global linear convergence rates for strongly convex objectives, providing theoretical guidance on how curvature, coupling strength, and graph partitioning affect scalability. 3. Develops graph-compliant surrogate updates and a hyperedge-splitting scheme to reduce per-iteration computation/communication costs and extend the method to hypergraphs while preserving convergence.",
      "summary": "The paper proposes MP-Jacobi, a decentralized algorithm for solving graph-structured nonlinear programs by partitioning the graph into tree clusters and combining min-sum message passing within clusters with Jacobi-style updates for inter-cluster couplings. This design uses only single-hop communication and is proven to converge linearly for strongly convex problems. Experiments show it outperforms decentralized gradient baselines, offering a scalable primitive for graph optimization.",
      "mindmap": "graph TB\n        A[A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[图结构非线性规划/Graph-structured Nonlinear Programs]\n        B --> B2[局部交互与循环图/Local Interactions & Loopy Graphs]\n        C --> C1[MP-Jacobi 框架/MP-Jacobi Framework]\n        C1 --> C1_1[树簇分区/Tree Clusters Partition]\n        C1 --> C1_2[簇内消息传递/Intra-cluster Message Passing]\n        C1 --> C1_3[簇间雅可比更新/Inter-cluster Jacobi Updates]\n        C --> C2[图合规代理/Graph-compliant Surrogates]\n        C --> C3[超图扩展/Hypergraph Extension]\n        D --> D1[线性收敛/Linear Convergence]\n        D --> D2[单跳通信/Single-hop Communication]\n        D --> D3[优于基线/Outperforms Baselines]"
    },
    {
      "title": "Fairness-Aware Insurance Pricing: A Multi-Objective Optimization Approach",
      "authors": "Tim J. Boonen, Xinyue Fan, Zixiao Quan",
      "institution": "University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.24747",
      "code": null,
      "tags": [
        "fairness in machine learning",
        "multi-objective optimization",
        "NSGA-II",
        "fairness criteria",
        "insurance pricing",
        "Pareto front"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ea23717323b55541619e16c4dd6ff704d96b756316680eeebec5f447f78a5d2_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-objective optimization framework for insurance pricing that jointly optimizes accuracy, group fairness, individual fairness, and counterfactual fairness. 2. Employs the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to generate a diverse Pareto front of trade-off solutions, moving beyond single-objective optimization. 3. Introduces a specific selection mechanism to extract a balanced premium from the Pareto front, demonstrating a consistent and superior compromise compared to single-model approaches.",
      "summary": "This paper addresses the trade-offs between accuracy and multiple fairness criteria in machine learning for insurance pricing. It proposes a multi-objective optimization framework using NSGA-II to generate a Pareto front of solutions and a method to select a balanced premium. The results show the proposed method achieves a better compromise than existing single-objective models.",
      "mindmap": "graph TB\n        A[Fairness-Aware Insurance Pricing: A Multi-Objective Optimization Approach] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[ML improves accuracy but worsens fairness trade-offs<br>ML提高准确性但加剧公平性权衡]\n        C --> C1[Multi-objective framework via NSGA-II<br>通过NSGA-II的多目标框架]\n        C --> C2[Jointly optimizes four criteria<br>联合优化四个标准]\n        C --> C3[Generates Pareto front & selects premium<br>生成帕累托前沿并选择保费]\n        D --> D1[XGBoost: high accuracy, high disparity<br>XGBoost: 高精度, 高差异]\n        D --> D2[Orthogonal: best group fairness<br>Orthogonal: 最佳群体公平性]\n        D --> D3[Synthetic Control: best individual/counterfactual fairness<br>Synthetic Control: 最佳个体/反事实公平性]\n        D --> D4[Proposed method: balanced compromise<br>所提方法: 平衡的折衷]"
    },
    {
      "title": "Limits of quantum generative models with classical sampling hardness",
      "authors": "Sabrina Herbst, Ivona Brandić, Adrián Pérez-Salinas",
      "institution": "TU Wien, ETH Zurich",
      "link": "https://arxiv.org/pdf/2512.24801",
      "code": null,
      "tags": [
        "quantum machine learning",
        "quantum generative models",
        "anticoncentration",
        "barren plateaus",
        "classical simulability",
        "surrogate sampling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db3f019ba893a7df48b4f7eb38a379b3b7ed4612455ba75a7c9068c960c84acc_w640_q70.webp",
      "contributions": "1. Shows that quantum generative models which anticoncentrate (a property linked to classical hardness) are not trainable on average, creating a trade-off between advantage and learnability. 2. Demonstrates that models outputting sparse distributions can be trained, and explores special cases to enhance trainability. 3. Links this trainability trade-off to quantum process verification and identifies that quantum advantage in generative models must stem from sources other than anticoncentration.",
      "summary": "This paper studies the trainability of quantum generative models, finding a fundamental trade-off: models that output anticoncentrated distributions (which are classically hard to sample from) suffer from untrainable loss landscapes, while models with sparse outputs are trainable. The authors link this to verification and conclude that quantum advantage in generative modeling is still possible but must originate from mechanisms distinct from anticoncentration.",
      "mindmap": "graph TB\n        Root(”Limits of quantum generative models with classical sampling hardness<br>量子生成模型与经典采样难度的局限”) --> Problem(”核心问题/Problem<br>Quantum advantage vs. trainability in generative models<br>生成模型中量子优势与可训练性的矛盾”)\n        Root --> Method(”主要方法/Method<br>Analyze models via output distribution properties (anticoncentration, sparsity)<br>通过输出分布特性（反集中性、稀疏性）分析模型”)\n        Root --> Results(”关键结果/Results<br>Anticoncentrating models are not trainable; sparse ones are. Advantage source distinct from anticoncentration.<br>反集中模型不可训练；稀疏模型可训练。优势来源不同于反集中性。”)"
    },
    {
      "title": "Learning Temporally Consistent Turbulence Between Sparse Snapshots via Diffusion Models",
      "authors": "Mohammed Sardar, Małgorzata J. Zimoń, Samuel Draycott, Alistair Revell, Alex Skillen",
      "institution": "The University of Manchester, IBM Research Europe",
      "link": "https://arxiv.org/pdf/2512.24813",
      "code": null,
      "tags": [
        "diffusion models",
        "Denoising Diffusion Probabilistic Models (DDPMs)",
        "turbulence interpolation",
        "generative surrogate",
        "Kolmogorov Flow",
        "Kelvin-Helmholtz Instability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0eee4dacebc613c047d4355fd35c417d91dbabc771afd4a894ccdc60312874e4_w640_q70.webp",
      "contributions": "1. Proposes a conditional DDPM-based method for temporally interpolating coherent turbulent dynamics between sparse, decorrelated flow snapshots. 2. Demonstrates the method as a proof-of-concept generative surrogate on both a 2D Kolmogorov Flow and a 3D Kelvin-Helmholtz Instability case. 3. Evaluates the generated sequences through statistical turbulence analysis, including turbulent kinetic energy spectra and the temporal decay of structures, showing the ability to capture evolving flow statistics.",
      "summary": "This paper proposes using conditional Denoising Diffusion Probabilistic Models (DDPMs) as a generative surrogate to reconstruct statistically accurate turbulent flow sequences between sparse snapshots. The method is demonstrated on 2D and 3D turbulent flow cases. The analysis shows the generated sequences capture key turbulent statistics and the evolution of flow structures.",
      "mindmap": "graph TB\n        A[Learning Temporally Consistent Turbulence Between Sparse Snapshots via Diffusion Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[从稀疏、不相关的湍流快照重建中间动态/Reconstruct intermediate dynamics from sparse, decorrelated turbulent snapshots]\n        C --> C1[使用条件去噪扩散概率模型/Conditional Denoising Diffusion Probabilistic Models (DDPMs)]\n        C --> C2[作为生成式代理模型/As a generative surrogate model]\n        D --> D1[分析生成序列的统计特性/Analyze statistical properties of generated sequences]\n        D --> D2[评估湍流能量谱和结构衰减/Evaluate turbulent kinetic energy spectra & structure decay]\n        D --> D3[在2D Kolmogorov流和3D KHI上验证/Validated on 2D Kolmogorov Flow & 3D KHI]"
    },
    {
      "title": "Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach",
      "authors": "Yuchen Jiao, Na Li, Changxiao Cai, Gen Li",
      "institution": "The Chinese University of Hong Kong, Zhejiang University, University of Michigan",
      "link": "https://arxiv.org/pdf/2512.24927",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion model",
        "training-free acceleration",
        "first-order sampler",
        "forward-value discretization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef8de5dcbba9edb9daa0ab1ba8e254668331d9851247da681521a06ad8b83b22_w640_q70.webp",
      "contributions": "1. Challenges the prevailing belief that higher-order ODE solvers are inherently faster for DPM sampling, proposing that the placement of DPM evaluations is a crucial, independent design factor. 2. Introduces a novel, training-free first-order sampler that approximates the forward-value evaluation using a cheap one-step lookahead predictor, resulting in a leading discretization error with the opposite sign to DDIM. 3. Provides theoretical guarantees for the sampler's approximation of the ideal forward-value trajectory while maintaining first-order convergence, and demonstrates empirical competitiveness with higher-order samplers on standard benchmarks.",
      "summary": "This paper challenges the view that first-order diffusion samplers are inherently slower than higher-order ones. It proposes a new first-order sampler that uses a forward-value approach with a lookahead predictor to better place model evaluations. The method is theoretically sound and empirically matches or outperforms state-of-the-art higher-order samplers on image generation tasks under the same computational budget.",
      "mindmap": "graph TB\n        A[”Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach<br>论文标题”] --> B[”核心问题/Problem<br>Higher-order solvers are standard; first-order methods are seen as slower.<br>高阶求解器是标准；一阶方法被认为更慢。”]\n        A --> C[”主要方法/Method<br>Proposes a training-free first-order sampler using forward-value evaluation via a one-step lookahead predictor.<br>提出一种免训练的一阶采样器，通过一步前瞻预测器进行前向值评估。”]\n        A --> D[”关键结果/Results<br>Sampler improves quality under same NFE budget, competitive with higher-order methods.<br>采样器在相同NFE预算下提升质量，与高阶方法竞争。”]"
    },
    {
      "title": "Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis",
      "authors": "Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani",
      "institution": "University of California, Berkeley, Columbia University, New York University",
      "link": "https://arxiv.org/pdf/2512.24999",
      "code": null,
      "tags": [
        "optimization theory",
        "basic inequalities",
        "implicit regularization",
        "gradient descent",
        "mirror descent",
        "statistical risk analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/056dd77c9a8e63bc02d7a61166503bba0d0961e3c1f64d12e92c35803aa0bd9a_w640_q70.webp",
      "contributions": "1. Introduces a formal framework of \"basic inequalities\" that connects implicit and explicit regularization in first-order optimization. 2. Applies the framework to derive new results for mirror descent with Bregman divergence, generalized linear models trained by gradient/exponentiated gradient descent, and randomized predictors. 3. Revisits and refines known results on gradient descent, demonstrating the framework's versatility for analyzing training dynamics and prediction risk.",
      "summary": "This paper proposes a framework of \"basic inequalities\" to analyze first-order optimization algorithms, linking the number of iterations to an effective regularization parameter. The method provides a unified tool to derive statistical risk bounds for algorithms like gradient descent and mirror descent. The main conclusion is that this framework simplifies and generalizes the analysis of implicit regularization across various optimization settings.",
      "mindmap": "graph TB\n    A[Basic Inequalities for First-Order Optimization] --> B[核心问题/Problem: Connecting implicit and explicit regularization in iterative optimization]\n    A --> C[主要方法/Method: Introduce ”basic inequalities” framework to bound f(θ_T)-f(z) using distances and step sizes]\n    A --> D[关键结果/Results: New risk bounds for mirror descent, GLMs, randomized predictors; refines gradient descent analysis]"
    },
    {
      "title": "SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets",
      "authors": "Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte",
      "institution": "Jülich Research Centre, RWTH Aachen University, University of Groningen, University of Coimbra",
      "link": "https://arxiv.org/pdf/2512.24977",
      "code": null,
      "tags": [
        "sequence learning",
        "formal language theory",
        "symbolic sequences",
        "benchmark suite",
        "cognitive modeling",
        "sequence processing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp",
      "contributions": "1. Introduces SymSeq, a tool for the rigorous generation and analysis of structured symbolic sequences. 2. Introduces SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks for evaluating AI systems. 3. Provides a unified, domain-agnostic framework (SymSeqBench) based on Formal Language Theory to standardize experiments across cognitive science and AI.",
      "summary": "The paper introduces SymSeqBench, a unified software framework combining a symbolic sequence generator/analyzer (SymSeq) and a benchmark suite (SeqBench) for evaluating sequence learning. It is based on Formal Language Theory to provide a domain-agnostic, formal link between computation and cognition. The main conclusion is that this modular, open-source tool offers a versatile and standardized way to investigate sequential structure across diverse fields like psycholinguistics, cognitive psychology, and AI.",
      "mindmap": "graph TB\n        Root[SymSeqBench: 统一框架] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[评估序列学习/Evaluating Sequence Learning]\n        Problem --> P2[领域无关的评估/Domain-Agnostic Evaluation]\n        Problem --> P3[连接形式理论与认知/Linking Formal Theory & Cognition]\n        Method --> M1[SymSeq: 生成与分析/SymSeq: Generation & Analysis]\n        Method --> M2[SeqBench: 基准测试套件/SeqBench: Benchmark Suite]\n        Method --> M3[基于形式语言理论/Based on Formal Language Theory]\n        Results --> R1[跨领域多功能/Versatile Across Domains]\n        Results --> R2[标准化实验/Standardizes Experiments]\n        Results --> R3[模块化开源工具/Modular Open-Source Tool]"
    },
    {
      "title": "Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses",
      "authors": "Yongyu Wang",
      "institution": "Michigan Technological University",
      "link": "https://arxiv.org/pdf/2512.22128",
      "code": null,
      "tags": [
        "graph neural networks",
        "adversarial robustness",
        "graph pruning",
        "message passing",
        "spurious connections",
        "graph defense"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp",
      "contributions": "1. Introduces a novel graph pruning framework that uses adversarial robustness evaluation to identify fragile graph components. 2. Proposes a method that selectively prunes edges based on robustness scores to improve model reliability and resilience. 3. Validates the framework by instantiating it on three representative GNN architectures and demonstrating significant defense enhancement in high-perturbation regimes.",
      "summary": "This paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks and noise in graph structure. It proposes a pruning framework that uses adversarial robustness scores to identify and remove detrimental edges, resulting in cleaner and more resilient graph representations. Experiments show this method significantly strengthens GNN defenses against high levels of perturbation.",
      "mindmap": "graph TB\n        A[Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(GNNs对图结构和特征的扰动敏感/GNNs vulnerable to graph & feature perturbations)\n        C --> C1(基于对抗鲁棒性评分的剪枝框架/Pruning framework guided by adversarial robustness scores)\n        D --> D1(显著增强高扰动下的防御能力/Significantly enhances GNN defense in high-perturbation regime)"
    },
    {
      "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling",
      "authors": "Conor Wallace, Umer Siddique, Yongcan Cao",
      "institution": "University of Texas at San Antonio",
      "link": "https://arxiv.org/pdf/2512.22129",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "ad-hoc teamwork",
        "retrieval-augmented generation",
        "teammate modeling",
        "Overcooked"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp",
      "contributions": "1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.",
      "summary": "This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.",
      "mindmap": "graph TB\n        Root(”RECOLLAB: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Ad-hoc Teammate Modeling<br>Ad-hoc队友建模”)\n        Problem --> P2(”Brittle Conventional Models<br>传统模型脆弱性”)\n        Method --> M1(”COLLAB: LLM-based Framework<br>基于LLM的框架”)\n        Method --> M2(”RECOLLAB: Adds RAG<br>增加RAG检索”)\n        Results --> R1(”Improved Adaptation<br>提升适应性”)\n        Results --> R2(”Pareto-Optimal Trade-offs<br>帕累托最优权衡”)"
    },
    {
      "title": "On Harnessing Idle Compute at the Edge for Foundation Model Training",
      "authors": "Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina",
      "institution": "The University of Edinburgh, Johns Hopkins University, Cisco Research",
      "link": "https://arxiv.org/pdf/2512.22142",
      "code": null,
      "tags": [
        "llm training",
        "edge computing",
        "tensor parallelism",
        "parameter server",
        "device heterogeneity",
        "fault-tolerance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp",
      "contributions": "1. A novel selective hybrid tensor parallelism method to finely partition training operations for edge devices. 2. A parameter server-centric training framework to cope with device memory limits and avoid communication bottlenecks. 3. A cost optimization model to guide device selection and workload distribution, effectively handling device heterogeneity and churn.",
      "summary": "The paper addresses the challenge of decentralized foundation model training on edge devices, which is hindered by memory limits, communication overhead, and device heterogeneity. It proposes Cleave, a new paradigm that uses selective hybrid tensor parallelism and a parameter server framework to partition training efficiently. The evaluation shows Cleave matches cloud-based training performance, scales to thousands of devices, and handles failures with much faster recovery than prior methods.",
      "mindmap": "graph TB\n    A[On Harnessing Idle Compute at the Edge for Foundation Model Training] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有边缘训练方法性能不足/Existing edge training falls short]\n    B --> B2[设备内存与通信瓶颈/Device memory & communication bottlenecks]\n    B --> B3[设备异构性与动态性/Device heterogeneity & dynamism]\n    C --> C1[选择性混合张量并行/Selective hybrid tensor parallelism]\n    C --> C2[参数服务器框架/Parameter server framework]\n    C --> C3[成本优化模型/Cost optimization model]\n    D --> D1[匹配云端训练性能/Matches cloud-based training]\n    D --> D2[扩展至数千设备/Scales to thousands of devices]\n    D --> D3[快速故障恢复/Fast failure recovery]"
    },
    {
      "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs",
      "authors": "Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong",
      "institution": "School of Software Engineering, Xi’an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.22147",
      "code": null,
      "tags": [
        "gpu kernels",
        "Minimal Executable Program (MEP)",
        "Automatic Error Repair",
        "Performance Pattern Inheritance",
        "iterative optimization",
        "cross-platform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.",
      "summary": "The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.",
      "mindmap": "graph TB\n        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]\n        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]\n        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]\n        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]\n        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]\n        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]\n        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]"
    },
    {
      "title": "Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders",
      "authors": "Hans Jarett J. Ong, Brian Godwin S. Lim, Dominic Dayta, Renzo Roel P. Tan, Kazushi Ikeda",
      "institution": "Nara Institute of Science and Technology, Kyoto University, Ateneo de Manila University",
      "link": "https://arxiv.org/pdf/2512.22150",
      "code": null,
      "tags": [
        "causal representation learning",
        "additive noise model",
        "Wasserstein auto-encoder",
        "identifiability",
        "unsupervised learning",
        "causal discovery"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp",
      "contributions": "1. Proposed LANCA, a novel autoencoder framework that operationalizes the Additive Noise Model (ANM) as an inductive bias for unsupervised causal representation learning., 2. Provided a theoretical analysis showing that the ANM constraint restricts admissible transformations to the affine class, resolving component-wise indeterminacy., 3. Introduced a deterministic WAE architecture with a differentiable ANM layer to explicitly optimize for residual independence, overcoming limitations of stochastic VAEs.",
      "summary": "This paper addresses the challenge of unsupervised causal representation learning by proposing LANCA, a method that uses the Additive Noise Model as an inductive bias within a deterministic autoencoder framework. Theoretically, it shows this constraint narrows down the solution space, and empirically, LANCA outperforms existing methods on synthetic and photorealistic benchmarks by being more robust to spurious correlations.",
      "mindmap": "graph TB\n        A[Towards Unsupervised Causal Representation Learning via LANCA] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[无监督因果表示学习的可识别性挑战/Identifiability Challenge in Unsupervised Causal Representation Learning]\n        C --> C1[提出LANCA：使用ANM作为归纳偏置/Propose LANCA: Using ANM as Inductive Bias]\n        C --> C2[确定性WAE与可微ANM层/Deterministic WAE with Differentiable ANM Layer]\n        D --> D1[理论：限制变换为仿射类/Theory: Restricts Transformations to Affine Class]\n        D --> D2[实证：在基准测试中优于SOTA/Empirical: Outperforms SOTA on Benchmarks]"
    },
    {
      "title": "SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models",
      "authors": "Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan",
      "institution": "Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.22170",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "reward model",
        "video generation",
        "reward hacking",
        "bradley-terry loss",
        "hierarchical attention"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp",
      "contributions": "1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.",
      "summary": "This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.",
      "mindmap": "graph TB\n        A[SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据标注噪声/Annotation Noise]\n        B --> B2[奖励黑客攻击/Reward Hacking]\n        B --> B3[模型架构设计不足/Under-explored RM Architecture]\n        C --> C1[单项目二元标注与跨提示配对/Single-item Binary Annotations & Cross-prompt Pairing]\n        C --> C2[分层渐进查询注意力/Hierarchical Progressive Query Attention]\n        C --> C3[改进的BT损失函数/Modified BT Loss for Win-Tie]\n        D --> D1[奖励模型评估指标提升/Improved Direct RM Evaluation Metrics]\n        D --> D2[视频生成后训练效果增强/Enhanced Efficacy of Post-training on Video Generation Models]"
    },
    {
      "title": "Wireless Traffic Prediction with Large Language Model",
      "authors": "Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini",
      "institution": "Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.22178",
      "code": null,
      "tags": [
        "spatio-temporal forecasting",
        "large language model",
        "wireless traffic prediction",
        "spatial-temporal correlation",
        "prompt engineering",
        "fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp",
      "contributions": "1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.",
      "summary": "This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.",
      "mindmap": "graph TB\n        A[Wireless Traffic Prediction with Large Language Model] --> B(核心问题/Problem: LLMs overlook spatial dependencies in city-scale wireless traffic)\n        A --> C(主要方法/Method: TIDES framework with clustering, prompt engineering, and DeepSeek spatial alignment module)\n        A --> D(关键结果/Results: Outperforms SOTA baselines in accuracy and robustness for 6G network management)"
    },
    {
      "title": "Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection",
      "authors": "Rajeeb Thapa Chhetri, Zhixiong Chen, Saurab Thapa",
      "institution": "Mercy University",
      "link": "https://arxiv.org/pdf/2512.22179",
      "code": null,
      "tags": [
        "anomaly detection",
        "manifold learning",
        "normalizing flows",
        "dual-centroid compactness loss",
        "out-of-distribution detection",
        "zero-shot generalization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp",
      "contributions": "1. Proposes Latent Sculpting, a novel two-stage framework that decouples manifold structure learning from density estimation for OOD anomaly detection. 2. Introduces the Dual-Centroid Compactness Loss (DCCL) to actively sculpt a compact, low-entropy latent manifold for benign data. 3. Demonstrates superior zero-shot generalization on the CIC-IDS-2017 benchmark, significantly outperforming supervised and unsupervised baselines on complex distribution shifts like \"Infiltration\".",
      "summary": "The paper addresses the problem of \"Generalization Collapse\" in supervised models when detecting Out-of-Distribution (OOD) anomalies. It proposes Latent Sculpting, a two-stage method that first uses a novel loss to sculpt a compact latent manifold for benign data and then applies a normalizing flow for density estimation. The results show this approach enables robust zero-shot anomaly detection, significantly outperforming existing methods on unseen attack scenarios.",
      "mindmap": "graph TB\n        A[Latent Sculpting for Zero-Shot Generalization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[泛化崩溃 / Generalization Collapse]\n        B --> B2[OOD异常检测失败 / OOD Anomaly Detection Failure]\n        C --> C1[潜在空间雕刻 / Latent Sculpting]\n        C1 --> C2[阶段1: DCCL损失 / Stage 1: DCCL Loss]\n        C1 --> C3[阶段2: MAF密度估计 / Stage 2: MAF Density Estimation]\n        D --> D1[零样本F1分数0.87 / Zero-Shot F1-Score 0.87]\n        D --> D2[渗透攻击检测率88.89% / Infiltration Detection 88.89%]"
    },
    {
      "title": "Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery",
      "authors": "Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir",
      "institution": "National University of Science and Technology (NUST)",
      "link": "https://arxiv.org/pdf/2512.22182",
      "code": null,
      "tags": [
        "dimensionality reduction",
        "Locally Linear Embedding (LLE)",
        "AI-enhanced LLE",
        "medical data analysis",
        "medical billing",
        "transcription"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp",
      "contributions": "1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.",
      "summary": "This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.",
      "mindmap": "graph TB\n    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)\n    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))\n    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)"
    },
    {
      "title": "Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks",
      "authors": "Vishnu Mohan",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.22186",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Dueling Double Deep Q-Network",
        "curriculum learning",
        "tennis simulation",
        "sequential decision-making",
        "sports analytics"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp",
      "contributions": "1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.",
      "summary": "This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.",
      "mindmap": "graph TB\n        A[Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks] --> B(核心问题/Problem: Tennis strategy optimization as a sequential decision-making challenge with hierarchical scoring, stochasticity, and opponent adaptation)\n        A --> C(主要方法/Method: Dueling Double Deep Q-Network (DDQN) trained with curriculum learning in a custom tennis simulation environment)\n        A --> D(关键结果/Results: High win rates (98-100%) and stable convergence, but reveals a defensive policy bias, highlighting reward design limitations)"
    },
    {
      "title": "Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants",
      "authors": "Jose I. Aizpurua",
      "institution": "University of the Basque Country (UPV/EHU)",
      "link": "https://arxiv.org/pdf/2512.22190",
      "code": null,
      "tags": [
        "prognostics & health management (phm)",
        "Neural Networks",
        "Convolutional Neural Networks",
        "Reinforcement Learning",
        "Uncertainty Quantification",
        "Physics-Informed Machine Learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp",
      "contributions": "1. Introduces the application of Neural Networks (NNs) and their variants, specifically Convolutional Neural Networks (CNNs), for transformer condition monitoring using diverse data modalities. 2. Discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control in transformer health management. 3. Provides perspectives on emerging research directions at the intersection of physics-informed machine learning and transformer Prognostics & Health Management (PHM).",
      "summary": "This paper addresses the limitations of traditional, rule-based transformer condition monitoring by proposing the use of machine learning, particularly Neural Networks and their variants. It explores Convolutional Neural Networks for processing diverse sensor data and discusses Reinforcement Learning for control, concluding that physics-informed ML provides a powerful framework for more accurate diagnostics, prognostics, and decision-making in power transformer health management.",
      "mindmap": "graph TB\n        Root[”Physics-Informed ML for Transformer Condition Monitoring – Part I”] --> Problem[”核心问题/Problem: Traditional monitoring struggles with uncertainty & complexity”]\n        Root --> Method[”主要方法/Method: Use Neural Networks, CNNs, and Reinforcement Learning”]\n        Root --> Results[”关键结果/Results: Enables accurate diagnostics, prognostics, and control”]"
    },
    {
      "title": "Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification",
      "authors": "Jose I. Aizpurua",
      "institution": "University of the Basque Country (UPV/EHU)",
      "link": "https://arxiv.org/pdf/2512.22189",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "Physics-Informed Neural Networks (PINNs)",
        "Bayesian PINNs",
        "uncertainty quantification",
        "Prognostics & Health Management (PHM)",
        "transformer condition monitoring"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp",
      "contributions": "1. Introduces Physics-Informed Neural Networks (PINNs) for integrating physics into neural network training for transformer applications like thermal modeling and insulation ageing. 2. Presents Bayesian PINNs as a framework to quantify epistemic uncertainty, enabling robust predictions under sparse data conditions. 3. Outlines emerging research directions for physics-aware and trustworthy machine learning in the management of critical power assets.",
      "summary": "This paper proposes using Physics-Informed Neural Networks (PINNs) and their Bayesian extension to improve transformer condition monitoring. The method integrates physical laws directly into the learning process and quantifies model uncertainty, aiming to deliver more reliable predictions with limited data. The work highlights the potential of physics-aware machine learning for robust prognostics and health management of critical power infrastructure.",
      "mindmap": "graph TB\n        A[Physics-Informed Machine Learning for Transformer Condition Monitoring – Part II] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统神经网络缺乏物理机制，数据稀疏下不可靠/Traditional NNs lack physics, unreliable with sparse data]\n        C --> C1[物理信息神经网络/PINNs]\n        C --> C2[贝叶斯PINNs量化认知不确定性/Bayesian PINNs for epistemic UQ]\n        D --> D1[提高稀疏数据下的鲁棒性/Improved robustness under sparse data]\n        D --> D2[为关键资产提供可信的机器学习/Trustworthy ML for critical assets]"
    },
    {
      "title": "Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks",
      "authors": "Jiahao Lu",
      "institution": "The Chinese University of Hong Kong, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.22192",
      "code": "https://github.com/lujiahao760/FrequencyRegularization",
      "tags": [
        "regularization theory",
        "Spectral Bias",
        "L2 Regularization",
        "Frequency Principle",
        "Spectral Suppression Ratio",
        "Inductive Bias"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp",
      "contributions": "1. Introduced a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during CNN training. 2. Proposed a novel metric, the Spectral Suppression Ratio (SSR), to quantify the \"low-pass filtering\" intensity of different regularizers. 3. Empirically revealed a critical Accuracy-Robustness Trade-off in L2-regularized models, showing their sensitivity to broadband noise but superior robustness to high-frequency information loss.",
      "summary": "This paper investigates the spectral inductive bias of deep neural networks, focusing on how regularization techniques like L2 affect feature frequency selection. The authors propose a visual diagnostic framework and a new metric to quantify spectral suppression, demonstrating that L2 regularization strongly suppresses high-frequency energy and leads to a trade-off between accuracy and robustness. The work confirms that regularization enforces a strong spectral bias towards low-frequency structures, providing a signal-processing perspective on generalization.",
      "mindmap": "graph TB\n        A[Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[理解正则化的物理机制<br/>Understanding the physical mechanism of regularization]\n        B --> B2[探索特征频率选择<br/>Exploring feature frequency selection]\n        C --> C1[视觉诊断框架<br/>Visual Diagnostic Framework]\n        C --> C2[谱抑制比 (SSR)<br/>Spectral Suppression Ratio (SSR)]\n        D --> D1[L2正则化抑制高频能量<br/>L2 regularization suppresses high-frequency energy]\n        D --> D2[揭示准确率-鲁棒性权衡<br/>Reveals Accuracy-Robustness Trade-off]"
    },
    {
      "title": "MatKV: Trading Compute for Flash Storage in LLM Inference",
      "authors": "Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee",
      "institution": "Seoul National University, Samsung Electronics",
      "link": "https://arxiv.org/pdf/2512.22195",
      "code": "https://github.com/kunwooshin/MatKV",
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "key-value cache",
        "flash storage",
        "prefill optimization",
        "power efficiency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp",
      "contributions": "1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.",
      "summary": "The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.",
      "mindmap": "graph TB\n        Root[”MatKV: Trading Compute for Flash Storage in LLM Inference”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>RAG推理中prefill阶段计算开销大<br>High compute cost of prefill in RAG inference”]\n        Method[”主要方法/Method<br>预计算并物化KV向量到闪存<br>Precompute & materialize KVs to flash storage”]\n        Results[”关键结果/Results<br>推理时间与能耗减半<br>Halves inference time & power consumption”]"
    },
    {
      "title": "AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.22196",
      "code": null,
      "tags": [
        "semantic change detection",
        "diachronic embeddings",
        "orthogonal Procrustes",
        "lexical drift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp",
      "contributions": "1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.",
      "summary": "This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.",
      "mindmap": "graph TB\n        A[AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”数字人文中语义变迁分析<br>Digital Humanities Semantic Shift Analysis”)\n        C --> C1(”可复现的专家系统流程<br>Reproducible Expert-System Pipeline”)\n        C1 --> C2(”分时段词嵌入与对齐<br>Temporal Embeddings & Alignment”)\n        C2 --> C3(”几何位移与邻域变化度量<br>Geometric & Neighborhood Metrics”)\n        D --> D1(”可视化法律概念演变<br>Visualizing Legal Concept Evolution”)\n        D1 --> D2(”揭示与历史事件的关联<br>Revealing Links to Historical Events”)"
    },
    {
      "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
      "authors": "Yogeswar Reddy Thota",
      "institution": "University of Texas at Dallas",
      "link": "https://arxiv.org/pdf/2512.22206",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "dynamic routing",
        "residual networks",
        "cosine incompatibility",
        "Gumbel-Softmax",
        "FLOPs regularization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp",
      "contributions": "1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.",
      "summary": "The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.",
      "mindmap": "graph TB\n        A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --> B[核心问题/Problem: Modern residual networks perform redundant computation for all inputs]\n        A --> C[主要方法/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]\n        A --> D[关键结果/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]"
    },
    {
      "title": "Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents",
      "authors": "Dhruv Tiwari",
      "institution": "Lovely Professional University",
      "link": "https://arxiv.org/pdf/2512.22200",
      "code": null,
      "tags": [
        "reinforcement learning",
        "intrinsic motivation",
        "homeostatic control",
        "adaptive optimization",
        "non-stationary learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp",
      "contributions": "1. Proposes a novel framework, Emotion-Inspired Learning Signals (EILS), that models emotions as continuous, homeostatic appraisal signals (e.g., Curiosity, Stress, Confidence) for adaptive control. 2. Formalizes these signals as vector-valued internal states derived from interaction history to dynamically modulate the agent's optimization landscape in real-time. 3. Hypothesizes that this closed-loop homeostatic regulation enables superior sample efficiency and adaptation to non-stationary environments compared to standard baselines like PPO.",
      "summary": "The paper identifies the fragility of standard AI agents that rely on static, external rewards in open-ended environments. It proposes the Emotion-Inspired Learning Signals (EILS) framework, which uses bio-inspired internal signals like curiosity and stress to dynamically control learning. The authors hypothesize this approach will lead to more robust, adaptive, and sample-efficient autonomous agents.",
      "mindmap": "graph TB\n        Root[EILS: A Homeostatic Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[静态外部奖励/Static Extrinsic Reward]\n        Problem --> P2[脆弱性，无法适应/Fragile, Non-Adaptive]\n        Method[主要方法/Method] --> M1[情绪启发信号/Emotion-Inspired Signals]\n        Method --> M2[动态稳态调节/Dynamic Homeostatic Control]\n        Results[关键结果/Results] --> R1[假设: 更高样本效率/Hypothesis: Higher Sample Efficiency]\n        Results --> R2[假设: 更好非平稳适应/Hypothesis: Better Non-Stationary Adaptation]"
    },
    {
      "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
      "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang",
      "institution": "Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC",
      "link": "https://arxiv.org/pdf/2512.22208",
      "code": "https://github.com/moxin-org/Moxin-LLM",
      "tags": [
        "multimodal large language models",
        "Moxin",
        "open-source LLM",
        "vision-language-action",
        "Model Openness Framework",
        "multimodal models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp",
      "contributions": "1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.",
      "summary": "This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.",
      "mindmap": "graph TB\n        A[Open-Source Multimodal Moxin Models<br/>开源多模态Moxin模型] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Proprietary vs. Open-Source LLMs<br/>闭源与开源大语言模型]\n        B --> B2[Need for transparent, capable open models<br/>需要透明、强大的开源模型]\n        C --> C1[Develop Moxin 7B under Model Openness Framework<br/>基于模型开放框架开发Moxin 7B]\n        C --> C2[Create variants: VLM, VLA, Chinese<br/>创建变体: VLM, VLA, 中文模型]\n        D --> D1[Superior performance in evaluations<br/>在评估中表现优异]\n        D --> D2[Full release of models, code, data<br/>完整发布模型、代码、数据]"
    },
    {
      "title": "Transformer Reconstructed with Dynamic Value Attention",
      "authors": "Xiaowei Wang",
      "institution": "College of Artificial Intelligence, China University of Petroleum (Beijing)",
      "link": "https://arxiv.org/pdf/2512.22212",
      "code": null,
      "tags": [
        "transformer architecture",
        "dynamic value attention",
        "single-head attention",
        "feed forward network removal"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp",
      "contributions": "1. Proposed Dynamic Value Attention (DVA), a method to dynamically decide a value for each query, addressing the limitation of static values in standard attention heads. 2. Enabled the removal of redundant multi-head attention, reducing the architecture to a single-head attention mechanism. 3. Demonstrated that the subsequent feed-forward network can be entirely removed, as the revised embeddings already capture sufficient contextual information.",
      "summary": "The paper identifies a limitation in the standard Transformer where static values are used for all queries within an attention head. It proposes Dynamic Value Attention (DVA), which computes a dynamic value per query, allowing the model to use only a single attention head and remove the feed-forward network entirely. Experiments show DVA reduces training time by 37.6% while improving learning capability.",
      "mindmap": "graph TB\n        Root(”Transformer Reconstructed with Dynamic Value Attention”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”静态值/Static Value”)\n        Problem --> P2(”多头冗余/Multi-head Redundancy”)\n        Method --> M1(”动态值注意力/Dynamic Value Attention (DVA)”)\n        Method --> M2(”单头架构/Single-head Architecture”)\n        Method --> M3(”移除前馈网络/Remove Feed-Forward Network”)\n        Results --> R1(”训练时间减少37.6%/37.6% Training Time Saved”)\n        Results --> R2(”学习能力提升/Learning Capability Increased”)"
    },
    {
      "title": "On the Existence and Behaviour of Secondary Attention Sinks",
      "authors": "Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao",
      "institution": "Imperial College London, UnlikelyAI",
      "link": "https://arxiv.org/pdf/2512.22213",
      "code": null,
      "tags": [
        "llm inference",
        "attention sinks",
        "transformer",
        "mlp",
        "attention mechanism",
        "large language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp",
      "contributions": "1. Identifies and characterizes a new class of \"secondary attention sinks\" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink's direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.",
      "summary": "This paper identifies a new phenomenon called \"secondary attention sinks\" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.",
      "mindmap": "graph TB\n        A[”On the Existence and Behaviour of Secondary Attention Sinks<br/>二次注意力汇的存在与行为”] --> B[”核心问题/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>先前研究仅关注持久的主汇（如BOS）”]\n        A --> C[”主要方法/Method<br/>Extensive experiments across 11 model families<br/>对11个模型系列进行广泛实验”]\n        A --> D[”关键结果/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>次级汇通过中间层MLP形成<br/>2. L2-norm determines sink score & lifetime<br/>L2范数决定汇分数与寿命<br/>3. Sink levels are deterministic in large models<br/>大模型中汇层级更确定”]"
    },
    {
      "title": "Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning",
      "authors": "Soroush Vahidi",
      "institution": "New Jersey Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22221",
      "code": null,
      "tags": [
        "graph neural networks",
        "heterophily",
        "interpretability",
        "combinatorial inference",
        "hybrid learning",
        "node classification"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp",
      "contributions": "1. Proposes an interpretable, adaptive framework for node classification using explicit combinatorial inference instead of deep message passing, with a tunable scoring function. 2. Introduces a validation-gated hybrid strategy that optionally refines combinatorial predictions with a lightweight neural model only when beneficial. 3. Ensures a leakage-free evaluation protocol by computing all adaptation signals strictly from training data.",
      "summary": "This paper addresses the challenge of node classification on heterophilic graphs where standard GNNs struggle. It proposes an interpretable framework based on combinatorial scoring and a conditional hybrid learning strategy, achieving competitive performance with modern GNNs while offering better interpretability, tunability, and efficiency.",
      "mindmap": "graph TB\n        Root(”Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”GNNs在异配图上表现不佳/GNNs struggle on heterophilic graphs”)\n        Method --> M1(”组合推理与评分函数/Combinatorial Inference & Scoring”)\n        Method --> M2(”验证门控混合策略/Validation-gated Hybrid Strategy”)\n        Results --> R1(”性能有竞争力/Competitive Performance”)\n        Results --> R2(”可解释性与高效性/Interpretability & Efficiency”)"
    },
    {
      "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs",
      "authors": "Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia",
      "institution": "Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University",
      "link": "https://arxiv.org/pdf/2512.22219",
      "code": "https://github.com/mirage-project/mirage",
      "tags": [
        "llm inference",
        "megakernel",
        "kernel fusion",
        "SM-level graph",
        "software pipelining",
        "CUDA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp",
      "contributions": "1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency.",
      "summary": "The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits.",
      "mindmap": "graph TB\n        A[Mirage Persistent Kernel<br>幻影持久内核] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Kernel-per-operator execution<br>limits GPU optimization<br>逐算子内核执行限制GPU优化]\n        C --> C1[SM-level graph &<br>mega-kernel runtime<br>SM级图与巨型内核运行时]\n        D --> D1[Reduces inference latency<br>by up to 1.7x<br>推理延迟降低高达1.7倍]"
    },
    {
      "title": "Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases",
      "authors": "Gnankan Landry Regis N'guessan",
      "institution": "Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre",
      "link": "https://arxiv.org/pdf/2512.22222",
      "code": null,
      "tags": [
        "neural network architecture",
        "Müntz-Szász Networks",
        "fractional power bases",
        "physics-informed neural networks",
        "universal approximation",
        "singular function approximation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp",
      "contributions": "1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.",
      "summary": "The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN's universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.",
      "mindmap": "graph TB\n    A[Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases] --> B[核心问题/Problem: Standard neural networks poorly approximate singular/fractional power functions common in physics]\n    A --> C[主要方法/Method: Proposes MSN with learnable fractional power bases, replacing fixed activations]\n    A --> D[关键结果/Results: MSN achieves superior approximation rates, lower error with fewer parameters, and significant improvement on PINN benchmarks]"
    },
    {
      "title": "VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs",
      "authors": "Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao",
      "institution": "University of Science and Technology of China, Ant Group",
      "link": "https://arxiv.org/pdf/2512.22226",
      "code": "https://github.com/zheng980629/VideoScaffold",
      "tags": [
        "video understanding",
        "streaming video",
        "multimodal large language models",
        "event segmentation",
        "hierarchical representation",
        "elastic-scale"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",
      "contributions": "1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.",
      "summary": "This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.",
      "mindmap": "graph TB\n        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"
    },
    {
      "title": "ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis",
      "authors": "Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam",
      "institution": "North Carolina A&T State University, University of Texas at San Antonio",
      "link": "https://arxiv.org/pdf/2512.22223",
      "code": "https://github.com/270771/llm-traffictraffic",
      "tags": [
        "rag (retrieval-augmented generation)",
        "retrieval-augmented generation (RAG)",
        "network traffic analysis",
        "large language models (LLMs)",
        "hierarchical retrieval",
        "explainable AI"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp",
      "contributions": "1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.",
      "summary": "The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.",
      "mindmap": "graph TB\n        A[ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Traditional traffic analysis systems have high false positives and lack interpretability.]\n        C[主要方法/Method: Multi-stage framework using traffic summarization, RAG, and LLM reasoning with a hierarchical retrieval pipeline.]\n        D[关键结果/Results: Achieves 95.95%-98.82% accuracy, outperforms baselines, and provides explainable, evidence-grounded responses.]"
    },
    {
      "title": "Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces",
      "authors": "Sophie Zhao",
      "institution": "Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22227",
      "code": null,
      "tags": [
        "representation analysis",
        "sentence embeddings",
        "probing",
        "hierarchical geometry",
        "transformer models",
        "cognitive states"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp",
      "contributions": "1. Constructed a novel dataset of 480 sentences annotated with continuous energy scores and discrete tier labels for seven ordered cognitive categories. 2. Demonstrated that both continuous scores and discrete tier labels are reliably decodable from fixed transformer sentence embeddings using linear and nonlinear probes, with nonlinear probes providing consistent gains. 3. Provided statistical and qualitative evidence (via permutation tests, UMAP visualizations, and confusion matrices) that the embedding space exhibits a hierarchical geometric organization aligned with human-defined cognitive attributes, beyond surface word statistics.",
      "summary": "This paper investigates whether transformer-based sentence embeddings encode a hierarchical structure aligned with cognitive states. The authors construct an annotated dataset and use linear and nonlinear probes to decode continuous scores and discrete labels from embeddings, finding reliable recoverability and a structured geometric gradient. The results show that transformer embedding spaces exhibit a systematic organization corresponding to interpretable cognitive attributes.",
      "mindmap": "graph TB\n        Root(”Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces”) --> Problem(”核心问题/Problem: Do sentence embeddings encode hierarchical cognitive structure?”)\n        Root --> Method(”主要方法/Method: Probe analysis on annotated dataset with linear/nonlinear classifiers”)\n        Root --> Results(”关键结果/Results: Reliable decoding, hierarchical geometry aligned with cognitive attributes”)"
    },
    {
      "title": "We are not able to identify AI-generated images",
      "authors": "Adrien Pavão",
      "institution": "(Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)",
      "link": "https://arxiv.org/pdf/2512.22236",
      "code": null,
      "tags": [
        "image forensics",
        "AI-generated images",
        "human evaluation",
        "MidJourney",
        "CC12M",
        "synthetic media detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp",
      "contributions": "1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.",
      "summary": "The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.",
      "mindmap": "graph TB\n        Root[We are not able to identify AI-generated images] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Can humans reliably distinguish AI-generated images from real photos?]\n        Method[主要方法/Method: Interactive web experiment with a curated dataset of 120 difficult images (CC12M real vs. MidJourney AI-generated)]\n        Results[关键结果/Results: Average human accuracy is 54% (near random), response time ~7.3s, highlighting human insufficiency and need for guidelines]"
    },
    {
      "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
      "authors": "Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu",
      "institution": "Fudan University, Shanghai Innovation Institute, OpenMoss Team",
      "link": "https://arxiv.org/pdf/2512.22234",
      "code": "https://github.com/OpenMOSS/DiRL",
      "tags": [
        "post-training (sft/rlhf)",
        "Diffusion Language Models",
        "FlexAttention",
        "Group Relative Policy Optimization",
        "LMDeploy",
        "blockwise training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp",
      "contributions": "1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.",
      "summary": "This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.",
      "mindmap": "graph TB\n        A[DiRL: An Efficient Post-Training Framework for Diffusion Language Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[dLLMs后训练低效/Post-training for dLLMs is inefficient]\n        B --> B2[训练与推理目标不匹配/Training-Inference objective mismatch]\n        C --> C1[DiRL框架/DiRL Framework]\n        C1 --> C1_1[整合FlexAttention与LMDeploy/Integrates FlexAttention & LMDeploy]\n        C1 --> C1_2[两阶段后训练/Two-stage post-training (SFT+RL)]\n        C --> C2[DiPO算法/DiPO Algorithm]\n        C2 --> C2_1[无偏GRPO实现/Unbiased GRPO for dLLMs]\n        D --> D1[高效训练与推理/Efficient Training & Inference]\n        D --> D2[数学SOTA性能/Math SOTA Performance]\n        D --> D3[超越Qwen2.5系列/Surpasses Qwen2.5 series]"
    },
    {
      "title": "Enhanced geometry prediction in laser directed energy deposition using meta-learning",
      "authors": "Abdul Malik Al Mardhouf Al Saadi, Amrita Basak",
      "institution": "The Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.22241",
      "code": null,
      "tags": [
        "meta-learning",
        "meta-learning",
        "model-agnostic meta-learning",
        "reptile",
        "laser-directed energy deposition",
        "bead geometry prediction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp",
      "contributions": "1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.",
      "summary": "This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.",
      "mindmap": "graph TB\n        A[Enhanced geometry prediction in L-DED using meta-learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Data scarcity & heterogeneity in L-DED geometry prediction]\n        C[主要方法/Method: Meta-learning (MAML & Reptile) for cross-dataset knowledge transfer]\n        D[关键结果/Results: Accurate prediction with few examples, outperforms conventional NN]"
    },
    {
      "title": "Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening",
      "authors": "Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf",
      "institution": "Radboud University Medical Center, Radboud University",
      "link": "https://arxiv.org/pdf/2512.22242",
      "code": null,
      "tags": [
        "medical imaging",
        "algorithmic fairness",
        "subgroup performance analysis",
        "JustEFAB framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",
      "contributions": "1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.",
      "summary": "This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.",
      "mindmap": "graph TB\n        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"
    },
    {
      "title": "Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning",
      "authors": "Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell",
      "institution": "Maynooth University, University College Dublin, Queen's University Belfast, Teagasc, Agri-Food and Biosciences Institute (AFBI)",
      "link": "https://arxiv.org/pdf/2512.22243",
      "code": null,
      "tags": [
        "transfer learning",
        "TabPFN",
        "FT-Transformer",
        "permutation-based variable importance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp",
      "contributions": "1. Evaluated and compared the performance of multiple deep learning and transfer learning models (MLP, TabPFN, TabNet, FT-Transformer) for predicting mycotoxin contamination in oats. 2. Applied these models to a multi-response prediction task using a dataset of environmental, agronomic, and geographical predictors from Irish oat samples. 3. Conducted a permutation-based variable importance analysis, identifying weather history in the 90-day pre-harvest period and seed moisture content as the most influential predictors.",
      "summary": "This study uses neural networks and transfer learning to predict mycotoxin contamination in Irish oat crops. The models, including TabPFN, TabNet, and FT-Transformer, were evaluated on a multi-response task, with TabPFN achieving the best overall performance. The analysis found that weather patterns before harvest and seed moisture are the most critical factors for prediction.",
      "mindmap": "graph TB\n        A[Predicting Mycotoxin Contamination in Irish Oats<br>预测爱尔兰燕麦中的霉菌毒素污染] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Mycotoxin contamination risks food safety and agricultural productivity.<br>霉菌毒素污染威胁食品安全和农业生产力。]\n        C[主要方法/Method<br>Use neural networks and transfer learning (TabPFN, TabNet, FT-Transformer) for multi-response prediction.<br>使用神经网络和迁移学习进行多响应预测。]\n        D[关键结果/Results<br>TabPFN performed best; weather history and seed moisture are key predictors.<br>TabPFN表现最佳；天气历史和种子水分是关键预测因子。]"
    },
    {
      "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
      "authors": "Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.22238",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "knowledge distillation",
        "reinforcement learning",
        "vision-language models",
        "progressive masking",
        "offline RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp",
      "contributions": "1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.",
      "summary": "The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.",
      "mindmap": "graph TB\n        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]\n        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]\n        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]\n        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]\n        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]\n        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]\n        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]\n        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]"
    },
    {
      "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs",
      "authors": "Chama Bensmail",
      "institution": "University of Hertfordshire, Omics Data Solutions LTD",
      "link": "https://arxiv.org/pdf/2512.22240",
      "code": "https://github.com/bensmailchama-boop/EvoXplain",
      "tags": [
        "interpretability",
        "mechanistic multiplicity",
        "explanatory stability",
        "stochastic optimization",
        "model explanations",
        "diagnostic framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp",
      "contributions": "1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.",
      "summary": "The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.",
      "mindmap": "graph TB\n        A[EvoXplain Paper] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[高精度模型是否共享相同内部逻辑?<br/>Do high-accuracy models share the same internal logic?]\n        C --> C1[跨重复训练测量解释稳定性<br/>Measure explanation stability across repeated training]\n        C --> C2[将解释视为优化过程样本<br/>Treat explanations as samples from optimization]\n        D --> D1[发现解释的多模态性<br/>Found explanatory multimodality]\n        D --> D2[逻辑回归等模型也显示多种机制<br/>Models like Logistic Regression show multiple mechanisms]\n        D --> D3[重新定义可解释性为模型类属性<br/>Reframe interpretability as model-class property]"
    },
    {
      "title": "Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation",
      "authors": "Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda",
      "institution": "Meta (FAIR at Meta, Meta Superintelligence Labs)",
      "link": "https://arxiv.org/pdf/2512.22245",
      "code": null,
      "tags": [
        "llm inference",
        "uncertainty estimation",
        "calibration",
        "linear probe",
        "brier score",
        "llm-as-judge"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp",
      "contributions": "1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.",
      "summary": "This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model's hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.",
      "mindmap": "graph TB\n        A[Calibrating LLM Judges<br/>校准LLM法官] --> B[Problem: LLM judges lack efficient, calibrated uncertainty<br/>问题：LLM法官缺乏高效、校准的不确定性估计]\n        A --> C[Method: Linear probes on hidden states with Brier score loss<br/>方法：基于Brier分数损失的隐状态线性探针]\n        A --> D[Results: Better calibration, 10x speedup, robust generalization<br/>结果：更好的校准，10倍加速，鲁棒的泛化]"
    },
    {
      "title": "Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation",
      "authors": "Rohit Pandey, Rohan Pandey",
      "institution": "Bellevue High School, University of Washington",
      "link": "https://arxiv.org/pdf/2512.22248",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "amortized inference",
        "sim-to-real transfer",
        "model rocketry",
        "neural network",
        "parameter estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp",
      "contributions": "1. Formulates model rocket parameter estimation as an amortized inference problem and demonstrates neural networks can invert physics simulations from sparse observations. 2. Proposes a simulation-based amortized inference approach that enables zero-shot sim-to-real transfer for aerodynamic parameter estimation. 3. Provides quantitative analysis of the sim-to-real gap and shows the learned model reduces apogee prediction error compared to a traditional baseline (OpenRocket).",
      "summary": "This paper presents a simulation-based amortized inference method that trains a neural network on synthetic flight data to predict aerodynamic parameters like drag coefficient from a single apogee measurement. The trained model is applied directly to real flights without fine-tuning, achieving promising zero-shot sim-to-real transfer and reducing apogee prediction error compared to traditional tools.",
      "mindmap": "graph TB\n        Root(”Amortized Inference for Model Rocket Aerodynamics”) --> Problem(”核心问题/Problem: Estimating aerodynamic parameters from sparse real flight data is difficult and costly.”)\n        Root --> Method(”主要方法/Method: Train neural network on synthetic data to invert physics simulator for amortized inference.”)\n        Root --> Results(”关键结果/Results: Promising zero-shot sim-to-real transfer with reduced apogee prediction error.”)"
    },
    {
      "title": "The Affine Divergence: Aligning Activation Updates Beyond Normalisation",
      "authors": "George Bird",
      "institution": "University of Manchester",
      "link": "https://arxiv.org/pdf/2512.22247",
      "code": null,
      "tags": [
        "optimization theory",
        "activation updates",
        "gradient descent",
        "normalisation",
        "PatchNorm",
        "affine divergence"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp",
      "contributions": "1. Identifies a systematic mismatch between mathematically ideal and effective activation updates during gradient descent, providing a new theoretical framework for understanding optimization. 2. Derives normalisation from first principles as a solution to this mismatch and proposes a functionally distinct, non-scale-invariant alternative that outperforms conventional normalisers. 3. Introduces \"PatchNorm\", a new compositionally inseparable normaliser for convolutional layers, and argues for reframing normalisers as activation-function-like maps with parameterised scaling.",
      "summary": "The paper identifies a mismatch between the ideal steepest-descent direction for activations and their effective updates during gradient descent. It proposes a theoretical framework that derives normalisation as a solution and introduces a new alternative, including \"PatchNorm\" for convolutions, which empirically outperforms standard normalisers. This reframes normalisation's role and questions the standard affine+nonlinear model-building approach.",
      "mindmap": "graph TB\n        Root[The Affine Divergence: Aligning Activation Updates Beyond Normalisation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[激活更新不匹配/Activation update mismatch]\n        P1 --> P2[非理想缩放/Non-ideal scaling]\n        Method[主要方法/Method] --> M1[理论推导/Theoretical derivation]\n        M1 --> M2[提出新方案/Propose new solutions]\n        M2 --> M3[PatchNorm for convolution]\n        Results[关键结果/Results] --> R1[重构归一化/Reframe normalisation]\n        R1 --> R2[新方案有效/New methods outperform]"
    },
    {
      "title": "Analyzing Skill Element in Online Fantasy Cricket",
      "authors": "Sarthak Sarkar, Supratim Das, Purushottam Saha, Diganta Mukherjee, Tridib Mukherjee",
      "institution": "Indian Statistical Institute, Kolkata",
      "link": "https://arxiv.org/pdf/2512.22254",
      "code": null,
      "tags": [
        "game theory and decision making",
        "statistical framework",
        "team selection strategies",
        "dynamic tournament model",
        "softmax reweighting",
        "IPL dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp",
      "contributions": "1. Development of a statistical framework to assess the role of skill in online fantasy cricket. 2. Construction and analysis of a range of deterministic and stochastic team selection strategies. 3. Introduction of a dynamic tournament model with agent populations evolving via a softmax reweighting mechanism.",
      "summary": "This paper develops a statistical framework to analyze whether success in online fantasy cricket is driven by skill or chance. It constructs various team selection strategies and a dynamic tournament model, testing them on IPL 2024 data. The results provide quantitative evidence supporting the presence of a skill element in these platforms.",
      "mindmap": "graph TB\n        A[Analyzing Skill Element in Online Fantasy Cricket] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Fantasy sports: skill or chance?]\n        C --> C1[Statistical framework]\n        C1 --> C2[Team selection strategies]\n        C2 --> C3[Deterministic & Stochastic]\n        C1 --> C4[Dynamic tournament model]\n        C4 --> C5[Softmax reweighting]\n        C --> C6[Experiments on IPL 2024]\n        D --> D1[Evidence for skill element]"
    },
    {
      "title": "Graph Attention-based Adaptive Transfer Learning for Link Prediction",
      "authors": "Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu",
      "institution": "Jinan University, Great Bay University, JD Technology, University of Illinois Chicago",
      "link": "https://arxiv.org/pdf/2512.22252",
      "code": "https://github.com/DSI-Lab1/GAATNet",
      "tags": [
        "graph neural networks",
        "graph attention network",
        "link prediction",
        "transfer learning",
        "graph transformer",
        "contrastive loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp",
      "contributions": "1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.",
      "summary": "The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.",
      "mindmap": "graph TB\n        A[Graph Attention-based Adaptive Transfer Learning for Link Prediction] --> B[核心问题/Problem: Challenges in large-scale sparse graphs and cross-dataset transfer learning for link prediction]\n        A --> C[主要方法/Method: Proposes GAATNet with distant neighbor embeddings and lightweight self-adapter for adaptive transfer]\n        A --> D[关键结果/Results: Achieves SOTA performance on seven datasets, provides scalable GNN-transfer learning solution]"
    },
    {
      "title": "Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models",
      "authors": "Zheng Xing, Weibing Zhao",
      "institution": "Shenzhen University, Shenzhen MSU-BIT University",
      "link": "https://arxiv.org/pdf/2512.22249",
      "code": null,
      "tags": [
        "human motion segmentation",
        "temporal vision semantics",
        "subspace clustering",
        "large language model",
        "temporal regularizer",
        "feedback framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp",
      "contributions": "1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.",
      "summary": "This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models] --> B(核心问题 / Problem: 传统无监督人体运动分割方法忽略了时序语义的作用 / Traditional unsupervised HMS overlooks temporal semantics)\n        A --> C(主要方法 / Method: 利用LLM提取时序视觉语义，并融入子空间聚类框架 / Use LLM to extract TVS and integrate it into subspace clustering)\n        A --> D(关键结果 / Results: 在四个基准数据集上性能超越现有方法 / Outperforms SOTA on four benchmark datasets)\n        C --> C1(LLM查询 / LLM Query: 判断相邻帧是否描述相同运动 / Determine if consecutive frames depict the same motion)\n        C --> C2(时序正则化 / Temporal Regularizer: 诱导相邻帧共享相似子空间嵌入 / Induces similar subspace embeddings for temporal neighbors)\n        C --> C3(反馈优化 / Feedback Optimization: 基于分割结果迭代优化嵌入 / Iteratively optimizes embedding based on segmentation output)"
    },
    {
      "title": "Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs",
      "authors": "Pascal Passigan, Kevin zhu, Angelina Ning",
      "institution": "Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22251",
      "code": null,
      "tags": [
        "graph neural networks",
        "biomedical knowledge graph",
        "graph attention network",
        "gene perturbation",
        "multimodal embeddings",
        "PrimeKG++"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp",
      "contributions": "1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.",
      "summary": "This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.",
      "mindmap": "graph TB\n        A[Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Predicting granular gene expression changes (perturbations) from drugs, beyond binary drug-disease associations.]\n        C[主要方法/Method: Merge PrimeKG++ & LINCS L1000 into a BKG; use Graph Attention Network (GAT) to predict delta expression.]\n        D[关键结果/Results: Outperforms MLP baselines; ablation shows KG edges enhance prediction for mechanistic modeling.]"
    },
    {
      "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method",
      "authors": "Satvik Tripathi",
      "institution": "University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.22258",
      "code": "https://github.com/satviktri/LSP",
      "tags": [
        "prompt engineering",
        "Logic Sketch Prompting",
        "deterministic prompting",
        "interpretability",
        "rule adherence",
        "clinical decision support"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp",
      "contributions": "1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.",
      "summary": "The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.",
      "mindmap": "graph TB\n        A[Logic Sketch Prompting (LSP)] --> B[核心问题/Problem: LLMs unreliable on tasks needing strict rules & determinism]\n        A --> C[主要方法/Method: Lightweight framework with typed variables, condition evaluators, rule validator]\n        A --> D[关键结果/Results: Highest accuracy/F1 vs. baselines; suitable for clinical/safety-critical systems]"
    },
    {
      "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
      "authors": "Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville",
      "institution": "University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs",
      "link": "https://arxiv.org/pdf/2512.22255",
      "code": null,
      "tags": [
        "reasoning",
        "chain-of-thought",
        "synthetic data",
        "distribution shift",
        "fine-tuning",
        "reasoning robustness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp",
      "contributions": "1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model's reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model's distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.",
      "summary": "This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data's distribution is closer to the model's own. The key conclusion is that aligning the training data distribution with the model's is more critical for performance than the correctness of the final answers.",
      "mindmap": "graph TB\n        A[Shape of Thought / 思维形态] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Correctness vs. Distribution / 正确性与数据分布]\n        B1 --> B2{Does correctness guarantee better reasoning? / 正确性保证更好的推理吗?}\n        C --> C1[Train on Incorrect Synthetic CoT / 使用错误的合成CoT训练]\n        C --> C2[Paraphrase Human Traces / 改写人类标注的推理链]\n        C --> C3[Introduce Flawed Steps / 引入有缺陷的推理步骤]\n        D --> D1[Synthetic Incorrect > Human Correct / 错误的合成数据优于正确的人类数据]\n        D --> D2[Distribution Alignment is Key / 数据分布对齐是关键]\n        D --> D3[Final Answer ≠ Faithful Reasoning / 最终答案 ≠ 忠实推理过程]"
    },
    {
      "title": "Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data",
      "authors": "Daniil Burakov, Ivan Petrov, Dmitrii Khelimskii, Ivan Bessonov, Mikhail Lazarev",
      "institution": "HSE University, Meshalkin National Medical Research Center, Tyumen Cardiology Research Center",
      "link": "https://arxiv.org/pdf/2512.22259",
      "code": null,
      "tags": [
        "clinical prediction",
        "synthetic data",
        "class imbalance",
        "permutation feature importance",
        "tabular data",
        "mortality prediction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp",
      "contributions": "1. Developed and evaluated machine learning models for predicting 3-year cardiac mortality after PCI using a dataset of patients with bifurcation lesions. 2. Demonstrated that augmenting the training set with synthetic samples effectively addresses class imbalance, improving minority-class recall and probability quality with minimal impact on AUROC. 3. Identified key clinical predictors (Age, Ejection Fraction, Peripheral Artery Disease, Cerebrovascular Disease) through feature importance analysis and highlighted the brittleness of models on external validation.",
      "summary": "This study developed machine learning models to predict cardiac death within three years for patients undergoing percutaneous coronary intervention (PCI). To handle class imbalance, the authors augmented the real patient data with synthetic samples, which improved the models' ability to identify high-risk patients. The analysis identified key risk factors and demonstrated that data augmentation can reduce model brittleness in imbalanced clinical prediction tasks.",
      "mindmap": "graph TB\n        A[Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[预测PCI术后心脏死亡风险/Predict cardiac death risk post-PCI]\n        B --> B2[类别不平衡问题/Class imbalance issue]\n        C --> C1[应用多种机器学习模型/Apply multiple ML models]\n        C --> C2[生成并添加合成数据/Generate & add synthetic data]\n        C --> C3[使用排列特征重要性/Use permutation feature importance]\n        D --> D1[合成数据提升少数类召回/Synthetic data improves minority-class recall]\n        D --> D2[识别关键特征: 年龄, 射血分数等/Identify key features: Age, Ejection Fraction, etc.]\n        D --> D3[外部验证性能下降/Performance drop on external validation]"
    },
    {
      "title": "The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning",
      "authors": "Rahul D Ray",
      "institution": "BITS Pilani, Hyderabad Campus",
      "link": "https://arxiv.org/pdf/2512.22261",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "physics-constrained data generation",
        "ablation study",
        "grating coupler",
        "Fabry-Perot oscillations",
        "energy conservation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp",
      "contributions": "1. Identifies the \"physics constraint paradox,\" demonstrating that explicit energy conservation enforcement can be mathematically redundant in physically consistent models. 2. Shows that removing Fabry-Perot oscillations significantly reduces bandwidth variability and improves downstream ML model accuracy for bandwidth prediction. 3. Reveals a subtle pitfall where standard noise-addition-and-renormalization pipelines can introduce unphysical negative absorption values.",
      "summary": "This paper investigates physics-constrained data generation for machine learning through an ablation study of a grating coupler spectrum generator. It finds that explicitly enforcing certain physical constraints, like energy conservation, can be redundant, while others, like Fabry-Perot oscillations, can hinder machine learning performance for specific prediction tasks. The main conclusion is that increased physical realism in data generation does not always improve ML learnability, and ML performance can be used to diagnose the relevance of physical constraints.",
      "mindmap": "graph TB\n        Root[”The Physics Constraint Paradox / 物理约束悖论”] --> Problem[”核心问题/Problem: Physics-constrained data generation may over-constrain models. / 物理约束数据生成可能过度约束模型”]\n        Root --> Method[”主要方法/Method: Systematic ablation study of a physics-informed generator. / 对物理信息生成器进行系统消融研究”]\n        Root --> Results[”关键结果/Results: Explicit energy conservation is redundant; Removing Fabry-Perot oscillations improves ML learnability. / 显式能量守恒是冗余的；移除法布里-珀罗振荡可提升机器学习可学习性”]"
    },
    {
      "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs",
      "authors": "Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang",
      "institution": "Tianjin University, Beihang University, Guangxi Normal University",
      "link": "https://arxiv.org/pdf/2512.22266",
      "code": "https://github.com/Wjerry5/LLMTM",
      "tags": [
        "graph representation learning",
        "temporal motifs",
        "dynamic graphs",
        "llm agent",
        "structure-aware dispatcher",
        "prompting techniques"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp",
      "contributions": "1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.",
      "summary": "This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.",
      "mindmap": "graph TB\n        A[LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs处理动态图时态模体分析能力未知/LLMs' capability for temporal motif analysis on dynamic graphs is unexplored]\n        C --> C1[提出基准LLMTM与智能体/Propose benchmark LLMTM and an agent]\n        C --> C2[提出结构感知调度器/Propose structure-aware dispatcher]\n        D --> D1[调度器保持高精度并降低成本/Dispatcher maintains high accuracy while reducing cost]"
    },
    {
      "title": "Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions",
      "authors": "Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton",
      "institution": "George Mason University",
      "link": "https://arxiv.org/pdf/2512.22263",
      "code": null,
      "tags": [
        "object detection",
        "RGB-LWIR fusion",
        "multispectral imagery",
        "YOLO",
        "adaptive framework",
        "illumination conditions"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",
      "contributions": "1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.",
      "summary": "This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.",
      "mindmap": "graph TB\n        Root[Evaluating an Adaptive Multispectral Turret System] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[RGB在低光下表现差/RGB struggles in low-light]\n        Problem --> P2[热成像缺乏颜色纹理/Thermal lacks color & texture]\n        Method[主要方法/Method] --> M1[自适应RGB-LWIR融合框架/Adaptive RGB-LWIR fusion framework]\n        Method --> M2[训练33个YOLO模型/Trained 33 YOLO models]\n        Method --> M3[11种融合比例/11 fusion ratios]\n        Results[关键结果/Results] --> R1[全光模型: 92.8%置信度/Full-light model: 92.8% confidence]\n        Results --> R2[微光模型: 92.0%置信度/Dim-light model: 92.0% confidence]\n        Results --> R3[无光模型: 71.0%置信度/No-light model: 71.0% confidence]"
    },
    {
      "title": "LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training",
      "authors": "Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior",
      "institution": "Politecnico di Torino, Ghent University - imec, University College London",
      "link": "https://arxiv.org/pdf/2512.22264",
      "code": null,
      "tags": [
        "on-device ai",
        "photonic neural networks",
        "transfer matrix",
        "Slicing method",
        "back-propagation",
        "simulation framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp",
      "contributions": "1. Proposes the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation for training Photonic Neural Networks (PNNs). 2. Introduces LuxIA, a unified simulation and training framework that integrates the Slicing method to enable scalable PNN training. 3. Demonstrates through experiments that LuxIA surpasses existing tools in speed and scalability for training large-scale PNNs on standard datasets.",
      "summary": "This paper addresses the scalability challenges in simulating and training large-scale Photonic Neural Networks (PNNs) by introducing the Slicing method for efficient transfer matrix computation. The method is integrated into the LuxIA framework, which significantly reduces memory usage and training time. Experimental results show LuxIA outperforms existing tools, enabling the exploration of larger and more complex photonic architectures.",
      "mindmap": "graph TB\n        A[LuxIA: A Lightweight Unitary matriX-based Framework] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[当前PNN仿真工具可扩展性差<br>Current PNN simulation tools lack scalability]\n        C --> C1[提出Slicing方法<br>Propose the Slicing method]\n        C --> C2[构建LuxIA统一框架<br>Build the unified LuxIA framework]\n        D --> D1[显著降低内存与时间消耗<br>Significantly reduces memory and time consumption]\n        D --> D2[在速度与可扩展性上超越现有工具<br>Outperforms existing tools in speed and scalability]"
    },
    {
      "title": "Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization",
      "authors": "Amgad Ahmed Ali Ibrahim, Hein Htet, Ryoji Asahi",
      "institution": "Nagoya University",
      "link": "https://arxiv.org/pdf/2512.22279",
      "code": null,
      "tags": [
        "bayesian optimization",
        "Dirichlet Process Mixture Model",
        "Gaussian Process Regression",
        "Transformer",
        "TabNet",
        "XGBoost"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp",
      "contributions": "1. Proposed a hierarchical stacking optimization framework (SoDip) integrating a Transformer for text, TabNet/XGBoost for multimodal features, and GPR with DPMM for uncertainty. 2. Curated a diverse dataset for radiation-induced grafting using automated tools to handle numerical and textual variables. 3. Demonstrated ~33% performance improvement over standard GPR with calibrated confidence intervals for identifying low-reproducibility regimes.",
      "summary": "The paper addresses reproducibility issues in radiation-induced graft polymerization by proposing SoDip, a hierarchical data-driven framework that combines Transformer encoders, multimodal feature models, and Bayesian optimization with uncertainty quantification. The method integrates sparse textual and numerical data, showing a 33% improvement over Gaussian Process Regression and providing reliable confidence estimates. This establishes a foundation for morphology-aware, reproducible design in polymer research.",
      "mindmap": "graph TB\n        A[SoDip: Hierarchical Stacking Optimization] --> B(核心问题/Problem: Radiation-induced grafting reproducibility limited by unreported base-film morphology variability)\n        A --> C(主要方法/Method: Hierarchical framework with Transformer (text), TabNet/XGBoost (features), GPR+DPMM (uncertainty), Bayesian Optimization)\n        A --> D(关键结果/Results: ~33% improvement over GPR, calibrated confidence intervals, integrates sparse multimodal data)"
    },
    {
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "authors": "Varshith Gudur",
      "institution": "Independent Researcher (Valori Kernel Project)",
      "link": "https://arxiv.org/pdf/2512.22280",
      "code": "https://github.com/varshith-Git/Valori-Kernel",
      "tags": [
        "memory & caching",
        "deterministic memory",
        "fixed-point arithmetic",
        "vector embeddings",
        "approximate nearest neighbor search",
        "state machine"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp",
      "contributions": "1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.",
      "summary": "The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.",
      "mindmap": "graph TB\n        A[Valori: A Deterministic Memory Substrate for AI Systems] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: AI内存非确定性/AI Memory Non-Determinism]\n        C[主要方法/Method: 固定点算术与状态机/Fixed-Point Arithmetic & State Machine]\n        D[关键结果/Results: 跨平台比特一致性/Cross-Platform Bit-Identical Results]"
    },
    {
      "title": "Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation",
      "authors": "Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi",
      "institution": "Kyungpook National University",
      "link": "https://arxiv.org/pdf/2512.22287",
      "code": null,
      "tags": [
        "generative models",
        "Generative Adversarial Networks",
        "Non-Intrusive Load Monitoring",
        "Clustering",
        "LSTM",
        "Pattern Generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp",
      "contributions": "1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.",
      "summary": "This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model's interpretability and scalability.",
      "mindmap": "graph TB\n        Root[”Cluster Aggregated GAN (CAG)”] --> Problem[”核心问题/Problem: 缺乏标记数据，现有GAN方法对所有设备一视同仁，忽略间歇性和持续性设备的行为差异，导致训练不稳定和保真度有限”]\n        Root --> Method[”主要方法/Method: 提出混合生成框架，根据设备行为特征路由到专门分支：间歇性设备使用聚类模块和专用生成器；持续性设备使用LSTM生成器”]\n        Root --> Results[”关键结果/Results: 在UVIC数据集上实验，在真实性、多样性和训练稳定性上优于基线方法，聚类作为主动生成组件提高了可解释性和可扩展性”]"
    },
    {
      "title": "When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing",
      "authors": "Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R",
      "institution": "Anna University, National Institute of Technology Tiruchirappalli, University of Missouri",
      "link": "https://arxiv.org/pdf/2512.22290",
      "code": null,
      "tags": [
        "causal inference",
        "Double Machine Learning",
        "Moderated Mediation",
        "Algorithmic Control",
        "Nonmonotonic Effects",
        "Gig Economy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp",
      "contributions": "1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a \"murky middle\" of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.",
      "summary": "This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.",
      "mindmap": "graph TB\n        Root[”当算法管理人类: 估算算法控制对零工工人绩效和福祉非线性效应的双重机器学习方法 / When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing”]\n        Root --> Problem[”核心问题: 算法管理下，以人为本的管理能否持续？工人对算法的反应是非线性的 / Problem: Can person-centered management survive algorithmic management? Worker responses are nonlinear.”]\n        Root --> Method[”主要方法: 使用双重机器学习框架估算有调节的中介模型，无严格函数形式限制 / Method: Double Machine Learning framework to estimate a moderated mediation model without restrictive functional forms.”]\n        Root --> Results[”关键结果: 发现非单调模式。模糊的算法监督削弱绩效联系，透明可解释的监督则加强它 / Results: Found a nonmonotonic pattern. Murky oversight weakens the performance link, transparent and explainable oversight strengthens it.”]"
    },
    {
      "title": "Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model",
      "authors": "Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang",
      "institution": "Tsinghua University (Leap Lab), Anyverse Dynamics",
      "link": "https://arxiv.org/pdf/2512.22288",
      "code": "https://co-grpo.github.io",
      "tags": [
        "diffusion models",
        "Masked Diffusion Models",
        "Markov Decision Process",
        "Group Relative Policy Optimization",
        "inference schedule optimization",
        "trajectory-level training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp",
      "contributions": "1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.",
      "summary": "This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.",
      "mindmap": "graph TB\n        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]\n        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]\n        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]\n        C --> C1[统一MDP/Unified MDP]\n        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]\n        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]\n        D --> D1[提升生成质量/Improved Generation Quality]\n        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]"
    },
    {
      "title": "DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations",
      "authors": "Guokan Chen, Yao Xiao",
      "institution": "Fujian University of Technology",
      "link": "https://arxiv.org/pdf/2512.22283",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Physics-informed neural networks",
        "Kolmogorov-Arnold networks",
        "Adaptive weighting",
        "B-splines",
        "Partial differential equations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp",
      "contributions": "1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.",
      "summary": "This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.",
      "mindmap": "graph TB\n        Root[DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[PINNs struggle with multi-scale/high-frequency PDEs / PINNs在处理多尺度/高频PDE时遇到困难]\n        P1 --> P2[Issues: Gradient flow stiffness & spectral bias / 问题: 梯度流刚度和谱偏差]\n        Method[主要方法/Method] --> M1[Architecture: Kolmogorov-Arnold Network (KAN) with learnable B-splines / 架构: 基于可学习B样条的KAN]\n        Method --> M2[Strategy: Adaptive weighting with dynamic decay upper bound / 策略: 带动态衰减上界的自适应加权]\n        Results[关键结果/Results] --> R1[Faster convergence & higher accuracy / 更快的收敛和更高的精度]\n        R1 --> R2[Improvement: At least one order of magnitude / 提升: 至少一个数量级]\n        Results --> R3[Benchmarks: Klein-Gordon, Burgers, Helmholtz equations / 基准: Klein-Gordon, Burgers, Helmholtz方程]"
    },
    {
      "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against",
      "authors": "Tsogt-Ochir Enkhbayar",
      "institution": "Mongol-AI (inferred from email domain)",
      "link": "https://arxiv.org/pdf/2512.22293",
      "code": null,
      "tags": [
        "language model safety",
        "sparse autoencoder",
        "feature orthogonalization",
        "stealth slip",
        "pragmatic interpretation",
        "statistical co-occurrence"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp",
      "contributions": "1. Empirically demonstrates that warning-framed training data fails to teach language models to avoid warned-against behaviors, showing generation rates similar to direct exposure. 2. Provides a mechanistic interpretation using sparse autoencoders, identifying a failure of feature orthogonalization where \"describing\" and \"performing\" an action activate overlapping latent features. 3. Identifies and names the \"stealth slip\" phenomenon, where conversational preambles can rotate activations into subspaces undetectable by linear probes, and shows that training-time feature ablation, not prompting, is required to address the issue.",
      "summary": "This paper investigates why language models trained on warning-framed examples (e.g., \"DO NOT USE\") still learn to generate the warned-against content. Through behavioral experiments and sparse autoencoder analysis, it finds that models learn statistical co-occurrences rather than pragmatic intent, due to overlapping latent features for description and action. The core conclusion is that current architectures prioritize pattern completion over understanding speaker intent, requiring training-time interventions like feature ablation for correction.",
      "mindmap": "graph TB\n        Root(”Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against”) --> Problem(”核心问题/Problem: Do warning-framed examples teach models to avoid bad behavior?”)\n        Root --> Method(”主要方法/Method: Behavioral analysis & Sparse Autoencoder mechanistic interpretability”)\n        Root --> Results(”关键结果/Results: No. Models learn statistical co-occurrence, not pragmatic intent.”)"
    },
    {
      "title": "Multi-Head Spectral-Adaptive Graph Anomaly Detection",
      "authors": "Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou",
      "institution": "People's Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College",
      "link": "https://arxiv.org/pdf/2512.22291",
      "code": null,
      "tags": [
        "graph anomaly detection",
        "spectral graph neural network",
        "hypernetwork",
        "Chebyshev filter",
        "teacher-student contrastive learning",
        "Barlow Twins loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp",
      "contributions": "1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a 'spectral fingerprint'. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.",
      "summary": "The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.",
      "mindmap": "graph TB\n        A[Multi-Head Spectral-Adaptive Graph Anomaly Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[固定滤波器导致过平滑与缺乏适应性/Fixed filters cause over-smoothing & lack adaptability]\n        C --> C1[基于谱指纹的轻量级超网络/Lightweight hypernetwork based on spectral fingerprint]\n        C --> C2[动态生成切比雪夫滤波器参数/Dynamically generates Chebyshev filter parameters]\n        C --> C3[双正则化策略防止模式崩溃/Dual regularization prevents mode collapse]\n        D --> D1[有效保留高频异常信号/Effectively preserves high-frequency anomaly signals]\n        D --> D2[在异构数据集上性能优越/Outperforms SOTA on heterogeneous datasets]"
    },
    {
      "title": "Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing",
      "authors": "Reda Heddad, Lamiae Bouanane",
      "institution": "Al Akhawayn University",
      "link": "https://arxiv.org/pdf/2512.22296",
      "code": null,
      "tags": [
        "federated learning",
        "Quantum Machine Learning",
        "Mixture-of-Experts",
        "Parameterized Quantum Circuits",
        "Quantum Router",
        "Interference Hypothesis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp",
      "contributions": "1. Design of a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture with a Quantum Router for an ablation study to isolate the source of quantum advantage. 2. Validation of the Interference Hypothesis, demonstrating the Quantum Router's topological advantage and superior parameter efficiency on non-linearly separable data. 3. Empirical analysis of the architecture's robustness against quantum noise, confirming its feasibility for near-term (NISQ) hardware.",
      "summary": "This paper proposes a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture that uses a Quantum Router to address limitations like expert imbalance in classical MoE systems. The core finding is that the Quantum Router, leveraging quantum interference, provides a topological advantage for routing complex data more efficiently than classical routers. The method is shown to be robust to noise and feasible for near-term quantum hardware.",
      "mindmap": "graph TB\n        Root(”Hybrid Quantum-Classical Mixture of Experts”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”MoE挑战:专家不平衡,路由复杂/MoE Challenges: Expert Imbalance, Routing Complexity”)\n        Method --> M1(”混合量子-经典架构/Hybrid Quantum-Classical Architecture”)\n        Method --> M2(”量子路由网络/Quantum Router”)\n        Method --> M3(”利用量子干涉/Utilizes Quantum Interference”)\n        Results --> R1(”验证干涉假说/Validates Interference Hypothesis”)\n        Results --> R2(”拓扑优势,高效解耦数据/Topological Advantage, Efficiently Untangles Data”)\n        Results --> R3(”NISQ硬件可行/Feasible for NISQ Hardware”)"
    },
    {
      "title": "Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware",
      "authors": "Vesal Ahsani, Babak Hossein Khalaj",
      "institution": "Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.22298",
      "code": null,
      "tags": [
        "human activity recognition",
        "driver monitoring systems",
        "edge AI",
        "quantization",
        "temporal decision head",
        "confounder-aware labeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp",
      "contributions": "1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.",
      "summary": "This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.",
      "mindmap": "graph TB\n        A[Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[实时DMS需求 / Real-time DMS needs low latency, low cost, low power]\n        C --> C1[紧凑单摄像头系统 / Compact single-camera pipeline]\n        C1 --> C2[紧凑视觉模型 / Compact per-frame vision model]\n        C1 --> C3[抗混淆标签设计 / Confounder-aware label design]\n        C1 --> C4[时序决策头 / Temporal decision head]\n        D --> D1[性能: 16 FPS (RPi5), 25 FPS (Edge TPU) / Performance: 16 FPS (RPi5), 25 FPS (Edge TPU)]\n        D --> D2[验证: 真实车辆测试 / Validation: Real in-vehicle tests]"
    },
    {
      "title": "Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions",
      "authors": "Jennifer Sawyer, Julian Allagan",
      "institution": "Elizabeth City State University",
      "link": "https://arxiv.org/pdf/2512.22302",
      "code": null,
      "tags": [
        "transportation safety analytics",
        "Random Forest",
        "Kernel Density Estimation (KDE)",
        "Moran's I",
        "Highway Safety Manual (HSM)",
        "Negative Binomial Regression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp",
      "contributions": "1. Applied and compared advanced statistical and machine learning methods (Random Forest, KDE, Negative Binomial Regression) to rural highway crash data, demonstrating a methodological advancement beyond basic techniques. 2. Validated spatial clustering of accidents using Moran's I test and identified specific crash hotspots via KDE, extending previous hotspot analysis. 3. Showed that a Random Forest classifier for injury severity prediction outperformed the standard Highway Safety Manual (HSM) Safety Performance Function (SPF) model.",
      "summary": "This study analyzes traffic accident data from a rural highway using advanced statistical and machine learning techniques, including Random Forest and spatial analysis. The proposed Random Forest model for predicting injury severity achieved 67% accuracy, outperforming the standard HSM model, and spatial analysis confirmed crash clustering near intersections. The results provide actionable insights for targeted safety interventions on US 158.",
      "mindmap": "graph TB\n        Root(”Statistical and Machine Learning Analysis of Traffic Accidents on US 158”) --> Problem(”核心问题/Problem: Rural highway traffic safety analysis”)\n        Root --> Method(”主要方法/Method: KDE, Random Forest, Moran's I, HSM SPF comparison”)\n        Root --> Results(”关键结果/Results: RF outperforms HSM, hotspots identified, clustering confirmed”)"
    },
    {
      "title": "PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations",
      "authors": "Sultan Amed, Chan Yu Hang, Sayantan Banerjee",
      "institution": "Indian Institute of Management Indore, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.22305",
      "code": null,
      "tags": [
        "others",
        "MLOps",
        "champion-challenger framework",
        "out-of-time validation",
        "probability of default",
        "data drift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp",
      "contributions": "1. Proposes PDx, an adaptive, MLOps-driven decision system for end-to-end lifecycle management of credit risk models. 2. Introduces a dynamic champion-challenger framework with regular model updates and out-of-time validation to combat data drift. 3. Empirically demonstrates that decision tree-based ensemble models perform best for default classification but require frequent retraining, and validates PDx's effectiveness across multiple lending datasets.",
      "summary": "This paper proposes PDx, an adaptive credit risk forecasting system that uses an MLOps pipeline and a champion-challenger framework to continuously monitor, retrain, and validate models against data drift. The study finds decision tree ensembles are most effective but degrade without updates, and shows PDx mitigates value erosion in digital lending, especially for short-term loans.",
      "mindmap": "graph TB\n        A[PDx - Adaptive Credit Risk Forecasting Model / PDx - 自适应信用风险预测模型] --> B[Problem: Static PD models degrade, hard to deploy & maintain / 问题: 静态PD模型性能下降，难以部署维护]\n        A --> C[Method: MLOps pipeline with dynamic champion-challenger framework / 方法: 采用动态冠军-挑战者框架的MLOps流程]\n        A --> D[Results: Decision tree ensembles best, PDx mitigates value erosion / 结果: 决策树集成效果最佳，PDx减少价值侵蚀]"
    },
    {
      "title": "LLMBoost: Make Large Language Models Stronger with Boosting",
      "authors": "Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban",
      "institution": "Beihang University, China Telecom eSurfing Cloud",
      "link": "https://arxiv.org/pdf/2512.22309",
      "code": null,
      "tags": [
        "llm inference",
        "ensemble learning",
        "boosting",
        "cross-model attention",
        "chain training",
        "near-parallel inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp",
      "contributions": "1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.",
      "summary": "The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.",
      "mindmap": "graph TB\n        A[LLMBoost: Make Large Language Models Stronger with Boosting] --> B[核心问题/Problem: Existing LLM ensemble methods treat models as black boxes, ignoring internal representations.]\n        A --> C[主要方法/Method: A boosting-inspired framework with cross-model attention, chain training, and near-parallel inference.]\n        A --> D[关键结果/Results: Consistently boosts accuracy and reduces inference latency on reasoning tasks.]"
    },
    {
      "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators",
      "authors": "You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou",
      "institution": "Northwestern University",
      "link": "https://arxiv.org/pdf/2512.22307",
      "code": null,
      "tags": [
        "hardware security",
        "model protection",
        "logic locking",
        "intellectual property protection",
        "hardware accelerator",
        "model theft",
        "supply chain security"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp",
      "contributions": "1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).",
      "summary": "The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.",
      "mindmap": "graph TB\n        Root[LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators] --> Problem(核心问题/Problem: Model IP Protection & Supply Chain Threats)\n        Root --> Method(主要方法/Method: Hardware-Software Co-design with Logic Locking)\n        Root --> Results(关键结果/Results: Resists Attacks, <0.1% Overhead)\n        Problem --> P1(模型盗窃/Model Theft)\n        Problem --> P2(模型破坏/Model Corruption)\n        Problem --> P3(信息泄露/Information Leakage)\n        Method --> M1(软件侧: 神经元嵌入密钥/Software: Key Embedding in Neurons)\n        Method --> M2(硬件侧: 轻量级锁定模块/Hardware: Lightweight Locking Module)\n        Results --> R1(抵御优化攻击/Withstands Oracle-Guided Attacks)\n        Results --> R2(低计算开销/Low Computational Overhead)"
    },
    {
      "title": "Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making",
      "authors": "Wenzhang Du",
      "institution": "Mahanakorn University of Technology, International College (MUTIC)",
      "link": "https://arxiv.org/pdf/2512.22313",
      "code": null,
      "tags": [
        "reinforcement learning",
        "optimistic feasible search",
        "closed-loop decision-making",
        "demographic parity",
        "bandit feedback",
        "threshold policy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp",
      "contributions": "1. Proposed Optimistic Feasible Search (OFS), a simple grid-based method for constrained closed-loop threshold learning using optimism under uncertainty. 2. Introduced synthetic and semi-synthetic closed-loop benchmarks with stable contraction dynamics and real-world datasets (German Credit, COMPAS) to evaluate feedback effects. 3. Demonstrated that OFS achieves near-oracle performance with higher reward and lower cumulative constraint violation compared to unconstrained and primal-dual bandit baselines.",
      "summary": "The paper addresses online learning of a threshold policy under fairness and service constraints in closed-loop decision systems with bandit feedback. It proposes Optimistic Feasible Search (OFS), which selects thresholds based on optimistic confidence bounds to maximize reward while minimizing constraint violations. Experiments show OFS outperforms baselines and achieves near-oracle performance across synthetic and real-world benchmarks.",
      "mindmap": "graph TB\n    A[Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Closed-loop decision-making with fairness constraints and feedback effects]\n    C --> C1[Optimistic Feasible Search (OFS) using confidence bounds on a grid of thresholds]\n    D --> D1[Higher reward, lower constraint violation, near-oracle performance]"
    },
    {
      "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting",
      "authors": "Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan",
      "institution": "University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University",
      "link": "https://arxiv.org/pdf/2512.22317",
      "code": null,
      "tags": [
        "weather forecasting",
        "multimodal nowcasting",
        "rectified flow",
        "semantic motion constraint",
        "latent space integration",
        "large-scale dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp",
      "contributions": "1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.",
      "summary": "The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.",
      "mindmap": "graph TB\n        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]\n        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]\n        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]"
    },
    {
      "title": "Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough",
      "authors": "Chorok Lee",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.22318",
      "code": null,
      "tags": [
        "knowledge graph embeddings",
        "probabilistic embeddings",
        "uncertainty quantification",
        "out-of-distribution detection",
        "semantic uncertainty",
        "structural uncertainty"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp",
      "contributions": "1. Identifies and formalizes the fundamental limitation of relation-agnostic uncertainty in probabilistic KG embeddings, proving an impossibility result for detecting novel relational contexts using only entity-level statistics. 2. Proposes a novel decomposition of uncertainty into complementary semantic (entity variance) and structural (entity-relation co-occurrence) components, proving their non-redundancy and the superiority of their combination. 3. Introduces the CAGP method that combines semantic and structural uncertainty with learned weights, achieving significant improvements (60-80% relative gain) in temporal OOD detection and selective prediction performance.",
      "summary": "The paper identifies that probabilistic knowledge graph embeddings use relation-agnostic entity variances, which conflates two distinct types of out-of-distribution data: emerging entities and novel relational contexts. To address this, the authors propose decomposing uncertainty into semantic and structural components and introduce the CAGP method to combine them. This approach achieves a 60-80% relative improvement in temporal OOD detection performance over existing baselines.",
      "mindmap": "graph TB\n        A[Decomposing Uncertainty in Probabilistic KG Embeddings] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Relation-agnostic entity variance fails to detect novel relational contexts]\n        C[主要方法/Method: Decompose uncertainty into semantic (entity variance) and structural (entity-relation co-occurrence) components]\n        D[关键结果/Results: CAGP method achieves 60-80% relative improvement in OOD detection]"
    },
    {
      "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
      "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
      "institution": "Peking University, Tencent",
      "link": "https://arxiv.org/pdf/2512.22322",
      "code": "https://huggingface.co/collections/yolay/smartsnap",
      "tags": [
        "agent system",
        "self-verifying agent",
        "proactive evidence seeking",
        "LLM-as-a-Judge",
        "3C Principles",
        "agentic reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",
      "contributions": "1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.",
      "summary": "The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.",
      "mindmap": "graph TB\n        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]"
    },
    {
      "title": "Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers",
      "authors": "Sravan Karthick T",
      "institution": "RV College of Engineering (RVCE), Bengaluru, India",
      "link": "https://arxiv.org/pdf/2512.22326",
      "code": null,
      "tags": [
        "time series forecasting",
        "TimeXer",
        "Global M2 Liquidity",
        "exogenous variable",
        "long-horizon forecasting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp",
      "contributions": "1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.",
      "summary": "This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.",
      "mindmap": "graph TB\n        A[Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers] --> B(核心问题/Problem: Bitcoin价格长期预测的极端波动性和非平稳性/Bitcoin's extreme volatility & non-stationarity for long-horizon forecasting)\n        A --> C(主要方法/Method: 集成全球M2流动性作为外生变量，使用TimeXer架构/Integrate Global M2 Liquidity as exogenous variable using TimeXer architecture)\n        A --> D(关键结果/Results: 在70天预测范围内，MSE降低超过89%/At 70-day horizon, MSE reduced by over 89%)"
    },
    {
      "title": "Emotion classification using EEG headset signals and Random Forest",
      "authors": "Ricardo Vasquez, Diego Riofrío-Luzcando, Joe Carrion-Jumbo, Cesar Guevara",
      "institution": "Universidad Internacional SEK, Universidad Indoamérica, The Institute of Mathematical Sciences (ICMAT-CSIC)",
      "link": "https://arxiv.org/pdf/2512.22333",
      "code": null,
      "tags": [
        "affective computing",
        "EEG",
        "Random Forest",
        "emotion classification",
        "brain-computer interface",
        "real-time prediction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp",
      "contributions": "1. Developed a model for classifying human emotions (happiness, sadness, relaxation) using EEG signals from a consumer-grade headset (EMOTIV EPOC). 2. Applied the Random Forest algorithm to achieve high accuracy, particularly for happiness (97.21%). 3. Implemented a real-time emotion prediction system that captures EEG signals, processes them, and visually displays the predicted emotion.",
      "summary": "This paper proposes a system to classify human emotions (happiness, sadness, relaxation) from EEG signals using a Random Forest model. The model was trained on data from 50 participants and achieved high accuracy, especially for happiness. The work was extended to create a real-time prediction algorithm that outputs the result with representative images.",
      "mindmap": "graph TB\n        A[Emotion classification using EEG headset signals and Random Forest] --> B(核心问题/Problem: 如何从EEG信号中检测和分类情绪？/How to detect and classify emotions from EEG signals?)\n        A --> C(主要方法/Method: 使用EMOTIV EPOC采集EEG数据，并应用随机森林模型进行分类/Use EMOTIV EPOC to collect EEG data and apply Random Forest model for classification)\n        A --> D(关键结果/Results: 快乐分类准确率97.21%，实现实时情绪预测算法/Happiness classification accuracy 97.21%, implemented a real-time emotion prediction algorithm)"
    },
    {
      "title": "The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models",
      "authors": "Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell",
      "institution": "IBM Research, Mila, Université de Montréal",
      "link": "https://arxiv.org/pdf/2512.22337",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "LoRA",
        "catastrophic forgetting",
        "KL divergence",
        "instruction-tuning",
        "parameter-efficient fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp",
      "contributions": "1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model's general knowledge while maintaining plasticity for new tasks, applied to Qwen models.",
      "summary": "This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model's original capabilities while allowing adaptation to new instructions, with minimal computational overhead.",
      "mindmap": "graph TB\n        Root[”论文标题: The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: LoRA微调导致灾难性遗忘/Catastrophic forgetting in LoRA fine-tuning”]\n        Method[”主要方法/Method: 正则化近似回放/Regularized Approximate Replay (KL惩罚+交错数据/KL penalty + interleaved data)”]\n        Results[”关键结果/Results: 保留通用知识，维持可塑性/Preserves general knowledge without hindering plasticity”]"
    },
    {
      "title": "Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data",
      "authors": "Alaa Alahmadi, Mohamed Hasan",
      "institution": "Newcastle University, University of Leeds",
      "link": "https://arxiv.org/pdf/2512.22349",
      "code": null,
      "tags": [
        "medical image analysis",
        "pseudo-colouring",
        "few-shot learning",
        "prototypical networks",
        "ResNet-18",
        "explainability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp",
      "contributions": "1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.",
      "summary": "This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.",
      "mindmap": "graph TB\n        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1\n        A --> B2\n        A --> B3\n        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]\n        B1 --> C2[可解释性差/Limited explainability]\n        B1 --> C3[临床可靠性受限/Constrained clinical reliability]\n        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]\n        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]\n        D1 --> E2[结构化颜色表示/Structured colour representations]\n        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]\n        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]\n        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]\n        B3 --> F2[提升可解释性/Improve explainability (guide attention)]\n        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]"
    },
    {
      "title": "PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System",
      "authors": "Mohammad Zakaria Haider, Amit Kumar Podder, Prabin Mali, Aranya Chakrabortty, Sumit Paudyal, Mohammad Ashiqur Rahman",
      "institution": "Florida International University, North Carolina State University",
      "link": "https://arxiv.org/pdf/2512.22381",
      "code": null,
      "tags": [
        "Cyber-Physical Systems Security",
        "False Data Injection (FDI)",
        "Physics-Informed Neural Network (PINN)",
        "Multi-Agent Reinforcement Learning (MARL)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp",
      "contributions": "1. Proposes PHANTOM, a physics-aware adversarial attack framework that integrates a federated learning-enabled PINN as a digital twin for accurate modeling of EV charging systems. 2. Develops a multi-agent RL environment using DQN and SAC to generate stealthy FDI attack strategies that bypass conventional detection. 3. Constructs a T&D co-simulation platform to demonstrate the cascading, cross-boundary grid impacts (e.g., load imbalance, voltage instability) of the learned attacks.",
      "summary": "This paper proposes PHANTOM, a physics-aware adversarial attack framework against federated learning-coordinated EV charging management. It uses a PINN-based digital twin and multi-agent RL to generate stealthy false data injection attacks, which are shown through co-simulation to cause significant grid instability, highlighting the need for physics-aware cybersecurity in vehicle-grid integration.",
      "mindmap": "graph TB\n        A[PHANTOM: Physics-Aware Adversarial Attacks] --> B(核心问题/Problem: EV Charging Grid Security)\n        A --> C(主要方法/Method: PINN Digital Twin + Multi-Agent RL)\n        A --> D(关键结果/Results: Stealthy Attacks Cause Grid Instability)"
    },
    {
      "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
      "authors": "Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan",
      "institution": "The University of Hong Kong, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.22374",
      "code": null,
      "tags": [
        "diffusion models",
        "text-to-image generation",
        "flow matching",
        "self-evaluation",
        "any-step inference",
        "from-scratch training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp",
      "contributions": "1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables \"any-step\" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.",
      "summary": "The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.",
      "mindmap": "graph TB\n        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]\n        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]\n        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]"
    },
    {
      "title": "Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration",
      "authors": "Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi",
      "institution": "Apple, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.22382",
      "code": null,
      "tags": [
        "llm training",
        "hyperparameter transfer",
        "Complete(d)P parameterisation",
        "per-module hyperparameter optimisation",
        "scaling laws",
        "evolutionary strategy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp",
      "contributions": "1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.",
      "summary": "This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.",
      "mindmap": "graph TB\n        A[Completed Hyperparameter Transfer<br/>超参数迁移研究] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Hyperparameter tuning is critical for large models<br/>大模型超参数调优至关重要]\n        B --> B2[Transferring optimal HPs across scales is challenging<br/>跨规模最优超参数迁移困难]\n        C --> C1[Propose Complete(d)P parameterisation<br/>提出Complete(d)P参数化方法]\n        C --> C2[Enable per-module HP optimisation & transfer<br/>实现模块级超参数优化与迁移]\n        D --> D1[Direct HP transfer to ~600x larger scale<br/>超参数可直接迁移至约600倍规模]\n        D --> D2[Per-module HPs yield training speedup<br/>模块级超参数带来训练加速]"
    },
    {
      "title": "BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks",
      "authors": "Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin",
      "institution": "Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22388",
      "code": null,
      "tags": [
        "others",
        "Graph Neural Networks",
        "Multi-armed Bandits",
        "Layer-wise Sampling",
        "Node Importance",
        "Efficient Training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp",
      "contributions": "1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.",
      "summary": "The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.",
      "mindmap": "graph TB\n        A[BLISS: Bandit Layer Importance Sampling Strategy] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练大型图神经网络的计算成本高/High computational cost for training GNNs on large graphs]\n        C --> C1[使用多臂老虎机动态选择信息节点/Using multi-armed bandits to dynamically select informative nodes]\n        C --> C2[平衡探索与利用，自适应节点重要性/Balancing exploration and exploitation, adapting to node importance]\n        D --> D1[保持或超过全批次训练的精度/Maintains or exceeds full-batch training accuracy]"
    },
    {
      "title": "Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields",
      "authors": "Zihan Lin, QiZhi He",
      "institution": "University of Minnesota",
      "link": "https://arxiv.org/pdf/2512.22421",
      "code": null,
      "tags": [
        "diffusion models",
        "latent diffusion model",
        "differentiable physics",
        "inverse problems",
        "parameter estimation",
        "flow in porous media"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp",
      "contributions": "1. Proposes LD-DIM, a novel framework that integrates a pretrained latent diffusion prior with a differentiable PDE solver for high-dimensional inverse problems. 2. Enables stable gradient-based optimization in a low-dimensional latent space, improving numerical conditioning and preserving sharp discontinuities. 3. Demonstrates superior numerical stability and reconstruction accuracy compared to PINNs and physics-embedded VAE baselines.",
      "summary": "This paper introduces LD-DIM, a method that combines a latent diffusion model with a differentiable numerical solver to reconstruct heterogeneous parameter fields from sparse observations in PDE-constrained inverse problems. It shows improved stability and accuracy over existing baselines while maintaining sharp material interfaces.",
      "mindmap": "graph TB\n    A[Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields] --> B(核心问题/Problem: High-dimensional, ill-posed PDE inverse problems with sparse observations)\n    A --> C(主要方法/Method: LD-DIM integrates latent diffusion prior with differentiable PDE solver)\n    A --> D(关键结果/Results: Improved stability & accuracy, preserves sharp interfaces, outperforms PINNs/VAE)"
    },
    {
      "title": "Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy",
      "authors": "Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath",
      "institution": "UC Santa Barbara, Allen Institute for Cell Sciences",
      "link": "https://arxiv.org/pdf/2512.22423",
      "code": null,
      "tags": [
        "medical image segmentation",
        "hyperspherical learning",
        "native sparse attention",
        "anisotropic patch embed"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp",
      "contributions": "1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.",
      "summary": "The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.",
      "mindmap": "graph TB\n        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)\n        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)\n        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)"
    },
    {
      "title": "Causality-Inspired Safe Residual Correction for Multivariate Time Series",
      "authors": "Jianxiang Xie, Yuncheng Hua",
      "institution": "University of New South Wales, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.22428",
      "code": null,
      "tags": [
        "time series forecasting",
        "residual correction",
        "causality-inspired encoder",
        "non-degradation guarantee",
        "safety mechanism",
        "multivariate time series"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp",
      "contributions": "1. Proposes CRC, a plug-and-play residual correction framework explicitly designed to guarantee non-degradation of model performance. 2. Introduces a causality-inspired encoder that decouples self- and cross-variable dynamics to expose direction-aware structure for safer correction. 3. Designs a strict four-fold safety mechanism to govern the correction process and prevent harmful updates, ensuring high non-degradation rates.",
      "summary": "The paper identifies that existing post-hoc residual correction methods for multivariate time series forecasting are greedy and can degrade performance. To solve this, it proposes CRC, a causality-inspired safe residual correction framework with a four-fold safety mechanism. Experiments show CRC consistently improves accuracy while ensuring exceptionally high non-degradation rates.",
      "mindmap": "graph TB\n        A[Causality-Inspired Safe Residual Correction for Multivariate Time Series] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有残差校正方法贪婪且可能导致性能下降/Existing greedy correction can degrade performance]\n        C --> C1[因果启发的编码器分离变量动态/Causality-inspired encoder decouples dynamics]\n        C --> C2[四重安全机制防止有害更新/Four-fold safety mechanism prevents harmful updates]\n        D --> D1[CRC提升准确性并保证高非退化率/CRC improves accuracy & ensures high non-degradation rate]"
    },
    {
      "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience",
      "authors": "Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang",
      "institution": "Northeastern University, Kansas State University",
      "link": "https://arxiv.org/pdf/2512.22435",
      "code": null,
      "tags": [
        "agent system",
        "analog circuit design",
        "multi-agent framework",
        "stratified memory",
        "simulation-grounded feedback",
        "self-evolving"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp",
      "contributions": "1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice.",
      "summary": "The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation.",
      "mindmap": "graph TB\n        A[AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Analog circuit design is knowledge-intensive and relies on human intuition]\n        B --> B2[Existing LLM-based methods lack feedback and generalization]\n        C --> C1[Self-evolving multi-agent framework]\n        C --> C2[Three-stage agent explorations with stratified memory]\n        C --> C3[Iterative refinement via simulation-grounded feedback]\n        D --> D1[10x overall pass rate improvement]\n        D --> D2[48x Pass@1 improvement]\n        D --> D3[4x reduction in parameter search space]"
    },
    {
      "title": "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG",
      "authors": "Cattalyya Nuengsigkapian",
      "institution": "Google",
      "link": "https://arxiv.org/pdf/2512.22442",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "hierarchical filtering",
        "two-pass generation",
        "citation verification",
        "query formulation",
        "model cascade"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.",
      "summary": "This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.",
      "mindmap": "graph TB\n        A[HiFi-RAG] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[开放域RAG中的无关信息与意图对齐/Open-domain RAG faces irrelevant info & intent misalignment]\n        C --> C1[分层过滤与两阶段生成/Hierarchical Filtering & Two-Pass Generation]\n        C1 --> C2[使用Gemini Flash进行过滤/Use Gemini Flash for filtering]\n        C1 --> C3[使用Gemini Pro进行生成/Use Gemini Pro for generation]\n        D --> D1[在MMU-RAGent上超越基线/Outperforms baseline on MMU-RAGent]\n        D --> D2[在自定义测试集上显著提升/Substantial gains on custom test set]"
    },
    {
      "title": "AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing",
      "authors": "Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai",
      "institution": "Meituan, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22455",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "LoRA",
        "Parameter-Efficient Fine-Tuning",
        "Activation Function Annealing",
        "Non-linear Adaptation",
        "Model Merging"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp",
      "contributions": "1. Proposes AFA-LoRA, a novel training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability., 2. Introduces an annealed activation function that transitions from non-linear to linear during training, enabling strong initial learning and final linear integration., 3. Demonstrates the method's effectiveness across multiple tasks, including supervised fine-tuning, reinforcement learning, and speculative decoding, reducing the performance gap with full-parameter training.",
      "summary": "The paper addresses the limited expressive power of linear Low-Rank Adaptation (LoRA) by proposing AFA-LoRA, a method that uses an annealed activation function to enable non-linear training while ensuring the final adapter remains mergeable. This approach narrows the performance gap between LoRA and full-parameter fine-tuning across various tasks, offering a more powerful and practical parameter-efficient adaptation paradigm.",
      "mindmap": "graph TB\n        A[AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>LoRA线性适配的表达能力有限<br>LoRA's linear adaptation limits expressive power]\n        C[主要方法/Method<br>引入退火激活函数<br>Introduce annealed activation function]\n        D[关键结果/Results<br>缩小LoRA与全参数训练的差距<br>Reduces gap between LoRA and full-parameter training]"
    },
    {
      "title": "AMBIT: Augmenting Mobility Baselines with Interpretable Trees",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.22466",
      "code": null,
      "tags": [
        "urban computing",
        "spatial data science",
        "origin-destination flow prediction",
        "spatial interaction models",
        "gradient-boosted trees",
        "SHAP analysis",
        "gray-box model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp",
      "contributions": "1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.",
      "summary": "This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.",
      "mindmap": "graph TB\n        A[AMBIT: Augmenting Mobility Baselines with Interpretable Trees] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Conflicting needs for high accuracy and clear interpretability in OD flow prediction]\n        C[主要方法/Method: Gray-box framework augmenting physical baselines with interpretable tree models]\n        D[关键结果/Results: Physics-grounded residuals approach black-box accuracy; POI-anchored residuals are most robust]"
    },
    {
      "title": "GLUE: Gradient-free Learning to Unify Experts",
      "authors": "Jong-Ik Park, Shreyas Chaudhari, Srinivasa Pranav, Carlee Joe-Wong, José M. F. Moura",
      "institution": "Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.22467",
      "code": null,
      "tags": [
        "others",
        "expert mixing",
        "model initialization",
        "gradient-free optimization",
        "SPSA",
        "transfer learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp",
      "contributions": "1. Proposes GLUE, a gradient-free method to learn mixture coefficients for blending expert models into a single initialization prior for a target domain. 2. Introduces a two-point (SPSA) update rule that requires only two forward passes per step, avoiding expensive backpropagation through the full network. 3. Demonstrates that GLUE outperforms heuristic blending baselines and matches or outperforms gradient-based learning of mixture coefficients across multiple datasets and architectures.",
      "summary": "The paper addresses the problem of initializing a model for a new target domain by blending multiple pretrained expert models. It proposes GLUE, a gradient-free method that learns the blending coefficients efficiently using only forward passes. Experiments show GLUE creates a better initialization prior, leading to higher fine-tuned accuracy than heuristic methods and comparable or better performance than gradient-based approaches.",
      "mindmap": "graph TB\n        A[GLUE: Gradient-free Learning to Unify Experts] --> B[核心问题/Problem: 如何有效融合多个专家模型以初始化新目标域模型/How to effectively fuse multiple expert models to initialize a new target domain model]\n        A --> C[主要方法/Method: 提出GLUE，一种基于梯度自由的两点SPSA更新学习混合系数的方法/Proposes GLUE, a gradient-free method using two-point SPSA updates to learn mixture coefficients]\n        A --> D[关键结果/Results: GLUE在多个实验上超越了启发式基线，并与基于梯度的混合方法性能相当或更优/GLUE outperforms heuristic baselines and matches or outperforms gradient-based mixing across experiments]"
    },
    {
      "title": "The Bayesian Geometry of Transformer Attention",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Columbia University, Dream Sports, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.22471",
      "code": null,
      "tags": [
        "interpretability",
        "Bayesian inference",
        "transformer attention",
        "mechanistic interpretability",
        "Bayesian wind tunnels",
        "geometric analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp",
      "contributions": "1. Introduces \"Bayesian wind tunnels\" as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.",
      "summary": "This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled \"Bayesian wind tunnel\" tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.",
      "mindmap": "graph TB\n        Root[”The Bayesian Geometry of Transformer Attention<br/>Transformer注意力的贝叶斯几何”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Do transformers perform genuine Bayesian inference or just pattern matching?<br/>Transformer是进行真正的贝叶斯推理还是仅仅模式匹配?”]\n        Method[”主要方法/Method<br/>Construct 'Bayesian wind tunnels' with known posteriors<br/>构建具有已知后验的'贝叶斯风洞'”]\n        Results[”关键结果/Results<br/>Transformers implement Bayesian inference via geometric mechanism; MLPs fail<br/>Transformer通过几何机制实现贝叶斯推理；MLP失败”]"
    },
    {
      "title": "Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting",
      "authors": "Chuantao Li, Zhi Li, Jiahao Xu, Jie Li, Sheng Li",
      "institution": "Guangdong Ocean University, University of Electronic Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.22478",
      "code": "https://github.com/ChuantaoLi/DARG",
      "tags": [
        "imbalanced learning",
        "collaborative optimization",
        "density-aware",
        "region-guided boosting",
        "sample weight update",
        "dynamic sampling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp",
      "contributions": "1. Proposes a collaborative optimization Boosting model for multiclass imbalanced learning that integrates imbalanced learning and model training. 2. Designs a noise-resistant weight update mechanism and a dynamic sampling strategy using density and confidence factors. 3. Achieves tight integration of modules for weight updates, sample region partitioning, and region-guided sampling to enhance performance.",
      "summary": "This paper addresses the problem of classification bias in multiclass imbalanced data by proposing a collaborative optimization Boosting model. The method integrates density-aware and region-guided techniques to update sample weights and perform dynamic sampling, achieving improved performance over existing baselines on 20 public datasets.",
      "mindmap": "graph TB\n    A[Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting] --> B[核心问题/Problem: 类别不平衡导致分类偏差，现有方法未协同优化不平衡学习与模型训练]\n    A --> C[主要方法/Method: 提出协同优化Boosting模型，集成密度因子和置信度因子，设计抗噪声权重更新和动态采样策略]\n    A --> D[关键结果/Results: 在20个公共不平衡数据集上显著优于8个先进基线模型]"
    },
    {
      "title": "SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding",
      "authors": "Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu",
      "institution": "University of Electronic Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.22481",
      "code": null,
      "tags": [
        "biomedical signal processing",
        "self-supervised learning",
        "surface electromyography",
        "rotary position encoding",
        "spectral pre-training",
        "movement decoding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp",
      "contributions": "1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.",
      "summary": "The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.",
      "mindmap": "graph TB\n        A[SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Decoding fine-grained movement from noisy, non-stationary sEMG signals for prosthetic control]\n        C[主要方法/Method: Domain-specific SSL with spectral pre-training and Cylindrical Rotary Position Embedding (CyRoPE)]\n        D[关键结果/Results: New SOTA performance, outperforms supervised & generic SSL baselines, validated on amputation data]"
    },
    {
      "title": "Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment",
      "authors": "Hassan Wasswa, Timothy Lynar",
      "institution": "University of New South Wales",
      "link": "https://arxiv.org/pdf/2512.22488",
      "code": null,
      "tags": [
        "network intrusion detection",
        "concept drift",
        "latent space alignment",
        "graph neural network (GNN)",
        "IoT botnet detection",
        "variational autoencoder"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp",
      "contributions": "1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.",
      "summary": "This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework's effectiveness in dynamic IoT environments.",
      "mindmap": "graph TB\n        Root[”Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection / 面向真实世界物联网安全：概念漂移鲁棒的物联网僵尸网络检测”]\n        Root --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”AI模型依赖静态数据集 / AI models rely on stationary datasets”]\n        Problem --> P2[”真实流量存在概念漂移 / Real-world traffic suffers concept drift”]\n        Problem --> P3[”现有方案重训练开销大 / Existing solutions have high retraining cost”]\n        Method --> M1[”学习历史流量的潜在空间表示 / Learn latent space of historical traffic”]\n        Method --> M2[”对齐模型映射新流量 / Alignment model maps new traffic”]\n        Method --> M3[”图神经网络分类 / Graph Neural Network for classification”]\n        Results --> R1[”保持鲁棒检测性能 / Maintains robust detection performance”]\n        Results --> R2[”适用于动态大规模环境 / Suitable for dynamic, large-scale environments”]"
    },
    {
      "title": "The Quest for Winning Tickets in Low-Rank Adapters",
      "authors": "Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi",
      "institution": "Australian Institute for Machine Learning (Adelaide University), Monash University",
      "link": "https://arxiv.org/pdf/2512.22495",
      "code": null,
      "tags": [
        "parameter-efficient fine-tuning",
        "Lottery Ticket Hypothesis",
        "Low-Rank Adaptation",
        "Parameter-Efficient Fine-Tuning",
        "Sparse Subnetworks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp",
      "contributions": "1. Empirically validates that the Lottery Ticket Hypothesis (LTH) holds within Low-Rank Adaptation (LoRA) methods, revealing the existence of sparse, high-performing subnetworks (\"winning tickets\") in adapters. 2. Discovers that the effectiveness of these sparse subnetworks depends more on the sparsity distribution across layers than on the specific weights selected. 3. Proposes Partial-LoRA, a novel method to systematically identify and train these sparse low-rank adapters, achieving significant parameter reduction (up to 87%) while maintaining or improving performance across vision and language tasks.",
      "summary": "This paper investigates whether the Lottery Ticket Hypothesis extends to parameter-efficient fine-tuning, specifically Low-Rank Adaptation (LoRA). The authors propose Partial-LoRA, a method to identify and train sparse, task-aligned subnetworks within LoRA adapters. Experiments show Partial-LoRA can reduce trainable parameters by up to 87% while matching or surpassing the performance of dense adapters.",
      "mindmap": "graph TB\n        A[The Quest for Winning Tickets in Low-Rank Adapters] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Does LTH hold in PEFT/LoRA?<br>LTH是否适用于参数高效微调?]\n        C[主要方法/Method<br>Propose Partial-LoRA<br>提出Partial-LoRA方法]\n        D[关键结果/Results<br>Sparse adapters work,<br>up to 87% fewer params<br>稀疏适配器有效，参数减少高达87%]"
    },
    {
      "title": "Role-Based Fault Tolerance System for LLM RL Post-Training",
      "authors": "Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin",
      "institution": "Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing",
      "link": "https://arxiv.org/pdf/2512.22492",
      "code": null,
      "tags": [
        "fault-tolerance",
        "role-based fault tolerance",
        "RL post-training",
        "UCX communication",
        "warm standby",
        "Effective Training Time Ratio (ETTR)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp",
      "contributions": "1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.",
      "summary": "The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.",
      "mindmap": "graph TB\n        A[Role-Based Fault Tolerance System for LLM RL Post-Training] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[RL后训练混合训练与推理工作负载，易受双方故障影响/RL post-training mixes training & inference, vulnerable to faults from both]\n        B --> B2[现有容错框架未针对RL的异步执行优化/Existing FT frameworks not optimized for RL's async execution]\n        C --> C1[基于角色的故障隔离与恢复/Role-based fault isolation & recovery]\n        C --> C2[检测-重启-重连范式/Detect-Restart-Reconnect paradigm]\n        C2 --> C21[角色感知监控/Role-aware monitoring]\n        C2 --> C22[非中断式重启/Non-disruptive restart with warm standbys]\n        C2 --> C23[动态UCX点对点通信重连/Dynamic UCX P2P reconnection]\n        D --> D1[ETTR超过80%，优于基线的60%/ETTR >80%, better than baseline 60%]\n        D --> D2[端到端训练时间加快8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]"
    },
    {
      "title": "Decomposing Task Vectors for Refined Model Editing",
      "authors": "Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi",
      "institution": "Australian Institute for Machine Learning (University of Adelaide), Monash University",
      "link": "https://arxiv.org/pdf/2512.22511",
      "code": null,
      "tags": [
        "model editing",
        "task vector",
        "parameter decomposition",
        "invariant subspace",
        "concept interference",
        "LoRA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp",
      "contributions": "1. A principled decomposition method to separate a task vector into shared and unique components., 2. Application of the decomposition to improve multi-task merging in image classification, enable clean style mixing in diffusion models, and reduce toxicity in language models., 3. A new framework for understanding and controlling task vector arithmetic to address interference during concept composition.",
      "summary": "The paper addresses the problem of unpredictable outcomes when performing arithmetic operations on task vectors due to overlapping concepts. It proposes a method to decompose each task vector into shared and unique components using invariant subspaces. This enables more precise model editing, demonstrated by improved multi-task merging, clean style mixing, and significant toxicity reduction while preserving general knowledge.",
      "mindmap": "graph TB\n        Root[”Decomposing Task Vectors for Refined Model Editing”] --> Problem[”核心问题/Problem: Task vector arithmetic causes unpredictable interference due to overlapping concepts.”]\n        Root --> Method[”主要方法/Method: Decompose task vectors into shared and unique components via invariant subspaces.”]\n        Root --> Results[”关键结果/Results: Improved multi-task merging, clean style mixing in diffusion models, and toxicity reduction in language models.”]"
    },
    {
      "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
      "authors": "Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya",
      "institution": "Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University",
      "link": "https://arxiv.org/pdf/2512.22508",
      "code": null,
      "tags": [
        "hallucination detection",
        "correctness prediction",
        "metadata signals",
        "prompting strategies",
        "log probability",
        "response consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp",
      "contributions": "1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.",
      "summary": "This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.",
      "mindmap": "graph TB\n        A[Predicting LLM Correctness in Prosthodontics<br/>预测LLM在口腔修复学中的正确性] --> B(Problem/核心问题: LLM hallucinations in high-stakes healthcare domains<br/>高风险医疗领域中的LLM幻觉问题)\n        A --> C(Method/主要方法: Use metadata & hallucination signals to build correctness predictors<br/>使用元数据和幻觉信号构建正确性预测器)\n        A --> D(Results/关键结果: Improved accuracy & precision; metadata not reliable for hallucination prediction; prompting alters behavior<br/>提升准确率和精确度；元数据不能可靠预测幻觉；提示策略改变模型行为)"
    },
    {
      "title": "Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks",
      "authors": "Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance",
      "link": "https://arxiv.org/pdf/2512.22522",
      "code": null,
      "tags": [
        "adversarial robustness",
        "Spiking Neural Networks",
        "surrogate gradient",
        "adversarial attack",
        "gradient vanishing",
        "adaptive optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp",
      "contributions": "1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.",
      "summary": "This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.",
      "mindmap": "graph TB\n        Root(”Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”梯度消失/Gradient Vanishing”)\n        Problem --> P2(”对抗评估不可靠/Unreliable Adversarial Evaluation”)\n        Method --> M1(”理论分析梯度消失/Theoretical Analysis of Gradient Vanishing”)\n        Method --> M2(”自适应锐度替代梯度/Adaptive Sharpness Surrogate Gradient (ASSG)”)\n        Method --> M3(”稳定自适应投影梯度下降/Stable Adaptive PGD (SA-PGD)”)\n        Results --> R1(”攻击成功率大幅提升/Substantially Increased Attack Success Rate”)\n        Results --> R2(”揭示鲁棒性被高估/Revealed Overestimated Robustness”)\n        Results --> R3(”提供更可靠评估/Provided More Reliable Evaluation”)"
    },
    {
      "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting",
      "authors": "Jaebin Lee, Hankook Lee",
      "institution": "Sungkyunkwan University",
      "link": "https://arxiv.org/pdf/2512.22550",
      "code": "https://github.com/efficient-learning-lab/TimePerceiver",
      "tags": [
        "time-series forecasting",
        "encoder-decoder",
        "latent bottleneck representations",
        "learnable queries",
        "generalized forecasting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp",
      "contributions": "1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.",
      "summary": "This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.",
      "mindmap": "graph TB\n        A[TIMEPERCEIVER] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法侧重编码器，预测与训练分离/Prior work focuses on encoder, treats prediction & training separately]\n        C --> C1[广义预测任务: 外推、插值、填补/Generalized forecasting: extrapolation, interpolation, imputation]\n        C --> C2[编码: 潜在瓶颈表示/Encoding: Latent bottleneck representations]\n        C --> C3[解码: 可学习查询/Decoding: Learnable queries]\n        D --> D1[性能显著超越SOTA/Outperforms SOTAs significantly]"
    },
    {
      "title": "Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches",
      "authors": "Chuang-Chieh Lin, Chi-Jen Lu, Po-An Chen, Chih-Chieh Hung",
      "institution": "National Taiwan Ocean University, Academia Sinica, National Yang Ming Chiao Tung University, National Chung Hsing University",
      "link": "https://arxiv.org/pdf/2512.22552",
      "code": null,
      "tags": [
        "game theory",
        "pure-strategy Nash equilibrium",
        "continuous games",
        "policy competition",
        "gradient-based algorithm",
        "grid-based search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp",
      "contributions": "1. Formulated a two-party policy competition game and validated the isotonicity hypothesis of winning probability through simulations. 2. Proved the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional policy spaces. 3. Proposed and experimentally validated a decentralized gradient-based algorithm and a polynomial-time grid-based search algorithm for finding approximate PSNEs.",
      "summary": "This paper models two-party policy competition as a continuous non-cooperative game where parties choose policy vectors and a party's payoff is the expected utility for its supporters. The authors prove the existence of a pure-strategy Nash equilibrium and propose two algorithms—a gradient-based method and a grid-based search—that efficiently find approximate equilibria.",
      "mindmap": "graph TB\n        A[Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[建模两党政策竞争/Model two-party policy competition]\n        C --> C1[形式化为连续博弈/Formulate as continuous game]\n        C --> C2[验证等渗性假设/Validate isotonicity hypothesis]\n        C --> C3[设计梯度与网格算法/Design gradient & grid algorithms]\n        D --> D1[证明纯策略纳什均衡存在性/Prove PSNE existence]\n        D --> D2[算法快速收敛/Algorithm converges rapidly]\n        D --> D3[找到近似均衡/Find approximate equilibrium]"
    },
    {
      "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure",
      "authors": "Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
      "institution": "HKUST, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22560",
      "code": "https://github.com/alibaba/ROLL",
      "tags": [
        "agent system",
        "disaggregated infrastructure",
        "hardware-affinity mapping",
        "fine-grained asynchrony"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp",
      "contributions": "1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.",
      "summary": "The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.",
      "mindmap": "graph TB\n        Root[”RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure”] --> Problem[”核心问题/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure.”]\n        Root --> Method[”主要方法/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation.”]\n        Root --> Results[”关键结果/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs.”]"
    },
    {
      "title": "On Admissible Rank-based Input Normalization Operators",
      "authors": "Taeyun Kim",
      "institution": "Seoul National University",
      "link": "https://arxiv.org/pdf/2512.22587",
      "code": null,
      "tags": [
        "machine learning theory",
        "rank-based normalization",
        "differentiable sorting",
        "monotone invariance",
        "batch independence",
        "Lipschitz continuity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp",
      "contributions": "1. Identified and formalized three axioms (invariance and stability properties) that a valid rank-based input normalization operator must satisfy. 2. Proved a structural characterization theorem showing any admissible operator must factor into a feature-wise rank representation and a monotone-Lipschitz scalarization map. 3. Constructed a minimal operator meeting the proposed axioms, empirically demonstrating the non-triviality of the constraints and delineating the design space from existing differentiable sorting methods.",
      "summary": "The paper identifies that widely used differentiable sorting/ranking operators are structurally unstable under monotone transformations and batch variations. To address this, it proposes formal axioms for stable rank-based normalization and proves that any admissible operator must have a specific factored structure. The authors construct a minimal operator satisfying these axioms, formally separating valid normalization from existing differentiable sorting approaches.",
      "mindmap": "graph TB\n        Root[”On Admissible Rank-based Input Normalization Operators<br>可容许的基于排序的输入归一化算子”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>Existing differentiable sorting/ranking operators are unstable under monotone transforms & batch shifts.”]\n        Method[”主要方法/Method<br>Propose three axioms for invariance/stability; prove structural factorization theorem.”]\n        Results[”关键结果/Results<br>Construct minimal admissible operator; delineate design space from prior methods.”]"
    },
    {
      "title": "Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining",
      "authors": "Jewel Rana Palit, Vijayalakshmi K Kumarasamy, Osama A. Osman",
      "institution": "University of Tennessee at Chattanooga, Collier County Government",
      "link": "https://arxiv.org/pdf/2512.22589",
      "code": null,
      "tags": [
        "data mining",
        "K-means clustering",
        "Association Rule Mining",
        "crash pattern analysis",
        "automated vehicles",
        "NHTSA data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp",
      "contributions": "1. Developed a two-stage data mining framework combining K-means clustering and Association Rule Mining to analyze AV crash data. 2. Applied the framework to a large-scale dataset of over 2,500 AV crash records from NHTSA, covering SAE Levels 2 and 4. 3. Uncovered interpretable multivariate relationships between crash patterns and environmental/operational factors, providing actionable insights for AV safety.",
      "summary": "This study analyzes crash patterns in SAE Level 2 and 4 Automated Vehicles using a large-scale NHTSA dataset. It proposes a two-stage data mining framework: first using K-means clustering to segment crashes into behavioral clusters, then applying Association Rule Mining to find relationships between crash factors within each cluster. The results provide actionable guidance for improving AV safety and deployment strategies.",
      "mindmap": "graph TB\n        Root[”Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining<br/>论文标题”]\n        Root --> Problem[”AV safety concerns in mixed traffic; Limited analysis across SAE levels<br/>核心问题/Problem”]\n        Root --> Method[”Two-stage framework: K-means clustering + Association Rule Mining (ARM)<br/>主要方法/Method”]\n        Root --> Results[”Uncovered crash dynamics & multivariate relationships; Actionable guidance for stakeholders<br/>关键结果/Results”]"
    },
    {
      "title": "Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units",
      "authors": "Milad Asadpour, Alireza Rezaee, Farshid Hajati",
      "institution": "University of Tehran, University of New England",
      "link": "https://arxiv.org/pdf/2512.22599",
      "code": null,
      "tags": [
        "time series forecasting",
        "Gated Recurrent Units",
        "parallel neural networks",
        "cryptocurrency price prediction",
        "mean absolute percentage error",
        "recurrent neural networks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp",
      "contributions": "1. Proposes a novel Parallel Gated Recurrent Units (PGRU) model for cryptocurrency price forecasting. 2. Employs parallel and independent recurrent neural networks with distinct price-related feature inputs. 3. Demonstrates higher accuracy and efficiency with lower computational cost and less input data compared to existing methods.",
      "summary": "This paper introduces a new deep learning model called Parallel Gated Recurrent Units (PGRU) for predicting cryptocurrency prices. The model uses parallel recurrent neural networks that process different price features independently, and their outputs are combined by another neural network for the final forecast. Experimental results show the model achieves low prediction errors (e.g., 2.641% MAPE) with higher efficiency and lower computational cost than previous methods.",
      "mindmap": "graph TB\n        A[Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units] --> B[核心问题/Problem: Cryptocurrency price forecasting challenge]\n        A --> C[主要方法/Method: Parallel Gated Recurrent Units (PGRU) model]\n        A --> D[关键结果/Results: Achieves low MAPE (e.g., 2.641%), higher accuracy & efficiency]"
    },
    {
      "title": "Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification",
      "authors": "Guikun Xu, Xiaohan Yi, Peilin Zhao, Yatao Bian",
      "institution": "Shanghai Jiao Tong University, National University of Singapore, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.22597",
      "code": "https://github.com/Rich-XGK/EnFlow.git",
      "tags": [
        "generative models",
        "flow matching",
        "conformer generation",
        "energy guidance",
        "ground-state identification",
        "molecular geometry"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp",
      "contributions": "1. Proposes EnFlow, a unified framework that couples flow matching with an explicitly learned energy model for conformer generation. 2. Introduces an energy-guided sampling scheme along a non-Gaussian flow matching path to steer trajectories toward lower-energy regions, improving fidelity in few-step regimes. 3. Enables accurate ground-state identification by using the learned energy function to rank generated ensembles, reducing prediction errors.",
      "summary": "The paper presents EnFlow, a method that integrates flow matching with an energy model to guide the generation of molecular conformers. By using energy-gradient guidance during sampling, it improves conformational accuracy with few steps and enables better identification of the ground-state structure. Experiments show it outperforms state-of-the-art methods on standard benchmarks.",
      "mindmap": "graph TB\n        A[Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification] --> B[核心问题/Problem: Fragmented learning-based approaches for conformer generation lack reliable energy calibration or ensemble diversity.]\n        A --> C[主要方法/Method: EnFlow couples flow matching with a learned energy model and uses energy-guided sampling along a non-Gaussian path.]\n        A --> D[关键结果/Results: Improves generation metrics with 1-2 steps and reduces ground-state prediction errors on GEOM datasets.]"
    },
    {
      "title": "Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer",
      "authors": "Hesam Taghipour, Alireza Rezaee, Farshid Hajati",
      "institution": "University of Tehran, University of New England",
      "link": "https://arxiv.org/pdf/2512.22606",
      "code": null,
      "tags": [
        "time series forecasting",
        "LSTM",
        "Multilayer Perceptron (MLP)",
        "Gray Wolf Optimizer (GWO)",
        "gold price prediction",
        "trading strategy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp",
      "contributions": "1. Proposed a hybrid LSTM-MLP model for multi-timeframe (daily and monthly) gold price forecasting. 2. Utilized the Gray Wolf Optimizer (GWO) to optimize the number of neurons in the neural networks for improved accuracy. 3. Developed and backtested a trading strategy based on the model's predictions, reporting a high simulated return of 171% over three months.",
      "summary": "This paper presents an AI-based model for predicting gold prices. It uses two LSTM networks for daily and monthly forecasts, integrates their outputs with an MLP, and optimizes the network structure using the Gray Wolf Optimizer. The model achieved low prediction errors and a high simulated trading return, demonstrating its potential for financial forecasting.",
      "mindmap": "graph TB\n        A[Gold Price Prediction Using LSTM and MLP with GWO] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[金融市场预测复杂且具有挑战性/Financial market forecasting is complex and challenging]\n        C --> C1[使用两个LSTM网络进行每日和每月预测/Use two LSTM networks for daily and monthly forecasting]\n        C --> C2[通过MLP整合LSTM输出/Integrate LSTM outputs via MLP]\n        C --> C3[使用灰狼优化器(GWO)优化网络神经元/Use Gray Wolf Optimizer (GWO) to optimize network neurons]\n        D --> D1[日收盘价预测MAE为$0.21/Daily closing price prediction MAE is $0.21]\n        D --> D2[月价格预测MAE为$22.23/Monthly price prediction MAE is $22.23]\n        D --> D3[模拟交易策略三个月回报率171%/Simulated trading strategy achieved 171% return in three months]"
    },
    {
      "title": "Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback",
      "authors": "Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani",
      "institution": "University of California, Irvine; University of British Columbia; Vector Institute",
      "link": "https://arxiv.org/pdf/2512.22623",
      "code": null,
      "tags": [
        "federated learning",
        "communication compression",
        "error feedback",
        "biased compression",
        "control variates",
        "distributed gradient descent"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp",
      "contributions": "1. Proposed Compressed Aggregate Feedback (CAFe), a novel framework for biased compression in distributed learning that uses the previous round's aggregated update as a shared control variate, eliminating the need for client-side state. 2. Proposed Server-Guided Compressed Aggregate Feedback (CAFe-S), an extension that leverages a small server-side dataset to generate a more accurate predictor update, improving convergence when server data is representative. 3. Provided theoretical convergence guarantees for both CAFe and CAFe-S in non-convex settings, proving CAFe's superiority over standard distributed compressed gradient descent and showing CAFe-S's improved rate with more representative server data.",
      "summary": "This paper addresses the communication bottleneck and privacy issues in Federated Learning by proposing two novel compression frameworks, CAFe and CAFe-S, which enable biased compression without requiring client-side state. CAFe uses the previous aggregated update as a shared control variate, while CAFe-S leverages a small server dataset for a better predictor. Theoretical and experimental results demonstrate their superiority over existing compression schemes.",
      "mindmap": "graph TB\n    A[Communication Compression for Distributed Learning] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[通信瓶颈与隐私/Communication Bottleneck & Privacy]\n    B1 --> B2[有偏压缩需要客户端状态/Biased Compression Needs Client State]\n    C --> C1[CAFe: 聚合反馈/CAFe: Aggregate Feedback]\n    C --> C2[CAFe-S: 服务器引导反馈/CAFe-S: Server-Guided Feedback]\n    C1 --> C3[使用上一轮聚合更新作为共享控制变量/Use Previous Aggregate Update as Shared Control Variate]\n    C2 --> C4[使用服务器数据生成预测器/Use Server Data to Generate Predictor]\n    D --> D1[理论收敛保证/Theoretical Convergence Guarantees]\n    D --> D2[优于现有方案/Superior to Existing Schemes]"
    },
    {
      "title": "Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks",
      "authors": "Irched Chafaa, Giacomo Bacci, Luca Sanguinetti",
      "institution": "University of Pisa",
      "link": "https://arxiv.org/pdf/2512.22639",
      "code": null,
      "tags": [
        "llm inference",
        "binary tree compression",
        "Transformer",
        "scalable inference",
        "power allocation",
        "cell-free"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp",
      "contributions": "1. Proposes a novel hybrid Tree-Transformer architecture for scalable per-user power allocation in wireless networks. 2. Introduces a method that compresses user features via a binary tree to a global root representation, applies a Transformer encoder only to the root, and decodes powers with a shared decoder, achieving logarithmic depth and linear total complexity. 3. Demonstrates that the model achieves near-optimal performance for the max-min fairness problem in cell-free massive MIMO systems while significantly reducing inference time compared to full-attention baselines, without needing retraining for different network sizes.",
      "summary": "This paper addresses the high computational cost of Transformer models for power allocation in large-scale wireless networks. It proposes a hybrid Tree-Transformer architecture that compresses user features into a root node for processing, achieving linear complexity and scalable inference. The model demonstrates near-optimal performance with significantly reduced inference time for cell-free massive MIMO systems.",
      "mindmap": "graph TB\n        Root[”Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks”]\n        Root --> Problem[”核心问题/Problem<br>Transformer计算成本高，难以扩展/Poor scalability of Transformer for power allocation”]\n        Root --> Method[”主要方法/Method<br>树-Transformer混合架构/Tree-Transformer hybrid architecture<br>（二叉树压缩，根节点处理/Binary tree compression, root processing）”]\n        Root --> Results[”关键结果/Results<br>接近最优性能，显著降低推理时间/Near-optimal performance, significantly reduced inference time”]"
    },
    {
      "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
      "authors": "Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang",
      "institution": "University of Minnesota",
      "link": "https://arxiv.org/pdf/2512.22650",
      "code": "https://minnesotanlp.github.io/insight-scaling-webpage",
      "tags": [
        "agent system",
        "Test-Time Scaling",
        "multi-agent pipeline",
        "process-based refinement",
        "LLM-as-Judge",
        "unverifiable rewards"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp",
      "contributions": "1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.",
      "summary": "This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.",
      "mindmap": "graph TB\n        A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[多阶段任务缺乏可验证奖励/Multi-stage tasks lack verifiable rewards]\n        B --> B2[基于评判的优化易累积误差/Judge-based refinement prone to error accumulation]\n        C --> C1[选择性测试时扩展/Selective Test-Time Scaling]\n        C --> C2[跨阶段分配计算资源/Distribute compute across stages]\n        C --> C3[早期剪枝低质量分支/Prune low-quality branches early]\n        D --> D1[提升平均分数/Increased mean scores (61.64 to 65.86)]\n        D --> D2[降低方差/Reduced variance]\n        D --> D3[与人类专家对齐的评判模型/Judge model aligned with human experts]"
    },
    {
      "title": "Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification",
      "authors": "Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta",
      "institution": "Indian Institute of Technology, Kharagpur",
      "link": "https://arxiv.org/pdf/2512.22656",
      "code": null,
      "tags": [
        "medical signal processing",
        "electroencephalography",
        "multi-disorder classification",
        "sensitivity-oriented modeling",
        "clinical calibration",
        "feature importance analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp",
      "contributions": "1. Proposes a clinically calibrated, sensitivity-prioritized machine learning framework for classifying eleven diverse neurological disorders from EEG data, addressing severe class imbalance. 2. Establishes realistic performance baselines for multi-disorder EEG classification, demonstrating recall exceeding 80% for most disorders with significant gains for low-prevalence conditions after threshold calibration. 3. Provides physiologically plausible feature importance analysis that aligns with established clinical EEG markers, validating the model's clinical relevance.",
      "summary": "This study addresses the challenge of automated, multi-disorder screening from clinical EEG data by developing disorder-aware machine learning models with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity. The method uses a multi-domain feature set and is evaluated on a large, heterogeneous dataset, achieving high recall for most neurological disorder categories. The results establish performance baselines and demonstrate that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in clinical practice.",
      "mindmap": "graph TB\n        A[Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Manual EEG interpretation is slow and variable; existing automation lacks multi-disorder support.]\n        C[主要方法/Method: Use multi-domain EEG features and train sensitivity-calibrated models under class imbalance.]\n        D[关键结果/Results: High recall (>80%) for most disorders; feature importance aligns with clinical knowledge.]"
    },
    {
      "title": "INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading",
      "authors": "Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni",
      "institution": "University Hospital Bonn, Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.22666",
      "code": null,
      "tags": [
        "medical image analysis",
        "multi-task learning",
        "inter-task consistency",
        "digital pathology",
        "foundation models",
        "combinatorial partial supervision"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp",
      "contributions": "1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.",
      "summary": "The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.",
      "mindmap": "graph TB\n        Root[”INTERACT-CMIL: CMIL分级<br>INTERACT-CMIL: CMIL Grading”]\n        Root --> Problem[”核心问题/Problem<br>CMIL分级困难，主观性强<br>CMIL grading is difficult and subjective”]\n        Root --> Method[”主要方法/Method<br>多任务共享学习与任务间一致性<br>Multi-task Shared Learning & Inter-Task Consistency”]\n        Root --> Results[”关键结果/Results<br>性能显著提升，提供可解释预测<br>Significant performance gains, interpretable predictions”]"
    },
    {
      "title": "Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos",
      "authors": "Shravan Saranyan, Pramit Saha",
      "institution": "Branham High School, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22657",
      "code": null,
      "tags": [
        "medical image analysis",
        "3D Inception",
        "two-stream networks",
        "CNN-RNN",
        "EchoNet-Dynamic",
        "video analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp",
      "contributions": "1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.",
      "summary": "This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.",
      "mindmap": "graph TB\n        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]\n        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]\n        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]\n        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]\n        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]"
    },
    {
      "title": "Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations",
      "authors": "Achraf Hsain, Fouad Mohammed Abbou",
      "institution": "Al Akhawayn University",
      "link": "https://arxiv.org/pdf/2512.22672",
      "code": null,
      "tags": [
        "others",
        "Quantum Generative Models",
        "Computational Fluid Dynamics",
        "Lattice Boltzmann Method",
        "Vector Quantized Variational Autoencoder",
        "Quantum Circuit Born Machine"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp",
      "contributions": "1. A complete open-source pipeline bridging CFD simulation and quantum machine learning. 2. The first empirical study of quantum generative modeling on compressed latent representations of physics simulations. 3. A comparative analysis of quantum (QCBM, QGAN) and classical (LSTM) generative models for a physics-derived latent distribution.",
      "summary": "This paper explores the application of quantum generative models to Computational Fluid Dynamics (CFD) data. The authors compress fluid simulation data into a discrete latent space using a VQ-VAE and then compare quantum (QCBM, QGAN) and classical (LSTM) models for generating samples from this distribution. Under their experimental conditions, the quantum models, particularly the QCBM, outperformed the classical baseline in generating samples closer to the true distribution.",
      "mindmap": "graph TB\n        Root[”Quantum Generative Models for CFD: Latent Space Learning in LBM Simulations”] --> Problem[”核心问题/Problem: Modeling compressed CFD latent distributions”]\n        Root --> Method[”主要方法/Method: VQ-VAE compression + QCBM/QGAN vs LSTM”]\n        Root --> Results[”关键结果/Results: Quantum models (QCBM best) outperformed classical LSTM”]"
    },
    {
      "title": "Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2",
      "authors": "Pere Martra",
      "institution": "Universidad Internacional Menéndez Pelayo (UIMP)",
      "link": "https://arxiv.org/pdf/2512.22671",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "width pruning",
        "expansion ratio",
        "Maximum Absolute Weight (MAW)",
        "GLU-MLP",
        "instruction-following"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp",
      "contributions": "1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.",
      "summary": "This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.",
      "mindmap": "graph TB\n        A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: How does structured width pruning affect different LLM capabilities?]\n        C[主要方法/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]\n        D[关键结果/Results: Knowledge ↓, Instruction-following ↑, Truthfulness ↑, Efficiency trade-offs]"
    },
    {
      "title": "Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning",
      "authors": "Donghwa Kang, Shana Moothedath",
      "institution": "Iowa State University",
      "link": "https://arxiv.org/pdf/2512.22675",
      "code": null,
      "tags": [
        "federated learning",
        "decentralized learning",
        "multi-task representation learning",
        "low-rank structure",
        "communication complexity",
        "projected gradient descent"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp",
      "contributions": "1. Proposes a new alternating projected gradient descent and minimization algorithm for decentralized multi-task representation learning with provable accuracy guarantees. 2. Provides comprehensive theoretical analysis of time, communication, and sample complexities, showing communication complexity is independent of target accuracy. 3. Identifies regimes (e.g., large number of nodes, low bandwidth) where the decentralized algorithm can outperform centralized federated learning approaches.",
      "summary": "This paper studies decentralized multi-task representation learning where tasks share a low-rank structure. The authors propose a new alternating projected gradient descent algorithm with provable guarantees, showing its communication cost is independent of the target accuracy. Numerical simulations validate the theory and demonstrate scenarios where decentralized learning outperforms centralized federated methods.",
      "mindmap": "graph TB\n    A[Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[数据稀缺环境下的去中心化多任务表示学习/Decentralized Multi-Task Representation Learning in Data-Scarce Environments]\n    C --> C1[交替投影梯度下降与最小化算法/Alternating Projected Gradient Descent and Minimization Algorithm]\n    D --> D1[通信复杂度与目标精度无关/Communication Complexity Independent of Target Accuracy]\n    D --> D2[去中心化算法在特定场景下优于中心化方法/Decentralized Algorithm Outperforms Centralized Counterpart in Certain Regimes]"
    },
    {
      "title": "Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors",
      "authors": "Salvador Rodriguez-Sanz, Monica Hernandez",
      "institution": "University of Zaragoza, Aragon Institute for Engineering Research (I3A)",
      "link": "https://arxiv.org/pdf/2512.22689",
      "code": null,
      "tags": [
        "medical image registration",
        "Neural ODEs",
        "Structural Descriptors",
        "Diffeomorphic Registration",
        "Multimodal",
        "Local Mutual Information"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp",
      "contributions": "1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.",
      "summary": "This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.",
      "mindmap": "graph TB\n        A[Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统算法假设强度相关，限于单模态/Traditional methods assume intensity correlation, limited to monomodal]\n        B --> B2[学习模型需要大量数据，泛化性差/Learning-based models need large datasets, poor generalization]\n        C --> C1[基于实例的框架/Instance-specific framework]\n        C --> C2[使用神经ODE与结构描述符/Using Neural ODEs & Structural Descriptors]\n        C --> C3[整合局部互信息/Integrating Local Mutual Information]\n        D --> D1[超越SOTA结果/Surpassing SOTA results]\n        D --> D2[对正则化鲁棒/Robust to varying regularization]\n        D --> D3[高效且适用于不同尺度/Efficient & suitable for varying scales]"
    },
    {
      "title": "Learning with the $p$-adics",
      "authors": "André F. T. Martins",
      "institution": "Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações",
      "link": "https://arxiv.org/pdf/2512.22692",
      "code": null,
      "tags": [
        "representation learning",
        "p-adic numbers",
        "ultrametric space",
        "hierarchical representation",
        "non-archimedean geometry",
        "semantic networks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp",
      "contributions": "1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.",
      "summary": "This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.",
      "mindmap": "graph TB\n    A[Learning with the p-adics] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[现有ML基于实数域/Existing ML uses real numbers]\n    B --> B2[是否可用其他域?/Alternative fields possible?]\n    C --> C1[研究p-adic数域/Study p-adic number field Q_p]\n    C --> C2[利用超度量结构/Exploit ultrametric structure]\n    D --> D1[构建分类回归模型/Build classification & regression models]\n    D --> D2[表示学习与语义网络/Representation learning & semantic networks]\n    D --> D3[开启新研究方向/Open new research directions]"
    },
    {
      "title": "Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors",
      "authors": "Antar Kumar Biswas, Masoud H. Nazari",
      "institution": "Wayne State University",
      "link": "https://arxiv.org/pdf/2512.22699",
      "code": null,
      "tags": [
        "predictive modeling",
        "power outage prediction",
        "LSTM",
        "socio-economic factors",
        "machine learning",
        "resilience"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp",
      "contributions": "1. Proposes a novel learning-based framework specifically for predicting low-probability, high-consequence (LPHC) power outages during extreme events. 2. Integrates a comprehensive set of features from public data, including weather, socio-economic, infrastructure, and seasonal event data, to reveal community vulnerability patterns. 3. Empirically validates the framework on a large-scale Michigan dataset, demonstrating that the LSTM model achieves the lowest prediction error and identifying correlations between economic/infrastructure factors and outage occurrence.",
      "summary": "This paper proposes a machine learning framework to predict power outages caused by extreme events by integrating weather, socio-economic, and infrastructure data. The authors evaluate four models (RF, SVM, AdaBoost, LSTM) on a dataset from Michigan and find that the LSTM performs best, with results showing that better economic conditions and infrastructure are linked to fewer outages.",
      "mindmap": "graph TB\n        A[Predictive Modeling of Power Outages during Extreme Events<br>极端事件下的停电预测建模] --> B(核心问题/Problem: Predicting LPHC power outages<br>预测低概率高后果停电)\n        A --> C(主要方法/Method: ML framework integrating multi-source data<br>集成多源数据的机器学习框架)\n        A --> D(关键结果/Results: LSTM best; Socio-economic factors matter<br>LSTM最优；社会经济因素重要)\n        B --> E(Data: EAGLE-I, weather, socio-economic, infrastructure<br>数据: 停电记录、天气、社会经济、基础设施)\n        C --> F(Models: RF, SVM, AdaBoost, LSTM<br>模型: 随机森林、支持向量机、自适应提升、长短期记忆网络)\n        D --> G(Conclusion: Stronger economy & infrastructure → fewer outages<br>结论: 经济与基础设施更强→停电更少)"
    },
    {
      "title": "What Matters in Deep Learning for Time Series Forecasting?",
      "authors": "Valentina Moretti, Andrea Cini, Ivan Marisca, Cesare Alippi",
      "institution": "IDSIA (Università della Svizzera italiana), Politecnico di Milano",
      "link": "https://arxiv.org/pdf/2512.22702",
      "code": null,
      "tags": [
        "time series forecasting",
        "channel-independence",
        "locality",
        "globality",
        "forecasting model card"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp",
      "contributions": "1. Analyzes the design space of deep learning for time series forecasting, emphasizing principles like locality and globality over specific sequence modeling layers. 2. Highlights how overlooked implementation details (e.g., parameter sharing) fundamentally alter model classes and empirical results. 3. Proposes an auxiliary forecasting model card to systematically characterize architectures based on key design choices, advocating for improved benchmarking practices.",
      "summary": "This paper critically examines deep learning architectures for time series forecasting, arguing that foundational design principles (e.g., locality vs. globality) are more crucial for accuracy than complex sequence modeling layers. It shows that simple, well-designed models can match state-of-the-art performance and reveals how implementation details significantly impact results. The authors propose a forecasting model card to standardize architecture characterization and call for a rethink of benchmarking practices.",
      "mindmap": "graph TB\n        Root(What Matters in Deep Learning for Time Series Forecasting?) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(大量新架构与矛盾结果难以评估组件贡献/Many new architectures and contradictory results make it hard to assess component contributions)\n        Method --> M1(基于时间序列预测原则设计模型/Grounding model design on forecasting principles)\n        Method --> M2(分析局部性与全局性等概念/Analyzing concepts like locality and globality)\n        Method --> M3(提出辅助预测模型卡/Proposing an auxiliary forecasting model card)\n        Results --> R1(设计原则比特定序列层更重要/Design principles more relevant than specific sequence layers)\n        Results --> R2(简单设计可匹配SOTA/Simple, well-designed architectures can match SOTA)\n        Results --> R3(呼吁重新思考基准测试实践/Call for rethinking benchmarking practices)"
    },
    {
      "title": "GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages",
      "authors": "Ahmed Abdullah, Sana Fatima, Haroon Mahmood",
      "institution": "FAST-National University, Al Ain University",
      "link": "https://arxiv.org/pdf/2512.22705",
      "code": null,
      "tags": [
        "hope speech detection",
        "transformer models",
        "multilingual classification",
        "low-resource languages",
        "XLM-RoBERTa",
        "UrduBERT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp",
      "contributions": "1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.",
      "summary": "This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.",
      "mindmap": "graph TB\n        A[GHaLIB: 多语言希望语音检测框架 / GHaLIB: Multilingual Hope Speech Detection Framework] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[希望语音在NLP中代表性不足 / Hope speech underrepresented in NLP]\n        B --> B2[低资源语言(如乌尔都语)缺乏资源 / Lack of resources for low-resource languages (e.g., Urdu)]\n        C --> C1[使用预训练多语言Transformer模型 / Use pretrained multilingual Transformer models]\n        C --> C2[简单预处理与分类器训练 / Simple preprocessing & classifier training]\n        D --> D1[乌尔都语二元分类F1: 95.2% / Urdu binary F1: 95.2%]\n        D --> D2[乌尔都语多类分类F1: 65.2% / Urdu multi-class F1: 65.2%]\n        D --> D3[多语言模型适用于低资源环境 / Multilingual models viable for low-resource settings]"
    },
    {
      "title": "Memento-II: Learning by Stateful Reflective Memory",
      "authors": "Jun Wang",
      "institution": "University College London (UCL)",
      "link": "https://arxiv.org/pdf/2512.22716",
      "code": null,
      "tags": [
        "reinforcement learning",
        "stateful reflective decision process",
        "episodic memory",
        "policy iteration",
        "continual learning",
        "retrieval-augmented generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp",
      "contributions": "1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.",
      "summary": "This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.",
      "mindmap": "graph TB\n        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]\n        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]\n        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]\n        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]\n        C --> F[读取对应策略改进/Reading corresponds to policy improvement]"
    },
    {
      "title": "Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data",
      "authors": "Md Badsha Biswas",
      "institution": "George Mason University",
      "link": "https://arxiv.org/pdf/2512.22732",
      "code": null,
      "tags": [
        "text classification",
        "data augmentation",
        "imbalanced dataset",
        "social media analysis",
        "natural language processing",
        "pregnancy outcome"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp",
      "contributions": "1. Proposes a novel approach to use public social media data (e.g., Twitter) as an adjunctive resource for studying negative pregnancy outcomes, addressing data scarcity in traditional epidemiological research. 2. Constructs an NLP pipeline to automatically identify and classify pregnancy experiences from unstructured, noisy social media text, distinguishing between positive and negative outcomes. 3. Investigates and evaluates various data augmentation techniques specifically to address the severe class imbalance inherent in social media data for this sensitive health domain.",
      "summary": "This paper addresses the challenge of classifying negative pregnancy outcomes from imbalanced social media data. It proposes an NLP pipeline to extract and categorize pregnancy experiences from Twitter and investigates data augmentation techniques to balance the dataset. The research demonstrates the viability of social media data as a supplementary resource for epidemiological studies on pregnancy.",
      "mindmap": "graph TB\n        A[Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[婴儿死亡率高，负面妊娠结局数据稀缺 / High infant mortality, scarce data on negative pregnancy outcomes]\n        B --> B2[社交媒体数据不平衡、有噪声 / Social media data is imbalanced and noisy]\n        C --> C1[构建NLP流水线分类妊娠结局 / Build NLP pipeline to classify pregnancy outcomes]\n        C --> C2[使用数据增强处理不平衡数据 / Use data augmentation for imbalanced data]\n        D --> D1[验证社交媒体数据作为辅助资源的可行性 / Validate social media data as an adjunctive resource]\n        D --> D2[为未来健康研究提供框架 / Provide a framework for future health studies]"
    },
    {
      "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
      "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
      "institution": "Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI",
      "link": "https://arxiv.org/pdf/2512.22733",
      "code": "https://github.com/SHAO-Jiaqi757/FoldAct",
      "tags": [
        "agent system",
        "context folding",
        "long-horizon RL",
        "non-stationary observation",
        "gradient dilution",
        "selective segment training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp",
      "contributions": "1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.",
      "summary": "The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.",
      "mindmap": "graph TB\n        A[FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[非平稳观测分布 / Non-stationary Observation Distribution]\n        B --> B2[梯度稀释 / Gradient Dilution]\n        B --> B3[计算成本高 / High Computational Cost]\n        C --> C1[分离损失计算 / Separated Loss Computation]\n        C --> C2[全上下文一致性损失 / Full Context Consistency Loss]\n        C --> C3[选择性片段训练 / Selective Segment Training]\n        D --> D1[稳定训练 / Stable Training]\n        D --> D2[5.19倍加速 / 5.19× Speedup]"
    },
    {
      "title": "When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction",
      "authors": "Sungwoo Kang",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.22740",
      "code": null,
      "tags": [
        "multi-task learning",
        "negative transfer",
        "data imbalance",
        "task independence",
        "metal alloys",
        "property prediction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp",
      "contributions": "1. Empirical demonstration of a dichotomy where MTL degrades regression but improves classification for metal alloy property prediction. 2. Quantitative analysis linking MTL failure to severe data imbalance and near-zero inter-task dependencies. 3. Practical guidelines for materials informatics: use independent models for precise regression and MTL for classification tasks requiring high recall.",
      "summary": "This paper tests the assumption of Multi-Task Learning (MTL) in materials informatics by predicting three metal alloy properties. The results show MTL harms regression performance due to negative transfer from data imbalance but improves classification recall, as the properties are found to be independent. The work concludes with recommendations for when to use or avoid MTL in material discovery.",
      "mindmap": "graph TB\n        A[When Does Multi-Task Learning Fail? <br/>多任务学习何时失效？] --> B{核心问题/Problem};\n        A --> C{主要方法/Method};\n        A --> D{关键结果/Results};\n        B --> B1[MTL Assumption in Materials <br/>材料中的MTL假设];\n        C --> C1[Compare STL, MTL, Structured MTL <br/>比较单任务、标准MTL、结构化MTL];\n        D --> D1[Regression Degrades <br/>回归性能下降];\n        D --> D2[Classification Improves <br/>分类性能提升];\n        D --> D3[Properties Independent <br/>属性相互独立];"
    },
    {
      "title": "Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL",
      "authors": "Rihong Qiu, Zhibang Yang, Xinke Jiang, Weibin Liao, Xin Gao, Xu Chu, Junfeng Zhao, Yasha Wang",
      "institution": "Peking University",
      "link": "https://arxiv.org/pdf/2512.22744",
      "code": null,
      "tags": [
        "text-to-sql",
        "semantic validation",
        "hierarchical representation",
        "logical plan",
        "abstract syntax tree",
        "nested message passing neural network"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp",
      "contributions": "1. Proposed HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans) and local details (via Abstract Syntax Trees) for semantic validation. 2. Employed a Nested Message Passing Neural Network (NMPNN) to capture relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. 3. Introduced an AST-driven sub-SQL augmentation strategy to generate high-quality negative samples for robust optimization of fine-grained semantic inconsistencies.",
      "summary": "The paper addresses the challenge of semantic validation in Text-to-SQL systems by proposing HEROSQL, a hierarchical representation method that combines global intent and local SQL details using Logical Plans and Abstract Syntax Trees, enhanced by a Nested Message Passing Neural Network. It also introduces an AST-driven augmentation strategy for generating negative samples. Experiments show that HEROSQL outperforms state-of-the-art methods in detecting semantic inconsistencies, improving AUPRC by 9.40% and AUROC by 12.35% on average.",
      "mindmap": "graph TB\n        A[HEROSQL: Bridging Global Intent with Local Details] --> B[核心问题/Problem: Semantic validation in Text-to-SQL, capturing global intent and local details]\n        A --> C[主要方法/Method: Hierarchical representation (LP + AST), NMPNN, AST-driven augmentation]\n        A --> D[关键结果/Results: Outperforms SOTA, +9.40% AUPRC, +12.35% AUROC]"
    },
    {
      "title": "From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms",
      "authors": "Rui Ai, David Simchi-Levi, Feng Zhu",
      "institution": "Massachusetts Institute of Technology (MIT)",
      "link": "https://arxiv.org/pdf/2512.22749",
      "code": null,
      "tags": [
        "reinforcement learning",
        "demand learning",
        "instrumental variables",
        "deep neural networks",
        "regret bound",
        "confounding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp",
      "contributions": "1. Developed an algorithm for dynamic service fee pricing with optimal regret, showing a phase transition based on supply-side noise. 2. Demonstrated that non-i.i.d. actions can serve as instrumental variables to address confounding in demand learning. 3. Proposed a novel homeomorphic construction to establish estimation bounds for learning demand with deep neural networks without requiring star-shapedness assumptions.",
      "summary": "This paper studies the problem of dynamic service fee pricing on third-party platforms, where only equilibrium price and quantity are observable, creating a confounding demand learning problem. The authors develop an algorithm with optimal regret, using non-i.i.d. actions as instrumental variables and a novel homeomorphic construction for deep neural network-based demand estimation. The results show that supply-side noise fundamentally impacts learnability and the method is validated with simulations and real-world data from Zomato and Lyft.",
      "mindmap": "graph TB\n        Root[”From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms”]\n        Root --> Problem[”核心问题/Problem: 在混杂因素下学习需求/Demand Learning under Confounding”]\n        Root --> Method[”主要方法/Method: 使用非独立同分布行动作为工具变量/Using non-i.i.d. actions as Instrumental Variables”]\n        Root --> Results[”关键结果/Results: 最优遗憾与相变/Optimal Regret & Phase Transition”]"
    },
    {
      "title": "A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants",
      "authors": "Eswarasanthosh Kumar Mamillapalli, Nishtha Sharma",
      "institution": "n/a (Inferred from email: nishtha sharma@nh.gov suggests potential affiliation with a state health department, but no clear academic/research institution is specified. The other author's email is a personal Gmail address.)",
      "link": "https://arxiv.org/pdf/2512.22758",
      "code": null,
      "tags": [
        "public health informatics",
        "multi-level modeling",
        "XGBoost",
        "environmental vulnerability index",
        "NHANES",
        "machine learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp",
      "contributions": "1. Proposed a novel micro-macro machine learning framework that integrates individual-level health data with macro-level environmental data to predict childhood obesity risk. 2. Constructed a composite environmental vulnerability index (EnvScore) from USDA and EPA data to quantify state-level structural risk factors. 3. Demonstrated a scalable, data-driven modeling pipeline that reveals geographic alignment between high environmental burden and predicted individual obesity risk, enabling identification of environment-driven health disparities.",
      "summary": "This paper introduces a machine learning framework that combines individual health data from NHANES with environmental data (food access, air quality) from USDA/EPA to predict childhood obesity. The best-performing model was XGBoost, and a state-level environmental risk score was created. The study found a strong geographic correlation between areas of high environmental burden and high predicted obesity risk, showing the value of multi-scale data integration for public health.",
      "mindmap": "graph TB\n        A[A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk<br>预测儿童肥胖风险的微-宏观机器学习框架] --> B(Problem: Childhood obesity is influenced by multi-level factors, but studies often analyze them independently.<br>核心问题: 儿童肥胖受多层面因素影响，但研究常孤立分析)\n        A --> C(Method: Integrated NHANES microdata with USDA/EPA macrodata. Used ML models (e.g., XGBoost) and created an EnvScore index.<br>主要方法: 整合NHANES微观数据与USDA/EPA宏观数据。使用ML模型并创建EnvScore指数)\n        A --> D(Results: XGBoost performed best. High environmental burden states aligned with high predicted obesity risk.<br>关键结果: XGBoost表现最佳。高环境负担州与高预测肥胖风险分布一致)"
    },
    {
      "title": "Active Constraint Learning in High Dimensions from Demonstrations",
      "authors": "Zheng Qiu, Chih-Yuan Chiu, Glen Chou",
      "institution": "Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22757",
      "code": null,
      "tags": [
        "robot learning",
        "active learning",
        "constraint inference",
        "Gaussian processes",
        "learning from demonstration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp",
      "contributions": "1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.",
      "summary": "This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.",
      "mindmap": "graph TB\n        Root(”Active Constraint Learning in High Dimensions from Demonstrations”) --> Problem(”核心问题/Problem: Data-inefficient constraint inference from demonstrations”)\n        Root --> Method(”主要方法/Method: Iterative active learning with Gaussian Processes”)\n        Root --> Results(”关键结果/Results: Outperforms baseline with sparse, informative demonstrations”)"
    },
    {
      "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer",
      "authors": "Nikhil Ghosh, Denny Wu, Alberto Bietti",
      "institution": "Flatiron Institute, New York University",
      "link": "https://arxiv.org/pdf/2512.22768",
      "code": null,
      "tags": [
        "hyperparameter optimization",
        "hyperparameter transfer",
        "scale-aware hyperparameters",
        "Maximal Update Parameterization (μP)",
        "compute-optimal grid search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp",
      "contributions": "1. Develops a formal conceptual framework defining \"fast\" hyperparameter transfer and proves its equivalence to \"useful\" transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.",
      "summary": "This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.",
      "mindmap": "graph TB\n        Root[”Understanding the Mechanisms of Fast Hyperparameter Transfer<br>理解快速超参数迁移的机制”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Standard HP tuning is too expensive for large models.<br>标准HP调优对于大模型过于昂贵”] --> P1[”子问题/Sub-problem<br>How to define and understand 'fast' HP transfer?<br>如何定义和理解'快速'HP迁移？”]\n        Method[”主要方法/Method<br>Develop a formal framework for HP transfer.<br>建立HP迁移的形式化框架”] --> M1[”方法步骤/Step<br>Define 'fast' vs 'useful' transfer.<br>定义'快速'与'有用'迁移”]\n        Method --> M2[”方法步骤/Step<br>Analyze problem structure & µP.<br>分析问题结构与µP”]\n        Method --> M3[”方法步骤/Step<br>Propose trajectory decomposition hypothesis.<br>提出轨迹分解假设”]\n        Results[”关键结果/Results<br>Fast transfer is equivalent to useful transfer.<br>快速迁移等价于有用迁移”] --> R1[”结果/Result<br>Transfer success depends on problem structure.<br>迁移成功取决于问题结构”]\n        Results --> R2[”结果/Result<br>Empirical evidence supports the hypothesis.<br>实证证据支持该假设”]"
    },
    {
      "title": "Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning",
      "authors": "Kasra Jalaldoust, Elias Bareinboim",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.22777",
      "code": null,
      "tags": [
        "causal inference",
        "causal transportability",
        "domain adaptation",
        "few-shot learning",
        "circuit composition",
        "distribution shift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp",
      "contributions": "1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.",
      "summary": "This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.",
      "mindmap": "graph TB\n    A[Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning] --> B[核心问题/Problem: Generalization under distribution shift]\n    A --> C[主要方法/Method: Circuit-TR algorithm based on causal transportability]\n    A --> D[关键结果/Results: Theoretical characterization of few-shot learnability]\n    B --> B1[领域泛化与适应/Domain Generalization & Adaptation]\n    C --> C1[模块学习与电路组合/Module Learning & Circuit Composition]\n    C --> C2[因果图与机制共享/Causal Graph & Mechanism Sharing]\n    D --> D1[可迁移性标准/Transportability Criteria]\n    D --> D2[电路规模复杂度/Circuit Size Complexity]"
    },
    {
      "title": "GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks",
      "authors": "Xuyan Li, Jie Wang, Zheng Yan",
      "institution": "Xidian University",
      "link": "https://arxiv.org/pdf/2512.22772",
      "code": null,
      "tags": [
        "graph neural networks",
        "temporal graph neural networks",
        "explainable ai",
        "graph explanation",
        "recurrent neural networks",
        "breadth-first search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp",
      "contributions": "1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.",
      "summary": "This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.",
      "mindmap": "graph TB\n        A[GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[TGNN缺乏透明度和可解释性/Lack of TGNN transparency & explainability]\n        B --> B2[现有方法通用性差、效率低、不友好/Existing methods lack generality, efficiency, user-friendliness]\n        C --> C1[提取节点序列作为统一特征/Extract node sequences as unified features]\n        C --> C2[使用BFS和时间信息构建序列/Use BFS & temporal info to construct sequences]\n        C --> C3[基于RNN的生成模型/RNN-based generative model]\n        D --> D1[在6个数据集上实验/Experiments on 6 datasets]\n        D --> D2[优于现有基线方法/Outperforms existing baselines]\n        D --> D3[通用、高效、用户友好/Generality, efficiency, user-friendliness]"
    },
    {
      "title": "Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization",
      "authors": "Truong Son Nguyen",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.22774",
      "code": null,
      "tags": [
        "quantum-inspired machine learning",
        "spectral decomposition",
        "Hamiltonian learning",
        "semantic wavefunctions",
        "operator calculus",
        "emergent manifolds"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp",
      "contributions": "1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.",
      "summary": "This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.",
      "mindmap": "graph TB\n        Root[”Schrödinger AI: A Unified Spectral-Dynamical Framework<br>薛定谔AI: 统一谱-动力学框架”] --> Problem[”核心问题/Problem<br>Limitations of conventional ML<br>传统机器学习的局限”]\n        Root --> Method[”主要方法/Method<br>Quantum-inspired framework<br>量子启发的框架”]\n        Root --> Results[”关键结果/Results<br>Empirical demonstrations<br>实证演示”]\n        Problem --> P1[”Struggle with uncertainty & adaptation<br>难以处理不确定性与适应性”]\n        Problem --> P2[”Brittle symbolic reasoning<br>脆弱的符号推理”]\n        Method --> M1[”Time-independent wave-energy solver<br>时间无关波能求解器”]\n        Method --> M2[”Time-dependent dynamical solver<br>时间相关动力学求解器”]\n        Method --> M3[”Low-rank operator calculus<br>低秩算子演算”]\n        Results --> R1[”Emergent semantic manifolds<br>涌现的语义流形”]\n        Results --> R2[”Dynamic reasoning adaptation<br>动态推理适应”]\n        Results --> R3[”Exact operator generalization<br>精确算子泛化”]"
    },
    {
      "title": "Discovering Transmission Dynamics of COVID-19 in China",
      "authors": "Zhou Yang, Edward Dougherty, Chen Zhang, Zhenhe Pan, Fang Jin",
      "institution": "George Washington University, Roger Williams University, Texas Tech University",
      "link": "https://arxiv.org/pdf/2512.22787",
      "code": null,
      "tags": [
        "information extraction",
        "natural language processing",
        "transmission chain",
        "epidemiological analysis",
        "data mining",
        "statistical analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp",
      "contributions": "1. Constructed transmission/tracking chains for COVID-19 in China by applying NLP and manual curation to data mined from diverse public sources. 2. Conducted a comprehensive spatiotemporal analysis by integrating case tracking data with population mobility data from Wuhan. 3. Quantified key transmission dynamics, revealing regional differences, hospitalization timelines, and the evolution of infection sources over the course of the pandemic.",
      "summary": "This paper investigates the transmission dynamics of COVID-19 in China by mining and processing public case reports using NLP to construct transmission chains. The analysis integrates these chains with mobility data to quantify spatiotemporal spread. Key findings include significant regional differences in infection rates, rapid hospitalization of symptomatic cases, and a shift in infection sources from travel to social activities over time.",
      "mindmap": "graph TB\n        A[Discovering Transmission Dynamics of COVID-19 in China] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[分析中国COVID-19传播机制/Analyze China COVID-19 transmission mechanisms]\n        C --> C1[从多源收集病例数据/Collect case data from multiple sources]\n        C --> C2[应用NLP构建传播链/Apply NLP to construct transmission chains]\n        C --> C3[结合移动数据进行统计分析/Combine mobility data for statistical analysis]\n        D --> D1[大城市感染更多，存在地区差异/Larger cities have more infections, regional differences exist]\n        D --> D2[79%有症状者5天内住院/79% symptomatic hospitalized within 5 days]\n        D --> D3[感染源从旅行转向社交活动/Infection source shifted from travel to social activities]"
    },
    {
      "title": "CNSight: Evaluation of Clinical Note Segmentation Tools",
      "authors": "Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong",
      "institution": "University of Southern California",
      "link": "https://arxiv.org/pdf/2512.22795",
      "code": null,
      "tags": [
        "text segmentation",
        "clinical note segmentation",
        "transformer models",
        "large language models",
        "MIMIC-IV",
        "rule-based baselines"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp",
      "contributions": "1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.",
      "summary": "This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.",
      "mindmap": "graph TB\n        Root[CNSight: 临床笔记分割工具评估 / CNSight: Evaluation of Clinical Note Segmentation Tools]\n        Root --> Problem[临床笔记非结构化 / Clinical Notes Unstructured]\n        Root --> Method[评估规则/变换器/大语言模型 / Evaluate Rule-based/Transformer/LLMs]\n        Root --> Results[大模型性能最佳 / Large Models Best Performance]"
    },
    {
      "title": "SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance",
      "authors": "Shuai Chen, Chen Wang, Ziran Wang",
      "institution": "School of Mechanical Engineering, Shandong University",
      "link": "https://arxiv.org/pdf/2512.22792",
      "code": null,
      "tags": [
        "open-set recognition",
        "spherical normalization",
        "Mahalanobis distance",
        "electronic nose",
        "open-set recognition",
        "feature drift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp",
      "contributions": "1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.",
      "summary": "This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.",
      "mindmap": "graph TB\n        Root[”SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Feature drift & unknown gas interference in E-nose”] --> P1[”信号漂移/Feature Distribution Shift”]\n        Problem --> P2[”未知气体干扰/Unknown Gas Interference”]\n        Method[”主要方法/Method<br>SNM-Net Framework”] --> M1[”几何解耦/Geometric Decoupling<br>Cascaded Batch & L2 Norm”]\n        Method --> M2[”马氏距离评分/Mahalanobis Distance Scoring”]\n        Method --> M3[”架构无关/Architecture-Agnostic<br>CNN, RNN, Transformer”]\n        Results[”关键结果/Results<br>State-of-the-art performance”] --> R1[”高AUROC/High AUROC: 0.9977”]\n        Results --> R2[”高未知气体检测率/High Unknown Detection: 99.57%”]\n        Results --> R3[”强鲁棒性/High Robustness<br>Low std. dev.”]"
    },
    {
      "title": "ReDiF: Reinforced Distillation for Few Step Diffusion",
      "authors": "Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee",
      "institution": "Sharif University of Technology, Alan Turing Institute, London School of Economics",
      "link": "https://arxiv.org/pdf/2512.22802",
      "code": null,
      "tags": [
        "diffusion models",
        "reinforcement learning",
        "knowledge distillation",
        "policy optimization",
        "denoising paths",
        "model agnostic"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp",
      "contributions": "1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.",
      "summary": "This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.",
      "mindmap": "graph TB\n        A[ReDiF: Reinforced Distillation for Few Step Diffusion] --> B(核心问题/Problem: Diffusion模型采样慢/Slow sampling in diffusion models)\n        A --> C(主要方法/Method: 基于强化学习的蒸馏框架/Reinforcement learning based distillation framework)\n        A --> D(关键结果/Results: 更少步骤，性能更优/Fewer steps, superior performance)"
    },
    {
      "title": "MoR: Mixture Of Representations For Mixed-Precision Training",
      "authors": "Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani",
      "institution": "Nvidia, Meta",
      "link": "https://arxiv.org/pdf/2512.22804",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "mixed-precision training",
        "FP8",
        "dynamic quantization",
        "tensor representation",
        "low-precision training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp",
      "contributions": "1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.",
      "summary": "This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.",
      "mindmap": "graph TB\n        Root[MoR: Mixture Of Representations For Mixed-Precision Training] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Successful mixed-precision training requires the right combination of methods.]\n        Method[主要方法/Method<br>Dynamic, property-aware quantization framework selecting between representations (e.g., FP8/BF16).]\n        Results[关键结果/Results<br>Achieves 98.38% FP8 quantization, preserves model quality, enables lower precision formats.]"
    },
    {
      "title": "Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models",
      "authors": "Scott A. Martin, Noah Brenowitz, Dale Durran, Michael Pritchard",
      "institution": "NVIDIA Research, University of Washington",
      "link": "https://arxiv.org/pdf/2512.22814",
      "code": null,
      "tags": [
        "climate informatics",
        "long-range distillation",
        "synthetic training data",
        "subseasonal-to-seasonal forecasting",
        "probabilistic forecasting",
        "autoregressive models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp",
      "contributions": "1. Proposes long-range distillation, a method to train a long-timestep probabilistic model using massive synthetic data from an autoregressive teacher model. 2. Demonstrates the generation and use of over 10,000 years of simulated climate data from the DLESyM model for training. 3. Shows that distilled models achieve S2S forecast skill comparable to ECMWF ensembles after fine-tuning, with skill scaling with synthetic data volume.",
      "summary": "This paper addresses the challenge of long-range weather forecasting by introducing long-range distillation, a method that trains a single-step probabilistic model using a massive synthetic dataset generated by an autoregressive AI model. The distilled model, trained on over 10,000 years of simulated climate, achieves subseasonal-to-seasonal forecast skill comparable to state-of-the-art ensemble methods, demonstrating that AI-generated synthetic data can effectively scale long-range forecast skill.",
      "mindmap": "graph TB\n        A[Long-Range Distillation Paper] --> B(核心问题/Problem: Long-range AI weather forecasting is limited by error accumulation and scarce training data for slow climate modes.)\n        A --> C(主要方法/Method: Distill a long-timestep probabilistic student model using 10,000+ years of synthetic data from an autoregressive teacher model (DLESyM).)\n        A --> D(关键结果/Results: Student model skill scales with synthetic data, approaches teacher skill, and matches ECMWF ensemble skill after ERA5 fine-tuning.)"
    },
    {
      "title": "TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning",
      "authors": "Gaurav Chaudhary, Laxmidhar Behera",
      "institution": "IIT Kanpur",
      "link": "https://arxiv.org/pdf/2512.22824",
      "code": null,
      "tags": [
        "reinforcement learning",
        "curriculum learning",
        "goal-conditioned reinforcement learning",
        "temporal variance",
        "student-teacher paradigm",
        "Q-function"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp",
      "contributions": "1. Proposes a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum for accelerating Goal-Conditioned RL. 2. Establishes a theoretical connection between the temporal variance of Q-values and policy evolution. 3. Demonstrates the algorithm-agnostic nature of the approach, showing consistent improvements across 11 robotic manipulation and maze navigation tasks.",
      "summary": "This paper addresses the sample inefficiency of uniform goal selection in multi-goal reinforcement learning. It proposes a TEACH framework where a teacher module dynamically selects goals with the highest temporal variance in Q-values to create an adaptive curriculum. The method is shown to improve learning efficiency over state-of-the-art curriculum learning methods across diverse robotic tasks.",
      "mindmap": "graph TB\n        A[TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Uniform goal selection is sample inefficient in multi-goal RL/多目标RL中均匀目标选择样本效率低]\n        C --> C1[Student-Teacher paradigm with Temporal Variance-Driven Curriculum/基于时序方差的师生课程学习范式]\n        C --> C2[Teacher prioritizes goals with highest Q-value temporal variance/教师模块优先选择Q值时序方差最高的目标]\n        D --> D1[Consistent improvements over SOTA methods/相比SOTA方法取得一致改进]\n        D --> D2[Evaluated on 11 robotic manipulation and navigation tasks/在11个机器人操作与导航任务上验证]"
    },
    {
      "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
      "authors": "Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang",
      "institution": "University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.22854",
      "code": "https://neutrinoliu.github.io/byteloom/",
      "tags": [
        "video generation",
        "Human-Object Interaction",
        "Diffusion Transformer",
        "Relative Coordinate Maps",
        "Progressive Curriculum Learning",
        "Geometry Consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp",
      "contributions": "1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.",
      "summary": "This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.",
      "mindmap": "graph TB\n        A[ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法缺乏多视图信息注入机制/Existing methods lack multi-view injection]\n        B --> B2[严重依赖手部网格标注/Heavy reliance on hand mesh annotations]\n        C --> C1[提出RCM-cache机制/Propose RCM-cache mechanism]\n        C --> C2[设计渐进式课程学习/Design progressive curriculum learning]\n        D --> D1[保持物体几何一致性/Preserves object geometry consistency]\n        D --> D2[生成平滑运动视频/Generates smooth motion videos]"
    },
    {
      "title": "Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks",
      "authors": "Soham Padia, Dhananjay Vaidya, Ramchandra Mangrulkar",
      "institution": "Northeastern University, Dwarkadas J. Sanghvi College of Engineering",
      "link": "https://arxiv.org/pdf/2512.22860",
      "code": null,
      "tags": [
        "blockchain security",
        "IoT security",
        "adversarial machine learning",
        "Fully Homomorphic Encryption",
        "Attribute-Based Access Control",
        "Multi-Agent Reinforcement Learning",
        "Byzantine Fault Tolerance",
        "Trust-Based Consensus"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp",
      "contributions": "1. Proposes a novel trust-based delegated consensus framework for blockchain IoT that integrates Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation. 2. Systematically compares the performance of three reinforcement learning approaches (RL, DRL, MARL) against five distinct and sophisticated adversarial attack families. 3. Empirically demonstrates that Multi-Agent RL (MARL) provides superior defense against collusive attacks and identifies the catastrophic vulnerability of all learning agents to Time-Delayed Poisoning (sleeper) attacks.",
      "summary": "This paper addresses securing blockchain-enabled IoT networks by proposing a trust-based consensus framework that combines privacy-preserving techniques (FHE and ABAC) with learning-based defenses. It compares RL, DRL, and MARL against five attack types, finding MARL most effective against collusive attacks but revealing that all methods are highly vulnerable to time-delayed poisoning attacks.",
      "mindmap": "graph TB\n        Root[”Adaptive Trust Consensus for Blockchain IoT<br/>区块链物联网自适应信任共识”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”Securing Blockchain IoT Against Attacks<br/>保护区块链物联网免受攻击”] --> A1[”Naive Malicious Attack (NMA)<br/>简单恶意攻击”]\n        Problem --> A2[”Collusive Rumor Attack (CRA)<br/>合谋谣言攻击”]\n        Problem --> A3[”Adaptive Adversarial Attack (AAA)<br/>自适应对抗攻击”]\n        Problem --> A4[”Byzantine Fault Injection (BFI)<br/>拜占庭故障注入”]\n        Problem --> A5[”Time-Delayed Poisoning (TDP)<br/>时间延迟投毒”]\n    \n        Method[”Trust Framework with FHE & ABAC + Learning Defenses<br/>基于FHE和ABAC的信任框架与学习防御”] --> M1[”Reinforcement Learning (RL)<br/>强化学习”]\n        Method --> M2[”Deep RL (DRL)<br/>深度强化学习”]\n        Method --> M3[”Multi-Agent RL (MARL)<br/>多智能体强化学习”]\n    \n        Results[”Key Experimental Findings<br/>关键实验结果”] --> R1[”MARL best vs. Collusive Attacks<br/>MARL对合谋攻击最佳”]\n        Results --> R2[”DRL & MARL perfect vs. Adaptive Attacks<br/>DRL和MARL完美防御自适应攻击”]\n        Results --> R3[”All agents fail vs. Time-Delayed Poisoning<br/>所有智能体在延迟投毒攻击下失效”]"
    },
    {
      "title": "Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks",
      "authors": "Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov",
      "institution": "Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University",
      "link": "https://arxiv.org/pdf/2512.22876",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Reinforcement Networks",
        "directed acyclic graph (DAG)",
        "credit assignment",
        "LevelEnv",
        "hierarchical RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp",
      "contributions": "1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.",
      "summary": "The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.",
      "mindmap": "graph TB\n        A[Reinforcement Networks] --> B[核心问题/Problem: End-to-end training of multi-component AI systems]\n        A --> C[主要方法/Method: MARL agents organized in a DAG (Reinforcement Networks)]\n        A --> D[关键结果/Results: Improved performance, unified framework for structured MARL]"
    },
    {
      "title": "Fundamental Novel Consistency Theory: $H$-Consistency Bounds",
      "authors": "Yutao Zhong",
      "institution": "New York University",
      "link": "https://arxiv.org/pdf/2512.22880",
      "code": null,
      "tags": [
        "statistical learning theory",
        "H-consistency bounds",
        "surrogate loss",
        "minimizability gaps",
        "adversarial robustness",
        "comp-sum losses"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp",
      "contributions": "1. Introduces a novel theoretical framework for deriving H-consistency bounds, which provide stronger and more informative guarantees than Bayes-consistency or H-calibration by accounting for the hypothesis set. 2. Establishes the first H-consistency bounds for a wide range of losses in binary and multi-class classification, including convex surrogates, max/sum/constrained losses, and comp-sum losses (e.g., cross-entropy), and extends the analysis to adversarial scenarios. 3. Analyzes the growth rates of H-consistency bounds, proving a universal square-root growth rate for smooth surrogates, and introduces the analysis of minimizability gaps to guide the selection of surrogate loss functions for learning.",
      "summary": "This paper presents a new theoretical framework for analyzing the estimation error of target losses using surrogate losses in machine learning. It introduces H-consistency bounds, which offer stronger guarantees by incorporating the hypothesis set, and derives these bounds for various loss functions in both standard and adversarial settings. The main conclusion is that these bounds provide a more precise tool for understanding surrogate loss performance and can guide the design of robust learning algorithms.",
      "mindmap": "graph TB\n        A[Fundamental Novel Consistency Theory: H-Consistency Bounds] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”训练损失与目标损失不一致<br/>Surrogate vs. Target Loss Mismatch”)\n        C --> C1(”提出H-一致性边界理论框架<br/>Propose H-Consistency Bounds Framework”)\n        C --> C2(”分析二元/多类分类的损失函数<br/>Analyze Losses for Binary/Multi-class”)\n        C --> C3(”扩展到对抗性场景<br/>Extend to Adversarial Setting”)\n        D --> D1(”得到紧的分布依赖/独立边界<br/>Tight Distribution-Dependent/Independent Bounds”)\n        D --> D2(”首次为多种损失推导H-一致性边界<br/>First H-Consistency Bounds for Various Losses”)\n        D --> D3(”证明平滑代理的平方根增长速率<br/>Square-Root Growth Rate for Smooth Surrogates”)"
    },
    {
      "title": "Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral",
      "authors": "Anqi Mao",
      "institution": "New York University",
      "link": "https://arxiv.org/pdf/2512.22886",
      "code": null,
      "tags": [
        "machine learning theory",
        "multi-expert deferral",
        "abstention",
        "H-consistency",
        "surrogate losses",
        "two-stage learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp",
      "contributions": "1. Introduced new surrogate loss families and proved strong consistency guarantees for multi-class learning with abstention, resolving open questions. 2. Designed new surrogate losses with H-consistency bounds for general multi-expert deferral in classification, leading to effective algorithms. 3. Proposed a novel framework and surrogate losses for regression with deferral, accommodating multiple experts and various cost structures.",
      "summary": "This thesis addresses the problems of learning with abstention and multi-expert deferral to improve the reliability and efficiency of models like LLMs. It proposes new surrogate loss formulations for classification and regression, proves strong theoretical consistency guarantees, and demonstrates the empirical effectiveness of the resulting algorithms on datasets like CIFAR-10 and CIFAR-100.",
      "mindmap": "graph TB\n        Root[”Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题 / Problem<br>LLMs face hallucinations & high cost<br>Need reliable & efficient prediction”] --> P1[”子问题1 / Sub-Problem 1<br>Learning with Abstention”]\n        Problem --> P2[”子问题2 / Sub-Problem 2<br>Multi-Expert Deferral”]\n        Problem --> P3[”子问题3 / Sub-Problem 3<br>Regression with Deferral”]\n        Method[”主要方法 / Method<br>Design new surrogate losses<br>Prove H-consistency guarantees”] --> M1[”方法1 / Method 1<br>For abstention (score-based & predictor-rejector)”]\n        Method --> M2[”方法2 / Method 2<br>For multi-expert deferral”]\n        Method --> M3[”方法3 / Method 3<br>For regression deferral framework”]\n        Results[”关键结果 / Results<br>Strong consistency guarantees<br>Superior empirical performance”] --> R1[”结果1 / Result 1<br>Resolved open questions in abstention”]\n        Results --> R2[”结果2 / Result 2<br>New effective algorithms for deferral”]\n        Results --> R3[”结果3 / Result 3<br>Versatile framework for regression”]"
    },
    {
      "title": "Debugging Tabular Log as Dynamic Graphs",
      "authors": "Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You",
      "institution": "University of Illinois Urbana-Champaign, Amazon",
      "link": "https://arxiv.org/pdf/2512.22903",
      "code": null,
      "tags": [
        "others",
        "dynamic graph",
        "graph neural network",
        "tabular log",
        "log debugging",
        "heterogeneous nodes"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp",
      "contributions": "1. Proposes GraphLogDebugger, a novel framework that models tabular log data as dynamic graphs with heterogeneous nodes for objects and events, 2. Demonstrates that a simple dynamic GNN can outperform large language models (LLMs) in debugging tasks using this graph representation, 3. Validates the approach on real-world datasets from computer systems and academic papers, showing improved flexibility and scalability over LLM-based methods.",
      "summary": "This paper introduces GraphLogDebugger, a framework that converts tabular log data into dynamic graphs to detect inconsistencies in real-world systems. By representing logs as evolving graphs with object and event nodes, a lightweight dynamic Graph Neural Network effectively debugs logs, outperforming larger LLM-based models in experiments on system and academic log datasets.",
      "mindmap": "graph TB\n        A[DEBUGGING TABULAR LOG AS DYNAMIC GRAPHS] --> B[核心问题/Problem: LLMs and heavy models lack flexibility and scalability for tabular log debugging]\n        A --> C[主要方法/Method: GraphLogDebugger models logs as dynamic graphs with heterogeneous nodes and edges]\n        A --> D[关键结果/Results: Simple dynamic GNN outperforms LLMs in debugging on real-world datasets]"
    },
    {
      "title": "A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments",
      "authors": "Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu",
      "institution": "University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.22901",
      "code": null,
      "tags": [
        "on-device ai",
        "Casing Collar Locator (CCL)",
        "ARM Cortex-M7",
        "Depthwise Separable Convolutions",
        "MACs",
        "Inference Latency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp",
      "contributions": "1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight \"Collar Recognition Nets\" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.",
      "summary": "This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight \"Collar Recognition Nets\" optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.",
      "mindmap": "graph TB\n        Root[”A Neural Network-Based Real-time Casing Collar Recognition System<br>基于神经网络的实时套管接箍识别系统”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”信号衰减导致井下定位不准确<br>Signal degradation compromises downhole positioning”]\n        Method[”为ARM Cortex-M7优化的轻量级CRN网络<br>Lightweight CRNs optimized for ARM Cortex-M7”]\n        Results[”8208 MACs, F1=0.972, 343.2µs延迟<br>8208 MACs, F1=0.972, 343.2µs latency”]"
    },
    {
      "title": "Federated Multi-Task Clustering",
      "authors": "S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong",
      "institution": "South China University of Technology, Dalian University of Technology, Xidian University",
      "link": "https://arxiv.org/pdf/2512.22897",
      "code": null,
      "tags": [
        "federated learning",
        "federated clustering",
        "spectral clustering",
        "multi-task learning",
        "tensor methods",
        "ADMM"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp",
      "contributions": "1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm.",
      "summary": "This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods.",
      "mindmap": "graph TB\n        A[Federated Multi-Task Clustering] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Centralized models inapplicable to decentralized environments/集中式模型不适用于去中心化环境]\n        B --> B2[Poor generalization due to unreliable pseudo-labels/伪标签不可靠导致泛化性能差]\n        B --> B3[Failure to capture latent client correlations/未能捕获客户端间的潜在关联]\n        C --> C1[Client-side personalized clustering module/客户端个性化聚类模块]\n        C --> C2[Server-side tensorial correlation module/服务器端张量关联模块]\n        C --> C3[ADMM-based distributed algorithm/基于ADMM的分布式算法]\n        D --> D1[Outperforms baselines and SOTA/性能优于基线和前沿方法]\n        D --> D2[Validated on real-world datasets/在真实数据集上得到验证]"
    },
    {
      "title": "MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning",
      "authors": "Jin Wu, Chanjin Zheng",
      "institution": "Shanghai Institute of Artificial Intelligence for Education, East China Normal University",
      "link": "https://arxiv.org/pdf/2512.22904",
      "code": null,
      "tags": [
        "educational data mining",
        "cognitive diagnosis",
        "meta-learning",
        "continual learning",
        "long-tailed distribution",
        "parameter protection mechanism"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp",
      "contributions": "1. Proposes MetaCD, a novel framework that integrates meta-learning and continual learning for cognitive diagnosis. 2. Uses meta-learning to learn an optimal initialization to alleviate the long-tailed data problem, enabling good performance with few samples. 3. Incorporates a continual learning parameter protection mechanism to adapt to dynamic data changes and new tasks while preventing catastrophic forgetting.",
      "summary": "The paper proposes MetaCD, a meta-learning framework based on continual learning, to address the challenges of long-tailed data distribution and dynamic changes in cognitive diagnosis for intelligent education. It uses meta-learning for optimal initialization and a parameter protection mechanism for continual adaptation, achieving superior accuracy and generalization on five real-world datasets compared to baselines.",
      "mindmap": "graph TB\n        Root[MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[长尾数据分布/Long-tailed Data Distribution]\n        Problem --> P2[数据的动态变化/Dynamic Data Changes]\n        Method[主要方法/Method] --> M1[元学习最优初始化/Meta-learning for Optimal Initialization]\n        Method --> M2[持续学习参数保护机制/Continual Learning Parameter Protection]\n        Results[关键结果/Results] --> R1[提升单任务可塑性/Improves Single-task Plasticity]\n        Results --> R2[确保序列任务稳定性与泛化性/Ensures Sequential Task Stability & Generalization]\n        Results --> R3[在五个真实数据集上超越基线/Outperforms Baselines on Five Real-world Datasets]"
    },
    {
      "title": "Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning",
      "authors": "Ünver Çiftçi",
      "institution": "Tekirdağ Namık Kemal University",
      "link": "https://arxiv.org/pdf/2512.22910",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Q-learning",
        "ensemble learning",
        "satisficing",
        "distillation",
        "bounded rationality"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp",
      "contributions": "1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.",
      "summary": "The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.",
      "mindmap": "graph TB\n        A[Sat-EnQ] --> B[核心问题/Problem: Deep Q-Learning Instability]\n        A --> C[主要方法/Method: Two-Phase Satisficing Ensemble]\n        A --> D[关键结果/Results: Variance Reduction & Robustness]\n        B --> B1[早期训练不稳定/Early Training Instability]\n        B --> B2[高方差与灾难性失败/High Variance & Catastrophic Failure]\n        C --> C1[阶段1: 满足化集成训练/Phase 1: Satisficing Ensemble Training]\n        C --> C2[阶段2: 蒸馏与微调/Phase 2: Distillation & Fine-tuning]\n        D --> D1[3.8倍方差降低/3.8x Variance Reduction]\n        D --> D2[0%灾难性失败/0% Catastrophic Failure]\n        D --> D3[2.5倍计算效率提升/2.5x Compute Efficiency]"
    },
    {
      "title": "Geometric Structural Knowledge Graph Foundation Model",
      "authors": "Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab",
      "institution": "University of Stuttgart, University of Southampton, Shahrood University of Technology",
      "link": "https://arxiv.org/pdf/2512.22931",
      "code": null,
      "tags": [
        "knowledge graph reasoning",
        "structural foundation model",
        "geometric attention",
        "inductive link prediction",
        "multi-head transformation",
        "relational fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp",
      "contributions": "1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.",
      "summary": "This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.",
      "mindmap": "graph TB\n        A[Geometric Structural Knowledge Graph Foundation Model] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖单一关系转换，表达能力受限/Existing methods rely on single relational transformation, limiting expressiveness]\n        C --> C1[引入多头几何注意力/Multi-head geometric attention]\n        C --> C2[并行多种几何变换/Parallel geometric transformations]\n        C --> C3[关系条件注意力融合/Relational conditioned attention fusion]\n        D --> D1[在56个KG上超越ULTRA/Outperforms ULTRA on 56 KGs]\n        D --> D2[零样本归纳链接预测性能提升/Improves zero-shot inductive link prediction]"
    },
    {
      "title": "Multiple Token Divergence: Measuring and Steering In-Context Computation Density",
      "authors": "Vincent Herrmann, Eric Alcaide, Michael Wand, Jürgen Schmidhuber",
      "institution": "The Swiss AI Lab IDSIA/USI/SUPSI, King Abdullah University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22944",
      "code": null,
      "tags": [
        "language model interpretability",
        "in-context computation",
        "KL divergence",
        "decoding method",
        "computational effort",
        "prediction head"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp",
      "contributions": "1. Proposes Multiple Token Divergence (MTD), a simple and non-invasive metric to measure a language model's in-context computational effort by comparing the output distributions of the full model and a shallow auxiliary head. 2. Introduces Divergence Steering, a novel decoding method that uses MTD to control the computational character of generated text. 3. Empirically demonstrates that MTD effectively distinguishes task complexity, correlates with problem difficulty, and that lower MTD is associated with more accurate reasoning.",
      "summary": "This paper addresses the challenge of measuring the in-context computational effort of language models. It proposes Multiple Token Divergence (MTD), a lightweight metric based on KL divergence between output distributions, and a corresponding decoding method called Divergence Steering. The authors show that MTD effectively correlates with task difficulty and can be used to analyze and steer model computation.",
      "mindmap": "graph TB\n        A[Multiple Token Divergence<br/>多令牌散度] --> B[核心问题/Problem<br/>如何衡量语言模型的上下文计算努力？<br/>How to measure in-context computational effort?]\n        A --> C[主要方法/Method<br/>提出多令牌散度(MTD)<br/>Propose Multiple Token Divergence (MTD)]\n        A --> D[关键结果/Results<br/>MTD有效区分任务复杂度<br/>MTD effectively distinguishes task complexity]\n        B --> B1[现有指标如下一令牌损失无效<br/>Metrics like next-token loss fail]\n        C --> C1[计算完整模型与浅层辅助头的输出分布KL散度<br/>KL divergence between full model and shallow head outputs]\n        C --> C2[提出散度引导解码方法<br/>Propose Divergence Steering decoding]\n        D --> D1[MTD与问题难度正相关<br/>MTD correlates positively with problem difficulty]\n        D --> D2[低MTD与更准确的推理相关<br/>Lower MTD associated with more accurate reasoning]"
    },
    {
      "title": "APO: Alpha-Divergence Preference Optimization",
      "authors": "Wang Zixian",
      "institution": "China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch",
      "link": "https://arxiv.org/pdf/2512.22953",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "alpha-divergence",
        "preference optimization",
        "mode collapse",
        "anchored coordinates",
        "gradient variance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp",
      "contributions": "1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.",
      "summary": "This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.",
      "mindmap": "graph TB\n        Root[APO: Alpha-Divergence Preference Optimization] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[两种分歧权衡 / Two Divergence Trade-off]\n        P1 --> P2[前向KL覆盖但保守 / Forward KL: Mode-Covering but Conservative]\n        P1 --> P3[反向KL寻求但易崩溃 / Reverse KL: Mode-Seeking but Collapses]\n        Method[主要方法/Method] --> M1[锚定框架 / Anchored Framework]\n        M1 --> M2[使用α-散度插值 / Use α-Divergence to Interpolate]\n        M2 --> M3[调度α值 / Schedule α Value]\n        Results[关键结果/Results] --> R1[竞争性性能 / Competitive Performance]\n        Results --> R2[保持稳定性 / Maintains Training Stability]"
    },
    {
      "title": "FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing",
      "authors": "Wafaa El Husseini",
      "institution": "Not explicitly stated. Affiliation inferred from email domain (gmail) is insufficient. Likely independent or institutional affiliation not provided in the excerpt.",
      "link": "https://arxiv.org/pdf/2512.22956",
      "code": null,
      "tags": [
        "synthetic data generation",
        "synthetic dataset",
        "longitudinal data",
        "feedback-driven simulation",
        "behavioral modeling",
        "benchmarking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp",
      "contributions": "1. Introduces FLOW, a novel synthetic longitudinal dataset modeling daily interactions between workload, lifestyle, and wellbeing. 2. Provides a configurable data generation tool for reproducible experimentation under adjustable assumptions. 3. Creates a publicly available, controlled experimental environment for methodological development and benchmarking where real-world data is inaccessible.",
      "summary": "The paper introduces FLOW, a synthetic longitudinal dataset generated via a rule-based, feedback-driven simulation to model daily interactions between work and wellbeing variables. It addresses the lack of accessible real-world data due to privacy and logistical constraints. The dataset and its configurable generation tool are released as a public resource to support reproducible research, methodological benchmarking, and education.",
      "mindmap": "graph TB\n        A[FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[缺乏可访问的纵向工作与福祉数据/Lack of accessible longitudinal work-wellbeing data]\n        B --> B2[隐私、伦理和后勤约束/Privacy, ethical, and logistical constraints]\n        C --> C1[基于规则的反馈驱动模拟/Rule-based, feedback-driven simulation]\n        C --> C2[生成合成数据集/Generate synthetic dataset]\n        D --> D1[包含1000个个体的两年每日数据/2-year daily data for 1000 individuals]\n        D --> D2[可配置的数据生成工具/Configurable data generation tool]\n        D --> D3[公开可用的基准测试资源/Publicly available benchmarking resource]"
    },
    {
      "title": "A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging",
      "authors": "Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr",
      "institution": "Shahid Beheshti University",
      "link": "https://arxiv.org/pdf/2512.22976",
      "code": null,
      "tags": [
        "biomedical signal processing",
        "multi-scale feature extraction",
        "hierarchical BiLSTM",
        "class-weighted loss",
        "temporal modeling",
        "sleep staging"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp",
      "contributions": "1. Proposes a context-aware and interpretable framework combining compact multi-scale feature extraction with hierarchical temporal modeling (BiLSTM) for single-channel EEG sleep staging. 2. Addresses class imbalance, especially for the N1 stage, using class-weighted loss functions and data augmentation techniques. 3. Introduces a sub-epoch chunking and probability averaging strategy to enhance contextual representation and robustness in predictions.",
      "summary": "This paper proposes a novel deep learning framework for automatic sleep staging using single-channel EEG. The method integrates multi-scale feature extraction with hierarchical sequence learning (BiLSTM) and employs strategies like chunk-based probability averaging to handle class imbalance and improve context modeling. The approach achieves state-of-the-art performance on the SleepEDF datasets, with a significant improvement in detecting the challenging N1 sleep stage, while maintaining model interpretability.",
      "mindmap": "graph TB\n        A[论文标题: A Context-Aware Temporal Modeling for EEG Sleep Staging] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[睡眠分期/Sleep Staging]\n        B --> B2[类别不平衡/Class Imbalance]\n        B --> B3[模型可解释性/Interpretability]\n        C --> C1[多尺度特征提取/Multi-scale Feature Extraction]\n        C --> C2[分层时序建模/Hierarchical Temporal Modeling (BiLSTM)]\n        C --> C3[数据增强与加权损失/Data Augmentation & Weighted Loss]\n        D --> D1[总体准确率 89.72%/Overall Accuracy 89.72%]\n        D --> D2[宏平均F1分数 85.46%/Macro F1-score 85.46%]\n        D --> D3[N1阶段F1分数 61.7%/N1 Stage F1-score 61.7%]"
    },
    {
      "title": "Fusion or Confusion? Multimodal Complexity Is Not All You Need",
      "authors": "Tillmann Rheude, Roland Eils, Benjamin Wild",
      "institution": "Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Intelligent Medicine Institute at Fudan University, Freie Universität Berlin",
      "link": "https://arxiv.org/pdf/2512.22991",
      "code": null,
      "tags": [
        "multi-modal training",
        "multimodal learning",
        "late-fusion",
        "hyperparameter tuning",
        "empirical study",
        "reliability checklist"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp",
      "contributions": "1. A large-scale benchmark of 19 multimodal architectures under a unified experimental protocol across nine diverse datasets. 2. The proposal of SimBaMM, a simple late-fusion Transformer baseline, which performs comparably to more complex methods under standardized conditions. 3. The provision of a pragmatic reliability checklist to promote robust and trustworthy future evaluations in multimodal learning.",
      "summary": "This paper challenges the assumption that architectural complexity is necessary for performance in multimodal learning. Through a large-scale empirical study, it shows that a simple late-fusion Transformer baseline (SimBaMM) performs comparably to 19 more complex methods when all are rigorously tuned and evaluated under standardized conditions. The authors argue for a shift in research focus from architectural novelty to methodological rigor.",
      "mindmap": "graph TB\n        A[Fusion or Confusion? Multimodal Complexity Is Not All You Need] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 多模态学习中的架构复杂性是否必要？<br>Is architectural complexity necessary in multimodal learning?]\n        C[主要方法/Method: 大规模实证研究，提出简单基线SimBaMM<br>Large-scale empirical study, propose simple baseline SimBaMM]\n        D[关键结果/Results: 复杂方法并不稳定优于简单基线<br>Complex methods do not reliably outperform simple baseline]"
    },
    {
      "title": "Merge before Forget: A Single LoRA Continual Learning via Continual Merging",
      "authors": "Fuli Qiao, Mehrdad Mahdavi",
      "institution": "The Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.23017",
      "code": null,
      "tags": [
        "llm training",
        "continual learning",
        "LoRA",
        "catastrophic forgetting",
        "orthogonal basis",
        "parameter-efficient"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp",
      "contributions": "1. Proposes a novel continual learning method that sequentially merges LoRA updates into a single unified LoRA, maintaining constant memory complexity with respect to the number of tasks. 2. Introduces orthogonal basis extraction from previous LoRA to initialize new task learning, minimizing task interference. 3. Employs a time-aware scaling mechanism to balance new and old knowledge during merging, improving performance over asymmetric LoRA merging.",
      "summary": "This paper addresses the issues of memory growth and task interference in LoRA-based continual learning for LLMs. It proposes a method that orthogonally initializes and sequentially merges LoRAs into a single LoRA, using a time-aware scaling mechanism. The approach demonstrates effectiveness and efficiency in mitigating catastrophic forgetting while maintaining constant memory usage.",
      "mindmap": "graph TB\n        Root[”Merge before Forget: A Single LoRA Continual Learning via Continual Merging”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Catastrophic forgetting, memory growth, task interference in LoRA continual learning”] --> P1[”参数增长/Parameter growth”]\n        Problem --> P2[”任务干扰/Task interference”]\n        Method[”主要方法/Method<br>Orthogonal initialization & sequential merging of LoRAs”] --> M1[”正交初始化/Orthogonal basis initialization”]\n        Method --> M2[”持续合并/Continual merging”]\n        Method --> M3[”时间感知缩放/Time-aware scaling”]\n        Results[”关键结果/Results<br>Constant memory, minimized interference, improved performance”] --> R1[”恒定内存/Constant memory complexity”]\n        Results --> R2[”最小化干扰/Minimized task interference”]\n        Results --> R3[”性能提升/Improved performance”]"
    },
    {
      "title": "Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization",
      "authors": "Kerem Zaman, Shashank Srivastava",
      "institution": "UNC Chapel Hill",
      "link": "https://arxiv.org/pdf/2512.23032",
      "code": null,
      "tags": [
        "interpretability",
        "chain-of-thought",
        "faithfulness",
        "causal mediation analysis",
        "biasing features",
        "explainability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp",
      "contributions": "1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.",
      "summary": "This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.",
      "mindmap": "graph TB\n        A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Biasing Features 指标将不完整性误判为不忠实性<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]\n        C --> C1[提出 faithful@k 指标并增加推理令牌预算<br/>Propose faithful@k metric & increase inference token budget]\n        C --> C2[使用因果中介分析<br/>Use Causal Mediation Analysis]\n        D --> D1[许多被标记为不忠实的 CoT 被其他指标判定为忠实<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]\n        D --> D2[更大的令牌预算显著提高提示词显化率<br/>Larger token budgets greatly increase hint verbalization]\n        D --> D3[未显化的提示词仍可通过 CoT 因果中介预测<br/>Non-verbalized hints can causally mediate predictions through CoT]"
    },
    {
      "title": "Mechanistic Analysis of Circuit Preservation in Federated Learning",
      "authors": "Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail",
      "institution": "Lahore University of Management Sciences",
      "link": "https://arxiv.org/pdf/2512.23043",
      "code": "https://github.com/ha405/FedMI",
      "tags": [
        "federated learning",
        "Mechanistic Interpretability",
        "circuit collapse",
        "weight sparsity",
        "Intersection-over-Union",
        "Non-IID data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp",
      "contributions": "1. Proposes a novel mechanistic interpretability (MI) framework to analyze the internal failure mode of FedAvg under Non-IID data, introducing the concept of \"circuit collapse\". 2. Demonstrates the use of inherently interpretable, weight-sparse neural networks to identify and track functional circuits across clients and communication rounds in FL. 3. Provides the first mechanistic evidence, quantified via Intersection-over-Union (IoU), that Non-IID data causes structural circuit divergence and degradation, reframing statistical drift as a failure of mechanistic preservation.",
      "summary": "This paper investigates why Federated Learning (FedAvg) performance degrades under Non-IID data by applying Mechanistic Interpretability. The method uses weight-sparse networks to identify and track functional \"circuits\" across clients, measuring their preservation with IoU. The main conclusion is that Non-IID data causes \"circuit collapse\" due to conflicting updates, providing a mechanistic explanation for the accuracy drop.",
      "mindmap": "graph TB\n        A[Mechanistic Analysis of Circuit Preservation in Federated Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: FL在Non-IID数据下性能下降的机制原因/Mechanistic cause of FL performance drop under Non-IID data]\n        C[主要方法/Method: 使用可解释的稀疏网络识别和追踪电路/Using interpretable sparse networks to identify & track circuits]\n        D[关键结果/Results: Non-IID数据导致电路崩溃，首次提供机制性证据/Non-IID data causes circuit collapse, first mechanistic evidence]"
    },
    {
      "title": "PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations",
      "authors": "Min Zhu, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer, Lu Lu",
      "institution": "Yale University, Johns Hopkins University, University of Notre Dame, University of California Los Angeles",
      "link": "https://arxiv.org/pdf/2512.23056",
      "code": null,
      "tags": [
        "scientific machine learning",
        "physics-informed neural networks",
        "multimodal foundation model",
        "partial differential equations",
        "multi-operator learning",
        "zero-shot fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp",
      "contributions": "1. Proposes a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing PDEs during both pretraining and adaptation, moving beyond purely data-driven approaches. 2. Introduces a method that takes symbolic PDE representations as input and automatically assembles PDE residual losses via vectorized derivative computation, enabling unified physics-informed training across diverse equation families. 3. Demonstrates effective zero-shot physics-informed fine-tuning to unseen PDE families, achieving low error using only PDE residuals and initial/boundary conditions without labeled solution data.",
      "summary": "The paper proposes PI-MFM, a physics-informed multimodal foundation model framework for solving partial differential equations. It integrates governing equations directly into the training and adaptation process, enabling data-efficient and transferable learning of PDE solution operators. The method outperforms data-driven models, especially with sparse data, and shows strong zero-shot adaptation capabilities to new PDE families.",
      "mindmap": "graph TB\n        A[PI-MFM: Physics-informed multimodal foundation model for solving PDEs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有多算子学习方法数据依赖性强且忽略物理规律/Existing multi-operator learning is data-hungry and neglects physics]\n        C --> C1[提出物理信息多模态基础模型框架/Propose physics-informed multimodal foundation model (PI-MFM) framework]\n        C --> C2[输入符号PDE表示，自动组装残差损失/Input symbolic PDEs, auto-assemble residual losses]\n        C --> C3[在预训练和适配中直接强制执行控制方程/Directly enforce governing equations in pretraining & adaptation]\n        D --> D1[在13个PDE基准上优于纯数据驱动模型/Outperforms data-driven models on 13 PDE benchmarks]\n        D --> D2[在稀疏数据、噪声下更鲁棒/More robust with sparse data & noise]\n        D --> D3[实现零样本物理信息微调至未见PDE族/Achieves zero-shot physics-informed fine-tuning to unseen PDE families]"
    },
    {
      "title": "The Reward Model Selection Crisis in Personalized Alignment",
      "authors": "Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales",
      "institution": "University of Edinburgh, Agency for Science, Technology and Research (A*STAR)",
      "link": "https://arxiv.org/pdf/2512.23067",
      "code": null,
      "tags": [
        "alignment & personalization",
        "reward-guided decoding",
        "policy accuracy",
        "Pref-LaMP benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp",
      "contributions": "1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.",
      "summary": "This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.",
      "mindmap": "graph TB\n        A[The Reward Model Selection Crisis in Personalized Alignment] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Standard RM accuracy fails to predict deployment performance for personalized alignment]\n        C[主要方法/Method<br>Introduce policy accuracy metric and Pref-LaMP benchmark for direct evaluation]\n        D[关键结果/Results<br>Weak correlation between RM & policy accuracy; ICL outperforms reward-guided decoding]"
    },
    {
      "title": "Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution",
      "authors": "Shuhuan Wang, Yuzhen Xie, Jiayi Li, Yinliang Diao",
      "institution": "South China Agricultural University",
      "link": "https://arxiv.org/pdf/2512.23068",
      "code": null,
      "tags": [
        "memory & caching",
        "Selective State Space Models",
        "analytical differentiation",
        "memory complexity",
        "Tiled Operator-Space Evolution",
        "Phase Gradient Flow"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp",
      "contributions": "1. Introduces Phase Gradient Flow (PGF), a framework for computing exact analytical derivatives for SSMs without materializing the intermediate computational graph. 2. Proposes Tiled Operator-Space Evolution (TOSE) to reframe SSM dynamics, achieving O(1) memory complexity relative to sequence length. 3. Demonstrates significant practical improvements, including a 94% reduction in peak VRAM and a 23x throughput increase, enabling chromosome-scale sensitivity analysis on a single GPU.",
      "summary": "The paper addresses the O(L) memory bottleneck in gradient-based sensitivity analysis for Selective State Space Models (SSMs). It proposes Phase Gradient Flow (PGF), which uses Tiled Operator-Space Evolution (TOSE) to compute exact analytical derivatives with O(1) memory complexity. This enables the handling of extreme-length sequences (e.g., 128,000 steps) on consumer hardware, significantly reducing memory usage and increasing throughput compared to standard backpropagation.",
      "mindmap": "graph TB\n    A[Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[SSM梯度分析存在O(L)内存墙/SSM Gradient Analysis has O(L) Memory Wall]\n    C --> C1[相位梯度流 (PGF) 框架/Phase Gradient Flow (PGF) Framework]\n    C --> C2[平铺算子空间演化 (TOSE)/Tiled Operator-Space Evolution (TOSE)]\n    D --> D1[O(1) 内存复杂度/O(1) Memory Complexity]\n    D --> D2[94% VRAM减少, 23x 吞吐提升/94% VRAM Reduction, 23x Throughput Gain]\n    D --> D3[支持超长序列 (128k步) 分析/Enables Extreme-Length (128k-step) Analysis]"
    },
    {
      "title": "Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models",
      "authors": "Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu",
      "institution": "Northeastern University",
      "link": "https://arxiv.org/pdf/2512.23073",
      "code": "https://github.com/Ming-K9/MFT-VLM",
      "tags": [
        "vision-language models",
        "Mask Fine-Tuning (MFT)",
        "Parameter Efficient Fine-Tuning (PEFT)",
        "Low-Rank Adaptation (LoRA)",
        "structural reparameterization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp",
      "contributions": "1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model's existing knowledge, not just from updating weights.",
      "summary": "This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.",
      "mindmap": "graph TB\n        Root[”Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models<br>重新思考微调：解锁视觉语言模型的隐藏能力”]\n        Root --> Problem[”Problem: Traditional fine-tuning overlooks underutilized structures in pre-trained VLMs.<br>核心问题：传统微调忽略了预训练VLM中未充分利用的结构。”]\n        Root --> Method[”Method: Apply Mask Fine-Tuning (MFT) to VLMs for structural reparameterization.<br>主要方法：将掩码微调（MFT）应用于VLM进行结构重参数化。”]\n        Root --> Results[”Results: MFT surpasses LoRA and full fine-tuning without altering the backbone.<br>关键结果：MFT超越了LoRA和全参数微调，且不改变主干网络。”]"
    },
    {
      "title": "FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment",
      "authors": "Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun",
      "institution": "The affiliations are not explicitly listed in the provided content. Based on the author names and common patterns, it is likely from a Chinese university or research institute (e.g., Tsinghua University, Peking University, Chinese Academy of Sciences). A specific institution cannot be reliably inferred.",
      "link": "https://arxiv.org/pdf/2512.23070",
      "code": null,
      "tags": [
        "federated learning",
        "Mixture-of-Experts",
        "Federated Learning",
        "Load Balancing",
        "Expert Assignment",
        "Non-IID Data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp",
      "contributions": "1. Proposes FLEX-MoE, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. 2. Introduces client-expert fitness scores to quantify expert suitability for local datasets using training feedback. 3. Employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide.",
      "summary": "The paper addresses the challenges of deploying Mixture-of-Experts models in Federated Learning, specifically resource constraints on edge devices and expert load imbalance caused by non-IID data. It proposes FLEX-MoE, a framework that uses client-expert fitness scores and an optimization algorithm to assign experts to clients for specialization while balancing system-wide expert utilization. Experiments on three datasets show that FLEX-MoE achieves superior performance and maintains balanced expert utilization in resource-constrained scenarios.",
      "mindmap": "graph TB\n        Root[FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[资源受限设备/Resource-constrained Edge Devices]\n        Problem --> P2[专家负载不均衡/Expert Load Imbalance]\n        Method[主要方法/Method] --> M1[客户端-专家适应度分数/Client-Expert Fitness Scores]\n        Method --> M2[基于优化的专家分配/Optimization-based Expert Assignment]\n        Results[关键结果/Results] --> R1[性能优越/Superior Performance]\n        Results --> R2[保持专家负载均衡/Maintains Balanced Expert Utilization]"
    },
    {
      "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning",
      "authors": "Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang",
      "institution": "(Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)",
      "link": "https://arxiv.org/pdf/2512.23075",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "trust region",
        "policy gradient",
        "off-policy mismatch",
        "KL divergence",
        "sequence-level masking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp",
      "contributions": "1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.",
      "summary": "The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.",
      "mindmap": "graph TB\n        A[Trust Region Masking for Long-Horizon LLM RL] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Off-policy mismatch in LLM-RL<br/>导致经典信任域边界失效]\n        C --> C1[提出信任域掩码(TRM)<br/>Propose Trust Region Masking (TRM)]\n        C1 --> C2[序列级掩码<br/>Sequence-level Masking]\n        D --> D1[推导更紧的理论边界<br/>Derive Tighter Bounds (O(T), O(T^{3/2}))]\n        D --> D2[提供非平凡的单调改进保证<br/>Provide Non-vacuous Guarantees]"
    },
    {
      "title": "Multimodal Functional Maximum Correlation for Emotion Recognition",
      "authors": "Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu",
      "institution": "Southeast University, Westlake University, Vrije Universiteit Amsterdam",
      "link": "https://arxiv.org/pdf/2512.23076",
      "code": "https://github.com/DY9910/MFMC",
      "tags": [
        "multimodal learning",
        "self-supervised learning",
        "dual total correlation",
        "functional maximum correlation analysis",
        "affective computing",
        "physiological signals"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp",
      "contributions": "1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.",
      "summary": "The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method's effectiveness.",
      "mindmap": "graph TB\n        A[MFMC for Emotion Recognition] --> B[核心问题/Problem: 情感状态表现为跨系统的协调但异质的生理反应，现有自监督方法难以捕捉多模态高阶交互。]\n        A --> C[主要方法/Method: 提出MFMC框架，通过Dual Total Correlation目标和Functional Maximum Correlation Analysis最大化高阶多模态依赖性。]\n        A --> D[关键结果/Results: 在多个基准测试中达到SOTA或竞争性性能，显著提升CEAP-360VR数据集上的准确率，对主体间变异性鲁棒。]"
    },
    {
      "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning",
      "authors": "Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang",
      "institution": "(Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)",
      "link": "https://arxiv.org/pdf/2512.23087",
      "code": null,
      "tags": [
        "llm training",
        "reinforcement learning",
        "training-inference mismatch",
        "vocabulary pruning",
        "gradient estimation",
        "numerical stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp",
      "contributions": "1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability \"tail\" tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.",
      "summary": "The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability \"tail\" tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.",
      "mindmap": "graph TB\n        A[Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练-推理不匹配 / Training-Inference Mismatch]\n        B1 --> B2[尾部token导致梯度不稳定 / Tail tokens destabilize gradient estimation]\n        C --> C1[动态剪枝词汇表 / Dynamic Vocabulary Pruning]\n        C1 --> C2[排除极端尾部token / Exclude extreme tail tokens]\n        D --> D1[实现稳定训练 / Achieves stable training]\n        D --> D2[理论界定优化偏差 / Theoretically bounds optimization bias]"
    },
    {
      "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms",
      "authors": "Yingru Li, Ziniu Li, Jiacai Liu",
      "institution": "Not explicitly stated in provided content.",
      "link": "https://arxiv.org/pdf/2512.23097",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Imitation Learning",
        "Reinforcement Learning",
        "KL divergence",
        "Dense Gradient",
        "Sparse Gradient"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp",
      "contributions": "1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.",
      "summary": "This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.",
      "mindmap": "graph TB\n        A[Hybrid Online RL and IL for LLMs] --> B[核心问题/Problem: Train-inference distribution mismatch in LLM fine-tuning]\n        A --> C[主要方法/Method: Unified framework combining Imitation Learning and Reinforcement Learning]\n        A --> D[关键结果/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]"
    },
    {
      "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
      "authors": "Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa",
      "institution": "Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn",
      "link": "https://arxiv.org/pdf/2512.23090",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reinforcement learning",
        "vision-language model",
        "supervised fine-tuning",
        "generalization paradox",
        "cross-dataset transferability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp",
      "contributions": "1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.",
      "summary": "The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model's ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.",
      "mindmap": "graph TB\n        A[Benchmark Success, Clinical Failure<br>基准成功，临床失败] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[RL优化提升基准性能但损害泛化<br>RL improves benchmarks but harms generalization]\n        C --> C1[使用SFT+GRPO训练ChexReason VLM<br>Train ChexReason VLM with SFT+GRPO]\n        D --> D1[GRPO提升CheXpert性能23%<br>GRPO improves CheXpert by 23%]\n        D --> D2[GRPO导致NIH性能下降19%<br>GRPO degrades NIH by 19%]\n        D --> D3[SFT检查点提升跨数据集泛化<br>SFT checkpoint improves cross-dataset generalization]"
    },
    {
      "title": "Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation",
      "authors": "Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari",
      "institution": "University of Messina, University of Klagenfurt, University of Innsbruck",
      "link": "https://arxiv.org/pdf/2512.23096",
      "code": null,
      "tags": [
        "federated learning",
        "self-supervised learning",
        "representation learning",
        "distributed learning",
        "decentralized clustering",
        "contextual data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp",
      "contributions": "1. Introduces Osmotic Learning (OSM-L), a novel self-supervised paradigm for learning from distributed data without raw data exchange. 2. Proposes an \"osmosis\" process that aligns local representations to converge to a dynamic equilibrium, capturing contextual patterns. 3. Demonstrates that OSM-L functions as a decentralized clustering mechanism, identifying correlated data groups during training.",
      "summary": "This paper proposes Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that extracts higher-level latent knowledge from decentralized data sources without sharing raw data. It achieves this through an iterative \"osmosis\" process that aligns local representations to converge to a contextual equilibrium, also enabling decentralized clustering. Experimental results show OSM-L achieves high accuracy in local information alignment while preserving contextual integrity.",
      "mindmap": "graph TB\n        A[Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation] --> B[核心问题/Problem: Extracting meaningful knowledge from distributed, heterogeneous data without raw data exchange]\n        A --> C[主要方法/Method: Osmotic Learning (OSM-L) - self-supervised paradigm using iterative alignment and ”osmosis” for representation convergence]\n        A --> D[关键结果/Results: Achieves >0.99 alignment accuracy and preserves contextual integrity; enables decentralized clustering]"
    },
    {
      "title": "How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure",
      "authors": "Paul M. Thompson",
      "institution": "Stevens Institute of Neuroimaging and Informatics, University of Southern California",
      "link": "https://arxiv.org/pdf/2512.23109",
      "code": null,
      "tags": [
        "statistical learning theory",
        "uniform convergence",
        "calibration",
        "low-dimensional structure",
        "vision-language models",
        "sample complexity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp",
      "contributions": "1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.",
      "summary": "This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.",
      "mindmap": "graph TB\n        A[How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现代生成和视觉语言模型在科学/医疗决策中需要准确且校准良好的概率预测 / Modern generative & VLMs need accurate, calibrated predictions for scientific/medical decisions]\n        B --> B2[平均性能良好时，罕见情况或特定子群仍可能出现大误差 / Large errors can persist for rare conditions/subgroups despite low average loss]\n        B --> B3[需要何种结构假设才能实现具有实用样本量的均匀泛化？ / What structural assumptions enable uniform generalization with practical sample sizes?]\n        C --> C1[分析由提示或语义嵌入在受限表示空间中诱导出的分类器族 / Analyze induced families of classifiers from varying prompts/embeddings in a restricted space]\n        C --> C2[假设模型输出对低维语义表示平滑依赖 / Assume model outputs depend smoothly on a low-dimensional semantic representation]\n        C --> C3[应用经典均匀收敛工具 / Apply classical uniform convergence tools]\n        D --> D1[在Lipschitz稳定性下，为VLM诱导分类器的准确性和校准功能提供有限样本均匀收敛界 / Provide finite-sample uniform convergence bounds for accuracy & calibration of VLM-induced classifiers under Lipschitz stability]\n        D --> D2[样本复杂度取决于内在/有效维度，而非环境维度 / Sample complexity depends on intrinsic/effective dimension, not ambient dimension]\n        D --> D3[谱相关边界阐明特征值衰减如何控制数据需求 / Spectrum-dependent bounds show how eigenvalue decay governs data requirements]"
    },
    {
      "title": "SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals",
      "authors": "Yankang Li, Changsheng Li",
      "institution": "Nanjing University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23131",
      "code": null,
      "tags": [
        "regression",
        "squeeze and excitation",
        "channel attention",
        "residual connections",
        "multi-layer perceptron",
        "penetration acceleration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp",
      "contributions": "1. Proposed SE-MLP, a novel MLP architecture integrating a channel attention mechanism for feature prediction. 2. Incorporated residual connections into the MLP framework to enhance model stability and performance. 3. Demonstrated the model's superior accuracy, generalization, and engineering applicability for rapidly predicting penetration acceleration features.",
      "summary": "This paper proposes SE-MLP, a multi-layer perceptron model enhanced with squeeze-and-excitation channel attention and residual connections, to rapidly predict prior acceleration features for penetration signals. The model establishes a nonlinear mapping from physical parameters to acceleration features, outperforming baseline models like MLP, XGBoost, and Transformer in accuracy and stability. The results validate its feasibility and provide a practical basis for engineering applications in penetration fuse design.",
      "mindmap": "graph TB\n        A[SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals] --> B[核心问题/Problem: 侵彻加速度先验特征获取耗时耗力/Prior acceleration features are expensive and time-consuming to obtain]\n        A --> C[主要方法/Method: 提出SE-MLP模型，集成通道注意力与残差连接/Proposed SE-MLP integrating channel attention and residual connections]\n        A --> D[关键结果/Results: 预测精度高，泛化性好，工程误差可接受/High prediction accuracy, good generalization, acceptable engineering error]"
    },
    {
      "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization",
      "authors": "Yu Li, Tian Lan, Zhengling Qi",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.23126",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "preference optimization",
        "direct preference optimization",
        "self-reflection",
        "invariance",
        "bradley-terry model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp",
      "contributions": "1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO's superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.",
      "summary": "The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.",
      "mindmap": "graph TB\n        A[InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DPO Limitations<br/>DPO的局限性]\n        B1 --> B2[Lacks Invariance<br/>缺乏不变性]\n        B1 --> B3[Suboptimal Use of Data<br/>数据利用次优]\n        C --> C1[Propose InSPO<br/>提出InSPO]\n        C1 --> C2[Globally Optimal Policy<br/>全局最优策略]\n        C2 --> C3[Conditions on Context & Alternative<br/>基于上下文与备选答案]\n        D --> D1[Theoretical Superiority<br/>理论优越性]\n        D --> D2[Practical Improvement<br/>实际提升]\n        D2 --> D3[Better Win Rates<br/>更高的胜率]\n        D2 --> D4[No Inference Overhead<br/>无推理开销]"
    },
    {
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "authors": "Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh",
      "institution": "Polytechnique Montréal (based on author affiliations and sMIEEE notation)",
      "link": "https://arxiv.org/pdf/2512.23132",
      "code": null,
      "tags": [
        "machine learning security",
        "TTPs",
        "threat graph",
        "multi-agent RAG",
        "model stealing",
        "jailbreaking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp",
      "contributions": "1. Conducted a large-scale empirical analysis of ML security, extracting 93 distinct threats from multiple sources including real-world incidents and code repositories. 2. Developed a multi-agent RAG system to automatically build an ontology-driven threat graph linking TTPs, vulnerabilities, and lifecycle stages from over 300 articles. 3. Identified unreported threats and dominant attack patterns (e.g., commercial LLM API model stealing, preference-guided jailbreaks) and highlighted vulnerability clusters in ML libraries with poor patch propagation.",
      "summary": "This paper characterizes modern security risks in AI systems by analyzing threats from multiple sources and using a multi-agent RAG system to construct a threat graph. The analysis uncovers unreported attack vectors and dominant TTPs, concluding that adaptive, ML-specific security frameworks are urgently needed to mitigate supply-chain and inference-time risks.",
      "mindmap": "graph TB\n    A[Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[传统安全框架缺乏针对ML的威胁建模/Traditional cybersecurity lacks ML-specific threat modeling]\n    C --> C1[多智能体RAG系统分析威胁/Multi-agent RAG system analyzes threats]\n    C1 --> C2[构建本体驱动的威胁图谱/Builds ontology-driven threat graph]\n    D --> D1[识别未报告的威胁/Identifies unreported threats]\n    D --> D2[发现主要的攻击TTPs/Identifies dominant attack TTPs]\n    D --> D3[强调自适应ML安全框架的必要性/Highlights need for adaptive ML security frameworks]"
    },
    {
      "title": "Principled Algorithms for Optimizing Generalized Metrics in Binary Classification",
      "authors": "Anqi Mao, Mehryar Mohri, Yutao Zhong",
      "institution": "Courant Institute of Mathematical Sciences (NYU), Google Research",
      "link": "https://arxiv.org/pdf/2512.23133",
      "code": null,
      "tags": [
        "binary classification",
        "metric optimization",
        "H-consistency",
        "surrogate loss",
        "cost-sensitive learning",
        "METRO"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp",
      "contributions": "1. Reformulated the optimization of generalized classification metrics (e.g., Fβ, Jaccard) as a generalized cost-sensitive learning problem. 2. Designed novel surrogate loss functions with provable H-consistency guarantees for this problem. 3. Developed the METRO algorithm with strong finite-sample generalization bounds, offering a principled alternative to existing threshold-based methods.",
      "summary": "This paper addresses the challenge of directly optimizing non-decomposable binary classification metrics like the Fβ-measure. The authors propose a principled framework that reformulates metric optimization as a cost-sensitive learning problem, leading to new surrogate losses and the METRO algorithm. Experiments show the proposed method is effective and outperforms prior baseline approaches.",
      "mindmap": "graph TB\n        A[Principled Algorithms for Optimizing Generalized Metrics in Binary Classification] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[优化不平衡/代价敏感指标如Fβ, Jaccard/Optimizing imbalanced/cost-sensitive metrics e.g., Fβ, Jaccard]\n        B --> B2[现有基于阈值的方法缺乏理论保证/Existing threshold-based methods lack theoretical guarantees]\n        C --> C1[将指标优化重构为代价敏感学习问题/Reformulate metric optimization as cost-sensitive learning]\n        C --> C2[设计具有H-一致性的替代损失函数/Design surrogate losses with H-consistency]\n        C --> C3[提出METRO算法/Propose METRO algorithm]\n        D --> D1[提供有限样本泛化保证/Provide finite-sample generalization bounds]\n        D --> D2[实验证明优于基线/Experiments show superiority over baselines]"
    },
    {
      "title": "Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use",
      "authors": "Runzhi Zhou, Xi Luo",
      "institution": "The University of Texas Health Science Center at Houston",
      "link": "https://arxiv.org/pdf/2512.23137",
      "code": null,
      "tags": [
        "graph neural networks",
        "graph neural networks",
        "transformer",
        "dynamic functional connectivity",
        "longitudinal fMRI",
        "multimodal fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp",
      "contributions": "1. Proposes a novel time-aware Graph Neural Network model with Transformer Fusion (GNN-TF) for integrating non-Euclidean brain connectivity and Euclidean tabular data. 2. Introduces an end-to-end framework that leverages the temporal order of longitudinal data for forecasting future clinical outcomes. 3. Demonstrates superior predictive performance for forecasting future tobacco use compared to established machine learning and deep learning models on a longitudinal fMRI dataset.",
      "summary": "This paper introduces GNN-TF, a time-aware model that integrates dynamic brain connectivity graphs and tabular data using a transformer for fusion, to forecast future tobacco use. It is evaluated on longitudinal fMRI data from the NCANDA study and is shown to outperform other state-of-the-art methods in predictive accuracy.",
      "mindmap": "graph TB\n        A[Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use] --> B(核心问题/Problem: Integrating non-Euclidean brain imaging and Euclidean tabular data for forecasting future outcomes in longitudinal studies is challenging.)\n        A --> C(主要方法/Method: Proposes a time-aware Graph Neural Network with Transformer Fusion (GNN-TF) to integrate dynamic brain connectivity and tabular data.)\n        A --> D(关键结果/Results: GNN-TF outperforms state-of-the-art models in predicting future tobacco use on the NCANDA dataset.)"
    },
    {
      "title": "A Weak Signal Learning Dataset and Its Baseline Method",
      "authors": "Xianqi Liu, Xiangru Li, Lefeng He, Ziyu Fang",
      "institution": "South China Normal University",
      "link": "https://arxiv.org/pdf/2512.23160",
      "code": null,
      "tags": [
        "weak signal learning",
        "weak signal learning",
        "dual-view representation",
        "class imbalance",
        "low SNR",
        "multi-source complementarity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp",
      "contributions": "1. Constructed the first specialized dataset for weak signal feature learning, featuring low SNR dominance and extreme class imbalance., 2. Proposed a dual-view representation (vector + time-frequency map) and the PDVFN model tailored for low SNR, distribution skew, and dual imbalance., 3. Established a foundational benchmark for future weak signal learning (WSL) research, demonstrating improved accuracy and robustness in challenging scenarios.",
      "summary": "The paper addresses the challenge of weak signal learning (WSL) by constructing the first dedicated dataset with low SNR and class imbalance, and proposes a dual-view PDVFN model to extract complementary features. The method shows higher accuracy and robustness in handling weak signals, noise, and imbalance. This work provides a dataset, baseline model, and foundation for future WSL research.",
      "mindmap": "graph TB\n        A[A Weak Signal Learning Dataset and Its Baseline Method<br>弱信号学习数据集及其基线方法] --> B(Problem: Lack of dedicated datasets for weak signal learning<br>核心问题: 缺乏专门的弱信号学习数据集)\n        A --> C(Method: Propose dual-view representation & PDVFN model<br>主要方法: 提出双视图表示和PDVFN模型)\n        A --> D(Results: Higher accuracy/robustness, provides dataset & baseline<br>关键结果: 更高准确率/鲁棒性，提供数据集和基线)"
    },
    {
      "title": "Diffusion-based Decentralized Federated Multi-Task Representation Learning",
      "authors": "Donghwa Kang, Shana Moothedath",
      "institution": "Iowa State University",
      "link": "https://arxiv.org/pdf/2512.23161",
      "code": null,
      "tags": [
        "federated learning",
        "decentralized learning",
        "multi-task representation learning",
        "projected gradient descent",
        "diffusion-based consensus",
        "sample complexity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp",
      "contributions": "1. Proposes a novel decentralized projected gradient descent-based algorithm for multi-task representation learning using a diffusion-based communication strategy. 2. Provides theoretical guarantees, including a lower bound on sample complexity and an upper bound on iteration complexity for the proposed algorithm. 3. Demonstrates through analysis and simulations that the algorithm is fast, communication-efficient, and outperforms benchmark methods.",
      "summary": "This paper addresses the problem of decentralized multi-task representation learning, where multiple linear regression models share a common low-dimensional representation across a network of nodes. The authors propose a diffusion-based decentralized algorithm using alternating projected gradient descent and minimization to recover the shared feature matrix. Theoretical analysis proves the algorithm's efficiency in terms of sample and iteration complexity, and numerical simulations validate its superior performance compared to benchmarks.",
      "mindmap": "graph TB\n        Root(”Diffusion-based Decentralized Federated Multi-Task Representation Learning”) --> Problem(”核心问题/Problem: Decentralized Multi-Task Representation Learning is underexplored”)\n        Root --> Method(”主要方法/Method: Diffusion-based Decentralized Projected Gradient Descent Algorithm”)\n        Root --> Results(”关键结果/Results: Provable guarantees, Fast and communication-efficient”)"
    },
    {
      "title": "Evaluating Parameter Efficient Methods for RLVR",
      "authors": "Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu",
      "institution": "Zhejiang University, HKUST, WUST, USTC, Brown University, Hong Kong Polytechnic University, INSAIT",
      "link": "https://arxiv.org/pdf/2512.23165",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Parameter-Efficient Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "LoRA",
        "Spectral Collapse",
        "Mathematical Reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp",
      "contributions": "1. Conducted the first comprehensive evaluation of over 12 PEFT methods for RLVR, challenging the default use of standard LoRA. 2. Identified that structural PEFT variants (DoRA, AdaLoRA, MiSS) consistently outperform LoRA in this setting. 3. Discovered and explained the failure of SVD-informed initialization methods (e.g., PiSSA) due to a \"spectral collapse\" phenomenon and misalignment with RL optimization.",
      "summary": "This paper systematically evaluates Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR) on mathematical reasoning tasks. It finds that structural variants like DoRA outperform standard LoRA, while SVD-based methods fail due to spectral collapse, and extreme parameter reduction bottlenecks performance. The work provides a guide for selecting PEFT methods in RLVR.",
      "mindmap": "graph TB\n        A[Evaluating Parameter Efficient Methods for RLVR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[RLVR中最佳PEFT架构未知 / Optimal PEFT architecture for RLVR is unknown]\n        C --> C1[系统评估12+种PEFT方法 / Systematically evaluate 12+ PEFT methods]\n        C --> C2[在数学推理基准上测试 / Test on mathematical reasoning benchmarks]\n        D --> D1[结构变体优于标准LoRA / Structural variants outperform standard LoRA]\n        D --> D2[SVD初始化导致谱崩溃 / SVD initialization causes spectral collapse]\n        D --> D3[极端参数减少损害推理能力 / Extreme parameter reduction harms reasoning]"
    },
    {
      "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search",
      "authors": "Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue",
      "institution": "IBM T.J. Watson Research Center, Vanderbilt University",
      "link": "https://arxiv.org/pdf/2512.23167",
      "code": "https://github.com/IBM/SPIRAL",
      "tags": [
        "agent system",
        "LLM planning",
        "Monte Carlo Tree Search (MCTS)",
        "multi-agent architecture",
        "symbolic reasoning",
        "self-correction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp",
      "contributions": "1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.",
      "summary": "The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.",
      "mindmap": "graph TB\n        A[SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search] --> B[核心问题/Problem: LLMs falter at complex planning, linear reasoning lacks self-correction]\n        A --> C[主要方法/Method: Integrates three LLM agents (Planner, Simulator, Critic) into MCTS loop]\n        A --> D[关键结果/Results: Outperforms SOTA agents, achieves 83.6% accuracy on DailyLifeAPIs, superior token efficiency]"
    },
    {
      "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning",
      "authors": "Yu Jiang, Xindi Tong, Ziyao Liu, Xiaoxi Zhang, Kwok-Yan Lam, Chee Wei Tan",
      "institution": "Nanyang Technological University, Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.23171",
      "code": null,
      "tags": [
        "federated learning",
        "vertical federated learning",
        "machine unlearning",
        "primal-dual optimization",
        "sample unlearning",
        "label unlearning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp",
      "contributions": "1. Proposes FedORA, a primal-dual optimization framework for sample and label unlearning in Vertical Federated Learning (VFL). 2. Introduces a new unlearning loss function that promotes classification uncertainty instead of misclassification. 3. Employs an adaptive step size and an asymmetric batch design to enhance stability and reduce computational costs.",
      "summary": "This paper addresses the challenge of data removal (unlearning) in Vertical Federated Learning (VFL), where different parties hold different features of the same data samples. The authors propose FedORA, a method that formulates unlearning as a constrained optimization problem solved via a primal-dual algorithm. Experiments show FedORA achieves unlearning effectiveness and model utility comparable to retraining from scratch, but with lower computational and communication costs.",
      "mindmap": "graph TB\n        Root[”Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning”]\n        Root --> Problem[”核心问题/Problem<br>Unlearning in VFL is challenging due to distributed features and cross-party coordination.”]\n        Root --> Method[”主要方法/Method<br>Propose FedORA: a primal-dual optimization framework with a new uncertainty-promoting loss.”]\n        Root --> Results[”关键结果/Results<br>Achieves effective unlearning & utility preservation with lower overhead vs. retraining.”]"
    },
    {
      "title": "HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction",
      "authors": "Seungeon Lee, Takuto Koyama, Itsuki Maeda, Shigeyuki Matsumoto, Yasushi Okuno",
      "institution": "Kyoto University",
      "link": "https://arxiv.org/pdf/2512.23175",
      "code": null,
      "tags": [
        "molecular language modeling",
        "HELM notation",
        "DeBERTa",
        "cyclic peptide",
        "membrane permeability",
        "peptide-protein interaction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp",
      "contributions": "1. Proposes HELM-BERT, the first encoder-based peptide language model trained on HELM notation, designed to capture hierarchical dependencies. 2. Pre-trains the model on a curated corpus of 39,079 chemically diverse linear and cyclic peptides. 3. Demonstrates superior performance over SMILES-based models in downstream tasks like cyclic peptide membrane permeability and peptide-protein interaction prediction.",
      "summary": "This paper introduces HELM-BERT, a transformer model based on DeBERTa and trained on HELM notation to better represent therapeutic peptides. It shows that this approach significantly outperforms existing SMILES-based models in predicting key peptide properties, demonstrating the data-efficiency advantages of topology-aware representations.",
      "mindmap": "graph TB\n        A[HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有分子表示(如SMILES)无法有效捕捉肽的化学与拓扑复杂性/Existing molecular representations fail to capture peptide complexity]\n        C --> C1[基于DeBERTa, 使用HELM符号训练首个编码器肽语言模型/Based on DeBERTa, first encoder peptide LM trained on HELM notation]\n        C --> C2[在39,079个多样化肽的语料库上进行预训练/Pre-trained on a corpus of 39,079 diverse peptides]\n        D --> D1[在膜渗透性和肽-蛋白相互作用预测上显著优于SMILES模型/Significantly outperforms SMILES models on permeability & interaction prediction]\n        D --> D2[HELM表示提供数据效率优势/HELM representations offer data-efficiency advantages]"
    },
    {
      "title": "Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR",
      "authors": "Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry",
      "institution": "Milwaukee School of Engineering, Emory University",
      "link": "https://arxiv.org/pdf/2512.23177",
      "code": null,
      "tags": [
        "medical image classification",
        "vocal cord ultrasound",
        "image segmentation",
        "VIPRnet",
        "vocal cord paralysis",
        "classification model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp",
      "contributions": "1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.",
      "summary": "This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.",
      "mindmap": "graph TB\n    A[Project VIPR: Machine Learning-Assisted Vocal Cord Ultrasound Examination] --> B[核心问题/Problem: VCUS accuracy is operator-dependent]\n    A --> C[主要方法/Method: Use ML models for vocal cord segmentation and VCP classification]\n    A --> D[关键结果/Results: Segmentation accuracy 96%, VIPRnet classification accuracy 99%]"
    },
    {
      "title": "PGOT: A Physics-Geometry Operator Transformer for Complex PDEs",
      "authors": "Zhuo Zhang, Xi Yang, Yuan Zhao, Canqun Yang",
      "institution": "National University of Defense Technology, National SuperComputer Center in Tianjin",
      "link": "https://arxiv.org/pdf/2512.23192",
      "code": null,
      "tags": [
        "neural operator learning",
        "Physics-Geometry Operator Transformer",
        "Spectrum-Preserving Geometric Attention",
        "geometric aliasing",
        "linear complexity",
        "spatially adaptive routing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp",
      "contributions": "1. Proposes PGOT, a novel transformer architecture designed to reconstruct physical feature learning through explicit geometry awareness for solving PDEs on complex geometries. 2. Introduces Spectrum-Preserving Geometric Attention (SpecGeo-Attention), which uses a physics slicing-geometry injection mechanism to incorporate multi-scale geometric encodings, preserving critical boundary information while maintaining linear computational complexity. 3. Implements a dynamic routing mechanism that adaptively selects low-order linear paths for smooth regions and high-order non-linear paths for discontinuities, enabling high-precision, spatially adaptive modeling.",
      "summary": "The paper addresses the challenge of modeling PDEs on large-scale unstructured meshes with complex geometries using transformers, where efficient architectures often lose critical boundary information due to geometric aliasing. It proposes the Physics-Geometry Operator Transformer (PGOT), which introduces a geometry-aware attention mechanism and adaptive computational routing to preserve multi-scale features and model shocks precisely. PGOT achieves state-of-the-art performance on standard benchmarks and excels in large-scale industrial design tasks.",
      "mindmap": "graph TB\n        A[PGOT: A Physics-Geometry Operator Transformer for Complex PDEs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Transformers建模复杂几何PDEs的挑战/Challenge: Modeling PDEs on complex geometries]\n        B --> B2[几何混叠导致边界信息丢失/Geometric Aliasing loses boundary info]\n        C --> C1[提出PGOT框架/Propose PGOT framework]\n        C --> C2[谱保持几何注意力/SpecGeo-Attention]\n        C --> C3[动态路径路由/Dynamic path routing]\n        D --> D1[四个基准测试SOTA/SOTA on four benchmarks]\n        D --> D2[工业设计任务优异/Excels in industrial design tasks]"
    },
    {
      "title": "A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization",
      "authors": "Yi-Han Wang, Peng Zhao, Zhi-Hua Zhou",
      "institution": "National Key Laboratory for Novel Software Technology, Nanjing University; School of Artificial Intelligence, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.23190",
      "code": null,
      "tags": [
        "online learning",
        "Online Newton Step",
        "Mahalanobis projection",
        "regret minimization",
        "exp-concave optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp",
      "contributions": "1. Proposes LightONS, a simple variant of ONS that reduces computational cost by delaying expensive Mahalanobis projections via a hysteresis mechanism. 2. Achieves optimal O(d log T) regret with a total runtime of O(d²T + d^ω √(T log T)), improving over ONS's O(d^ω T) runtime. 3. Provides an SXO algorithm with runtime ~O(d³/ε), solving the COLT'13 open problem posed by Koren.",
      "summary": "This paper addresses the computational bottleneck of the Online Newton Step (ONS) algorithm for online exp-concave optimization, where the Mahalanobis projection step is costly. The authors propose LightONS, a simple variant that introduces a hysteresis mechanism to delay expensive projections, preserving optimal regret while significantly reducing runtime. This leads to an efficient stochastic optimization method that resolves a long-standing open problem.",
      "mindmap": "graph TB\n        A[”A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization<br/>在线指数凹优化的简单、最优且高效算法”] --> B[”核心问题/Problem”]\n        A --> C[”主要方法/Method”]\n        A --> D[”关键结果/Results”]\n        B --> B1[”在线指数凹优化(OXO)中，ONS算法的Mahalanobis投影计算成本高<br/>High computational cost of Mahalanobis projection in ONS for OXO”]\n        B --> B2[”随机指数凹优化(SXO)存在COLT'13开放问题<br/>COLT'13 open problem for SXO”]\n        C --> C1[”提出LightONS算法<br/>Propose LightONS algorithm”]\n        C --> C2[”利用迟滞机制延迟昂贵投影<br/>Delay expensive projections via hysteresis mechanism”]\n        D --> D1[”保持最优O(d log T)遗憾<br/>Preserves optimal O(d log T) regret”]\n        D --> D2[”总运行时间降至O(d²T + d^ω √(T log T))<br/>Total runtime reduced to O(d²T + d^ω √(T log T))”]\n        D --> D3[”解决SXO开放问题，运行时间~O(d³/ε)<br/>Solves SXO open problem with runtime ~O(d³/ε)”]"
    },
    {
      "title": "Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing",
      "authors": "Ziru Niu, Hai Dong, A.K. Qin, Tao Gu, Pengcheng Zhang",
      "institution": "RMIT University, Swinburne University of Technology, Macquarie University, Hohai University",
      "link": "https://arxiv.org/pdf/2512.23200",
      "code": null,
      "tags": [
        "federated learning",
        "Ordered Layer Freezing",
        "Tensor Operation Approximation",
        "Non-IID Data",
        "Edge Computing",
        "Model Compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp",
      "contributions": "1. Proposed Federated Learning with Ordered Layer Freezing (FedOLF), a method that freezes model layers in a predefined order before training to reduce computation and memory requirements. 2. Introduced Tensor Operation Approximation (TOA) as a lightweight alternative to conventional quantization to further reduce communication and energy costs while preserving model accuracy. 3. Demonstrated superior performance of FedOLF over non-IID data across multiple datasets and model architectures, achieving higher accuracy, energy efficiency, and lower memory footprint compared to existing works.",
      "summary": "This paper introduces FedOLF, a federated learning method that freezes model layers in a predefined order to reduce resource demands on edge devices, combined with a Tensor Operation Approximation technique for efficient communication. The proposed approach is shown to achieve higher accuracy and better energy/memory efficiency than existing methods when training on non-IID data across several benchmark datasets.",
      "mindmap": "graph TB\n        A[Energy and Memory-Efficient Federated Learning with Ordered Layer Freezing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[FL在资源受限的IoT设备上效率低/FL inefficiency on resource-constrained IoT devices]\n        C --> C1[有序层冻结/FedOLF: Ordered Layer Freezing]\n        C --> C2[张量操作近似/TOA: Tensor Operation Approximation]\n        D --> D1[更高准确率/Higher accuracy on non-IID data]\n        D --> D2[更高能效与更低内存占用/Higher energy efficiency & lower memory footprint]"
    },
    {
      "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation",
      "authors": "Saif Khalfan Saif Al Mazrouei",
      "institution": "University of Wisconsin-Madison",
      "link": "https://arxiv.org/pdf/2512.23214",
      "code": null,
      "tags": [
        "llm inference",
        "Domain-Specific Language",
        "Constrained Syntax",
        "Code Generation",
        "Data Transformation Pipeline",
        "In-Context Learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp",
      "contributions": "1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management.",
      "summary": "This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation.",
      "mindmap": "graph TB\n        A[Anka: A Domain-Specific Language for Reliable LLM Code Generation] --> B[核心问题/Problem: LLMs make systematic errors in complex multi-step code generation]\n        A --> C[主要方法/Method: Design Anka, a constrained DSL for data transformation pipelines]\n        A --> D[关键结果/Results: High parse success & task accuracy; Anka outperforms Python on multi-step tasks]"
    },
    {
      "title": "FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs",
      "authors": "Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren",
      "institution": "Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.23235",
      "code": null,
      "tags": [
        "federated learning",
        "graph federated learning",
        "fairness",
        "overlapping subgraphs",
        "privacy-preserving",
        "weighted aggregation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp",
      "contributions": "1. Uncover and theoretically analyze the unfairness issue in graph federated learning caused by imbalanced overlapping subgraphs across clients. 2. Propose FairGFL, a novel algorithm that uses a privacy-preserving estimation of overlapping ratios and an interpretable weighted aggregation approach to enhance cross-client fairness. 3. Improve the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function.",
      "summary": "The paper identifies a fairness problem in graph federated learning when client subgraphs overlap in an imbalanced way. To solve this, it proposes FairGFL, a method that uses privacy-preserving overlap estimation and a fairness-aware regularizer to balance utility and fairness. Experiments show FairGFL outperforms baselines in both utility and fairness on benchmark datasets.",
      "mindmap": "graph TB\n    A(FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs) --> B(核心问题/Problem: Imbalanced overlapping subgraphs cause unfairness in GFL)\n    A --> C(主要方法/Method: FairGFL with privacy-preserving overlap estimation, weighted aggregation, and fairness regularizer)\n    A --> D(关键结果/Results: Outperforms baselines in model utility and fairness)"
    },
    {
      "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
      "authors": "Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu",
      "institution": "Meta Platforms",
      "link": "https://arxiv.org/pdf/2512.23236",
      "code": null,
      "tags": [
        "gpu kernels",
        "agentic kernel coding",
        "heterogeneous accelerators",
        "retrieval-augmented prompt synthesis",
        "graph-based search",
        "Triton/CuTe DSL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp",
      "contributions": "1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.",
      "summary": "This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.",
      "mindmap": "graph TB\n        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]\n        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]\n        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]\n        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]\n        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]\n        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]\n        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]\n        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]"
    },
    {
      "title": "On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases",
      "authors": "Alexander Korotin, Gudmund Pammer",
      "institution": "Applied AI Institute, Graz University of Technology",
      "link": "https://arxiv.org/pdf/2512.23265",
      "code": null,
      "tags": [
        "generative models",
        "flow matching",
        "inverse problem",
        "uniqueness",
        "generative AI",
        "continuity equation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp",
      "contributions": "1. Formally defines the inverse problem of flow matching (FM) for distributions with finite exponential moment, 2. Establishes the uniqueness of the solution to the inverse FM problem in the one-dimensional (D=1) setting, 3. Establishes the uniqueness of the solution to the inverse FM problem in the Gaussian case.",
      "summary": "This paper studies the inverse problem of flow matching, aiming to recover the original transport plan given the initial distribution and the learned velocity field. It proves that the solution to this inverse problem is unique in two specific cases: one-dimensional distributions and Gaussian distributions. The general multidimensional case remains an open problem for future research.",
      "mindmap": "graph TB\n        Root[”On the Inverse Flow Matching Problem<br>逆流匹配问题”] --> Problem[”研究逆流匹配问题<br>Inverse Flow Matching Problem”]\n        Root --> Method[”分析一维与高斯情形<br>Analyze 1D & Gaussian Cases”]\n        Root --> Results[”证明解的唯一性<br>Prove Solution Uniqueness”]"
    },
    {
      "title": "PFed-Signal: An ADR Prediction Model based on Federated Learning",
      "authors": "Tao Li, Peilin Li, Kui Lu, Yilei Wang, Junliang Shang, Guangshun Li, Huiyu Zhou",
      "institution": "Qufu Normal University, University of Leicester",
      "link": "https://arxiv.org/pdf/2512.23262",
      "code": null,
      "tags": [
        "federated learning",
        "federated learning",
        "transformer",
        "euclidean distance",
        "adverse drug reaction",
        "data cleaning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp",
      "contributions": "1. Proposed PFed-Split, a method to split the original FAERS dataset based on Adverse Drug Reactions (ADRs). 2. Introduced a federated learning-based biased data identification method that uses Euclidean distance to filter out noisy records and generate a clean dataset. 3. Developed an ADR prediction model based on the Transformer architecture, trained on the cleaned dataset, which achieves superior performance in accuracy and signal detection metrics (ROR, PRR) compared to traditional statistical methods.",
      "summary": "This paper proposes PFed-Signal, a federated learning-based model for predicting Adverse Drug Reactions (ADRs). The method first cleans biased data from the FAERS database using Euclidean distance within a federated framework and then trains a Transformer model on the cleaned data for prediction. The results show that this approach outperforms traditional statistical methods in key metrics like accuracy, F1 score, and AUC.",
      "mindmap": "graph TB\n        A[PFed-Signal: An ADR Prediction Model based on Federated Learning] --> B(核心问题/Problem: Biased data in FAERS leads to inaccurate ADR prediction)\n        A --> C(主要方法/Method: Use federated learning & Euclidean distance to clean data, then train Transformer model)\n        A --> D(关键结果/Results: Higher accuracy, F1, recall, AUC than baselines; improved ROR/PRR)"
    },
    {
      "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
      "authors": "Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.23260",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Sparse Autoencoders (SAEs)",
        "Low-Rank Adaptation (LoRA)",
        "Safety Alignment",
        "Interpretability",
        "Parameter-efficient Fine-tuning (PEFT)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp",
      "contributions": "1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.",
      "summary": "This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.",
      "mindmap": "graph TB\n        Root(”Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LoRA缺乏可解释性/LoRA lacks interpretability”)\n        Problem --> P2(”子空间学习是黑盒的/Subspace learning is black-box”)\n        Method --> M1(”利用预训练SAE/Use pre-trained SAEs”)\n        Method --> M2(”构建显式低秩子空间/Construct explicit low-rank subspace”)\n        Results --> R1(”高安全率99.6%/High safety rate 99.6%”)\n        Results --> R2(”参数高效0.19%/Parameter-efficient 0.19%”)\n        Results --> R3(”提供可解释性/Provides interpretability”)"
    },
    {
      "title": "Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning",
      "authors": "Mahdi Kchaou, Francesco Contino, Diederik Coppitters",
      "institution": "Institute of Mechanics, Materials and Civil Engineering (iMMC), Université catholique de Louvain (UCLouvain)",
      "link": "https://arxiv.org/pdf/2512.23284",
      "code": null,
      "tags": [
        "Energy Systems Optimization",
        "Modeling to Generate Alternatives",
        "Interpretable Machine Learning",
        "Decision Trees",
        "Energy System Optimization Model",
        "E-molecules"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp",
      "contributions": "1. Applied Modeling to Generate Alternatives (MGA) to produce a diverse set of near-cost-optimal e-molecule import pathway designs, moving beyond a single optimal solution. 2. Used interpretable machine learning (specifically decision trees) to extract high-level insights and design archetypes from the complex, multi-dimensional solution space generated by MGA. 3. Demonstrated the flexibility of hydrogen import pathways, showing that specific technologies (solar, wind, storage) are not strictly required to stay within a 10% cost margin, and revealed how constraints shift the preferred design archetypes.",
      "summary": "This paper addresses the limitation of single cost-optimal designs for green e-molecule import pathways by using Modeling to Generate Alternatives to create diverse near-optimal solutions and then applying interpretable machine learning to analyze them. The method is applied to hydrogen import pathways considering different carriers. The main finding is a broad near-optimal space with significant flexibility, where specific renewable sources are not strictly necessary, and constraints shift preferences toward different carrier and technology combinations.",
      "mindmap": "graph TB\n        A[揭示电子分子进口路径的设计原型与灵活性<br>Revealing design archetypes and flexibility in e-molecule import pathways] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>单一成本最优解脆弱<br>Single cost-optimal solution is fragile]\n        C[主要方法/Method<br>MGA与可解释机器学习<br>Modeling to Generate Alternatives & Interpretable ML]\n        D[关键结果/Results<br>广阔的近优空间与灵活性<br>Broad near-optimal space with flexibility]"
    },
    {
      "title": "Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions",
      "authors": "Yuchen Xie, Honghang Chi, Haopeng Quan, Yahui Wang, Wei Wang, Yu Ma",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.23295",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Physics-Informed Neural Networks",
        "Neural Tangent Kernel",
        "spectral analysis",
        "hard constraints",
        "boundary functions"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp",
      "contributions": "1. Established a rigorous Neural Tangent Kernel (NTK) framework for Hard-Constraint PINNs (HC-PINNs), deriving the explicit kernel composition law. 2. Revealed that the boundary function acts as a multiplicative spatial modulator and spectral filter, fundamentally altering the learning landscape and potentially causing spectral collapse. 3. Identified the effective rank of the residual kernel as a superior, deterministic predictor of training convergence compared to classical condition numbers.",
      "summary": "This paper investigates the training dynamics of Physics-Informed Neural Networks with hard constraints (HC-PINNs). It establishes an NTK framework to show that the boundary function acts as a spectral filter, and identifies the kernel's effective rank as a key predictor of convergence. The work provides a theoretical foundation for designing boundary functions to avoid optimization stagnation.",
      "mindmap": "graph TB\n        Root[”Spectral Analysis of Hard-Constraint PINNs<br/>硬约束PINN的谱分析”] --> Problem[”核心问题/Problem<br/>HC-PINN训练动态机制不明<br/>HC-PINN training dynamics unexplored”]\n        Root --> Method[”主要方法/Method<br/>建立NTK框架与谱分析<br/>Establish NTK framework & spectral analysis”]\n        Root --> Results[”关键结果/Results<br/>边界函数是谱滤波器<br/>有效秩预测收敛性<br/>Boundary function is spectral filter<br/>Effective rank predicts convergence”]"
    },
    {
      "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control",
      "authors": "Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam",
      "institution": "Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi",
      "link": "https://arxiv.org/pdf/2512.23292",
      "code": null,
      "tags": [
        "reinforcement learning",
        "domain-specific foundation model",
        "agentic physical ai",
        "variance collapse",
        "physics-based validation",
        "policy distillation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp",
      "contributions": "1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.",
      "summary": "The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model's behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.",
      "mindmap": "graph TB\n        Root(”Agentic Physical AI for Nuclear Reactor Control”) --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>General-purpose models fail at physical control<br>通用模型在物理控制中失败”]\n        Method[”主要方法/Method<br>Agentic Physical AI with physics-based validation<br>基于物理验证的智能体物理AI”]\n        Results[”关键结果/Results<br>Variance collapse & emergent policy distillation<br>方差崩溃与策略蒸馏涌现”]\n    \n        Problem --> P1[”Input unfaithfulness / 输入不忠实”]\n        Problem --> P2[”Semantic vs. physical correctness / 语义与物理正确性冲突”]\n    \n        Method --> M1[”Compact LM (360M params) / 紧凑语言模型”]\n        Method --> M2[”Physics-driven optimization / 物理驱动优化”]\n        Method --> M3[”Synthetic data scaling (10^3 to 10^5) / 合成数据缩放”]\n    \n        Results --> R1[”Phase transition & >500x variance collapse / 相变与方差崩溃”]\n        Results --> R2[”Autonomous policy distillation / 自主策略蒸馏”]\n        Results --> R3[”Transferable representations / 可迁移表征”]"
    },
    {
      "title": "Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL",
      "authors": "Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer",
      "institution": "University of Innsbruck, Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.23310",
      "code": null,
      "tags": [
        "llm inference",
        "Lyapunov Optimization",
        "Deep Reinforcement Learning",
        "Edge-Cloud Partitioning",
        "Transformer Decomposition",
        "Queue Stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp",
      "contributions": "1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.",
      "summary": "The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.",
      "mindmap": "graph TB\n        A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --> B[核心问题/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]\n        A --> C[主要方法/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]\n        A --> D[关键结果/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]"
    },
    {
      "title": "Deep learning for pedestrians: backpropagation in Transformers",
      "authors": "Laurent Boué",
      "institution": "Oracle, Microsoft",
      "link": "https://arxiv.org/pdf/2512.23329",
      "code": null,
      "tags": [
        "backpropagation",
        "backpropagation",
        "transformers",
        "gradient derivation",
        "LoRA",
        "PyTorch implementation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp",
      "contributions": "1. Provides a vectorized, index-free derivation of backpropagation for transformer architectures, extending previous work on CNNs. 2. Derives gradient expressions for key transformer components like embedding, multi-headed self-attention, layer normalization, and LoRA layers. 3. Includes a complete PyTorch implementation of a minimal GPT-like network alongside analytical gradient expressions for pedagogical clarity.",
      "summary": "This paper manually derives the backpropagation algorithm for transformer-based next-token-prediction models using a vectorized, index-free methodology. It provides gradient expressions for core layers (embedding, self-attention, layer norm) and LoRA, aiming to build deeper intuition for how operations influence the final output. A complete PyTorch implementation is also provided to illustrate the theoretical derivations.",
      "mindmap": "graph TB\n        A[Deep learning for pedestrians: backpropagation in Transformers] --> B[核心问题/Problem: Understanding the mechanics of backpropagation in transformers]\n        A --> C[主要方法/Method: Apply lightweight, index-free methodology to derive gradients for embedding, self-attention, layer norm, and LoRA]\n        A --> D[关键结果/Results: Provides analytical gradient expressions and a complete PyTorch implementation for a GPT-like network]"
    },
    {
      "title": "Visual Language Hypothesis",
      "authors": "Xiu Li",
      "institution": "Bytedance",
      "link": "https://arxiv.org/pdf/2512.23335",
      "code": null,
      "tags": [
        "representation learning",
        "visual language hypothesis",
        "fiber bundle",
        "semantic quotient",
        "expand-and-snap",
        "topology change"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp",
      "contributions": "1. Proposes the \"Visual Language Hypothesis,\" framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an \"expand-and-snap\" process to achieve the necessary topology change for semantic abstraction.",
      "summary": "This paper proposes the \"Visual Language Hypothesis,\" which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific \"expand-and-snap\" process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.",
      "mindmap": "graph TB\n        A[Visual Language Hypothesis] --> B[核心问题/Problem: What structural properties enable semantic abstraction in vision?]\n        A --> C[主要方法/Method: Propose hypothesis of discrete semantic language, derive geometric (fiber bundle) structure]\n        A --> D[关键结果/Results: Semantic invariance needs discriminative target; Model needs ”expand-and-snap” for topology change]"
    },
    {
      "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models",
      "authors": "Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.23340",
      "code": null,
      "tags": [
        "scaling laws",
        "scaling laws",
        "model ensembling",
        "multi-model collaboration",
        "cross-entropy loss",
        "parameter budget"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp",
      "contributions": "1. Proposes the \"Law of Multi-model Collaboration,\" a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.",
      "summary": "This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the \"Law of Multi-model Collaboration,\" a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.",
      "mindmap": "graph TB\n        Root[”The Law of Multi-Model Collaboration<br>多模型协作定律”] --> Problem[”核心问题/Problem<br>Lack of scaling theory for multi-model collaboration<br>缺乏多模型协作的扩展理论”]\n        Root --> Method[”主要方法/Method<br>Propose Law of Multi-model Collaboration<br>提出多模型协作定律”]\n        Root --> Results[”关键结果/Results<br>Ensembles scale better than single models<br>集成模型比单一模型扩展性更好”]"
    },
    {
      "title": "ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling",
      "authors": "Hai Duong Nguyen, Xuan-The Tran",
      "institution": "HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University",
      "link": "https://arxiv.org/pdf/2512.23347",
      "code": null,
      "tags": [
        "medical signal processing",
        "ECG classification",
        "morphology-rhythm disentanglement",
        "Mamba",
        "zero-shot generalization",
        "Power Mean pooling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp",
      "contributions": "1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.",
      "summary": "The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.",
      "mindmap": "graph TB\n        A[ECG-RAMBA: Zero-Shot ECG Generalization] --> B[核心问题/Problem: Poor cross-dataset generalization in ECG classification]\n        A --> C[主要方法/Method: Morphology-Rhythm Disentanglement & Long-Range Mamba Modeling]\n        A --> D[关键结果/Results: Strong zero-shot AUC on CPSC-2021 & PTB-XL]"
    },
    {
      "title": "ISOPO: Proximal policy gradients without pi-old",
      "authors": "Nilin Abrahamsen",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.23353",
      "code": null,
      "tags": [
        "reinforcement learning",
        "natural policy gradient",
        "proximal policy optimization",
        "reinforcement learning fine-tuning",
        "Fisher metric",
        "neural tangent kernel"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp",
      "contributions": "1. Introduces ISOPO, a method to approximate the natural policy gradient in a single gradient step, contrasting with existing methods like GRPO/PPO that require multiple steps. 2. Proposes a simple form of ISOPO that normalizes log-probability gradients in the Fisher metric before contracting with advantages. 3. Presents a variant that transforms microbatch advantages based on the neural tangent kernel layer-wise, enabling efficient implementation with negligible overhead compared to REINFORCE.",
      "summary": "This paper introduces Isometric Policy Optimization (ISOPO), a new method for approximating the natural policy gradient in reinforcement learning fine-tuning. Unlike existing proximal policy methods like GRPO/PPO which require multiple gradient steps with a reference policy, ISOPO performs the approximation in a single step by normalizing gradients or transforming advantages, achieving this with minimal computational overhead.",
      "mindmap": "graph TB\n        A[ISOPO: Proximal policy gradients without pi-old] --> B[核心问题/Problem: Existing methods (GRPO/PPO) require multiple steps to approximate natural policy gradient]\n        A --> C[主要方法/Method: ISOPO approximates natural gradient in single step via Fisher metric normalization or NTK-based advantage transformation]\n        A --> D[关键结果/Results: Efficient single-step approximation with negligible overhead vs. REINFORCE]"
    },
    {
      "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2",
      "authors": "Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang",
      "institution": "Tianjin University",
      "link": "https://arxiv.org/pdf/2512.23367",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "post-training quantization",
        "W8A8",
        "W4A8",
        "Ascend NPU",
        "Chain-of-Thought (CoT)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp",
      "contributions": "1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.",
      "summary": "This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.",
      "mindmap": "graph TB\n        A[Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CoT推理带来高内存与延迟 / CoT reasoning causes high memory & latency]\n        B --> B2[Ascend NPU部署挑战 / Deployment challenge on Ascend NPU]\n        C --> C1[低比特量化 / Low-bit Quantization]\n        C --> C2[统一推理框架 / Unified Inference Framework]\n        C --> C3[支持W8A8与W4A8 / Supports W8A8 & W4A8]\n        D --> D1[INT8保持>90%精度 / INT8 preserves >90% accuracy]\n        D --> D2[1.5倍预填充加速 / 1.5x prefill speedup]\n        D --> D3[W4A8显著减少内存 / W4A8 greatly reduces memory]"
    },
    {
      "title": "Diffusion priors enhanced velocity model building from time-lag images using a neural operator",
      "authors": "Xiao Ma, Mohammad Hasyim Taufik, Tariq Alkhalifah",
      "institution": "King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.23375",
      "code": null,
      "tags": [
        "diffusion models",
        "neural operator",
        "velocity model building",
        "reverse time migration",
        "diffusion model",
        "automatic differentiation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp",
      "contributions": "1. Proposes a novel framework that combines generative models (diffusion priors) with neural operators for velocity model building. 2. Uses a neural operator as a fast surrogate for the forward modeling and migration process to generate time-lag images from velocity models. 3. Employs the trained neural operator and automatic differentiation to update the migration velocity, enhanced by a generative model as a regularizer to produce high-resolution, cleaner predictions.",
      "summary": "This paper proposes a new deep learning framework for efficient, high-resolution velocity model building in seismic imaging. The method combines a neural operator, which acts as a fast surrogate for seismic modeling and migration, with a diffusion generative model that serves as a prior to regularize the solution. Experiments on synthetic and field data show the approach effectively builds cleaner, higher-resolution velocity models.",
      "mindmap": "graph TB\n        Root(”扩散先验增强的基于神经算子的时滞图像速度建模<br>Diffusion priors enhanced velocity model building from time-lag images using a neural operator”)\n        Root --> Problem(”核心问题/Problem: 传统速度建模方法计算成本高、耗时<br>Conventional VMB is computationally expensive and time-consuming”)\n        Root --> Method(”主要方法/Method: 结合神经算子与生成模型（扩散先验）<br>Combines neural operator with generative model (diffusion prior)”)\n        Root --> Results(”关键结果/Results: 合成与实地数据验证了方法的有效性<br>Synthetic and field data demonstrate effectiveness”)"
    },
    {
      "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
      "authors": "Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee",
      "institution": "Urmia University of Technology",
      "link": "https://arxiv.org/pdf/2512.23380",
      "code": "https://github.com/your-repo/CoLog",
      "tags": [
        "log anomaly detection",
        "collaborative transformers",
        "multi-head impressed attention",
        "modality adaptation layer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp",
      "contributions": "1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.",
      "summary": "The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.",
      "mindmap": "graph TB\n        Root[”A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers”] --> Problem[”核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions”]\n        Root --> Method[”主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer”]\n        Root --> Results[”关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA”]"
    },
    {
      "title": "Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons",
      "authors": "Abd Ullah Khan, Uman Khalid, Muhammad Tanveer, Trung Q. Duong, Hyundong Shin",
      "institution": "Kyung Hee University, University of Management and Technology, Memorial University of Newfoundland, Queen's University Belfast",
      "link": "https://arxiv.org/pdf/2512.23400",
      "code": null,
      "tags": [
        "communication & networking",
        "beyond-diagonal RIS",
        "passive beamforming",
        "hybrid quantum-classical ML",
        "6G networks",
        "reconfigurable intelligent surfaces"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp",
      "contributions": "1. Provides a systematic introduction to Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS), detailing its principles, architecture, advantages, and classification. 2. Presents a case study comparing four beamforming algorithms for BD-RIS, analyzing their performance in terms of sum rate and computation cost. 3. Proposes and analyzes hybrid quantum-classical machine learning models to enhance beam prediction for 6G BD-RIS, validated using the real-world DeepSense 6G dataset.",
      "summary": "This paper introduces Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) as a key technology for 6G networks to overcome high-frequency propagation challenges. It systematically reviews BD-RIS principles, analyzes beamforming algorithms, and explores quantum-enhanced machine learning for beam prediction. The work concludes with insights into the practical implications and future potential of BD-RIS for advanced wireless communication.",
      "mindmap": "graph TB\n        A[Beyond-Diagonal RIS for 6G Networks<br/>超越对角可重构智能表面用于6G网络] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>6G高频段传播损耗与阻塞<br/>6G high-frequency propagation loss & blockage]\n        C[主要方法/Method<br/>系统综述、波束赋形案例研究、量子-经典混合ML<br/>Systematic review, beamforming case study, hybrid quantum-classical ML]\n        D[关键结果/Results<br/>获得BD-RIS实用见解与性能分析<br/>Derived practical insights & performance analysis for BD-RIS]"
    },
    {
      "title": "Task-driven Heterophilic Graph Structure Learning",
      "authors": "Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri",
      "institution": "Indian Institute of Science, University of Rochester",
      "link": "https://arxiv.org/pdf/2512.23406",
      "code": null,
      "tags": [
        "graph neural networks",
        "graph structure learning",
        "heterophilic graphs",
        "spectral filtering",
        "topology inference",
        "graph rewiring"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp",
      "contributions": "1. Proposes FgGSL, an end-to-end framework that jointly learns complementary homophilic and heterophilic graph structures using a learnable masking function and processes them with low- and high-pass graph filter banks. 2. Introduces a label-based structural loss to explicitly promote the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning, and provides theoretical stability bounds for this loss and robustness guarantees for the filters. 3. Demonstrates through experiments on six heterophilic benchmarks that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, validating the benefit of combining frequency information with supervised topology inference.",
      "summary": "This paper addresses the challenge of learning discriminative node representations on heterophilic graphs, where connected nodes often have dissimilar labels. The authors propose FgGSL, a framework that jointly learns homophilic and heterophilic graph structures using spectral filters and a task-driven structural loss. Experiments show FgGSL outperforms existing methods, highlighting the advantage of combining frequency guidance with supervised graph inference.",
      "mindmap": "graph TB\n        A[Task-driven Heterophilic Graph Structure Learning] --> B[核心问题/Problem: GNNs struggle on heterophilic graphs]\n        A --> C[主要方法/Method: FgGSL - Jointly learns complementary graphs with spectral filters & label-based loss]\n        A --> D[关键结果/Results: Outperforms SOTA on benchmarks, benefits of frequency-guided inference]"
    },
    {
      "title": "On the Sample Complexity of Learning for Blind Inverse Problems",
      "authors": "Nathan Buskulic, Luca Calatroni, Lorenzo Rosasco, Silvia Villa",
      "institution": "Università degli studi di Genova, Italian Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23405",
      "code": null,
      "tags": [
        "inverse problems",
        "blind inverse problems",
        "Linear Minimum Mean Square Estimators (LMMSEs)",
        "Tikhonov regularization",
        "random forward operators",
        "error bounds"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp",
      "contributions": "1. Deriving closed-form expressions for optimal Linear Minimum Mean Square Estimators (LMMSEs) for blind inverse problems and establishing their equivalence with distribution-dependent Tikhonov regularization. 2. Proving convergence results for these estimators under source condition assumptions. 3. Deriving rigorous finite-sample error bounds that quantify the impact of operator randomness, noise level, and sample count on estimator performance.",
      "summary": "This paper provides a theoretical analysis of learning for blind inverse problems, where the forward operator is unknown. It focuses on Linear Minimum Mean Square Estimators (LMMSEs), deriving their optimal forms and connecting them to Tikhonov regularization. The main conclusion is the establishment of rigorous error bounds and convergence rates that characterize how estimator performance depends on noise, problem conditioning, and the randomness of the forward operator.",
      "mindmap": "graph TB\n        A[On the Sample Complexity of Learning for Blind Inverse Problems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[盲逆问题缺乏理论保证/Blind inverse problems lack theoretical guarantees]\n        C --> C1[线性最小均方误差估计器框架/LMMSE framework]\n        C --> C2[连接吉洪诺夫正则化/Link to Tikhonov regularization]\n        D --> D1[闭式最优估计器/Closed-form optimal estimators]\n        D --> D2[有限样本误差界/Finite-sample error bounds]\n        D --> D3[收敛率分析/Convergence rate analysis]"
    },
    {
      "title": "Theoretical Foundations of Scaling Law in Familial Models",
      "authors": "Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.23407",
      "code": null,
      "tags": [
        "llm training",
        "familial models",
        "scaling law",
        "early exiting",
        "IsoFLOP design",
        "compute-optimal training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp",
      "contributions": "1. Theoretically and empirically extends the neural scaling law to the \"familial models\" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the \"train once, deploy many\" paradigm without compromising compute-optimality.",
      "summary": "This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.",
      "mindmap": "graph TB\n        A[Theoretical Foundations of Scaling Law in Familial Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统缩放定律忽略多模型范式/Traditional scaling laws overlook the multi-model paradigm]\n        C --> C1[引入粒度作为新变量/Introduce Granularity (G) as a new variable]\n        C --> C2[统一函数形式 L(N, D, G)/Unified functional form L(N, D, G)]\n        C --> C3[采用IsoFLOP实验设计/Employ rigorous IsoFLOP experimental design]\n        D --> D1[粒度惩罚遵循幂律/Granularity penalty follows a power law]\n        D --> D2[指数极小 (γ≈0.041)/Exponent is extremely small]\n        D --> D3[验证”一次训练，多次部署”/Validates ”train once, deploy many”]"
    },
    {
      "title": "Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks",
      "authors": "Yusuf Kalyoncuoglu",
      "institution": "RWTH Aachen University",
      "link": "https://arxiv.org/pdf/2512.23410",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "intrinsic dimension",
        "low-rank approximation",
        "subspace-native distillation",
        "weight matrices",
        "empirical spectral density"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp",
      "contributions": "1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces \"Subspace-Native Distillation\" as a novel paradigm to provide a stable geometric coordinate system for student models, enabling \"Train Big, Deploy Small\".",
      "summary": "The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.",
      "mindmap": "graph TB\n        Root[”Directly Constructing Low-Dimensional Solution Subspaces<br>直接构建低维解子空间”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Large models are redundant for representation but needed for optimization.<br>大模型对表示是冗余的，但对优化是必要的。”]\n        Method[”主要方法/Method<br>Construct low-dimensional subspaces, decouple solution geometry.<br>构建低维子空间，解耦解几何。”]\n        Results[”关键结果/Results<br>Head compression by 16x, Subspace-Native Distillation.<br>分类头压缩16倍，提出子空间原生蒸馏。”]"
    },
    {
      "title": "Towards Integrating Uncertainty for Domain-Agnostic Segmentation",
      "authors": "Jesse Brouwers, Xiaoyan Xing, Alexander Timans",
      "institution": "UvA-Bosch Delta Lab, University of Amsterdam",
      "link": "https://arxiv.org/pdf/2512.23427",
      "code": "https://github.com/JesseBrouw/UncertSAM",
      "tags": [
        "semantic segmentation",
        "uncertainty quantification",
        "domain-agnostic",
        "Segment Anything Model (SAM)",
        "Laplace approximation",
        "benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp",
      "contributions": "1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.",
      "summary": "This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.",
      "mindmap": "graph TB\n        A[Towards Integrating Uncertainty for Domain-Agnostic Segmentation] --> B[核心问题/Problem: SAM在域偏移或知识有限场景下脆弱/SAM vulnerable in shifted or limited-knowledge domains]\n        A --> C[主要方法/Method: 构建UncertSAM基准，评估后验不确定性方法，尝试不确定性引导优化/Build UncertSAM benchmark, evaluate post-hoc UQ methods, attempt uncertainty-guided refinement]\n        A --> D[关键结果/Results: 拉普拉斯近似不确定性估计与误差相关，初步验证不确定性整合潜力/Laplace approximation yields correlated uncertainty, preliminary potential of integrating UQ]"
    },
    {
      "title": "AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis",
      "authors": "Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao",
      "institution": "Huawei Technologies Co., Ltd., Hunan University",
      "link": "https://arxiv.org/pdf/2512.23424",
      "code": null,
      "tags": [
        "gpu kernels",
        "kernel generation",
        "multi-agent system",
        "domain-specific languages (DSLs)",
        "performance tuning",
        "Triton"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp",
      "contributions": "1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system's effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.",
      "summary": "This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.",
      "mindmap": "graph TB\n        A[AKG Kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[AI模型对高性能计算内核的需求 / AI Models Demand High-Performance Kernels]\n        B --> B2[硬件多样性与手动优化的瓶颈 / Hardware Diversity & Manual Optimization Bottleneck]\n        C --> C1[多智能体系统自动化内核生成与调优 / Multi-Agent System Automates Kernel Generation & Tuning]\n        C --> C2[支持多种DSL以面向不同硬件后端 / Supports Multiple DSLs for Different Hardware Backends]\n        D --> D1[在KernelBench上评估 / Evaluated on KernelBench]\n        D --> D2[平均加速1.46倍 / Average 1.46x Speedup Achieved]"
    },
    {
      "title": "Stochastic Siamese MAE Pretraining for Longitudinal Medical Images",
      "authors": "Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović",
      "institution": "Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton",
      "link": "https://arxiv.org/pdf/2512.23441",
      "code": null,
      "tags": [
        "medical image analysis",
        "masked autoencoder",
        "siamese network",
        "stochastic process",
        "longitudinal data",
        "variational inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp",
      "contributions": "1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer's Disease.",
      "summary": "The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.",
      "mindmap": "graph TB\n        A[Stochastic Siamese MAE Pretraining for Longitudinal Medical Images] --> B[核心问题/Problem: MAE lacks temporal awareness for longitudinal medical data.]\n        A --> C[主要方法/Method: STAMP - Stochastic Siamese MAE using conditional variational inference.]\n        A --> D[关键结果/Results: Outperforms existing methods on AMD and AD progression prediction.]"
    },
    {
      "title": "Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing",
      "authors": "Manuel Franco-Vivo",
      "institution": "University of Bristol",
      "link": "https://arxiv.org/pdf/2512.23445",
      "code": null,
      "tags": [
        "multi-agent systems",
        "Model Predictive Control",
        "Coverage-Based Testing",
        "Edge-Case Exploration",
        "Multi-Agent Simulation",
        "Behaviour Coverage"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp",
      "contributions": "1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics.",
      "summary": "This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks.",
      "mindmap": "graph TB\n    A[Assessing Behaviour Coverage in a Multi-Agent System Simulation for Autonomous Vehicle Testing] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[如何全面评估自动驾驶系统在模拟环境中的行为覆盖度？/How to comprehensively evaluate behaviour coverage of AV systems in simulation?]\n    C --> C1[定义场景与交互，提出MPC行人智能体/Define scenarios & interactions, propose MPC pedestrian agent]\n    D --> D1[行为覆盖度对验证系统有效性至关重要/Behaviour coverage is crucial for validating system effectiveness]"
    },
    {
      "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
      "authors": "Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao",
      "institution": "ByteDance, Renmin University of China",
      "link": "https://arxiv.org/pdf/2512.23447",
      "code": null,
      "tags": [
        "llm training",
        "Mixture-of-Experts",
        "Router-Expert Coupling",
        "Auxiliary Loss",
        "Expert Specialization",
        "Efficient Training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp",
      "contributions": "1. Proposes a novel lightweight auxiliary loss (ERC loss) to explicitly couple router decisions with expert capabilities in MoE models. 2. Introduces a computationally efficient method that scales with the square of the number of experts (n^2), independent of batch size, unlike prior token-dependent methods. 3. Enables flexible control and quantitative tracking of expert specialization levels during training, providing new insights into MoE model dynamics.",
      "summary": "This paper addresses the misalignment between router decisions and expert capabilities in Mixture-of-Experts (MoE) models. It proposes an Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that enforces constraints via perturbed router embeddings to ensure each expert specializes in its routed tokens and each router embedding faithfully represents its expert. The method is shown to be effective and computationally efficient, enabling better control and analysis of expert specialization during large-scale pre-training.",
      "mindmap": "graph TB\n        A[耦合专家与路由器<br/>Coupling Experts and Routers in Mixture-of-Experts] --> B[核心问题/Problem: 路由器决策与专家能力不匹配<br/>Router decisions misaligned with expert capabilities]\n        A --> C[主要方法/Method: 提出专家-路由器耦合损失 (ERC Loss)<br/>Propose Expert-Router Coupling (ERC) Loss]\n        A --> D[关键结果/Results: 提升模型性能，高效计算，可量化追踪专家专业化<br/>Improved performance, efficient computation, quantifiable tracking of specialization]\n        C --> E[方法原理/Mechanism: 使用扰动路由器嵌入作为代理令牌<br/>Use perturbed router embeddings as proxy tokens]\n        E --> F[约束/Constraints: 专家对自身代理令牌激活最高；代理令牌引发对应专家最强激活<br/>Expert highest activation for own proxy; Proxy elicits strongest activation from corresponding expert]"
    },
    {
      "title": "Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion",
      "authors": "Vladimer Khasia",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.23448",
      "code": "https://github.com/VladimerKhasia/DSC",
      "tags": [
        "llm inference",
        "Mixture of Experts",
        "Parameter-Efficient Fine-Tuning",
        "Low-Rank Adaptation",
        "Memory-Bandwidth Bottleneck",
        "Dynamic Sparse Dictionary Learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp",
      "contributions": "1. Proposes Dynamic Subspace Composition (DSC), a framework that models weight updates as a residual trajectory within a Star-Shaped Domain using Magnitude-Gated Simplex Interpolation for continuity. 2. Decouples storage and adaptation rank, constructing compositional approximations from a shared basis bank to reduce parameter complexity from O(Mrd) to O(Md) and memory traffic to O(Kd). 3. Introduces Frame-Theoretic regularization and spectral constraints to provide rigorous worst-case bounds on the dynamic update, addressing representation collapse and gradient instability.",
      "summary": "The paper addresses the memory-bandwidth bottleneck and optimization instability in Mixture of Experts (MoE) models. It proposes Dynamic Subspace Composition (DSC), a method that approximates context-dependent weights via a sparse expansion of a shared basis, reducing parameter complexity and memory traffic while ensuring stable updates. The main conclusion is that DSC offers a more efficient and theoretically grounded alternative to standard approaches like Mixture-of-LoRAs.",
      "mindmap": "graph TB\n        A[Dynamic Subspace Composition<br/>动态子空间组合] --> B[Problem<br/>核心问题]\n        A --> C[Method<br/>主要方法]\n        A --> D[Results<br/>关键结果]\n        B --> B1[MoE suffers from<br/>representation collapse &<br/>memory bottleneck<br/>MoE存在表示崩溃与内存瓶颈]\n        C --> C1[Dynamic Sparse Dictionary Learning<br/>动态稀疏字典学习]\n        C1 --> C2[Star-Shaped Domain &<br/>Magnitude-Gated Simplex Interpolation<br/>星形域与幅度门控单纯形插值]\n        D --> D1[Reduced complexity<br/>O(Md) & O(Kd)<br/>降低复杂度]\n        D --> D2[Frame-Theoretic bounds<br/>框架理论边界]"
    },
    {
      "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following",
      "authors": "Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song",
      "institution": "Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.23457",
      "code": "https://github.com/zhangkc97/HiR",
      "tags": [
        "reinforcement learning",
        "instruction following",
        "hindsight replay",
        "sample-efficient RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp",
      "contributions": "1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.",
      "summary": "This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.",
      "mindmap": "graph TB\n        Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[稀疏/不可区分的奖励阻碍学习<br>Sparse/Indistinguishable Rewards Impede Learning]\n        Method[后见指令重放 (HiR)<br>Hindsight instruction Replay (HiR)]\n        Results[跨任务有效且计算高效<br>Effective Across Tasks & Computationally Efficient]"
    },
    {
      "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
      "authors": "Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang",
      "institution": "Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data",
      "link": "https://arxiv.org/pdf/2512.23461",
      "code": "https://github.com/Qwen-Applications/DIR",
      "tags": [
        "reinforcement learning from human feedback (RLHF)",
        "reward model",
        "inductive bias",
        "information bottleneck",
        "mutual information",
        "reward hacking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp",
      "contributions": "1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method's ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR's effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.",
      "summary": "This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.",
      "mindmap": "graph TB\n        A[Eliminating Inductive Bias in Reward Models<br>消除奖励模型中的归纳偏差] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Low-quality RM data with inductive biases<br>导致过拟合和奖励攻击] --> B1[举例/Example<br>Response length bias<br>响应长度偏差]\n        C[主要方法/Method<br>DIR: Information-theoretic debiasing<br>基于信息瓶颈优化互信息] --> C1[目标/Objective<br>Max MI with preference, Min MI with bias<br>最大化偏好互信息，最小化偏差互信息]\n        D[关键结果/Results<br>Mitigates multiple biases & enhances RLHF<br>减轻多种偏差并提升RLHF性能] --> D1[验证的偏差/Verified Biases<br>Length, Sycophancy, Format<br>长度、迎合性、格式]"
    },
    {
      "title": "FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence",
      "authors": "Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu",
      "institution": "Beihang University, Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23485",
      "code": "https://github.com/Bane-Elvin/AAAI2026-FRoD",
      "tags": [
        "post-training (sft/rlhf)",
        "Parameter-efficient fine-tuning",
        "LoRA",
        "full-rank adaptation",
        "rotational degrees of freedom",
        "hierarchical joint decomposition"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp",
      "contributions": "1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.",
      "summary": "The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.",
      "mindmap": "graph TB\n        A[FRoD: Full-Rank Efficient Fine-Tuning] --> B[核心问题/Problem: Low-rank PEFT methods suffer from slow convergence and limited adaptation capacity]\n        A --> C[主要方法/Method: Hierarchical joint decomposition with rotational degrees of freedom for full-rank updates]\n        A --> D[关键结果/Results: Matches full fine-tuning accuracy using only 1.72% parameters and achieves faster convergence]"
    },
    {
      "title": "ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment",
      "authors": "Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki",
      "institution": "Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University",
      "link": "https://arxiv.org/pdf/2512.23487",
      "code": null,
      "tags": [
        "llm inference",
        "model selection",
        "capability-cost frontier",
        "constrained optimization",
        "deployment-aware leaderboards",
        "compliance trade-offs"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp",
      "contributions": "1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.",
      "summary": "The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.",
      "mindmap": "graph TB\n        Root(”ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”能力排行榜与部署决策脱节/Capability-Deployment Gap”)\n        Problem --> P2(”需平衡用户效用、成本、合规性/Balance Utility, Cost, Compliance”)\n        Method --> M1(”理论: 基于前沿的约束优化/Theoretical Constrained Optimization”)\n        Method --> M2(”实现: 提取、估计、学习、推荐/Pipeline: Extract, Estimate, Learn, Recommend”)\n        Results --> R1(”最优配置呈现三区结构/Optimal Configurations Show Three-Regime Structure”)\n        Results --> R2(”部署感知排名不同于能力排名/Deployment-Aware Rankings Differ from Capability-Only”)"
    },
    {
      "title": "Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization",
      "authors": "Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink",
      "institution": "Wuhan University, RWTH Aachen University",
      "link": "https://arxiv.org/pdf/2512.23493",
      "code": null,
      "tags": [
        "communication & networking",
        "URLLC",
        "Link Adaptation",
        "Device Scheduling",
        "Deep Reinforcement Learning",
        "Bayesian Optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp",
      "contributions": "1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.",
      "summary": "This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.",
      "mindmap": "graph TB\n        A[Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[URLLC IIoT网络的多设备动态调度与链路自适应/URLLC IIoT Multi-device Dynamic Scheduling & Link Adaptation]\n        B --> B2[不完美的信道状态信息/Imperfect Channel State Information]\n        B --> B3[严格的误块率约束/Strict Block Error Rate Constraints]\n        C --> C1[贝叶斯优化驱动的TD3方法/BO-driven TD3 Method]\n        C --> C2[自适应确定设备服务顺序与MCS/Adaptively Determine Device Order & MCS]\n        C --> C3[BO训练机制改进收敛/BO-based Training for Convergence]\n        D --> D1[更快的收敛速度/Faster Convergence]\n        D --> D2[更高的总速率性能/Higher Sum-rate Performance]"
    },
    {
      "title": "Trustworthy Machine Learning under Distribution Shifts",
      "authors": "Zhuo Huang",
      "institution": "The University of Sydney",
      "link": "https://arxiv.org/pdf/2512.23524",
      "code": null,
      "tags": [
        "trustworthy machine learning",
        "distribution shift",
        "robustness",
        "explainability",
        "adaptability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp",
      "contributions": "1. Proposes a systematic framework for studying Trustworthy Machine Learning by categorizing three common types of distribution shifts (Perturbation, Domain, Modality). 2. Rigorously investigates trustworthiness through three key aspects: Robustness, Explainability, and Adaptability. 3. Aims to provide effective solutions and fundamental insights to enhance critical ML problems like efficiency and safety under distribution shifts.",
      "summary": "This thesis addresses the core problem of distribution shift, which limits the reliability and trustworthiness of AI systems. The research proposes a framework that studies three types of distribution shifts and evaluates solutions through the lenses of robustness, explainability, and adaptability. The goal is to develop more reliable, versatile, and responsible machine learning models that can generalize effectively under real-world distribution shifts.",
      "mindmap": "graph TB\n        Root[”Trustworthy Machine Learning under Distribution Shifts<br>分布偏移下的可信机器学习”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Distribution shift limits AI reliability & trust<br>分布偏移限制AI可靠性与信任”] --> P1[”扰动偏移/Perturbation Shift”]\n        Problem --> P2[”域偏移/Domain Shift”]\n        Problem --> P3[”模态偏移/Modality Shift”]\n        Method[”主要方法/Method<br>Study trustworthiness via three aspects<br>从三个维度研究可信度”] --> M1[”鲁棒性/Robustness”]\n        Method --> M2[”可解释性/Explainability”]\n        Method --> M3[”适应性/Adaptability”]\n        Results[”关键结果/Results<br>Propose solutions & insights<br>提出解决方案与洞见”] --> R1[”增强关键问题/Enhance critical problems”]\n        R1 --> R1_Sub[”效率, 适应性, 安全<br>Efficiency, Adaptability, Safety”]"
    },
    {
      "title": "EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition",
      "authors": "Maryam Mirzaei, Farzaneh Shayegh, Hamed Narimani",
      "institution": "Based on author names and content, specific institution not provided. Could be inferred from typical academic affiliations in this field, but not explicitly stated in the given text.",
      "link": "https://arxiv.org/pdf/2512.23526",
      "code": null,
      "tags": [
        "affective computing",
        "domain adaptation",
        "graph regularization",
        "EEG",
        "emotion recognition",
        "cross-session"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp",
      "contributions": "1. Proposed EGDA, a novel framework integrating domain adaptation with graph-based regularization for cross-session EEG emotion recognition. 2. Introduced a method to jointly align both marginal and conditional distributions while preserving the intrinsic data structure. 3. Demonstrated the discriminative power of the Gamma frequency band and identified critical brain regions (central-parietal and prefrontal) for emotion recognition.",
      "summary": "The paper addresses the challenge of cross-session EEG emotion recognition by proposing EGDA, a framework that reduces distribution discrepancies through joint marginal and conditional alignment while using graph regularization to preserve data structure. Experiments on the SEED-IV dataset show EGDA outperforms baselines, achieving robust accuracies. The analysis further identifies the Gamma band and specific brain regions as key for reliable emotion recognition.",
      "mindmap": "graph TB\n        Root[EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Cross-session EEG distribution shifts hinder emotion recognition model generalization.]\n        Method[主要方法/Method: EGDA framework aligns marginal & conditional distributions with graph regularization.]\n        Results[关键结果/Results: Achieves robust accuracy on SEED-IV; Gamma band and central-parietal/prefrontal regions are most discriminative.]"
    },
    {
      "title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing",
      "authors": "Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang",
      "institution": "Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23562",
      "code": "https://github.com/K1nght/VL-RouterBench",
      "tags": [
        "multi-modal inference",
        "vision-language model routing",
        "benchmark",
        "cost-accuracy trade-off",
        "model selection",
        "evaluation protocol"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp",
      "contributions": "1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.",
      "summary": "This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.",
      "mindmap": "graph TB\n        Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>缺乏系统化、可复现的<br>VLM路由评估基准<br>Lack of systematic, reproducible<br>benchmark for VLM routing]\n        Method[主要方法/Method<br>基于原始推理日志构建<br>质量与成本矩阵<br>Construct quality & cost matrices<br>from raw inference logs]\n        Results[关键结果/Results<br>观察到显著的路由增益<br>但与理想性能仍有差距<br>Observe significant routability gain<br>but clear gap to ideal Oracle]"
    },
    {
      "title": "Distribution-Free Process Monitoring with Conformal Prediction",
      "authors": "Christopher Burger",
      "institution": "The University of Mississippi",
      "link": "https://arxiv.org/pdf/2512.23602",
      "code": null,
      "tags": [
        "anomaly detection",
        "Conformal Prediction",
        "Statistical Process Control",
        "Control Charts",
        "Anomaly Detection",
        "Quality Management"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp",
      "contributions": "1. A hybrid framework integrating Conformal Prediction's distribution-free guarantees into Statistical Process Control (SPC). 2. Conformal-Enhanced Control Charts that visualize process uncertainty and enable proactive signals like 'uncertainty spikes'. 3. Conformal-Enhanced Process Monitoring that reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart.",
      "summary": "This paper addresses the limitations of traditional Statistical Process Control (SPC), which relies on often-violated statistical assumptions, by proposing a hybrid framework that integrates distribution-free Conformal Prediction. The method introduces two novel applications: enhanced control charts for visualizing uncertainty and a p-value chart for formal anomaly detection. The framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability of classic methods.",
      "mindmap": "graph TB\n        A[Distribution-Free Process Monitoring with Conformal Prediction] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统SPC依赖统计假设，不可靠/Traditional SPC relies on statistical assumptions, unreliable]\n        C --> C1[将保形预测与SPC结合的混合框架/Hybrid framework integrating Conformal Prediction with SPC]\n        C1 --> C2[保形增强控制图/Conformal-Enhanced Control Charts]\n        C1 --> C3[保形增强过程监控/Conformal-Enhanced Process Monitoring]\n        D --> D1[更鲁棒、统计严谨的质量控制方法/More robust, statistically rigorous quality control]\n        D --> D2[保持经典方法的可解释性和易用性/Maintains interpretability and ease of use of classic methods]"
    },
    {
      "title": "Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning",
      "authors": "Deniz Akdemir",
      "institution": "None (Institution not specified in provided content)",
      "link": "https://arxiv.org/pdf/2512.23617",
      "code": null,
      "tags": [
        "transfer learning",
        "Le Cam Distortion",
        "Deficiency Distance",
        "Directional Simulability",
        "Unsupervised Domain Adaptation",
        "Negative Transfer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp",
      "contributions": "1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam's theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework's effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.",
      "summary": "The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful \"negative transfer\" by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam's theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.",
      "mindmap": "graph TB\n        Root[Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[标准UDA的缺陷/Flaw of Standard UDA]\n        Problem --> P2[负迁移与信息破坏/Negative Transfer & Information Destruction]\n        Method --> M1[Le Cam理论/Le Cam's Theory]\n        Method --> M2[方向可模拟性/Directional Simulability]\n        Method --> M3[Le Cam Distortion度量/Le Cam Distortion Metric]\n        Results --> R1[基因组学完美估计/Perfect Genomics Estimation]\n        Results --> R2[零源域损失/Zero Source Utility Loss]\n        Results --> R3[安全RL策略转移/Safe RL Policy Transfer]"
    },
    {
      "title": "Regret-Based Federated Causal Discovery with Unknown Interventions",
      "authors": "Federico Baldo, Charles K. Assaad",
      "institution": "Sorbonne Université, INSERM, Institut Pierre Louis d'Epidémiologie et de Santé Publique",
      "link": "https://arxiv.org/pdf/2512.23626",
      "code": null,
      "tags": [
        "federated learning",
        "causal discovery",
        "unknown interventions",
        "differential privacy",
        "Φ-CPDAG",
        "regret-based"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp",
      "contributions": "1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).",
      "summary": "The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm's effectiveness and privacy guarantees.",
      "mindmap": "graph TB\n        A[Regret-Based Federated Causal Discovery with Unknown Interventions] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 联邦因果发现中客户端存在未知异质干预/Federated causal discovery with unknown, heterogeneous client interventions]\n        C[主要方法/Method: 提出I-PERI算法，利用干预差异定向边/Propose I-PERI algorithm, orienting edges using intervention differences]\n        D[关键结果/Results: 定义Φ-CPDAG，提供理论与隐私保证/Define Φ-CPDAG, provide theoretical and privacy guarantees]"
    },
    {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "authors": "Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu",
      "institution": "Princeton University, Harvard University",
      "link": "https://arxiv.org/pdf/2512.23628",
      "code": "github.com/zlab-princeton/3d_mem",
      "tags": [
        "3D shape generation",
        "memorization",
        "diffusion models",
        "latent vector-set",
        "evaluation framework",
        "data leakage"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp",
      "contributions": "1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.",
      "summary": "This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.",
      "mindmap": "graph TB\n        A[Memorization in 3D Shape Generation: An Empirical Study] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(3D生成模型是否记忆训练数据?/Do 3D generative models memorize training data?)\n        C --> C1(设计量化框架/Design evaluation framework)\n        C --> C2(使用Vecset扩散模型进行控制实验/Use Vecset diffusion model for controlled experiments)\n        D --> D1(数据多样性和细粒度条件增加记忆/Data diversity & fine-grained conditioning increase memorization)\n        D --> D2(适度引导规模峰值记忆/Moderate guidance scale peaks memorization)\n        D --> D3(更长Vecsets和旋转增强可缓解记忆/Longer Vecsets & rotation augmentation mitigate memorization)"
    },
    {
      "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization",
      "authors": "Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong",
      "institution": "Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst",
      "link": "https://arxiv.org/pdf/2512.23631",
      "code": "https://github.com/iamxjy/BOAD-SWE-Agent",
      "tags": [
        "agent system",
        "multi-agent systems",
        "hierarchical agents",
        "bandit optimization",
        "software engineering agents",
        "SWE-bench"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp",
      "contributions": "1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.",
      "summary": "The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.",
      "mindmap": "graph TB\n        A[BOAD: 发现分层软件工程代理 / BOAD: Discovering Hierarchical Software Engineering Agents] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 单一LLM代理在长视野、分布外软件工程任务上泛化能力差 / Single-agent LLMs generalize poorly on long-horizon, out-of-distribution SWE tasks]\n        C[主要方法 / Method: 将分层发现建模为多臂老虎机问题，优化子代理协作 / Formulate hierarchy discovery as a multi-armed bandit problem to optimize sub-agent collaboration]\n        D[关键结果 / Results: 在SWE-bench上超越单代理和手动设计的多代理系统，36B模型排名第二 / Outperforms single-agent and manual multi-agent systems on SWE-bench, 36B model ranks second]"
    },
    {
      "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
      "authors": "LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão",
      "institution": "Google, Eedi",
      "link": "https://arxiv.org/pdf/2512.23633",
      "code": null,
      "tags": [
        "educational ai",
        "generative AI",
        "fine-tuning",
        "randomized controlled trial",
        "Socratic questioning",
        "pedagogical instruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp",
      "contributions": "1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.",
      "summary": "This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.",
      "mindmap": "graph TB\n        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]\n        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]\n        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]\n        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]\n        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]\n        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]\n        Results --> R2[学生表现相当或更好/Student performance equal or better]\n        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]"
    },
    {
      "title": "Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks",
      "authors": "Konstantin Yakovlev, Nikita Puchkin",
      "institution": "HSE University",
      "link": "https://arxiv.org/pdf/2512.23643",
      "code": null,
      "tags": [
        "diffusion models",
        "score function",
        "approximation theory",
        "deep neural networks",
        "curse of dimensionality",
        "Ornstein-Uhlenbeck process"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp",
      "contributions": "1. Presents a theory for simultaneous approximation of the score function and its derivatives, extending beyond the first-order setting. 2. Derives approximation error bounds that are free from the curse of dimensionality. 3. Relaxes the common assumption of bounded data support, enabling handling of distributions with low-dimensional structure and unbounded support.",
      "summary": "This paper develops a theoretical framework for using deep neural networks to approximate the score function and its derivatives simultaneously. The method relaxes the typical bounded support assumption and provides error bounds that avoid the curse of dimensionality. The main conclusion is that this theory enables more efficient handling of complex data distributions, which is crucial for improving the convergence of diffusion and ODE-based generative models.",
      "mindmap": "graph TB\n        A[Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[需要同时逼近分数函数及其导数/Need to approximate score function and its derivatives]\n        B --> B2[处理无界支撑和低维结构数据/Handle data with unbounded support & low-dim structure]\n        C --> C1[深度神经网络理论框架/Deep Neural Network theoretical framework]\n        D --> D1[逼近误差无维度诅咒/Approximation bounds free from curse of dimensionality]\n        D --> D2[放松有界支撑假设/Relaxes bounded support assumption]\n        D --> D3[保证任意阶导数逼近/Guarantees for derivatives of any order]"
    },
    {
      "title": "Random Controlled Differential Equations",
      "authors": "Francesco Piatti, Thomas Cass, William F. Turner",
      "institution": "Imperial College London",
      "link": "https://arxiv.org/pdf/2512.23670",
      "code": "https://github.com/FrancescoPiatti/RandomSigJax",
      "tags": [
        "time-series learning",
        "controlled differential equations",
        "random features",
        "signature kernels",
        "reservoir computing",
        "rough paths"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp",
      "contributions": "1. A training-efficient framework combining random features with Controlled Differential Equations (CDEs) to create continuous-time reservoirs for time-series learning. 2. Two novel variants: Random Fourier CDEs (RF-CDEs) for kernel-free RBF approximation and Random Rough DEs (R-RDEs) for stable, efficient modeling of rough-path inputs. 3. Theoretical proof that these models induce the RBF-lifted and rough signature kernels in the infinite-width limit, unifying random-feature reservoirs, continuous-time architectures, and path-signature theory.",
      "summary": "The paper introduces a fast and scalable framework for time-series learning by using large, randomly parameterized Controlled Differential Equations (CDEs) as continuous-time reservoirs, with only a linear readout layer trained. Two specific model variants, RF-CDEs and R-RDEs, are proposed and shown to approximate powerful signature kernels. The methods achieve competitive or state-of-the-art performance on benchmarks, offering a practical alternative to explicit signature computations.",
      "mindmap": "graph TB\n        A[Random Controlled Differential Equations] --> B(核心问题/Problem: Efficient learning for time-series data)\n        A --> C(主要方法/Method: Random-feature CDE reservoirs with linear readout)\n        C --> D[RF-CDEs: 随机傅里叶特征/Random Fourier Features]\n        C --> E[R-RDEs: 随机粗糙微分方程/Random Rough DEs]\n        A --> F(关键结果/Results: Competitive SOTA performance, induces signature kernels)"
    },
    {
      "title": "End-to-End Test-Time Training for Long Context",
      "authors": "Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun",
      "institution": "Stanford University, UC Berkeley, UC San Diego, Astera Institute, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.23675",
      "code": null,
      "tags": [
        "llm inference",
        "test-time training",
        "meta-learning",
        "sliding-window attention",
        "continual learning",
        "long-context modeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp",
      "contributions": "1. Formulates long-context modeling as a continual learning problem, enabling a standard sliding-window Transformer to learn at test time via next-token prediction, 2. Uses meta-learning during training to optimize the model's initialization for efficient test-time learning, 3. Achieves scaling performance comparable to full-attention Transformers while maintaining constant inference latency like RNNs, resulting in significant speedups for long contexts.",
      "summary": "This paper proposes an end-to-end test-time training method for long-context language modeling. It uses a standard sliding-window attention Transformer that learns continuously at test time via next-token prediction, with its initialization optimized via meta-learning during training. The method matches the scaling performance of full-attention Transformers while offering constant inference latency, making it 2.7x faster for 128K contexts.",
      "mindmap": "graph TB\n        A[End-to-End Test-Time Training for Long Context] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Long-context language modeling]\n        C[主要方法/Method: Test-Time Training with meta-learning & sliding-window attention]\n        D[关键结果/Results: Matches full-attention scaling, constant latency, 2.7x faster]"
    },
    {
      "title": "Eliciting Behaviors in Multi-Turn Conversations",
      "authors": "Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert",
      "institution": "Google DeepMind, Stanford University",
      "link": "https://arxiv.org/pdf/2512.23701",
      "code": null,
      "tags": [
        "llm evaluation",
        "behavior elicitation",
        "multi-turn conversation",
        "online methods",
        "dynamic benchmarks",
        "test case generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp",
      "contributions": "1. Proposes an analytical framework categorizing behavior elicitation methods into three families based on their interaction with the target model (prior knowledge, offline, online). 2. Introduces a generalized multi-turn formulation for online behavior elicitation methods, unifying single-turn and multi-turn settings. 3. Demonstrates the superior efficiency of online methods in discovering failure cases in multi-turn conversations compared to static benchmarks, advocating for a shift to dynamic evaluation.",
      "summary": "This paper studies the problem of efficiently eliciting specific behaviors from large language models in multi-turn conversational settings. It introduces a framework for categorizing existing elicitation methods and proposes a generalized online method for multi-turn interactions. The key finding is that online methods can discover many more failure cases with few queries than static benchmarks, highlighting the need for dynamic evaluation approaches.",
      "mindmap": "graph TB\n        A[Eliciting Behaviors in Multi-Turn Conversations] --> B(核心问题/Problem: How to efficiently elicit specific behaviors from LLMs in multi-turn conversations?)\n        A --> C(主要方法/Method: Categorizes methods into three families; Proposes a generalized multi-turn online formulation.)\n        A --> D(关键结果/Results: Online methods achieve high success rates with few queries, outperforming static benchmarks.)"
    },
    {
      "title": "UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing",
      "authors": "Gaofeng Dong, Kang Yang, Mani Srivastava",
      "institution": "University of California, Los Angeles (UCLA)",
      "link": "https://arxiv.org/pdf/2512.22143",
      "code": null,
      "tags": [
        "on-device ai",
        "Integrated Sensing and Communication (ISAC)",
        "Channel State Information (CSI)",
        "Attention Model",
        "Irregular Sampling",
        "Wi-Fi Sensing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp",
      "contributions": "1. Proposes UniFi, the first Wi-Fi ISAC framework that eliminates intrusive packet injection by exploiting irregularly sampled CSI from diverse communication packets across multiple bands. 2. Introduces a CSI sanitization pipeline to harmonize heterogeneous packets and a time-aware attention model that learns directly from non-uniform CSI sequences. 3. Presents CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic.",
      "summary": "This paper presents UniFi, a Wi-Fi sensing framework that solves the problem of communication degradation caused by high-rate probing packets. It achieves this by directly using irregularly sampled Channel State Information from existing communication traffic across multiple frequency bands, combined with a novel sanitization pipeline and a time-aware attention model. Evaluations show that UniFi achieves state-of-the-art sensing accuracy while fully preserving communication throughput.",
      "mindmap": "graph TB\n        A[UniFi: Wi-Fi Sensing] --> B[核心问题/Problem: 现有系统依赖注入探测包，降低通信性能/Existing systems rely on probing packets, degrading communication]\n        A --> C[主要方法/Method: 利用多频段通信包的非均匀CSI，使用净化管道和注意力模型/Exploit irregular CSI from multi-band comm packets with sanitization & attention]\n        A --> D[关键结果/Results: 消除包注入，保持通信吞吐量，实现SOTA精度/Eliminates packet injection, preserves throughput, achieves SOTA accuracy]"
    },
    {
      "title": "Training AI Co-Scientists Using Rubric Rewards",
      "authors": "Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse",
      "institution": "Meta Superintelligence Labs, ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, University of Oxford, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.23707",
      "code": null,
      "tags": [
        "reinforcement learning",
        "research plan generation",
        "self-grading",
        "rubric rewards"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp",
      "contributions": "1. A scalable method to automatically extract research goals and goal-specific grading rubrics from existing papers to build a training corpus. 2. A reinforcement learning framework with self-grading, where a frozen initial model acts as the grader using rubrics, enabling unsupervised improvement. 3. Demonstration of significant performance gains (12-22% relative improvement) and cross-domain generalization (e.g., to medical research) validated by human experts and frontier model juries.",
      "summary": "This paper addresses the challenge of training language models to generate high-quality, constraint-following research plans. The proposed method uses reinforcement learning with self-grading, where rubrics automatically extracted from research papers provide reward signals. The approach shows significant improvements in plan quality and generalizes across domains like machine learning and medicine, validated by human expert preference.",
      "mindmap": "graph TB\n        Root[”Training AI Co-Scientists Using Rubric Rewards”] --> Problem[”核心问题/Problem: LMs struggle to generate research plans that follow all constraints.”]\n        Root --> Method[”主要方法/Method: RL with self-grading using automatically extracted rubrics.”]\n        Root --> Results[”关键结果/Results: Human experts prefer finetuned model's plans; method generalizes across domains.”]"
    },
    {
      "title": "EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG",
      "authors": "Hanbeot Park, Yunjeong Cho, Hunhee Kim",
      "institution": "Pukyong National University",
      "link": "https://arxiv.org/pdf/2512.22146",
      "code": null,
      "tags": [
        "brain-computer interface",
        "EEG-to-Voice",
        "mel-spectrogram",
        "domain adaptation",
        "automatic speech recognition",
        "language model correction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp",
      "contributions": "1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy.",
      "summary": "This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance.",
      "mindmap": "graph TB\n        A[EEG-to-Voice Decoding of Spoken and Imagined speech] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: EEG-based speech reconstruction is challenging due to noise, low resolution, and lack of aligned targets for imagined speech.]\n        C[主要方法/Method: Direct, open-loop EEG-to-mel-spectrogram generation with subject-specific generators, domain adaptation from spoken to imagined speech, and optional LM-based ASR correction.]\n        D[关键结果/Results: Feasibility demonstrated for both speech types; stable acoustic/linguistic performance; LM correction reduces CER/WER without semantic distortion.]"
    },
    {
      "title": "Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms",
      "authors": "Emna Bouzid, Noura Baccar, Kamran Iqbal, Yassine Chaouch, Fares Ben Youssef, Amine Regayeg, Sarra Toumi, Houda Nsir, Amina Mseddi, Leila Costelle",
      "institution": "University of Arkansas, Little Rock; Mediterranean Institute of Technology, South Mediterranean University",
      "link": "https://arxiv.org/pdf/2512.22151",
      "code": null,
      "tags": [
        "on-device ai",
        "indoor vertical hydroponics",
        "IoT sensors",
        "LSTM",
        "DNN",
        "Linear Regression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp",
      "contributions": "1. Developed a prediction system for basil yield in IoT-enabled indoor vertical hydroponic farms using ML models. 2. Conducted a comparative performance analysis of Linear Regression, LSTM, and DNN models, evaluating accuracy, execution time, and RAM usage. 3. Identified DNN as offering an optimal balance between computational efficiency (speed/RAM) and high prediction accuracy (98%), making it suitable for real-world, resource-conscious deployment.",
      "summary": "This paper addresses water scarcity in agriculture by proposing a machine learning-based system to predict basil yield in IoT-enabled indoor vertical hydroponic farms. It compares Linear Regression, LSTM, and DNN models using sensor data, finding that DNN provides a good trade-off between high accuracy (98%) and computational efficiency, making it suitable for practical deployment.",
      "mindmap": "graph TB\n        A[Paper Title: Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Agriculture water scarcity, need for efficient solutions]\n        C[主要方法/Method: ML models (LR, LSTM, DNN) trained on IoT sensor data from hydroponic farm]\n        D[关键结果/Results: DNN balances accuracy (98%) and computational efficiency for real-life deployment]"
    },
    {
      "title": "The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence",
      "authors": "Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang",
      "institution": "Tsinghua University, National Climate Centre, China Meteorological Administration",
      "link": "https://arxiv.org/pdf/2512.22144",
      "code": null,
      "tags": [
        "climate informatics",
        "similarity-preserving representation",
        "latent space clustering",
        "physics-coherent monitoring"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp",
      "contributions": "1. Introduced an \"AI-for-theory\" paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.",
      "summary": "This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an \"AI-for-theory\" paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI's role as a discovery tool for complex systems.",
      "mindmap": "graph TB\n        A[The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Defining MJO lifecycle is challenging due to propagation; classical methods conflate artifacts with physics.]\n        C[主要方法/Method<br>AI-for-theory paradigm; Deep learning model (PhysAnchor-MJO-AE) learns similarity-preserving latent representation for objective clustering.]\n        D[关键结果/Results<br>First complete six-phase MJO anatomy; New physics-coherent monitoring framework reduces errors by an order of magnitude.]"
    },
    {
      "title": "Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data",
      "authors": "Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet",
      "institution": "IMT Atlantique, Ifremer, CNRS, Mercator Ocean International",
      "link": "https://arxiv.org/pdf/2512.22152",
      "code": null,
      "tags": [
        "spatiotemporal forecasting",
        "4DVarNet",
        "U-Net",
        "sequence-to-sequence",
        "sea level anomaly",
        "neural forecast"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp",
      "contributions": "1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.",
      "summary": "This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.",
      "mindmap": "graph TB\n        A[Neural Ocean Forecasting from Sparse Observations] --> B(核心问题/Problem: 稀疏卫星数据下的短期海洋预报/Short-term ocean forecasting from sparse satellite data)\n        A --> C(主要方法/Method: 基于U-Net和4DVarNet的端到端序列预测/End-to-end sequence forecasting using U-Net & 4DVarNet)\n        A --> D(关键结果/Results: 神经模型超越业务化基线，在多变区域改进显著/Neural model outperforms operational baseline, notable improvements in high-variability regions)"
    },
    {
      "title": "Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials",
      "authors": "Nicolas Zilberstein, Santiago Segarra, Luiz Chamon",
      "institution": "Rice University, École Polytechnique (Institut Polytechnique de Paris)",
      "link": "https://arxiv.org/pdf/2512.22153",
      "code": null,
      "tags": [
        "constrained sampling",
        "Langevin Monte Carlo",
        "navigation functions",
        "constrained sampling",
        "non-convex support",
        "adaptive temperature"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp",
      "contributions": "1. Introduces shielded Langevin Monte Carlo (LMC) for sampling from distributions with non-convex supports defined by convex sets with convex holes. 2. Incorporates a navigation function-inspired approach using a spatially adaptive temperature and repulsive drift to keep samples within feasible regions. 3. Demonstrates effectiveness through experiments on 2D Gaussian mixture and MIMO symbol detection, showing advantages over unconstrained methods.",
      "summary": "The paper proposes shielded Langevin Monte Carlo, a constrained sampling method that uses navigation potentials to sample from unnormalized target distributions over punctured (non-convex) supports. It modifies the Langevin diffusion with adaptive temperature and repulsive drift to avoid holes. Experiments on Gaussian mixtures and MIMO detection show it outperforms unconstrained sampling.",
      "mindmap": "graph TB\n        Root(”Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials”) --> Problem(”核心问题/Problem: Sampling from non-convex supports with convex holes”)\n        Root --> Method(”主要方法/Method: Shielded LMC with adaptive temperature and repulsive drift”)\n        Root --> Results(”关键结果/Results: Outperforms unconstrained sampling in 2D Gaussian mixture and MIMO detection”)"
    },
    {
      "title": "PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection",
      "authors": "Md Shahriar Sajid, Abhijit Kumar Ghosh, Fariha Nusrat",
      "institution": "Rajshahi University of Engineering & Technology, BRAC University, University of Asia Pacific",
      "link": "https://arxiv.org/pdf/2512.22172",
      "code": null,
      "tags": [
        "brain-computer interface (BCI)",
        "temporal convolution",
        "residual attention",
        "recurrent networks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp",
      "contributions": "1. Proposed PaperNet, a compact hybrid architecture combining temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block for EEG classification. 2. Demonstrated high performance (macro-F1 0.96) on the BEED dataset with only ~0.6M parameters under a subject-independent protocol. 3. Provided interpretability through channel-wise attention weights to reveal electrode relevance and validated efficiency for deployment on resource-constrained systems.",
      "summary": "This paper introduces PaperNet, a lightweight deep learning model that integrates temporal convolutions, channel residual attention, and a bidirectional recurrent block for efficient EEG epilepsy detection. It achieves a macro-F1 score of 0.96 on the BEED dataset with only about 0.6 million parameters, showing balanced performance across classes. The results indicate that combining temporal filtering, channel reweighting, and recurrent context modeling can deliver strong classification without high computational cost.",
      "mindmap": "graph TB\n        A[PaperNet: EEG癫痫检测 / PaperNet: EEG Epilepsy Detection] --> B[核心问题/Problem: EEG信号噪声多、变异性大 / Problem: EEG signals are noisy and variable]\n        A --> C[主要方法/Method: 时间卷积+通道残差注意力+轻量循环块 / Method: Temporal convolutions + Channel residual attention + Lightweight recurrent block]\n        A --> D[关键结果/Results: 宏F1=0.96, 参数0.6M, 高效部署 / Results: Macro-F1=0.96, 0.6M params, efficient deployment]"
    },
    {
      "title": "On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio",
      "authors": "Ernest Fokoué",
      "institution": "Rochester Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22284",
      "code": null,
      "tags": [
        "ensemble learning",
        "Fibonacci weighting",
        "Rao-Blackwell optimization",
        "variance reduction",
        "recursive ensemble",
        "orthogonalization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp",
      "contributions": "1. Introduces Fibonacci Ensembles, a novel ensemble learning framework using normalized Fibonacci weights optimized via orthogonalization and Rao-Blackwellization for systematic variance reduction. 2. Proposes a second-order recursive ensemble dynamic inspired by the Fibonacci sequence to enhance representational depth beyond classical boosting. 3. Develops a General Weighting Theory that unifies various ensemble methods (bagging, boosting, stacking, etc.) under a single mathematical framework as distributional operators.",
      "summary": "This paper proposes Fibonacci Ensembles, a new ensemble learning method inspired by the Fibonacci sequence, which uses mathematically optimized Fibonacci weights and a recursive dynamic to reduce variance and improve model depth. Experimental results on one-dimensional regression show it can match or outperform uniform averaging and integrates effectively with orthogonal Rao-Blackwellization. The work suggests Fibonacci ensembles offer a natural and interpretable design within ensemble theory.",
      "mindmap": "graph TB\n        A[On Fibonacci Ensembles] --> B[核心问题/Problem: How to design a principled ensemble learning method inspired by natural harmony?]\n        A --> C[主要方法/Method: Use Fibonacci weights with orthogonalization/Rao-Blackwell optimization and recursive dynamics]\n        A --> D[关键结果/Results: Fibonacci weighting matches/improves uniform averaging and integrates with Rao-Blackwellization]"
    },
    {
      "title": "A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue",
      "authors": "Qianqian Qi, Peter G. M. van der Heijden",
      "institution": "Hangzhou Dianzi University, Utrecht University, University of Southampton",
      "link": "https://arxiv.org/pdf/2512.22282",
      "code": null,
      "tags": [
        "matrix factorization",
        "nonnegative matrix factorization",
        "identifiability",
        "latent class analysis",
        "probabilistic latent semantic analysis",
        "end-member analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp",
      "contributions": "1. Highlights the similarities among five popular matrix factorization models (LBA, LCA, EMA, PLSA, NMF) that are often presented separately across different fields. 2. Proves a unified identifiability condition, showing that the solution uniqueness for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. 3. Provides a brief review of algorithms for these models and illustrates their application with a social science time budget dataset.",
      "summary": "This paper reviews and unifies five nonnegative matrix factorization models (NMF, PLSA, LBA, EMA, LCA) from different disciplines, focusing on the identifiability issue. It proves that the uniqueness of solutions for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. The work clarifies model similarities, reviews algorithms, and demonstrates application with a real-world dataset.",
      "mindmap": "graph TB\n        A[论文标题: A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 跨领域非负矩阵分解模型的相似性与可识别性/Similarity and identifiability of cross-domain nonnegative matrix factorization models]\n        C[主要方法/Method: 理论分析与统一证明/Theoretical analysis and unified proof]\n        D[关键结果/Results: 证明LBA, EMA, LCA, PLSA解的唯一性与NMF解的唯一性等价/Proved solution uniqueness of LBA, EMA, LCA, PLSA is equivalent to uniqueness of NMF solution]"
    },
    {
      "title": "A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure",
      "authors": "Ernest Fokoué",
      "institution": "Rochester Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22286",
      "code": null,
      "tags": [
        "ensemble learning",
        "weighting theory",
        "spectral complexity",
        "approximation geometry",
        "bias-variance decomposition",
        "constrained quadratic program"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp",
      "contributions": "1. Develops a general weighting theory for ensembles that moves beyond variance reduction, formalizing ensembles as linear operators with geometric and spectral constraints. 2. Derives a refined bias-variance-approximation decomposition showing how structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity. 3. Provides a unified theoretical framework that subsumes classical averaging, stacking, and recent Fibonacci-based ensembles, showing optimal weights arise from constrained quadratic programs.",
      "summary": "This paper proposes a new theoretical framework for ensemble learning that explains its effectiveness beyond the traditional variance-reduction argument, particularly for stable base learners. The method formalizes ensembles as linear operators and shows how structured, non-uniform weighting can optimize performance by managing spectral complexity and approximation geometry. The main conclusion is that the principal role of aggregation for low-variance learners is the redistribution of spectral complexity, establishing a foundation for structure-driven ensemble design.",
      "mindmap": "graph TB\n        A[A General Weighting Theory for Ensemble Learning<br>集成学习的一般加权理论] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”传统方差缩减理论无法解释稳定基学习器的集成效果<br>Traditional variance reduction fails to explain ensembles of stable learners”]\n        C --> C1[”将集成形式化为具有几何与谱约束的线性算子<br>Formalize ensembles as linear operators with geometric/spectral constraints”]\n        C --> C2[”推导偏差-方差-近似分解<br>Derive bias-variance-approximation decomposition”]\n        D --> D1[”结构化加权方案在理论上优于均匀平均<br>Structured weighting provably dominates uniform averaging”]\n        D --> D2[”最优权重是约束二次规划的解<br>Optimal weights are solutions to constrained QPs”]\n        D --> D3[”统一理论涵盖经典平均、堆叠等<br>Unified theory subsumes averaging, stacking, etc.”]"
    },
    {
      "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Dream Sports, Columbia University",
      "link": "https://arxiv.org/pdf/2512.22473",
      "code": null,
      "tags": [
        "transformer interpretability",
        "cross-entropy",
        "gradient dynamics",
        "attention mechanism",
        "expectation-maximization",
        "Bayesian inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp",
      "contributions": "1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.",
      "summary": "This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.",
      "mindmap": "graph TB\n    A[Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Transformer内部几何结构如何形成?/How is transformer internal geometry formed?]\n    C --> C1[推导注意力梯度动态/Derive attention gradient dynamics]\n    C --> C2[建立EM算法类比/Establish EM algorithm analogy]\n    D --> D1[发现优势路由与责任更新/Discover advantage-based routing & responsibility-weighted update]\n    D --> D2[梯度流塑造贝叶斯流形/Gradient flow sculpts Bayesian manifolds]"
    },
    {
      "title": "Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites",
      "authors": "Behzad Parvaresh, Rahmat K. Adesunkanmi, Adel Alaeddini",
      "institution": "Southern Methodist University",
      "link": "https://arxiv.org/pdf/2512.22397",
      "code": null,
      "tags": [
        "multi-target regression",
        "squeeze-and-excitation blocks",
        "wide-and-deep neural networks",
        "Latin Hypercube Sampling",
        "SHAP analysis",
        "multi-input multi-target learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp",
      "contributions": "1. Introduced a data-efficient multi-input, multi-target learning approach integrating Latin Hypercube Sampling (LHS) with a squeeze-and-excitation wide and deep neural network (SE-WDNN) for predicting mechanical and manufacturing properties of additively manufactured fiber-reinforced composites. 2. Demonstrated superior performance of SE-WDNN over baseline models (e.g., feedforward neural networks, XGBoost) with the lowest overall test error (MAPE=12.33%) and statistically significant improvements for several target variables. 3. Provided interpretability through SHAP analysis, identifying reinforcement strategy as the major influence on mechanical performance, enabling guided parameter selection balancing mechanical behavior and manufacturing metrics.",
      "summary": "This paper addresses the challenge of predicting multiple properties in additively manufactured fiber-reinforced composites, where performance is sensitive to process and material parameters. The authors propose a sample-efficient method combining Latin Hypercube Sampling with a novel squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict mechanical and manufacturing properties. The model outperforms several baseline machine learning models, achieving the lowest test error, and SHAP analysis reveals that reinforcement strategy is the most influential factor, demonstrating the approach's effectiveness for interpretable, multi-target prediction in this domain.",
      "mindmap": "graph TB\n        Root(”Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites”) --> Problem(”核心问题/Problem: Exhaustive testing of CFRC-AM properties is impractical due to complex parameter interactions.”)\n        Root --> Method(”主要方法/Method: Integrate LHS-guided experimentation with a novel SE-WDNN model for multi-target prediction.”)\n        Root --> Results(”关键结果/Results: SE-WDNN achieves lowest test error (MAPE=12.33%); SHAP shows reinforcement strategy is key.”)"
    },
    {
      "title": "Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks",
      "authors": "Y. Sungtaek Ju",
      "institution": "University of California, Los Angeles",
      "link": "https://arxiv.org/pdf/2512.22426",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "sparse variational Gaussian processes",
        "Kolmogorov-Arnold networks",
        "flow reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp",
      "contributions": "1. Proposes a novel machine learning framework (SVGP-KAN) for uncertainty-aware flow field reconstruction, combining sparse variational Gaussian processes with Kolmogorov-Arnold network topology. 2. Enables principled epistemic uncertainty quantification, extending classical methods like Linear Stochastic Estimation (LSE) and Spectral Analysis Modal Methods (SAMM). 3. Provides a systematic evaluation demonstrating that the method achieves accuracy comparable to established techniques while offering well-calibrated uncertainty estimates that reliably indicate prediction quality.",
      "summary": "This paper introduces a machine learning framework called SVGP-KAN for reconstructing time-resolved flow fields from sparse measurements. The method combines sparse variational Gaussian processes with Kolmogorov-Arnold networks to provide accurate reconstructions along with principled uncertainty estimates. The results show the framework achieves comparable accuracy to classical methods while offering reliable uncertainty quantification, which is valuable for experimental design in periodic flows.",
      "mindmap": "graph TB\n        A[Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[从稀疏测量中重建瞬态流场/Reconstruct time-resolved flow fields from sparse measurements]\n        C --> C1[SVGP-KAN框架/SVGP-KAN framework]\n        C1 --> C2[稀疏变分高斯过程/Sparse Variational Gaussian Processes]\n        C1 --> C3[Kolmogorov-Arnold网络拓扑/Kolmogorov-Arnold Network Topology]\n        D --> D1[精度与经典方法相当/Accuracy comparable to established methods]\n        D --> D2[提供良好校准的不确定性估计/Provides well-calibrated uncertainty estimates]"
    },
    {
      "title": "Likelihood-Preserving Embeddings for Statistical Inference",
      "authors": "Deniz Akdemir",
      "institution": "Not explicitly provided; inferred from email domain as independent researcher or unspecified institution.",
      "link": "https://arxiv.org/pdf/2512.22638",
      "code": null,
      "tags": [
        "statistical inference",
        "likelihood-preserving embeddings",
        "likelihood-ratio distortion",
        "Hinge Theorem",
        "approximate sufficient statistics",
        "surrogate MLE"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp",
      "contributions": "1. Introduces the Likelihood-Ratio Distortion metric and the Hinge Theorem, establishing it as the necessary and sufficient condition for preserving likelihood-based inference. 2. Proves an impossibility result for universal likelihood preservation, motivating model-class-specific guarantees. 3. Provides a constructive framework using neural networks as approximate sufficient statistics with explicit bounds linking training loss to inferential guarantees.",
      "summary": "This paper addresses the problem that modern machine learning embeddings often destroy the geometric structure needed for classical likelihood-based statistical inference. It proposes a theory of likelihood-preserving embeddings, centered on controlling the Likelihood-Ratio Distortion, and proves that this control is necessary and sufficient to preserve tests, Bayes factors, and MLEs. The main conclusion is that with this framework, neural network embeddings can be made compatible with classical inference workflows under specific, provable conditions.",
      "mindmap": "graph TB\n        A[Likelihood-Preserving Embeddings for Statistical Inference] --> B[核心问题/Problem: ML embeddings destroy geometric structure for likelihood-based inference]\n        A --> C[主要方法/Method: Theory of likelihood-preserving embeddings using Likelihood-Ratio Distortion metric and Hinge Theorem]\n        A --> D[关键结果/Results: Controlling distortion preserves tests & MLEs; framework for neural networks as approximate sufficient statistics]"
    },
    {
      "title": "Machine learning models for predicting catastrophe bond coupons using climate data",
      "authors": "Julia Kończal, Michał Balcerek, Krzysztof Burnecki",
      "institution": "Wrocław University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22660",
      "code": null,
      "tags": [
        "financial machine learning",
        "catastrophe bonds",
        "climate indicators",
        "extremely randomized trees",
        "gradient boosting",
        "risk pricing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp",
      "contributions": "1. Proposes a novel integration of large-scale climate indicators (e.g., ONI, NAO, SSTs) into the prediction of catastrophe bond coupon rates. 2. Systematically compares the performance of linear regression against advanced tree-based ensemble methods (RF, GBM, ERT, XGBoost) for this financial prediction task. 3. Demonstrates that including climate variables improves predictive accuracy across all models, with Extremely Randomized Trees achieving the best performance, quantifying the influence of climate variability on CAT bond pricing.",
      "summary": "This paper investigates the use of machine learning models to predict catastrophe bond coupons by incorporating climate data. The authors combine traditional financial features with climate indicators and compare models like linear regression, random forest, and gradient boosting. The results show that climate variables improve prediction accuracy, with extremely randomized trees performing best, indicating that climate variability significantly impacts bond pricing.",
      "mindmap": "graph TB\n        A[Machine learning models for predicting catastrophe bond coupons using climate data] --> B(核心问题/Problem: 如何预测巨灾债券息票？/How to predict CAT bond coupons?)\n        A --> C(主要方法/Method: 结合气候指标与机器学习模型/Combine climate indicators with ML models)\n        A --> D(关键结果/Results: 气候变量提升预测精度，极端随机树表现最佳/Climate variables improve accuracy, Extremely Randomized Trees perform best)"
    },
    {
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "authors": "Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan",
      "institution": "Başkent University, Gaziantep University",
      "link": "https://arxiv.org/pdf/2512.22564",
      "code": null,
      "tags": [
        "medical audio classification",
        "Audio Spectrogram Transformer",
        "Sharpness-Aware Minimization",
        "ICBHI 2017",
        "class imbalance",
        "loss landscape"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp",
      "contributions": "1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.",
      "summary": "This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.",
      "mindmap": "graph TB\n        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]\n        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]\n    \n        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]\n        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]\n        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]\n    \n        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]\n        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]"
    },
    {
      "title": "Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference",
      "authors": "Kiarash Vaziri, Lucine L. Oganesian, HyeongChan Jo, Roberto M.C. Vera, Charles Y. Liu, Brian Lee, Maryam M. Shanechi",
      "institution": "University of Southern California",
      "link": "https://arxiv.org/pdf/2512.22785",
      "code": null,
      "tags": [
        "computational neuroscience",
        "DFINE",
        "state-space models",
        "intracranial EEG",
        "neural forecasting",
        "brain-computer interfaces"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp",
      "contributions": "1. Extended the DFINE framework, originally for intracortical recordings, to model multisite human intracranial EEG (iEEG) signals. 2. Demonstrated that DFINE significantly outperforms linear state-space models (LSSMs) and matches or exceeds the accuracy of GRU models in forecasting future neural activity. 3. Showed that DFINE handles missing observations more robustly than baseline models, highlighting its flexible inference capability for practical BCI applications.",
      "summary": "This paper extends the DFINE framework to model nonlinear dynamics in human intracranial EEG (iEEG) data. DFINE combines neural networks with a linear state-space model backbone to enable accurate neural forecasting and robust handling of missing data. The results show DFINE outperforms linear models, matches or beats GRU performance, and is particularly effective in high gamma bands, making it a promising tool for next-generation brain-computer interfaces.",
      "mindmap": "graph TB\n        Root(”Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem”) --> P1(”线性模型无法捕捉神经活动中的非线性结构/Linear models fail to capture nonlinear neural structure”)\n        Problem --> P2(”RNN模型不直接处理缺失观测值/RNN models do not directly handle missing observations”)\n        Method(”主要方法/Method”) --> M1(”扩展DFINE框架至iEEG建模/Extend DFINE framework to iEEG modeling”)\n        Method --> M2(”结合神经网络与线性状态空间模型/Integrate neural networks with linear state-space models”)\n        Results(”关键结果/Results”) --> R1(”DFINE显著优于线性状态空间模型/DFINE significantly outperforms LSSM”)\n        Results --> R2(”DFINE匹配或超过GRU的预测精度/DFINE matches or exceeds GRU accuracy”)\n        Results --> R3(”DFINE更鲁棒地处理缺失数据/DFINE handles missing observations more robustly”)"
    },
    {
      "title": "Causal-Policy Forest for End-to-End Policy Learning",
      "authors": "Masahiro Kato",
      "institution": "The University of Tokyo",
      "link": "https://arxiv.org/pdf/2512.22846",
      "code": null,
      "tags": [
        "causal inference",
        "policy learning",
        "causal forest",
        "conditional average treatment effect (CATE)",
        "end-to-end learning",
        "random forests"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp",
      "contributions": "1. Establishes an equivalence between maximizing policy value and minimizing MSE for CATE under a specific regression model, providing a theoretical foundation. 2. Proposes the causal-policy forest, a novel end-to-end algorithm that modifies the widely-used causal forest for direct policy learning. 3. Integrates policy training steps more tightly than prior methods, avoiding separate nuisance parameter estimation and improving computational efficiency.",
      "summary": "This paper proposes an end-to-end algorithm for policy learning in causal inference, called the causal-policy forest. It modifies the causal forest method by leveraging a theoretical equivalence between policy value maximization and CATE estimation. The method unifies policy training steps, is computationally efficient, and bridges the gap between policy learning and CATE estimation in practice.",
      "mindmap": "graph TB\n        Root[Causal-Policy Forest for End-to-End Policy Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Policy learning from observational data to recommend optimal treatments] --> Problem_Sub[目标/Goal: Maximize policy value]\n        Method[主要方法/Method: Causal-Policy Forest] --> Method_Sub1[基础/Foundation: Equivalence of policy value max and CATE MSE min]\n        Method --> Method_Sub2[算法/Algorithm: Modify causal forest for end-to-end policy learning]\n        Results[关键结果/Results: Three advantages] --> Results_Sub1[优势1/Advantage 1: Bridges policy learning and CATE estimation]\n        Results --> Results_Sub2[优势2/Advantage 2: More end-to-end training]\n        Results --> Results_Sub3[优势3/Advantage 3: Computationally efficient]"
    },
    {
      "title": "A first-order method for nonconvex-strongly-concave constrained minimax optimization",
      "authors": "Zhaosong Lu, Sanyou Mei",
      "institution": "University of Minnesota, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22909",
      "code": null,
      "tags": [
        "optimization theory",
        "minimax optimization",
        "augmented Lagrangian method",
        "first-order method",
        "operation complexity",
        "nonconvex-strongly-concave"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp",
      "contributions": "1. Proposes a first-order augmented Lagrangian method for solving nonconvex-strongly-concave constrained minimax problems. 2. Develops a new first-order method to solve the resulting unconstrained minimax subproblems by leveraging the strong concavity structure. 3. Establishes an improved operation complexity of O(ε^\\{-3.5\\} log ε^\\{-1\\}) for finding an ε-KKT solution, which is a factor of ε^\\{-0.5\\} better than the previous best-known result.",
      "summary": "This paper addresses the problem of nonconvex-strongly-concave constrained minimax optimization. The authors propose a novel first-order augmented Lagrangian method, where the subproblems are solved by a specially designed first-order algorithm. The main result is that their method achieves an improved operation complexity of O(ε^\\{-3.5\\} log ε^\\{-1\\}) for finding an approximate KKT solution.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: A first-order method for nonconvex-strongly-concave constrained minimax optimization] --> B(核心问题 / Problem: 非凸-强凹约束极小极大优化 / Nonconvex-strongly-concave constrained minimax optimization)\n        A --> C(主要方法 / Method: 一阶增广拉格朗日法 / First-order augmented Lagrangian method)\n        A --> D(关键结果 / Results: 操作复杂度 O(ε^{-3.5} log ε^{-1}) / Operation complexity O(ε^{-3.5} log ε^{-1}))"
    },
    {
      "title": "Deep Learning for the Multiple Optimal Stopping Problem",
      "authors": "Mathieu Laurière, Mehdi Talbi",
      "institution": "Shanghai Center for Data Science; NYU-ECNU Institute of Mathematical Sciences at NYU Shanghai; NYU Shanghai; Laboratoire de Probabilités, Statistiques et Modélisation, Université Paris-Cité",
      "link": "https://arxiv.org/pdf/2512.22961",
      "code": null,
      "tags": [
        "reinforcement learning / optimal stopping",
        "multiple optimal stopping",
        "dynamic programming principle",
        "neural network approximation",
        "high-dimensional problems",
        "American basket options"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp",
      "contributions": "1. Proposes a novel deep learning framework combining the Dynamic Programming Principle with neural networks to solve high-dimensional multiple optimal stopping problems. 2. Provides theoretical error analysis for both the discrete-time problem (neural network training error) and continuous problems (discretization error). 3. Demonstrates the method's efficiency and scalability through numerical experiments on high-dimensional American basket options and nonlinear utility maximization.",
      "summary": "This paper introduces a deep learning framework to solve the challenging multiple optimal stopping problem in high dimensions. The method combines the Dynamic Programming Principle with neural network approximation of the value function. Numerical experiments show it is an efficient and scalable solution for problems like pricing American basket options.",
      "mindmap": "graph TB\n        Root[Deep Learning for the Multiple Optimal Stopping Problem] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[高维多停止问题/High-dim Multiple Stopping]\n        Problem --> P2[复杂递归依赖/Complex Recursive Dependencies]\n        Method --> M1[动态规划原理/DPP]\n        Method --> M2[神经网络值函数近似/Neural Network Value Approximation]\n        Results --> R1[理论误差分析/Theoretical Error Analysis]\n        Results --> R2[高效可扩展方法/Efficient & Scalable Method]"
    },
    {
      "title": "Risk-Averse Learning with Varying Risk Levels",
      "authors": "Siyi Wang, Zifan Wang, Karl H. Johansson",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22986",
      "code": null,
      "tags": [
        "online convex optimization",
        "Conditional Value-at-Risk (CVaR)",
        "dynamic regret",
        "zeroth-order optimization",
        "risk-level variation",
        "first-order optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp",
      "contributions": "1. Introduces a novel risk-level variation metric to capture the dynamics of changing risk preferences in online optimization. 2. Develops risk-averse learning algorithms for both first-order and zeroth-order information settings under a limited sampling budget. 3. Provides dynamic regret bounds for the proposed algorithms, analyzing their performance in terms of function variation, risk-level variation, and sample count.",
      "summary": "This paper tackles risk-averse online optimization in non-stationary environments where both the cost functions and the desired risk level can change over time. The authors propose new algorithms for first-order and zeroth-order settings that use Conditional Value-at-Risk (CVaR) and analyze their dynamic regret, showing adaptability to changing conditions. Numerical experiments validate the effectiveness of the proposed methods.",
      "mindmap": "graph TB\n        A[Risk-Averse Learning with Varying Risk Levels<br>风险规避学习与变化的风险水平] --> B(Problem: Safety-critical decision-making in dynamic, risk-sensitive environments<br>核心问题: 动态风险敏感环境中的安全关键决策)\n        A --> C(Method: CVaR-based online algorithms for first-order & zeroth-order settings<br>主要方法: 基于CVaR的一阶和零阶在线算法)\n        A --> D(Results: Dynamic regret bounds and numerical validation<br>关键结果: 动态遗憾界与数值验证)"
    },
    {
      "title": "JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference",
      "authors": "Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev",
      "institution": "Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22999",
      "code": null,
      "tags": [
        "simulation-based inference",
        "Bayesian adaptive design",
        "amortized inference",
        "diffusion models",
        "sequential experimental design",
        "policy learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp",
      "contributions": "1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.",
      "summary": "This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.",
      "mindmap": "graph TB\n        Root[JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Actively optimizing design variables for parameter estimation] --> Problem_Sub[子问题/Sub-problem: Sequential design and inference are typically treated separately]\n        Method[主要方法/Method: Jointly amortize design and inference via end-to-end training] --> Method_Sub1[网络/Networks: Policy, History, and Inference (Diffusion-based) networks]\n        Method --> Method_Sub2[损失函数/Loss: Aggregates incremental posterior error reduction]\n        Results[关键结果/Results: Superior/competitive performance on standard benchmarks]"
    },
    {
      "title": "Deep Learning for Art Market Valuation",
      "authors": "Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang",
      "institution": "Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy",
      "link": "https://arxiv.org/pdf/2512.23078",
      "code": null,
      "tags": [
        "multi-modal learning",
        "multi-modal deep learning",
        "visual embeddings",
        "Grad-CAM",
        "hedonic regression",
        "repeated-sales dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp",
      "contributions": "1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.",
      "summary": "This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.",
      "mindmap": "graph TB\n        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]\n        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]\n        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]"
    },
    {
      "title": "Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity",
      "authors": "Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell",
      "institution": "Åbo Akademi University",
      "link": "https://arxiv.org/pdf/2512.23071",
      "code": null,
      "tags": [
        "federated learning",
        "L0 regularization",
        "probabilistic gates",
        "communication efficiency",
        "model sparsity",
        "federated stochastic gradient descent"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp",
      "contributions": "1. Proposes a novel federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and their continuous relaxation to achieve target sparsity. 2. Derives the L0 constrained stochastic minimization objective from an entropy maximization problem of the stochastic gates. 3. Demonstrates that the method can achieve high target sparsity (down to ρ=0.005) under data and client heterogeneity with minimal loss in statistical performance, outperforming magnitude pruning-based methods.",
      "summary": "The paper addresses the problem of poor generalizability and communication inefficiency in Federated Learning due to overly dense models. It proposes a method to enforce L0 sparsity constraints via probabilistic gates, deriving the objective from entropy maximization and implementing it with federated stochastic gradient descent. The method is shown to be communication-efficient and achieves high target sparsity with better statistical performance than pruning-based baselines on synthetic and real datasets.",
      "mindmap": "graph TB\n        Root[Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 数据与模型固有的稀疏性未被解决，导致模型过密、泛化性差，且存在数据和客户端参与异质性。]\n        Method[主要方法/Method: 通过概率门及其连续松弛对非零参数密度施加L0约束，目标源自随机门的熵最大化问题，并基于联邦随机梯度下降。]\n        Results[关键结果/Results: 在数据和客户端异质性下，能达到目标密度(ρ)，统计性能损失最小，且比基于幅度的剪枝方法更优、通信高效。]"
    },
    {
      "title": "QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants",
      "authors": "Tim C. Pearce, Ahmed Ibrahim",
      "institution": "University of Leicester, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.23080",
      "code": null,
      "tags": [
        "generative models",
        "variational autoencoder",
        "QSAR",
        "molecular generation",
        "Fréchet ChemNet Distance",
        "retrosynthesis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp",
      "contributions": "1. A novel generative AI framework combining a VAE with a QSAR model to design novel odorant molecules from limited training data., 2. Demonstration of effective latent space structuring for odor likelihood, enabling exploration of novel chemical scaffolds beyond simple derivatization., 3. Comprehensive validation showing generated molecules are syntactically valid, unique, thermodynamically stable, and synthetically viable.",
      "summary": "This paper presents a generative AI framework that combines a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to design novel odorant molecules. The method structures the VAE's latent space based on odor probability, enabling the generation of valid, unique, and synthetically viable candidate molecules from a limited training set. The results show the model successfully explores novel chemical space, producing stable candidates with practical synthesis routes.",
      "mindmap": "graph TB\n        Root[”QSAR-Guided Generative Framework for Odorant Discovery<br>基于QSAR引导的生成式气味分子发现框架”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>Challenging to discover novel odorants from vast chemical space with limited data<br>从巨大化学空间中利用有限数据发现新气味分子具有挑战性”]\n        Method[”主要方法/Method<br>VAE + QSAR model for de novo molecular design<br>使用VAE与QSAR模型进行从头分子设计”]\n        Results[”关键结果/Results<br>Generates valid, unique, novel, stable, and synthetically viable odorant candidates<br>生成有效、独特、新颖、稳定且可合成的气味分子候选物”]"
    },
    {
      "title": "Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN",
      "authors": "Yuan-Sen Ting",
      "institution": "The Ohio State University, Max-Planck-Institut für Astronomie",
      "link": "https://arxiv.org/pdf/2512.23138",
      "code": "https://github.com/tingyuansen/LatentNN",
      "tags": [
        "statistical machine learning",
        "attenuation bias",
        "latent variable",
        "neural networks",
        "measurement error",
        "joint likelihood"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp",
      "contributions": "1. Demonstrates that neural networks suffer from attenuation bias, a systematic underestimation of extreme values due to input measurement errors. 2. Proposes LatentNN, a method that generalizes the latent variable solution from linear regression to neural networks by jointly optimizing network parameters and latent input values. 3. Validates the method's effectiveness in reducing bias across various scenarios, including low signal-to-noise astronomical data, and defines its effective operational regime.",
      "summary": "The paper addresses the problem of attenuation bias, where neural networks systematically underestimate extreme values due to noisy input measurements. It introduces LatentNN, a method that treats true inputs as latent variables and jointly optimizes them with the network parameters by maximizing the joint data likelihood. The results show that LatentNN effectively reduces this bias, especially in the low signal-to-noise regimes common in astronomy.",
      "mindmap": "graph TB\n        A[Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[神经网络存在衰减偏差 / Neural networks suffer from attenuation bias]\n        B1 --> B2[输入测量误差导致极端值被低估 / Input measurement errors cause underestimation of extreme values]\n        C --> C1[提出LatentNN方法 / Propose LatentNN method]\n        C1 --> C2[将真实输入作为潜变量 / Treat true inputs as latent variables]\n        C2 --> C3[联合优化网络参数和潜变量 / Jointly optimize network parameters and latent values]\n        D --> D1[有效减少衰减偏差 / Effectively reduces attenuation bias]\n        D1 --> D2[在低信噪比天文数据中表现良好 / Performs well in low-SNR astronomical data]"
    },
    {
      "title": "An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making",
      "authors": "Wendyam Eric Lionel Ilboudo, Saori C Tanaka",
      "institution": "Nara Institute of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23144",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Kullback-Leibler divergence",
        "decision paralysis",
        "intent selection",
        "affordance selection",
        "hierarchical decision process"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp",
      "contributions": "1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.",
      "summary": "This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.",
      "mindmap": "graph TB\n        Root[An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Decision Paralysis] --> P1[挑战/Challenge<br>Choice models assume ready-to-compare options]\n        Problem --> P2[现象/Phenomenon<br>Hesitation, freezing, failure to act]\n        Method[主要方法/Method<br>Computational Account] --> M1[架构/Architecture<br>Hierarchical decision process]\n        Method --> M2[形式化/Formalization<br>Intent vs. Affordance selection]\n        Method --> M3[目标/Objective<br>Mixture of reverse-KL & forward-KL]\n        Results[关键结果/Results<br>Simulation Outcomes] --> R1[行为/Behavior<br>Slow, heavy-tailed response times]\n        Results --> R2[失败模式/Failure Modes<br>Intent & Affordance saturation]\n        Results --> R3[解释/Interpretation<br>Autism as an extreme regime]"
    },
    {
      "title": "Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis",
      "authors": "Zijian Liu",
      "institution": "New York University",
      "link": "https://arxiv.org/pdf/2512.23178",
      "code": null,
      "tags": [
        "stochastic optimization",
        "gradient clipping",
        "heavy-tailed noise",
        "nonsmooth convex optimization",
        "convergence analysis",
        "Freedman's inequality"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp",
      "contributions": "1. Provided a refined analysis for Clipped SGD under heavy-tailed noise, achieving faster high-probability convergence rates that depend on a novel \"generalized effective dimension\" term. 2. Extended the refined analysis to convergence in expectation, obtaining new rates that break previously known lower bounds and proving their optimality by matching newly established lower bounds. 3. Established new lower bounds for both high-probability and in-expectation convergence, completing the theoretical landscape and confirming the optimality of the new in-expectation rates.",
      "summary": "This paper refines the theoretical analysis of Clipped Stochastic Gradient Descent (Clipped SGD) for nonsmooth convex optimization under heavy-tailed gradient noise. By improving the use of Freedman's inequality and providing finer bounds for clipping error, the authors derive faster high-probability convergence rates and new optimal in-expectation rates that surpass previous lower bounds. The work also establishes matching lower bounds, demonstrating the optimality of the proposed analysis for convergence in expectation.",
      "mindmap": "graph TB\n    A[Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Heavy-tailed gradient noise with bounded p-th moment (p∈(1,2]) / 具有有界p阶矩的重尾梯度噪声]\n    C --> C1[Refined analysis of Clipped SGD / 对Clipped SGD的精细分析]\n    C1 --> C2[Better use of Freedman's inequality / 更好地利用Freedman不等式]\n    C1 --> C3[Finer bounds for clipping error / 对裁剪误差的更精细边界]\n    D --> D1[Faster high-probability rates with generalized effective dimension / 具有广义有效维度的更快高概率收敛率]\n    D --> D2[New optimal in-expectation rates breaking known lower bounds / 打破已知下界的新最优期望收敛率]\n    D --> D3[New matching lower bounds established / 建立了新的匹配下界]"
    },
    {
      "title": "Persistent Homology via Finite Topological Spaces",
      "authors": "Selçuk Kayacan",
      "institution": "Bahçeşehir University",
      "link": "https://arxiv.org/pdf/2512.23348",
      "code": null,
      "tags": [
        "topological data analysis",
        "persistent homology",
        "finite topological spaces",
        "posets",
        "crosscut complexes",
        "stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp",
      "contributions": "1. Proposes a functorial framework for persistent homology using filtrations of finite topological spaces and posets, bypassing the need for inclusion relations between simplicial complexes. 2. Demonstrates that standard simplifications at the poset level preserve persistent invariants. 3. Proves the stability of the resulting persistence diagrams under metric perturbations in a density-based instantiation.",
      "summary": "The paper proposes a new functorial framework for persistent homology that starts from a finite metric space and constructs a filtration of finite topological spaces, then functorially maps these to posets and simplicial complexes via crosscut constructions. This approach decouples the metric from the homological analysis and does not require inclusion maps between complexes. The authors show that poset-level simplifications preserve persistent invariants and prove the stability of the resulting persistence diagrams.",
      "mindmap": "graph TB\n        A[Persistent Homology via Finite Topological Spaces] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Standard persistent homology relies on inclusion-based filtrations of simplicial complexes.]\n        C[主要方法/Method: A functorial framework using filtrations of finite topological spaces and posets, mapped to complexes via crosscut constructions.]\n        D[关键结果/Results: Poset-level simplifications preserve invariants; persistence diagrams are stable under metric perturbations.]"
    },
    {
      "title": "Probabilistic Modelling is Sufficient for Causal Inference",
      "authors": "Bruno Mlodozeniec, David Krueger, Richard E. Turner",
      "institution": "University of Cambridge, Max Planck Institute for Intelligent Systems, MILA",
      "link": "https://arxiv.org/pdf/2512.23408",
      "code": null,
      "tags": [
        "causal inference",
        "probabilistic modelling",
        "causal inference",
        "do-operator",
        "structural causal models",
        "Bayesian networks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp",
      "contributions": "1. Demonstrates that standard probabilistic modelling is sufficient for answering causal inference questions without requiring special causal frameworks. 2. Provides concrete examples showing how causal problems (interventional and counterfactual) can be solved by \"writing down the probability of everything\". 3. Reinterprets established causal tools (e.g., do-operator, do-calculus) as \"syntactic sugar\" emerging from standard probabilistic modelling, clarifying their utility.",
      "summary": "This paper argues that causal inference questions can be fully addressed using standard probabilistic modelling and inference, without needing specialized causal tools or notation. The core method is to \"write down the probability of everything\" to model and solve both interventional and counterfactual problems. The authors conclude that causal-specific frameworks are not fundamentally necessary but can be seen as convenient abstractions built upon probabilistic foundations.",
      "mindmap": "graph TB\n        A[Probabilistic Modelling is Sufficient for Causal Inference] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Confusion over tools for causal inference in ML] --> B1[声称需要专门因果框架/Claims need for bespoke causal frameworks]\n        C[主要方法/Method<br>Write down the probability of everything] --> C1[使用概率建模解决因果问题/Solve causal questions with probabilistic modelling]\n        D[关键结果/Results<br>Causal tools are syntactic sugar] --> D1[因果工具源于概率建模/Causal tools emerge from probabilistic modelling]"
    },
    {
      "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
      "authors": "Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua",
      "institution": "Shanghai Jiao Tong University, StepFun, FinStep",
      "link": "https://arxiv.org/pdf/2512.23515",
      "code": "https://github.com/FinStep-AI/Alpha-R1",
      "tags": [
        "reinforcement learning",
        "alpha screening",
        "large language models",
        "reinforcement learning",
        "factor investing",
        "economic reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp",
      "contributions": "1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.",
      "summary": "The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.",
      "mindmap": "graph TB\n        A[Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning] --> B[核心问题/Problem: Signal decay and regime shifts in non-stationary markets; existing methods overlook semantic rationale for factor relevance.]\n        A --> C[主要方法/Method: Alpha-R1, an 8B-parameter LLM trained via RL, reasons over factor logic and real-time news for context-aware alpha screening.]\n        A --> D[关键结果/Results: Outperforms benchmark strategies; exhibits improved robustness to alpha decay across multiple asset pools.]"
    },
    {
      "title": "Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains",
      "authors": "Jiada Huang, Hao Ma, Zhibin Shen, Yizhou Qiao, Haiyang Li",
      "institution": "National University of Defense Technology, Zhengzhou University of Aeronautics",
      "link": "https://arxiv.org/pdf/2512.23443",
      "code": null,
      "tags": [
        "graph neural networks",
        "graph U-Net",
        "adaptive pooling",
        "feature fusion",
        "strain field prediction",
        "solid rocket motor"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp",
      "contributions": "1. Proposes GrainGNet, an adaptive graph network with an adaptive pooling dynamic node selection mechanism to preserve key mechanical features in critical structural regions. 2. Utilizes feature fusion to transmit deep features and enhance the model's representational capacity for 3D strain field prediction. 3. Demonstrates significant performance improvements, including a 62.8% reduction in mean squared error and a sevenfold training efficiency gain over a baseline graph U-Net, with particular accuracy in high-strain regions.",
      "summary": "This paper proposes GrainGNet, an adaptive fusion graph network, to predict the 3D strain field in solid rocket motor grains, addressing the computational expense of traditional simulations. The model uses adaptive pooling and feature fusion to accurately capture high-strain regions. It achieves a 62.8% reduction in mean squared error and improved training efficiency compared to baseline methods, offering a high-fidelity approach for structural safety evaluation.",
      "mindmap": "graph TB\n    A[Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains] --> B[核心问题/Problem: Local high strain causes structural failure; traditional simulations are expensive; surrogate models lack geometric accuracy.]\n    A --> C[主要方法/Method: Proposes GrainGNet with adaptive pooling and feature fusion to preserve key features and enhance representation.]\n    A --> D[关键结果/Results: 62.8% MSE reduction vs. baseline; 7x training efficiency; 33% error reduction in high-strain regions.]"
    },
    {
      "title": "A general framework for deep learning",
      "authors": "William Kengne, Modou Wade",
      "institution": "Université Jean Monnet, CY Cergy Paris Université",
      "link": "https://arxiv.org/pdf/2512.23425",
      "code": null,
      "tags": [
        "statistical learning theory",
        "deep neural networks",
        "Bernstein-type inequality",
        "excess risk bound",
        "minimax optimality",
        "mixing processes"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp",
      "contributions": "1. Proposes a general theoretical framework for deep learning that unifies analysis for data satisfying a generalized Bernstein-type inequality, encompassing independent and various dependent (mixing) observations. 2. Introduces two novel estimators: a Non-Penalized Deep Neural Network (NPDNN) and a Sparse-Penalized Deep Neural Network (SPDNN) estimator. 3. Establishes minimax optimal (up to logarithmic factors) convergence rates for the expected excess risk of both estimators on Hölder smooth and composition function classes.",
      "summary": "This paper develops a general theoretical framework for analyzing deep neural network estimators in settings including nonparametric regression and classification. It proposes two estimators (NPDNN and SPDNN) and derives upper bounds for their expected excess risk for data satisfying a generalized Bernstein-type inequality, covering independent and various dependent data processes. The main conclusion is that both proposed estimators achieve minimax optimal convergence rates in many classical settings.",
      "mindmap": "graph TB\n        A[A general framework for deep learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[统一理论分析框架/Unified theoretical analysis framework]\n        B1 --> B2[数据满足广义Bernstein不等式/Data satisfies generalized Bernstein inequality]\n        B2 --> B3[包含独立与混合观测/Includes independent and mixing observations]\n        C --> C1[提出两种估计器/Propose two estimators]\n        C1 --> C2[非惩罚深度神经网络/NPDNN]\n        C1 --> C3[稀疏惩罚深度神经网络/SPDNN]\n        D --> D1[推导期望超额风险上界/Establish upper bounds for expected excess risk]\n        D1 --> D2[Hölder函数类/Hölder function classes]\n        D --> D3[证明极小极大最优性/Prove minimax optimality]\n        D3 --> D4[多种经典设置/Many classical settings]"
    },
    {
      "title": "From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints",
      "authors": "Dimitra Maoutsa",
      "institution": "Technical University of Berlin",
      "link": "https://arxiv.org/pdf/2512.23566",
      "code": null,
      "tags": [
        "stochastic system identification",
        "overdamped Langevin dynamics",
        "sparse observations",
        "geometric constraints",
        "stochastic control",
        "path augmentation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp",
      "contributions": "1. A new framework that reconciles geometric and temporal perspectives by reformulating inference as a stochastic control problem. 2. A method using geometry-driven path augmentation, guided by the system's invariant density, to reconstruct trajectories without assuming specific parametric models. 3. Demonstrating accurate recovery of stochastic dynamics from extremely undersampled data, outperforming existing methods in synthetic benchmarks.",
      "summary": "This paper addresses the problem of learning stochastic dynamics from sparse temporal observations. It proposes a new framework that uses geometry-driven path augmentation within a stochastic control formulation to infer the underlying laws without parametric assumptions. The method successfully recovers overdamped Langevin dynamics from highly undersampled data, outperforming existing approaches.",
      "mindmap": "graph TB\n        A[From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints] --> B(核心问题/Problem: How to learn stochastic dynamics from sparse observations?);\n        A --> C(主要方法/Method: Reformulate inference as a stochastic control problem with geometry-driven path augmentation);\n        A --> D(关键结果/Results: Accurately recovers dynamics from undersampled data, outperforms existing methods);"
    },
    {
      "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction",
      "authors": "Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou",
      "institution": "Columbia University, Stanford University",
      "link": "https://arxiv.org/pdf/2512.23596",
      "code": null,
      "tags": [
        "financial machine learning",
        "non-stationarity",
        "model selection",
        "adaptive window selection",
        "return prediction",
        "tournament procedure"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp",
      "contributions": "1. Identifies and formalizes the nonstationarity-complexity tradeoff in return prediction, where complex models reduce misspecification but require longer, more non-stationary training windows. 2. Proposes a novel model selection method that jointly optimizes model class and training window size using an adaptive tournament procedure evaluated on non-stationary validation data. 3. Provides theoretical analysis showing the method balances misspecification error, estimation variance, and non-stationarity, and demonstrates its empirical superiority with significant performance gains in out-of-sample prediction and trading strategy returns, especially during recessions.",
      "summary": "This paper addresses the challenge of predicting stock returns in non-stationary environments by identifying a tradeoff between model complexity and non-stationarity. It proposes a new model selection method that jointly chooses the model and its training window via an adaptive tournament. The method outperforms standard benchmarks, particularly during economic recessions, and generates higher cumulative returns for a trading strategy.",
      "mindmap": "graph TB\n        A[The Nonstationarity-Complexity Tradeoff in Return Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[预测中的非平稳性-复杂性权衡/Nonstationarity-Complexity Tradeoff in Prediction]\n        C --> C1[联合优化模型与窗口的锦标赛方法/Tournament Method for Joint Model & Window Selection]\n        D --> D1[OOS R²提升 & 衰退期表现优异/Improved OOS R² & Superior Recession Performance]"
    },
    {
      "title": "Calibrated Multi-Level Quantile Forecasting",
      "authors": "Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani",
      "institution": "University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.23671",
      "code": null,
      "tags": [
        "online learning",
        "forecasting",
        "quantile forecasting",
        "calibration",
        "online learning",
        "adversarial robustness",
        "no-regret guarantee"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp",
      "contributions": "1. Introduces Multi-Level Quantile Tracker (MultiQT), a lightweight online method that wraps any existing forecaster to guarantee multi-level quantile calibration against adversarial distribution shifts. 2. Provides theoretical guarantees including calibration and a no-regret property ensuring asymptotic performance is not worse than the base forecaster. 3. Ensures the corrected forecasts are properly ordered across different quantile levels.",
      "summary": "The paper addresses the problem of producing reliable multi-level quantile forecasts that are calibrated. It proposes MultiQT, an online wrapper method that guarantees calibration and proper ordering of forecasts even under adversarial conditions, without asymptotically worsening the base forecaster's performance. Experiments show it significantly improves calibration in epidemic and energy forecasting tasks.",
      "mindmap": "graph TB\n        Root[Calibrated Multi-Level Quantile Forecasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Need for reliable, calibrated multi-level quantile forecasts] --> P1[校准保证/Calibration Guarantee]\n        Problem --> P2[顺序性/Ordering Constraint]\n        Method[主要方法/Method: Multi-Level Quantile Tracker (MultiQT)] --> M1[在线包装器/Online Wrapper]\n        Method --> M2[对抗性鲁棒/Adversarially Robust]\n        Method --> M3[无遗憾保证/No-Regret Guarantee]\n        Results[关键结果/Results] --> R1[显著改进校准/Significantly Improves Calibration]\n        Results --> R2[实证验证/Empirical Validation in Real Problems]"
    },
    {
      "title": "Bellman Calibration for V-Learning in Offline Reinforcement Learning",
      "authors": "Lars van der Laan, Nathan Kallus",
      "institution": "University of Washington, Netflix, Cornell University",
      "link": "https://arxiv.org/pdf/2512.23694",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Bellman calibration",
        "off-policy evaluation",
        "value iteration",
        "doubly robust estimator",
        "Markov decision process"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp",
      "contributions": "1. Introduces Iterated Bellman Calibration, a model-agnostic, post-hoc procedure for calibrating value predictions in infinite-horizon MDPs. 2. Adapts classical calibration methods (histogram, isotonic) to the dynamic, counterfactual setting using a doubly robust pseudo-outcome for off-policy data. 3. Provides finite-sample guarantees for calibration and prediction without requiring Bellman completeness or realizability.",
      "summary": "This paper introduces Iterated Bellman Calibration, a post-hoc method to improve the accuracy of long-term value predictions in offline reinforcement learning. The method repeatedly regresses fitted Bellman targets onto a model's predictions, adapting classical calibration techniques to handle off-policy data. The analysis shows the approach provides finite-sample guarantees for calibrated predictions under weak assumptions, without needing Bellman completeness.",
      "mindmap": "graph TB\n        A[Bellman Calibration for V-Learning<br/>Bellman校准用于V学习] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[预测长期回报的校准<br/>Calibrating long-term value predictions]\n        B --> B2[离线、反事实数据<br/>Offline, counterfactual data]\n        C --> C1[迭代贝尔曼校准<br/>Iterated Bellman Calibration]\n        C --> C2[使用双稳健伪结果<br/>Using doubly robust pseudo-outcome]\n        C --> C3[直方图/保序回归适配<br/>Adapting histogram/isotonic regression]\n        D --> D1[有限样本保证<br/>Finite-sample guarantees]\n        D --> D2[无需贝尔曼完备性<br/>No Bellman completeness required]"
    },
    {
      "title": "INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer",
      "authors": "Piotr Keller, Mark Eastwood, Zedong Hu, Aimée Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, André D'Hoore, Cédric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas",
      "institution": "KU Leuven, University of Oxford, University of Cambridge, University of Manchester, University of Bristol, University of Edinburgh, University of Glasgow, University of Sheffield, University of Southampton, University of Warwick, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strath",
      "link": "https://arxiv.org/pdf/2512.22262",
      "code": null,
      "tags": [
        "computational pathology",
        "graph neural network",
        "survival prediction",
        "spatial transcriptomics",
        "colorectal cancer",
        "histology"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp",
      "contributions": "",
      "summary": "",
      "mindmap": ""
    },
    {
      "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum",
      "authors": "Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda",
      "institution": "Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.",
      "link": "https://arxiv.org/pdf/2512.21340",
      "code": null,
      "tags": [
        "on-device ai",
        "data spaces",
        "cloud-edge continuum",
        "containerized microservices",
        "edge AI",
        "intelligent infrastructure monitoring"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp",
      "contributions": "1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.",
      "summary": "This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.",
      "mindmap": "graph TB\n        A[Harnessing Data Spaces for Smart City Infrastructures] --> B[核心问题/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]\n        A --> C[主要方法/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]\n        A --> D[关键结果/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]"
    },
    {
      "title": "Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems",
      "authors": "Haaris Mian",
      "institution": "Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)",
      "link": "https://arxiv.org/pdf/2512.21349",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "physics-informed neural networks",
        "Floquet-Bloch eigenvalue problem",
        "honeycomb lattice",
        "band structure",
        "transfer learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp",
      "contributions": "1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schrödinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology.",
      "summary": "This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning.",
      "mindmap": "graph TB\n        A[Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[求解二维周期势中的Floquet-Bloch本征值问题/Solve Floquet-Bloch eigenvalue problem in 2D periodic potentials]\n        B --> B2[关注石墨烯等材料的蜂窝晶格和能带拓扑/Focus on honeycomb lattice & band topology for materials like graphene]\n        C --> C1[使用神经网络同时学习布洛赫函数和本征值/Use neural networks to learn Bloch functions & eigenvalues]\n        C --> C2[通过复合损失函数强制执行薛定谔方程和周期性/Enforce Schrödinger eq. & periodicity via composite loss]\n        C --> C3[在布里渊区训练并探索迁移学习/Train over Brillouin zone & explore transfer learning]\n        D --> D1[数值验证与传统方法一致/Numerical validation against traditional methods]\n        D --> D2[迁移学习捕捉能带拓扑变化/Transfer learning captures changes in band topology]"
    },
    {
      "title": "A Reinforcement Learning Approach to Synthetic Data Generation",
      "authors": "Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin",
      "institution": "Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis",
      "link": "https://arxiv.org/pdf/2512.21395",
      "code": null,
      "tags": [
        "reinforcement learning",
        "synthetic data generation",
        "reinforcement learning",
        "proximal policy optimization",
        "privacy",
        "biomedical data"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp",
      "contributions": "1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets.",
      "summary": "This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing.",
      "mindmap": "graph TB\n        A[A Reinforcement Learning Approach to Synthetic Data Generation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: State-of-the-art generative models need large datasets and complex training, limiting use in small-sample settings.]\n        C[主要方法/Method: Reframe SDG as RL; introduce RLSyn (stochastic policy optimized via PPO with discriminator rewards).]\n        D[关键结果/Results: RLSyn performs comparably to/better than GANs & diffusion models, especially on smaller datasets.]"
    },
    {
      "title": "Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme",
      "authors": "Doğukan Özbayrak, Ahmed Hareedy",
      "institution": "Middle East Technical University",
      "link": "https://arxiv.org/pdf/2512.21396",
      "code": null,
      "tags": [
        "storage systems",
        "constrained coding",
        "LOCO codes",
        "linear programming",
        "code reconfiguration",
        "two-dimensional magnetic recording (TDMR)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp",
      "contributions": "1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems.",
      "summary": "This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems.",
      "mindmap": "graph TB\n        Root(”Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”设备老化需要不同等级的数据保护/Device aging requires different levels of data protection”)\n        Problem --> P2(”基于预定时间戳的重配置忽略实际状态/Reconfiguration based on timestamps neglects actual device status”)\n        Method --> M1(”提出离线和在线学习方法/Propose offline and online learning methods”)\n        Method --> M2(”将训练数据拟合为多项式方程/Fit training data to polynomial equations”)\n        Method --> M3(”将重配置建模为线性规划问题/Model reconfiguration as a linear programming problem”)\n        Results --> R1(”解决方案是全局最优的/Solution is globally optimal”)\n        Results --> R2(”实验证明在TDMR系统中有效/Experiments demonstrate effectiveness in TDMR systems”)"
    },
    {
      "title": "kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning",
      "authors": "Giacomo Turri, Grégoire Pacreau, Giacomo Meanti, Timothée Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli",
      "institution": "Italian Institute of Technology, École Polytechnique, Inria, University College London",
      "link": "https://arxiv.org/pdf/2512.21409",
      "code": "https://github.com/Machine-Learning-Dynamical-Systems/kooplearn",
      "tags": [
        "dynamical systems learning",
        "Koopman operator",
        "transfer operator",
        "spectral decomposition",
        "scikit-learn API",
        "reduced-order models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp",
      "contributions": "1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison.",
      "summary": "The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling.",
      "mindmap": "graph TB\n        Root[kooplearn: A Scikit-Learn Compatible Library] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Need for a unified, easy-to-use library for learning evolution operators from dynamical systems data] --> Problem_Sub1[应用/Application: Analyze systems, forecast, reduce model dimension]\n        Method[主要方法/Method: Implement linear, kernel, and deep-learning estimators] --> Method_Sub1[接口/Interface: Scikit-learn compatible API]\n        Method --> Method_Sub2[模型/Model: Discrete-time (Koopman/Transfer) & continuous-time operators]\n        Results[关键结果/Results: kooplearn library released] --> Results_Sub1[特性/Features: Includes benchmark datasets for fair comparison]\n        Results --> Results_Sub2[目标/Goal: Facilitate integration, experimentation, and reproducibility]"
    },
    {
      "title": "A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding",
      "authors": "Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli",
      "institution": "California Institute of Technology, Stanford University",
      "link": "https://arxiv.org/pdf/2512.21414",
      "code": "https://github.com/christinaliu2020/tool-bottleneck-framework",
      "tags": [
        "medical image analysis",
        "tool-use framework",
        "vision-language model",
        "interpretability",
        "data-efficiency",
        "tool bottleneck model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",
      "contributions": "1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.",
      "summary": "This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.",
      "mindmap": "graph TB\n        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]\n        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]"
    },
    {
      "title": "A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning",
      "authors": "Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng",
      "institution": "Illinois Institute of Technology, California State University, San Bernardino",
      "link": "https://arxiv.org/pdf/2512.21412",
      "code": null,
      "tags": [
        "wireless networking",
        "Age of Information (AoI)",
        "reinforcement learning",
        "freshness optimization",
        "wireless networks",
        "multi-agent systems"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp",
      "contributions": "1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization.",
      "summary": "This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research.",
      "mindmap": "graph TB\n        A[A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有综述的不足: 经典AoI与泛化RL研究分离 / Gap: Classical AoI vs. Broad RL]\n        C --> C1[提出以AoI为中心的RL综述框架 / Propose AoI-centric RL Survey Framework]\n        C --> C2[构建AoI变体分类与策略中心分类法 / Build AoI Variant & Policy-Centric Taxonomies]\n        D --> D1[为B5G/6G建立学习式新鲜度优化的统一基础 / Establish Unified Foundation for Learning-based Freshness Optimization]\n        D --> D2[识别开放挑战: 延迟决策、随机性、跨层设计 / Identify Open Challenges: Delayed Decisions, Stochasticity, Cross-layer]"
    },
    {
      "title": "Scalable Deep Subspace Clustering Network",
      "authors": "Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami",
      "institution": "University of Quebec at Montreal",
      "link": "https://arxiv.org/pdf/2512.21434",
      "code": null,
      "tags": [
        "subspace clustering",
        "landmark-based approximation",
        "self-expression",
        "spectral clustering",
        "linear complexity",
        "convolutional auto-encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",
      "contributions": "1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.",
      "summary": "The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Deep Subspace Clustering Network”] --> Problem[”核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity”]\n        Root --> Method[”主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization”]\n        Root --> Results[”关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance”]"
    },
    {
      "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors",
      "authors": "Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen",
      "institution": "University of Texas at Dallas",
      "link": "https://arxiv.org/pdf/2512.21431",
      "code": null,
      "tags": [
        "software testing",
        "runtime error detection",
        "coverage-guided testing",
        "multi-agent reasoning",
        "large language models",
        "static analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp",
      "contributions": "1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.",
      "summary": "The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.",
      "mindmap": "graph TB\n        A[Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors] --> B(核心问题/Problem: Detecting runtime errors in code snippets without execution is crucial for software safety.)\n        A --> C(主要方法/Method: Uses LLMs for execution-free, coverage-guided testing with a two-phase feedback loop.)\n        A --> D(关键结果/Results: Outperforms conventional and learning-based frameworks by generating high-coverage tests and finding more errors.)"
    },
    {
      "title": "DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction",
      "authors": "Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello",
      "institution": "University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21433",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "lossy compression",
        "quality prediction",
        "deep-surrogate",
        "mixture-of-experts",
        "feature-extraction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",
      "contributions": "1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.",
      "summary": "This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.",
      "mindmap": "graph TB\n        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]\n        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]\n        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]\n        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]\n        D --> D2[显著优于现有方法/Significantly outperforms existing methods]"
    },
    {
      "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models",
      "authors": "Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi",
      "institution": "Institute of Intelligent Systems and Robotics, Sorbonne University",
      "link": "https://arxiv.org/pdf/2512.21439",
      "code": null,
      "tags": [
        "computational ethics",
        "moral context",
        "probabilistic clustering",
        "LLM semantics",
        "interpretable prediction",
        "human judgment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp",
      "contributions": "1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.",
      "summary": "The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.",
      "mindmap": "graph TB\n        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]\n        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]\n        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]"
    },
    {
      "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing",
      "authors": "Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen",
      "institution": "University of Texas at Dallas",
      "link": "https://arxiv.org/pdf/2512.21440",
      "code": null,
      "tags": [
        "fuzz testing",
        "initial corpus generation",
        "large language models",
        "multi-agent framework",
        "predictive code coverage",
        "mutation-based fuzzing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp",
      "contributions": "1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.",
      "summary": "The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.",
      "mindmap": "graph TB\n        A[FuzzWise: Intelligent Initial Corpus Generation for Fuzzing] --> B[核心问题/Problem: 为模糊测试生成高质量的初始种子语料库/Generating high-quality initial seed corpus for fuzzing]\n        A --> C[主要方法/Method: 基于LLM的多智能体框架，集成生成与预测性覆盖评估/LLM-based multi-agent framework integrating generation and predictive coverage assessment]\n        A --> D[关键结果/Results: 用更少的测试用例实现更高的代码覆盖率和错误发现率/Achieves higher code coverage and bug detection with fewer test cases]"
    },
    {
      "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning",
      "authors": "Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu",
      "institution": "University of Washington, University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.21446",
      "code": null,
      "tags": [
        "diffusion models",
        "masked diffusion language models",
        "reinforcement learning",
        "parallel decoding",
        "on-policy optimization",
        "unmasking planner"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp",
      "contributions": "1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.",
      "summary": "The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards \"diffusion supremacy\" over autoregressive models.",
      "mindmap": "graph TB\n        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]\n        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]\n        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]\n        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]\n        D --> D2[迈向”扩散霸权”/Moving towards ”diffusion supremacy”]"
    },
    {
      "title": "An Equivariance Toolbox for Learning Dynamics",
      "authors": "Yongyi Yang, Liu Ziyin",
      "institution": "University of Michigan, Massachusetts Institute of Technology, NTT Research",
      "link": "https://arxiv.org/pdf/2512.21447",
      "code": null,
      "tags": [
        "learning theory",
        "equivariance",
        "Noether's theorem",
        "Hessian constraints",
        "learning dynamics",
        "symmetry"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp",
      "contributions": "1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations.",
      "summary": "The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena.",
      "mindmap": "graph TB\n        Root[An Equivariance Toolbox for Learning Dynamics] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有分析是特定问题的/Existing analyses are problem-specific]\n        Problem --> P2[二阶结构理解不足/Second-order structure less understood]\n        Method[主要方法/Method] --> M1[构建等变性工具箱/Build general equivariance toolbox]\n        Method --> M2[扩展诺特定理分析/Extend Noether-type analyses]\n        Results[关键结果/Results] --> R1[统一一阶约束/Unify first-order constraints]\n        Results --> R2[提供二阶结构预测/Provide second-order structural predictions]\n        Results --> R3[连接变换结构与几何/Connect transformation structure to geometry]"
    },
    {
      "title": "RLLaVA: An RL-central Framework for Language and Vision Assistants",
      "authors": "Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang",
      "institution": "Beihang University",
      "link": "https://arxiv.org/pdf/2512.21450",
      "code": "https://github.com/TinyLoopX/RLLaVA",
      "tags": [
        "multi-modal training",
        "reinforcement learning",
        "vision-language models",
        "Markov decision process",
        "resource-efficient training",
        "modular framework"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp",
      "contributions": "1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU.",
      "summary": "The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks.",
      "mindmap": "graph TB\n        A[RLLaVA: An RL-central Framework for Language and Vision Assistants] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[缺乏专门的多模态RL框架 / Lack of specialized multi-modal RL framework]\n        B --> B2[现有框架资源消耗大 / Existing frameworks are resource-intensive]\n        C --> C1[解耦RL逻辑与架构 / Decouple RL logic from architecture & execution]\n        C --> C2[统一MDP建模 / Unified MDP formulation for VLMs]\n        C --> C3[支持多种算法与模型 / Supports broad RL methods & VLMs]\n        D --> D1[资源高效训练 / Resource-efficient training (e.g., 4B model on 24GB GPU)]\n        D --> D2[性能提升 / Models show improved performance]\n        D --> D3[任务可扩展性 / Task extensibility demonstrated]"
    },
    {
      "title": "CCAD: Compressed Global Feature Conditioned Anomaly Detection",
      "authors": "Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu",
      "institution": "Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC",
      "link": "https://arxiv.org/pdf/2512.21459",
      "code": "https://github.com/chloeqxq/CCAD",
      "tags": [
        "anomaly detection",
        "global feature conditioning",
        "adaptive compression",
        "reconstruction-based anomaly detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",
      "contributions": "1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.",
      "summary": "This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.",
      "mindmap": "graph TB\n        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]\n        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]\n        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]"
    },
    {
      "title": "Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US",
      "authors": "Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder",
      "institution": "Harvard University, Boston Children's Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21456",
      "code": null,
      "tags": [
        "time series forecasting",
        "LSTM",
        "SARIMA",
        "conformal prediction",
        "counterfactual estimation",
        "uncertainty quantification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp",
      "contributions": "1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning.",
      "summary": "This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions.",
      "mindmap": "graph TB\n        A[Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[估计疫情导致的药物过量超额死亡率/Estimating pandemic-attributable excess mortality]\n        B --> B2[传统方法在结构性变化下可能失效/Traditional methods may fail under structural change]\n        C --> C1[系统比较SARIMA与深度学习模型/Systematic comparison of SARIMA vs. DL models]\n        C --> C2[使用CDC数据(2015-2019)进行训练/Using CDC data (2015-2019) for training]\n        C --> C3[预测2020-2023年的反事实轨迹/Projecting counterfactual trajectories for 2020-2023]\n        D --> D1[LSTM在点估计和不确定性校准上表现最佳/LSTM achieves best point estimation & uncertainty calibration]\n        D --> D2[注意力模型因过拟合历史均值而表现不佳/Attention models underperform due to overfitting to historical means]\n        D --> D3[提供可部署的开源框架/Providing a deployable open-source framework]"
    },
    {
      "title": "When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning",
      "authors": "Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis",
      "institution": "Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C.",
      "link": "https://arxiv.org/pdf/2512.21486",
      "code": "https://github.com/OceanSTARLab/RR-FBTC",
      "tags": [
        "tensor decomposition",
        "Bayesian tensor completion",
        "multioutput Gaussian processes",
        "variational inference",
        "rank learning",
        "functional universality"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp",
      "contributions": "1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model.",
      "summary": "The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches.",
      "mindmap": "graph TB\n        A[When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法需已知张量秩/Existing methods require known tensor rank]\n        B --> B2[确定最优秩是NP难问题/Determining optimal rank is NP-hard]\n        C --> C1[提出RR-FBTC方法/Propose RR-FBTC method]\n        C --> C2[使用多输出高斯过程建模/Model with multioutput Gaussian processes]\n        C --> C3[变分推断与闭式更新/Variational inference with closed-form updates]\n        D --> D1[证明泛函逼近能力/Prove functional universal approximation]\n        D --> D2[实现自动秩学习/Achieve automatic rank learning]\n        D --> D3[实验验证有效性/Experiments validate effectiveness]"
    },
    {
      "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
      "authors": "Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson",
      "institution": "Dartmouth College",
      "link": "https://arxiv.org/pdf/2512.21506",
      "code": null,
      "tags": [
        "multi-modal training",
        "wearable sensing",
        "actigraphy encoder",
        "projection module",
        "frozen LLM",
        "behavioral summarization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",
      "contributions": "1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.",
      "summary": "The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.",
      "mindmap": "graph TB\n        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"
    },
    {
      "title": "Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering",
      "authors": "Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu",
      "institution": "University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University",
      "link": "https://arxiv.org/pdf/2512.21510",
      "code": null,
      "tags": [
        "multi-view clustering",
        "incomplete multi-view clustering",
        "missing pattern tree",
        "decision ensemble",
        "knowledge distillation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",
      "contributions": "1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.",
      "summary": "This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.",
      "mindmap": "graph TB\n        Root[”Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering”] --> Problem[”核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance.”]\n        Root --> Method[”主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation.”]\n        Root --> Results[”关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness.”]"
    },
    {
      "title": "First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions",
      "authors": "Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik",
      "institution": "King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.21521",
      "code": null,
      "tags": [
        "federated learning",
        "differential privacy",
        "convergence guarantees",
        "partial client participation",
        "local updates",
        "clipping bias"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp",
      "contributions": "1. Introduces Fed-α-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification.",
      "summary": "The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-α-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings.",
      "mindmap": "graph TB\n        Root[First Provable Guarantees for Practical Private FL] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有私有FL方法依赖不现实假设/Existing private FL relies on unrealistic assumptions]\n        Problem --> P2[忽略本地更新与部分客户端参与/Neglects local updates & partial participation]\n        Method[主要方法/Method] --> M1[提出Fed-α-NormEC框架/Propose Fed-α-NormEC framework]\n        Method --> M2[集成本地更新与独立学习率/Integrates local updates & separate stepsizes]\n        Method --> M3[支持部分客户端参与/Supports partial client participation]\n        Results[关键结果/Results] --> R1[提供可证明的收敛与DP保证/Provides provable convergence & DP guarantees]\n        Results --> R2[实验验证理论/Experiments corroborate theory]"
    },
    {
      "title": "Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training",
      "authors": "Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren",
      "institution": "Ant Group, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.21515",
      "code": null,
      "tags": [
        "llm training",
        "continual pre-training",
        "scaling laws",
        "perplexity",
        "data selection",
        "knowledge gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp",
      "contributions": "1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of \"perplexity landscapes\" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.",
      "summary": "This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.",
      "mindmap": "graph TB\n        A[Perplexity-Aware Data Scaling Law<br>困惑度感知数据缩放定律] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CPT中单纯增加数据收益递减<br>Diminishing returns from scaling data in CPT]\n        C --> C1[提出基于困惑度景观的缩放定律<br>Propose perplexity-landscape-based scaling law]\n        C1 --> C2[利用困惑度量化知识差距<br>Use perplexity to quantify knowledge gap]\n        C2 --> C3[自适应选择高价值数据子集<br>Adaptively select high-utility data subsets]\n        D --> D1[识别接近最优的训练子集<br>Identifies near-optimal training subsets]\n        D1 --> D2[在领域基准上取得优越性能<br>Achieves superior performance on domain benchmarks]"
    },
    {
      "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data",
      "authors": "Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu",
      "institution": "Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University",
      "link": "https://arxiv.org/pdf/2512.21516",
      "code": null,
      "tags": [
        "multi-view clustering",
        "contrastive learning",
        "incomplete multi-view data",
        "noise-robust clustering",
        "graph-guided learning",
        "imputation-free"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",
      "contributions": "1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.",
      "summary": "This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.",
      "mindmap": "graph TB\n        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]\n        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]\n        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]\n        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]\n        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]\n        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]\n        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]"
    },
    {
      "title": "Generative Actor Critic",
      "authors": "Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie",
      "institution": "Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University",
      "link": "https://arxiv.org/pdf/2512.21527",
      "code": "github.com/qayqaq/Generative-Actor-Critic",
      "tags": [
        "reinforcement learning",
        "generative modeling",
        "policy evaluation",
        "latent plan",
        "offline-to-online",
        "actor-critic"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp",
      "contributions": "1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards.",
      "summary": "This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments.",
      "mindmap": "graph TB\n        A[Generative Actor Critic] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统RL在线改进离线预训练模型存在挑战/Challenges in refining offline models online]\n        C --> C1[将策略评估重构为学习轨迹与回报的联合生成模型/Reframe policy evaluation as learning p(τ, y)]\n        C --> C2[将策略改进重构为在模型上进行多样化推理/Reframe policy improvement as versatile inference]\n        C --> C3[基于潜变量模型的实例化与新颖推理策略/Instantiation with latent plans & novel inference]\n        D --> D1[离线性能强大/Strong offline performance]\n        D --> D2[离线到在线改进显著/Enhanced offline-to-online improvement]"
    },
    {
      "title": "AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification",
      "authors": "Xinru Wen, Weizhong Lin, Xuan Xiao",
      "institution": "JCI (inferred from email domain `jci.edu.cn`)",
      "link": "https://arxiv.org/pdf/2512.21544",
      "code": null,
      "tags": [
        "bioinformatics",
        "adaptive gating mechanism",
        "contrastive learning",
        "transfer learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp",
      "contributions": "1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples.",
      "summary": "This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families.",
      "mindmap": "graph TB\n        Root[AVP-Fusion: 抗病毒肽识别 / Antiviral Peptide Identification] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题 / Problem] --> P1[现有方法难以捕捉复杂序列依赖 / Current methods struggle with sequence dependencies]\n        Problem --> P2[难以处理模糊样本 / Hard to handle ambiguous samples]\n        Method[主要方法 / Method] --> M1[构建全景特征空间 / Construct panoramic feature space]\n        Method --> M2[自适应门控机制融合特征 / Adaptive Gating Mechanism for feature fusion]\n        Method --> M3[对比学习与数据增强 / Contrastive learning & data augmentation]\n        Results[关键结果 / Results] --> R1[准确率0.9531, MCC 0.9064 / Accuracy 0.9531, MCC 0.9064]\n        Results --> R2[优于现有方法 / Outperforms SOTA]\n        Results --> R3[实现病毒家族亚类预测 / Enables viral family subclass prediction]"
    },
    {
      "title": "Discovering Sparse Recovery Algorithms Using Neural Architecture Search",
      "authors": "Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde",
      "institution": "New York University, University of California, Riverside",
      "link": "https://arxiv.org/pdf/2512.21563",
      "code": null,
      "tags": [
        "sparse recovery",
        "neural architecture search",
        "meta-learning",
        "iterative shrinkage thresholding algorithm",
        "sparse optimization",
        "algorithm discovery"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp",
      "contributions": "1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework's capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework's applicability to various data distributions and algorithms beyond ISTA/FISTA.",
      "summary": "This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions.",
      "mindmap": "graph TB\n    A[Discovering Sparse Recovery Algorithms Using Neural Architecture Search] --> B[核心问题/Problem: Automated discovery of sparse optimization algorithms is difficult and heuristic-driven]\n    A --> C[主要方法/Method: Meta-learning framework using Neural Architecture Search (NAS) for algorithm rediscovery]\n    A --> D[关键结果/Results: Rediscovered ISTA/FISTA elements; framework applies to various data and algorithms]"
    },
    {
      "title": "AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging",
      "authors": "Xiaobin Ren, Kaiqi Zhao, Katerina Taškova, Patricia Riddle",
      "institution": "University of Auckland, Harbin Institute of Technology, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.21569",
      "code": "https://github.com/xren451/Spatial-interpolation",
      "tags": [
        "spatio-temporal kriging",
        "graph neural networks",
        "incremental learning",
        "data stratification",
        "anchor locations",
        "incomplete features"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp",
      "contributions": "1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness.",
      "summary": "The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods.",
      "mindmap": "graph TB\n        A[AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging] --> B(核心问题/Problem: Sparse sensor distribution and incomplete features hinder accurate spatio-temporal kriging)\n        A --> C(主要方法/Method: Anchor-based stratification and dual-view graph learning for incremental feature integration)\n        A --> D(关键结果/Results: Outperforms state-of-the-art baselines on multiple benchmark datasets)"
    },
    {
      "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
      "authors": "Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang",
      "institution": "Canaan Inc.",
      "link": "https://arxiv.org/pdf/2512.21571",
      "code": "https://github.com/kendryte/nncase",
      "tags": [
        "compiler & ir",
        "e-graph",
        "term rewriting",
        "phase ordering",
        "NUMA abstraction",
        "auto vectorize"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a \"compile once, adapt everywhere\" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.",
      "summary": "The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.",
      "mindmap": "graph TB\n        A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --> B[核心问题/Problem: LLM部署受限于内存架构异构性，传统编译器流程碎片化/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]\n        A --> C[主要方法/Method: 基于e-graph的项重写引擎，统一NUMA抽象，集成自动向量化、分布、调度模块/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]\n        A --> D[关键结果/Results: 性能超越MLC LLM和Intel IPEX，接近手工优化的llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]"
    },
    {
      "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid",
      "authors": "Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng",
      "institution": "Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers",
      "link": "https://arxiv.org/pdf/2512.21577",
      "code": null,
      "tags": [
        "hallucination detection & evaluation",
        "hallucination",
        "world modeling",
        "knowledge conflict",
        "benchmark",
        "language model evaluation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp",
      "contributions": "1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.",
      "summary": "This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.",
      "mindmap": "graph TB\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Problem[”核心问题/Problem”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Method[”主要方法/Method”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Results[”关键结果/Results”]\n        Problem --> P1[”Hallucination persists in LLMs / 幻觉在LLM中持续存在”]\n        Method --> M1[”Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模”]\n        Method --> M2[”Framework: reference world & conflict policy / 框架：参考世界与冲突策略”]\n        Results --> R1[”Clarifies evaluation & terminology / 澄清评估与术语”]\n        Results --> R2[”Proposes new benchmark plans / 提出新基准计划”]"
    },
    {
      "title": "RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models",
      "authors": "Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic",
      "institution": "Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21572",
      "code": null,
      "tags": [
        "time series forecasting",
        "Schrödinger Bridge",
        "Low-rank Adaptation",
        "Time Series Foundation Models",
        "Financial Forecasting",
        "Generative Refinement"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp",
      "contributions": "1. Proposes RefineBridge, a novel post-processing module built on a tractable Schrödinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise.",
      "summary": "The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schrödinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks.",
      "mindmap": "graph TB\n        A[RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[TSFMs在金融预测中表现不佳<br/>TSFMs underperform in financial forecasting]\n        B --> B2[LoRA等适应方法存在局限<br/>Limitations of adaptation methods like LoRA]\n        C --> C1[提出基于薛定谔桥的RefineBridge模块<br/>Propose RefineBridge based on Schrödinger Bridge]\n        C --> C2[将TSFM预测作为先验进行迭代优化<br/>Iteratively refine TSFM predictions as prior]\n        D --> D1[在多个基准上提升TSFM性能<br/>Improves TSFM performance on multiple benchmarks]\n        D --> D2[对不同预测范围均有效<br/>Effective across different prediction horizons]"
    },
    {
      "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations",
      "authors": "Xin Liu, Haoran Li, Dongbin Zhao",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21586",
      "code": null,
      "tags": [
        "imitation learning",
        "behavior cloning",
        "latent representation",
        "self-supervised learning",
        "sample efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp",
      "contributions": "1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks.",
      "summary": "This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning.",
      "mindmap": "graph TB\n        A[Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”从视频模仿学习的挑战 / Challenges of Imitation Learning from Videos”]\n        C --> C1[”BCV-LR框架 / BCV-LR Framework”]\n        C1 --> C2[”自监督提取潜在特征 / Self-supervised Latent Feature Extraction”]\n        C1 --> C3[”基于动态的潜在动作预测 / Dynamics-based Latent Action Prediction”]\n        C1 --> C4[”在线对齐与迭代策略改进 / Online Alignment & Iterative Policy Improvement”]\n        D --> D1[”高样本效率 / High Sample Efficiency”]\n        D --> D2[”超越SOTA方法 / Outperforms SOTA Baselines”]\n        D --> D3[”首次证明视频可作为高效监督 / First to Show Videos as Efficient Supervision”]"
    },
    {
      "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming",
      "authors": "Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong",
      "institution": "East China Normal University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21596",
      "code": null,
      "tags": [
        "probabilistic programming and verification",
        "temporal posterior inference",
        "omega-regular properties",
        "stochastic barrier certificates",
        "Rabin automata",
        "quantitative verification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp",
      "contributions": "1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.",
      "summary": "This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.",
      "mindmap": "graph TB\n        Root(”Quantitative Verification of Omega-regular Properties in Probabilistic Programming”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”标准后验推断的局限/Limitation of Standard Posterior Inference”)\n        P1 --> P2(”无法捕捉程序执行的时间演化/Fails to capture temporal evolution”)\n        Method --> M1(”提出时间后验推断框架/Propose Temporal Posterior Inference (TPI)”)\n        M1 --> M2(”统一概率编程与时序逻辑/Unifies Probabilistic Programming & Temporal Logic”)\n        M2 --> M3(”基于随机屏障证书的定量验证方法/Quantitative Verification via Stochastic Barrier Certificates”)\n        Results --> R1(”实现原型工具 TPInfer/Implement Prototype Tool TPInfer”)\n        Results --> R2(”在基准测试中展示有效性与效率/Demonstrates Effectiveness & Efficiency on Benchmarks”)"
    },
    {
      "title": "Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care",
      "authors": "Yusuf Brima, Marcellin Atemkeng",
      "institution": "Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)",
      "link": "https://arxiv.org/pdf/2512.21602",
      "code": null,
      "tags": [
        "imbalanced classification",
        "XGBoost",
        "TabNet",
        "TabResNet",
        "Bayesian hyperparameter search",
        "class imbalance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",
      "contributions": "1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.",
      "summary": "This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.",
      "mindmap": "graph TB\n        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]\n        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]\n        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]"
    },
    {
      "title": "A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)",
      "authors": "Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen",
      "institution": "Tongji University, University of Chittagong",
      "link": "https://arxiv.org/pdf/2512.21610",
      "code": null,
      "tags": [
        "predictive modeling",
        "XGBoost",
        "SHAP analysis",
        "K-Fold Cross-Validation",
        "Isolation Forest",
        "hyperparameter tuning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp",
      "contributions": "1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing.",
      "summary": "This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests.",
      "mindmap": "graph TB\n        A[”A Data-Driven Multi-Objective Approach for Predicting UHPC Properties<br>预测UHPC性能的数据驱动多目标方法”] --> B[”Problem: Predicting UHPC mechanical performance, flowability, porosity<br>核心问题: 预测UHPC的力学性能、流动性和孔隙率”]\n        A --> C[”Method: Two-stage ML framework with XGBoost, data cleaning, SHAP<br>主要方法: 两阶段ML框架，使用XGBoost、数据清洗和SHAP”]\n        A --> D[”Results: High prediction accuracy, developed GUI for designers<br>关键结果: 高预测精度，为设计师开发了GUI”]"
    },
    {
      "title": "MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations",
      "authors": "Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu",
      "institution": "Hunan University, Capital Normal University",
      "link": "https://arxiv.org/pdf/2512.21633",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Neural Galerkin Method",
        "meta-learning",
        "parametric PDEs",
        "space-time decoupling",
        "randomized sparse updates"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp",
      "contributions": "1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy.",
      "summary": "The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability.",
      "mindmap": "graph TB\n        A[MAD-NG: Meta-Auto-Decoder Neural Galerkin Method<br>MAD-NG: 元自解码器神经伽辽金方法] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统神经求解器泛化与效率挑战<br>Traditional Neural Solvers' Generalization & Efficiency Challenges]\n        C --> C1[空间-时间解耦与元学习<br>Space-Time Decoupling & Meta-Learning]\n        C --> C2[随机稀疏更新<br>Randomized Sparse Updates]\n        D --> D1[物理一致的长时预测<br>Physically Consistent Long-Horizon Predictions]\n        D --> D2[较低计算开销<br>Lower Computational Overhead]"
    },
    {
      "title": "Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms",
      "authors": "Jagaran Chakma, Zhiguang Zhou, Badhan Chakma",
      "institution": "Tongji University, Chongqing Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.21638",
      "code": null,
      "tags": [
        "materials informatics",
        "hybrid machine learning",
        "SHAP analysis",
        "uncertainty quantification",
        "strength prediction",
        "high-performance concrete"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp",
      "contributions": "1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis.",
      "summary": "This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications.",
      "mindmap": "graph TB\n        A[Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Predict mechanical properties (CS, FS, TS) of fiber-reinforced HPC]\n        C[主要方法/Method<br>Develop & evaluate hybrid ML models (ET-XGB, RF-LGBM, Transformer-XGB) with SHAP & uncertainty analysis]\n        D[关键结果/Results<br>ET-XGB most accurate, RF-LGBM most stable for FS, key influential factors identified via SHAP]"
    },
    {
      "title": "Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search",
      "authors": "Maximilian Weichart",
      "institution": "University of Regensburg",
      "link": "https://arxiv.org/pdf/2512.21648",
      "code": "https://github.com/Max-We/inverse-rpo",
      "tags": [
        "reinforcement learning",
        "Monte Carlo Tree Search",
        "Upper Confidence Bound",
        "Variance-Aware",
        "Prior-Based Tree Policy",
        "Inverse-RPO"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp",
      "contributions": "1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.",
      "summary": "This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.",
      "mindmap": "graph TB\n        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]\n        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]\n        D[关键结果/Results: New policies outperform PUCT without extra cost]"
    },
    {
      "title": "Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation",
      "authors": "Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing",
      "institution": "Chongqing University",
      "link": "https://arxiv.org/pdf/2512.21650",
      "code": null,
      "tags": [
        "anomaly detection",
        "multimodal fusion",
        "causal modeling",
        "hierarchical modulation",
        "sensor guidance",
        "unsupervised learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp",
      "contributions": "1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process→Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency.",
      "summary": "The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness.",
      "mindmap": "graph TB\n        A[Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[因果盲区/Causal Blindness]\n        B --> B2[模态异质性/Modality Heterogeneity]\n        C --> C1[传感器引导调制/Sensor-Guided CHM Modulation]\n        C --> C2[因果分层架构/Causal-Hierarchical Architecture]\n        D --> D1[新基准/Weld-4M Benchmark]\n        D --> D2[SOTA性能/SOTA I-AUROC 90.7%]"
    },
    {
      "title": "Semantic Codebooks as Effective Priors for Neural Speech Compression",
      "authors": "Liuyang Bai, Weiyi Lu, Li Guo",
      "institution": "NYU Shanghai",
      "link": "https://arxiv.org/pdf/2512.21653",
      "code": null,
      "tags": [
        "speech compression",
        "semantic codebooks",
        "residual vector quantization (RVQ)",
        "HuBERT",
        "FiLM-conditioned decoder",
        "neural audio codec"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",
      "contributions": "1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.",
      "summary": "The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.",
      "mindmap": "graph TB\n        Root[”Semantic Codebooks as Effective Priors for Neural Speech Compression”] --> Problem[”核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure.”]\n        Root --> Method[”主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder.”]\n        Root --> Results[”关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps).”]"
    },
    {
      "title": "Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models",
      "authors": "Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do",
      "institution": "Monash University, University of Surrey",
      "link": "https://arxiv.org/pdf/2512.21651",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "1-bit quantization",
        "post-training quantization",
        "output alignment",
        "activation error",
        "large language models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp",
      "contributions": "1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead",
      "summary": "This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches.",
      "mindmap": "graph TB\n        Root[”Rethinking Output Alignment For 1-bit PTQ of LLMs<br>重新思考大语言模型1比特训练后量化的输出对齐”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”1-bit PTQ causes significant performance drop<br>1比特训练后量化导致性能显著下降”] --> P1[”Focus on weight alignment, not output<br>关注权重对齐而非输出”]\n        Problem --> P2[”Naive output-matching fails<br>简单的输出匹配方法失败”]\n        Method[”Propose a data-aware PTQ approach<br>提出一种数据感知的训练后量化方法”] --> M1[”Accounts for activation error accumulation<br>考虑激活误差累积”]\n        Method --> M2[”Keeps optimization efficient<br>保持优化高效”]\n        Results[”Consistently outperforms existing 1-bit PTQ methods<br>持续优于现有1比特训练后量化方法”] --> R1[”Minimal overhead<br>开销极小”]"
    },
    {
      "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds",
      "authors": "Subramanyam Sahoo, Jared Junkin",
      "institution": "University of California, Berkeley, Johns Hopkins University",
      "link": "https://arxiv.org/pdf/2512.21670",
      "code": "https://github.com/SubramanyamSahoo/The-Deepfake-Detective",
      "tags": [
        "deepfake detection",
        "mechanistic interpretability",
        "sparse autoencoder",
        "forensic manifold analysis",
        "feature selectivity",
        "vision-language model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",
      "contributions": "1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.",
      "summary": "This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.",
      "mindmap": "graph TB\n        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]\n        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]\n        C --> C2[法证流形分析/Forensic Manifold Analysis]\n        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]\n        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]"
    },
    {
      "title": "Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles",
      "authors": "Jalal Khan",
      "institution": "United Arab Emirates University",
      "link": "https://arxiv.org/pdf/2512.21673",
      "code": null,
      "tags": [
        "object detection",
        "YOLO-NAS",
        "YOLOv8",
        "perception",
        "autonomous vehicles",
        "custom dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",
      "contributions": "1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.",
      "summary": "This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.",
      "mindmap": "graph TB\n    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]\n    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\n    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]\n    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"
    },
    {
      "title": "Dictionary-Transform Generative Adversarial Networks",
      "authors": "Angshul Majumdar",
      "institution": "Indraprastha Institute of Information Technology Delhi (IIIT-D)",
      "link": "https://arxiv.org/pdf/2512.21677",
      "code": null,
      "tags": [
        "generative adversarial networks",
        "dictionary learning",
        "transform learning",
        "sparse modeling",
        "adversarial learning",
        "nash equilibrium"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp",
      "contributions": "1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data.",
      "summary": "This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure.",
      "mindmap": "graph TB\n        A[Dictionary-Transform GANs] --> B[核心问题/Problem: Classical GANs are theoretically fragile, with ill-posed objectives and unstable training.]\n        A --> C[主要方法/Method: Propose DT-GAN, a model-based framework with a sparse synthesis dictionary generator and an analysis transform discriminator.]\n        A --> D[关键结果/Results: Game is well-posed with Nash equilibrium; solutions are identifiable and stable; framework is interpretable and provably correct.]"
    },
    {
      "title": "RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting",
      "authors": "Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan",
      "institution": "Beijing Jiaotong University, Aalborg University",
      "link": "https://arxiv.org/pdf/2512.21685",
      "code": null,
      "tags": [
        "spatiotemporal forecasting",
        "probabilistic forecasting",
        "uncertainty estimation",
        "principal component analysis",
        "road impedance",
        "traffic flow"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp",
      "contributions": "1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.",
      "summary": "The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.",
      "mindmap": "graph TB\n        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)\n        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)\n        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)\n        C --> C2(主成分网络 / Principal Component Network)\n        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)"
    },
    {
      "title": "Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities",
      "authors": "Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin",
      "institution": "Kyung Hee University, Ghent University",
      "link": "https://arxiv.org/pdf/2512.21717",
      "code": null,
      "tags": [
        "communication & networking",
        "multiconnectivity",
        "SAGIN",
        "resource allocation",
        "agentic reinforcement learning",
        "heterogeneous networks"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp",
      "contributions": "1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.",
      "summary": "This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.",
      "mindmap": "graph TB\n        Root[”Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation”]\n        Method[”主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning”]\n        Results[”关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off”]"
    },
    {
      "title": "An Information Theoretic Perspective on Agentic System Design",
      "authors": "Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.21720",
      "code": null,
      "tags": [
        "agent system",
        "mutual information",
        "noisy channel",
        "compressor-predictor",
        "on-device AI",
        "information-theoretic"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp",
      "contributions": "1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.",
      "summary": "The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.",
      "mindmap": "graph TB\n        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance”)\n        C --> C1(”提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator”)\n        D --> D1(”更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate”)\n        D --> D2(”扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors”)"
    },
    {
      "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks",
      "authors": "Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21723",
      "code": null,
      "tags": [
        "agent system",
        "embodied agent",
        "hierarchical planning",
        "large language model",
        "household tasks",
        "open source LLM"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",
      "contributions": "1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.",
      "summary": "The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.",
      "mindmap": "graph TB\n        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"
    },
    {
      "title": "A Model of Causal Explanation on Neural Networks for Tabular Data",
      "authors": "Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda",
      "institution": "Sony Computer Science Laboratories, Inc., Sony Corporation of America",
      "link": "https://arxiv.org/pdf/2512.21746",
      "code": null,
      "tags": [
        "explainable ai",
        "CENNET",
        "structural causal models",
        "entropy",
        "causal explanation",
        "tabular data"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp",
      "contributions": "1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.",
      "summary": "This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.",
      "mindmap": "graph TB\n        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]\n        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]\n        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]"
    },
    {
      "title": "Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning",
      "authors": "Hengyi Wu, Zhenyi Wang, Heng Huang",
      "institution": "University of Maryland, College Park, University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.21743",
      "code": null,
      "tags": [
        "continual learning",
        "entropy scaling",
        "catastrophic forgetting",
        "stability-plasticity dilemma",
        "dynamic feedback",
        "layer-wise control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",
      "contributions": "1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.",
      "summary": "The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.",
      "mindmap": "graph TB\n        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]\n        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]\n        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]\n        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]\n        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]\n        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]"
    },
    {
      "title": "Approximation Capabilities of Feedforward Neural Networks with GELU Activations",
      "authors": "Konstantin Yakovlev, Nikita Puchkin",
      "institution": "HSE University",
      "link": "https://arxiv.org/pdf/2512.21749",
      "code": null,
      "tags": [
        "neural network approximation theory",
        "GELU activation",
        "feedforward neural networks",
        "approximation error bounds",
        "derivative approximation",
        "constructive approximation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp",
      "contributions": "1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators.",
      "summary": "This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives.",
      "mindmap": "graph TB\n    Root(”Approximation Capabilities of Feedforward Neural Networks with GELU Activations<br>GELU激活前馈神经网络的逼近能力”) --> Problem(”核心问题/Problem”)\n    Root --> Method(”主要方法/Method”)\n    Root --> Results(”关键结果/Results”)\n    Problem --> P1(”逼近函数及其导数<br>Approximating functions and their derivatives”)\n    Method --> M1(”构造性乘法逼近<br>Constructive multiplication approximation”)\n    Method --> M2(”扩展到除法和指数<br>Extension to division and exponent”)\n    Results --> R1(”同时误差界<br>Simultaneous error bounds”)\n    Results --> R2(”全局有界导数<br>Globally bounded derivatives”)\n    Results --> R3(”网络规模控制<br>Network size control”)"
    },
    {
      "title": "Assessing the Effectiveness of Membership Inference on Generative Music",
      "authors": "Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang",
      "institution": "University of California, Irvine",
      "link": "https://arxiv.org/pdf/2512.21762",
      "code": null,
      "tags": [
        "membership inference attacks",
        "membership inference attack (MIA)",
        "generative music",
        "MuseGAN",
        "privacy",
        "copyright"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp",
      "contributions": "1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio.",
      "summary": "This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks.",
      "mindmap": "graph TB\n        Root[”Assessing the Effectiveness of Membership Inference on Generative Music”] --> Problem[”核心问题/Problem: Lack of MIA study on generative music, privacy & copyright concerns”]\n        Root --> Method[”主要方法/Method: Apply existing MIAs to MuseGAN model”]\n        Root --> Results[”关键结果/Results: Music data is resilient to known MIAs”]"
    },
    {
      "title": "BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization",
      "authors": "Evgeny Alves Limarenko, Anastasiia Studenikina",
      "institution": "Moscow Institute of Physics and Technology",
      "link": "https://arxiv.org/pdf/2512.21769",
      "code": null,
      "tags": [
        "3d medical image analysis",
        "Masked Autoencoder",
        "Swin Transformer",
        "Self-Supervised Learning",
        "3D Vision Transformer",
        "Structural Priority Loss"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",
      "contributions": "1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.",
      "summary": "The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.",
      "mindmap": "graph TB\n        Root(”BertsWin: 3D MAE优化”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE”)\n        Problem --> P2(”破坏空间关系/Destroys Spatial Context”)\n        Method --> M1(”BertsWin混合架构/BertsWin Hybrid Architecture”)\n        Method --> M2(”完整3D令牌网格/Full 3D Token Grid”)\n        Method --> M3(”Swin窗口 & 结构损失/Swin Windows & Structural Loss”)\n        Results --> R1(”5.8x语义收敛加速/5.8x Faster Convergence”)\n        Results --> R2(”15倍训练轮次减少/15x Fewer Epochs”)\n        Results --> R3(”FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction”)"
    },
    {
      "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents",
      "authors": "Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin",
      "institution": "Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.21782",
      "code": null,
      "tags": [
        "scientific discovery agents",
        "autonomous goal evolution",
        "bi-level optimization",
        "LLM agents",
        "objective function design",
        "scientific discovery"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp",
      "contributions": "1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.",
      "summary": "This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.",
      "mindmap": "graph TB\n        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]\n        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]\n        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]\n        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]\n        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]"
    },
    {
      "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling",
      "authors": "Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana",
      "institution": "AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University",
      "link": "https://arxiv.org/pdf/2512.21791",
      "code": null,
      "tags": [
        "synthetic data generation",
        "synthetic financial data",
        "TimeGAN",
        "ARIMA-GARCH",
        "VAE",
        "Maximum Mean Discrepancy"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp",
      "contributions": "1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking.",
      "summary": "This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field.",
      "mindmap": "graph TB\n        Root[”Synthetic Financial Data Generation for Enhanced Financial Modelling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”数据稀缺与保密性<br/>Data Scarcity & Confidentiality”]\n        Method[”统一评估框架与三种生成模型<br/>Unified Evaluation Framework & Three Generative Models”]\n        Results[”TimeGAN最佳权衡与实用指南<br/>TimeGAN Best Trade-off & Practical Guidelines”]"
    },
    {
      "title": "Multi-agent Adaptive Mechanism Design",
      "authors": "Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao",
      "institution": "Massachusetts Institute of Technology, University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.21794",
      "code": null,
      "tags": [
        "mechanism design",
        "distributionally robust optimization",
        "online learning",
        "incentive compatibility",
        "adaptive mechanism",
        "regret analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp",
      "contributions": "1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\\tilde\\{O\\}(\\sqrt\\{T\\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.",
      "summary": "This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.",
      "mindmap": "graph TB\n        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]\n        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]\n        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\\tilde{O}(\\sqrt{T})$ regret]"
    },
    {
      "title": "VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance",
      "authors": "Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud",
      "institution": "National School of Artificial Intelligence (ENSIA)",
      "link": "https://arxiv.org/pdf/2512.21786",
      "code": null,
      "tags": [
        "bioinformatics",
        "Set Attention Transformer",
        "1D-CNN",
        "Multi-Path Network",
        "Interpretable Machine Learning",
        "Genomic Variant Analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics.",
      "summary": "This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics.",
      "mindmap": "graph TB\n        Root[VAMP-Net: Interpretable Multi-Path Framework] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem<br>Challenges in MTB Drug Resistance Prediction] --> P1[复杂的上位性相互作用/Complex Epistatic Interactions]\n        Problem --> P2[测序数据质量多变/Variable Sequencing Data Quality]\n    \n        Method[主要方法/Method<br>VAMP-Net Multi-Path Architecture] --> M1[路径1: 集合注意力变换器/Path-1: Set Attention Transformer]\n        Method --> M2[路径2: 质量感知1D-CNN/Path-2: Quality-Aware 1D-CNN]\n        Method --> M3[融合模块/Fusion Module]\n        M1 --> M1_Detail[处理变异集合/Processes Variant Sets]\n        M2 --> M2_Detail[分析质量指标/Analyzes Quality Metrics]\n    \n        Results[关键结果/Results<br>Superior Performance & Interpretability] --> R1[性能: >95% 准确率, ~97% AUC/Performance: >95% Acc, ~97% AUC]\n        Results --> R2[可解释性: 双层面分析/Interpretability: Dual-Layer Analysis]\n        R2 --> R2_1[注意力权重揭示上位网络/Attention Weights Reveal Epistatic Networks]\n        R2 --> R2_2[梯度分析识别关键位点与质量指标/Gradient Analysis Identifies Key Loci & Quality Metrics]"
    },
    {
      "title": "Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers",
      "authors": "Krishna Chaitanya Sunkara, Rambabu Konakanchi",
      "institution": "Oracle, Charles Schwab",
      "link": "https://arxiv.org/pdf/2512.21801",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "LSTM",
        "Random Forest",
        "MQTT",
        "InfluxDB",
        "Streamlit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp",
      "contributions": "1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.",
      "summary": "The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.",
      "mindmap": "graph TB\n        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]\n        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]\n        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]"
    },
    {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "authors": "Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang",
      "institution": "Australian National University, The University of Queensland, GE Research",
      "link": "https://arxiv.org/pdf/2512.21815",
      "code": null,
      "tags": [
        "adversarial attacks",
        "entropy-guided attacks",
        "vision-language models",
        "adversarial robustness",
        "harmful content generation",
        "transferability"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",
      "contributions": "1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.",
      "summary": "This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.",
      "mindmap": "graph TB\n        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]\n        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]\n        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]\n        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]\n        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]\n        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]"
    },
    {
      "title": "Scalable Class-Incremental Learning Based on Parametric Neural Collapse",
      "authors": "Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen",
      "institution": "Not explicitly stated in the provided content. Affiliation information is not included.",
      "link": "https://arxiv.org/pdf/2512.21845",
      "code": "this",
      "tags": [
        "class-incremental learning",
        "parametric neural collapse",
        "equiangular tight frame",
        "knowledge distillation",
        "adaptive expansion",
        "feature alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",
      "contributions": "1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.",
      "summary": "The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Class-Incremental Learning Based on Parametric Neural Collapse”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题 / Problem”] --> P1[”过拟合新数据 / Overfitting to new data”]\n        Problem --> P2[”灾难性遗忘旧数据 / Catastrophic forgetting of old data”]\n        Problem --> P3[”特征差异与类别错位 / Feature difference & Class misalignment”]\n    \n        Method[”主要方法 / Method”] --> M1[”SCL-PNC方法 / SCL-PNC Method”]\n        M1 --> M1_1[”自适应层扩展主干 / Adapt-layer for backbone expansion”]\n        M1 --> M1_2[”动态参数化ETF分类器 / Dynamic Parametric ETF Classifier”]\n        M1 --> M1_3[”并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation”]\n    \n        Results[”关键结果 / Results”] --> R1[”高效处理类别增长 / Efficiently handles increasing categories”]\n        Results --> R2[”解决类别错位 / Addresses class misalignment”]\n        Results --> R3[”确保特征一致性 / Ensures feature consistency”]"
    },
    {
      "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs",
      "authors": "Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville",
      "institution": "Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR",
      "link": "https://arxiv.org/pdf/2512.21852",
      "code": null,
      "tags": [
        "reinforcement learning",
        "KL divergence",
        "policy gradient",
        "on-policy sampling",
        "off-policy training",
        "gradient bias"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp",
      "contributions": "1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.",
      "summary": "This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.",
      "mindmap": "graph TB\n        Root[”A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差”] --> P1[”实践问题/Practical Issue<br>广泛使用但实现与目标不一致”]\n        Problem --> P2[”理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性”]\n        Method[”主要方法/Method<br>分析梯度偏差并进行实证验证”] --> M1[”分析/Analysis<br>研究多种估计器配置的梯度”]\n        Method --> M2[”实验/Experiments<br>RL微调多个LLM并评估性能”]\n        Results[”关键结果/Results<br>无偏梯度配置带来更好性能”] --> R1[”在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能”]\n        Results --> R2[”离线策略/Off-Policy<br>KL正则化有助于稳定异步训练”]"
    },
    {
      "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
      "authors": "Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan",
      "institution": "North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh",
      "link": "https://arxiv.org/pdf/2512.21861",
      "code": null,
      "tags": [
        "medical image classification",
        "feature-level fusion",
        "convolutional neural networks",
        "diabetic retinopathy screening",
        "EfficientNet",
        "DenseNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",
      "contributions": "1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.",
      "summary": "This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.",
      "mindmap": "graph TB\n        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"
    },
    {
      "title": "Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation",
      "authors": "Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh",
      "institution": "Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University",
      "link": "https://arxiv.org/pdf/2512.21866",
      "code": null,
      "tags": [
        "Privacy-preserving machine learning",
        "dataset distillation",
        "random forest",
        "synthetic data generation",
        "explainable AI",
        "membership-inference attack"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp",
      "contributions": "1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.",
      "summary": "This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.",
      "mindmap": "graph TB\n        Root(”Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection”)\n        Problem --> P2(”模型需要可解释性/Model needs explainability”)\n        Method --> M1(”将随机森林转换为规则区域/Convert random forest to rule regions”)\n        Method --> M2(”在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data”)\n        Results --> R1(”数据量减少85-93%/Data volume reduced by 85-93%”)\n        Results --> R2(”保持竞争性性能/Maintains competitive performance”)\n        Results --> R3(”抵抗成员推理攻击/Resists membership-inference attacks”)"
    },
    {
      "title": "MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction",
      "authors": "Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han",
      "institution": "Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University",
      "link": "https://arxiv.org/pdf/2512.21897",
      "code": null,
      "tags": [
        "multi-modal training",
        "multimodal fusion",
        "sparse mixture-of-experts",
        "schema-guided textualization",
        "clinical trial prediction",
        "temperature scaling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp",
      "contributions": "1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.",
      "summary": "The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.",
      "mindmap": "graph TB\n        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]\n        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Method --> M2[架构设计/Architecture Design]\n        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]\n        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]\n        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]\n        Results --> R2[消融研究<br>Ablation Studies]\n        Results --> R3[概率校准<br>Probability Calibration]"
    },
    {
      "title": "GQ-VAE: A gated quantized VAE for learning variable length tokens",
      "authors": "Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener",
      "institution": "Kempner Institute, Harvard University",
      "link": "https://arxiv.org/pdf/2512.21913",
      "code": "https://github.com/Theo-Datta-115/gq-vae",
      "tags": [
        "tokenization",
        "GQ-VAE",
        "variable-length tokens",
        "VQ-VAE",
        "neural tokenizer",
        "byte-pair encoding"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp",
      "contributions": "1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE.",
      "summary": "The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE's performance and, under equivalent compression, improving downstream language model learning.",
      "mindmap": "graph TB\n        Root[GQ-VAE: A gated quantized VAE for learning variable length tokens] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传统分词器如BPE是确定性的/Traditional tokenizers like BPE are deterministic]\n        Problem --> P2[神经分词器复杂且难集成/Neural tokenizers are complex and hard to integrate]\n        Method[主要方法/Method] --> M1[提出GQ-VAE架构/Propose GQ-VAE architecture]\n        Method --> M2[学习变长离散令牌/Learn variable-length discrete tokens]\n        Method --> M3[可独立预训练/Can be independently pre-trained]\n        Results[关键结果/Results] --> R1[压缩和语言建模性能接近BPE/Compression & LM performance approaches BPE]\n        Results --> R2[在同等压缩下提升下游LM学习/Improves downstream LM learning at equivalent compression]"
    },
    {
      "title": "Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs",
      "authors": "Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21915",
      "code": "https://github.com/windblow32/DATE",
      "tags": [
        "others",
        "Tabular Data Generation",
        "Large Language Models",
        "Multi-Arm Bandit",
        "Data Diversity",
        "In-context Learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp",
      "contributions": "1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data.",
      "summary": "This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance.",
      "mindmap": "graph TB\n        Root[”Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs”] --> Problem[”核心问题/Problem: Real-world tabular data is heterogeneous, making universal generation models challenging”]\n        Root --> Method[”主要方法/Method: DATE framework partitions data, uses LLMs with decision tree feedback, and applies Multi-Arm Bandit for selection”]\n        Root --> Results[”关键结果/Results: Outperforms SOTA methods, reduces error rate by 23.75%, improves DPO and LLM reasoning”]"
    },
    {
      "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model",
      "authors": "Nathan Kallus",
      "institution": "Netflix, Cornell University",
      "link": "https://arxiv.org/pdf/2512.21917",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "preference optimization",
        "single-index model",
        "semiparametric",
        "link function",
        "policy learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp",
      "contributions": "1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.",
      "summary": "The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.",
      "mindmap": "graph TB\n        Root[”Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment”]\n        Method[”主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model”]\n        Results[”关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees”]"
    },
    {
      "title": "AutoPP: Towards Automated Product Poster Generation and Optimization",
      "authors": "Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law",
      "institution": "JD.COM",
      "link": "https://arxiv.org/pdf/2512.21921",
      "code": "https://github.com/JD-GenX/AutoPP",
      "tags": [
        "image generation",
        "product poster generation",
        "click-through rate optimization",
        "isolated direct preference optimization",
        "AutoPP1M dataset",
        "unified design module"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",
      "contributions": "1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.",
      "summary": "The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.",
      "mindmap": "graph TB\n        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]\n        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]\n        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]\n        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]\n        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]\n        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]\n        D --> D2[代码与数据集公开/Code & dataset released]"
    },
    {
      "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
      "authors": "Yiquan Gao, John See",
      "institution": "Heriot-Watt University",
      "link": "https://arxiv.org/pdf/2512.21944",
      "code": null,
      "tags": [
        "low-light image enhancement",
        "Data Relativistic Uncertainty",
        "Unsupervised Learning",
        "EnlightenGAN",
        "Anime Scenery",
        "Domain Gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
      "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
      "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
    },
    {
      "title": "Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms",
      "authors": "Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong",
      "institution": "Southern University of Science and Technology, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.21925",
      "code": null,
      "tags": [
        "multi-armed bandits",
        "combinatorial multi-armed bandits",
        "probabilistically triggered arms",
        "hybrid learning",
        "offline data",
        "online interaction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp",
      "contributions": "1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm's consistent advantage over purely online or offline baselines.",
      "summary": "This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods.",
      "mindmap": "graph TB\n        Root(”Hybrid CMAB-T<br>混合组合多臂老虎机”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”在线方法成本高、适应慢<br>Online: High Cost, Slow”)\n        Problem --> P2(”离线方法受数据质量限制<br>Offline: Data Quality Limits”)\n        Method --> M1(”提出混合CMAB-T框架<br>Propose Hybrid CMAB-T Framework”)\n        Method --> M2(”设计混合CUCB算法<br>Design Hybrid CUCB Algorithm”)\n        M2 --> M2a(”利用离线数据引导探索<br>Use Offline Data to Guide”)\n        M2 --> M2b(”结合在线交互纠正偏差<br>Use Online to Correct Bias”)\n        Results --> R1(”理论悔恨界保证<br>Theoretical Regret Guarantee”)\n        Results --> R2(”实验显示一致优势<br>Empirical Consistent Advantage”)"
    },
    {
      "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
      "authors": "Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He",
      "institution": "Chongqing University, Xinjiang University",
      "link": "https://arxiv.org/pdf/2512.21999",
      "code": "https://github.com/hujiayu1223/ALEAHallu",
      "tags": [
        "multi-modal training",
        "hallucination mitigation",
        "adversarial parametric editing",
        "parameter clustering",
        "visual-language models",
        "activation dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.",
      "summary": "This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.",
      "mindmap": "graph TB\n        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]\n        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]\n        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]\n        C --> C1[激活数据集/Activation Dataset]\n        C --> C2[定位关键参数/Locate Critical Parameters]\n        C --> C3[对抗性编辑/Adversarial Editing]"
    },
    {
      "title": "DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction",
      "authors": "Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar",
      "institution": "National School of Artificial Intelligence (ENSIA)",
      "link": "https://arxiv.org/pdf/2512.22007",
      "code": null,
      "tags": [
        "computational biology",
        "protein language model",
        "ESM-2",
        "dual-stream architecture",
        "1D CNN",
        "transformer encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp",
      "contributions": "1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings.",
      "summary": "This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery.",
      "mindmap": "graph TB\n        A[DuaDeep-SeqAffinity: 序列抗原-抗体亲和力预测 / Sequence-Only Antigen-Antibody Affinity Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统方法依赖稀缺的3D结构 / Traditional methods rely on scarce 3D structures]\n        C --> C1[双流混合架构 / Dual-Stream Hybrid Architecture]\n        C1 --> C2[使用ESM-2嵌入 / Uses ESM-2 Embeddings]\n        C1 --> C3[1D CNN检测局部模式 / 1D CNN for Local Motifs]\n        C1 --> C4[Transformer编码全局上下文 / Transformer for Global Context]\n        C1 --> C5[融合模块整合特征 / Fusion Module Integrates Features]\n        D --> D1[性能超越SOTA / Outperforms SOTA]\n        D --> D2[皮尔逊相关: 0.688 / Pearson: 0.688]\n        D --> D3[AUC: 0.890]\n        D --> D4[证明序列嵌入的有效性 / Proves Efficacy of Sequence Embeddings]"
    },
    {
      "title": "HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness",
      "authors": "Chengyu Tian, Wenbin Pei",
      "institution": "Not explicitly stated in the provided content.",
      "link": "https://arxiv.org/pdf/2512.22014",
      "code": null,
      "tags": [
        "graph neural networks",
        "hypergraph isomorphism network",
        "hypergraph weisfeiler-lehman test",
        "higher-order network robustness",
        "hypergraph neural networks"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp",
      "contributions": "1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model's expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models.",
      "summary": "This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency.",
      "mindmap": "graph TB\n        A[HWL-HIN: Hypergraph-Level Hypergraph Isomorphism Network] --> B(核心问题/Problem: High computational cost of robustness assessment; Limited expressive power of HGNNs)\n        A --> C(主要方法/Method: Propose HWL-HIN framework inspired by GIN; Prove expressive power equivalent to Hypergraph WL test)\n        A --> D(关键结果/Results: Outperforms graph-based models and HGNNs; Maintains superior efficiency)"
    },
    {
      "title": "From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation",
      "authors": "Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni",
      "institution": "University College London, University of Urbino Carlo Bo",
      "link": "https://arxiv.org/pdf/2512.22031",
      "code": null,
      "tags": [
        "generative models for drug discovery",
        "hit-like molecule generation",
        "autoregressive models",
        "diffusion models",
        "docking scores",
        "multi-stage filtering"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp",
      "contributions": "1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.",
      "summary": "This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.",
      "mindmap": "graph TB\n        Root(”From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Hit identification is resource-intensive/命中识别资源密集”)\n        Method --> M1(”Propose tailored evaluation framework/提出定制评估框架”)\n        Method --> M2(”Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型”)\n        Results --> R1(”Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物”)\n        Results --> R2(”Selected hits synthesized & confirmed active/选定命中物被合成并确认有效”)"
    },
    {
      "title": "Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing",
      "authors": "Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt",
      "institution": "University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology",
      "link": "https://arxiv.org/pdf/2512.22024",
      "code": null,
      "tags": [
        "signal processing",
        "DOA estimation",
        "sparse arrays",
        "coarrays",
        "spatial smoothing",
        "MUSIC"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp",
      "contributions": "1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods.",
      "summary": "This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC.",
      "mindmap": "graph TB\n        A[Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DOA估计精度下降/DOA estimation accuracy degrades for correlated/coherent sources]\n        C --> C1[可变窗口空间平滑/Variable Window Size Spatial Smoothing]\n        C --> C2[压缩平滑孔径/Compressing the smoothing aperture]\n        C --> C3[VWS-CA-MUSIC算法/VWS-CA-MUSIC algorithm]\n        D --> D1[提高信噪子空间分离/Increased signal-noise subspace separation]\n        D --> D2[性能提升与复杂度降低/Performance improvements and complexity savings]"
    },
    {
      "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning",
      "authors": "Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo",
      "institution": "Nanjing University, University of Wollongong, University of Rochester",
      "link": "https://arxiv.org/pdf/2512.22029",
      "code": "https://github.com/RL-VIG/LibContinual",
      "tags": [
        "others",
        "catastrophic forgetting",
        "stability-plasticity dilemma",
        "modular architecture",
        "memory budget",
        "online continual learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp",
      "contributions": "1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.",
      "summary": "This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.",
      "mindmap": "graph TB\n        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]\n        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]\n        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]\n        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]\n        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]\n        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]\n        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]"
    },
    {
      "title": "Why Smooth Stability Assumptions Fail for ReLU Learning",
      "authors": "Ronald Katende",
      "institution": "Kabale University",
      "link": "https://arxiv.org/pdf/2512.22055",
      "code": null,
      "tags": [
        "optimization theory",
        "ReLU networks",
        "nonsmooth optimization",
        "stability analysis",
        "generalized derivatives",
        "learning dynamics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp",
      "contributions": "1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems.",
      "summary": "The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks.",
      "mindmap": "graph TB\n        A[Why Smooth Stability Assumptions Fail for ReLU Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Smooth stability assumptions are violated by ReLU networks.]\n        C[主要方法/Method<br>Provide counterexample and identify minimal generalized derivative condition.]\n        D[关键结果/Results<br>Classical bounds fail; stability can be restored under nonsmooth-aware condition.]"
    },
    {
      "title": "Scaling Adversarial Training via Data Selection",
      "authors": "Youran Ye, Dejin Wang, Ajinkya Bhandare",
      "institution": "Northeastern University",
      "link": "https://arxiv.org/pdf/2512.22069",
      "code": "https://github.com/youranye/Selective-Adversarial-Training",
      "tags": [
        "adversarial robustness",
        "adversarial training",
        "PGD",
        "sample selection",
        "gradient matching",
        "margin-based sampling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp",
      "contributions": "1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%.",
      "summary": "This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%.",
      "mindmap": "graph TB\n        A[Scaling Adversarial Training via Data Selection] --> B[核心问题/Problem: PGD计算成本高/High PGD Computational Cost]\n        A --> C[主要方法/Method: 选择性对抗训练/Selective Adversarial Training]\n        C --> D[选择标准1: 基于边界的采样/Margin-based Sampling]\n        C --> E[选择标准2: 梯度匹配采样/Gradient-matching Sampling]\n        A --> F[关键结果/Results: 鲁棒性相当，计算减少50%/Comparable Robustness, 50% Computation Reduction]"
    },
    {
      "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling",
      "authors": "Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras",
      "institution": "Uppsala University",
      "link": "https://arxiv.org/pdf/2512.22066",
      "code": null,
      "tags": [
        "llm inference",
        "SRAM",
        "frequency scaling",
        "energy-delay product",
        "systolic array",
        "memory bandwidth"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",
      "contributions": "1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.",
      "summary": "This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.",
      "mindmap": "graph TB\n        Root[”Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同”] --> Problem_Sub1[”SRAM大小与频率如何影响能效？”]\n        Problem --> Problem_Sub2[”内存带宽如何限制性能？”]\n        Method[”主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法”] --> Method_Sub1[”能耗建模/Energy Modeling”]\n        Method --> Method_Sub2[”延迟模拟/Latency Simulation”]\n        Method --> Method_Sub3[”操作强度分析/Operational Intensity”]\n        Results[”关键结果/Results”] --> Results_Sub1[”总能耗主要由SRAM大小决定<br>大缓存增加静态能耗”]\n        Results --> Results_Sub2[”高频可降低总能耗<br>（减少静态能耗）”]\n        Results --> Results_Sub3[”最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)”]"
    },
    {
      "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
      "authors": "Chiwun Yang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.22088",
      "code": null,
      "tags": [
        "learning theory",
        "scaling law",
        "learning dynamics",
        "generalization error",
        "transformer",
        "stochastic gradient descent"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp",
      "contributions": "1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\\{-1/6\\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.",
      "summary": "This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.",
      "mindmap": "graph TB\n        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]\n        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]\n        C --> C2[近似为核行为 / Approximate to kernel behaviors]\n        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]\n        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]\n        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]\n        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]\n        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]"
    },
    {
      "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting",
      "authors": "Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang",
      "institution": "University of Minnesota",
      "link": "https://arxiv.org/pdf/2512.22101",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent pipeline",
        "automated data analysis",
        "insight generation",
        "report synthesis",
        "visual analytics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp",
      "contributions": "1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.",
      "summary": "This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.",
      "mindmap": "graph TB\n        A[A2P-Vis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]\n        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]\n        B1 --> B3[组装成专业报告/Assembling coherent professional report]\n        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]\n        C1 --> C2[数据分析器/Data Analyzer]\n        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]\n        C1 --> C4[报告呈现器/Presenter]\n        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]\n        D --> D1[端到端协同分析/End-to-end co-analysis]\n        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]"
    },
    {
      "title": "Explainable Multimodal Regression via Information Decomposition",
      "authors": "Zhaozhao Ma, Shujian Yu",
      "institution": "Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway",
      "link": "https://arxiv.org/pdf/2512.22102",
      "code": "https://github.com/xxx/PIDReg",
      "tags": [
        "multimodal machine learning",
        "Partial Information Decomposition (PID)",
        "multimodal regression",
        "interpretability",
        "Gaussianity assumption",
        "conditional independence regularizer"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp",
      "contributions": "1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality.",
      "summary": "This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID's underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection.",
      "mindmap": "graph TB\n        A[Explainable Multimodal Regression via Information Decomposition<br/>可解释多模态回归与信息分解] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法缺乏量化模态贡献与交互的工具<br/>Existing methods lack tools to quantify modality contributions & interactions]\n        C --> C1[基于PID分解模态信息<br/>Decompose modality info via PID]\n        C --> C2[引入高斯性假设与正则化<br/>Introduce Gaussianity & regularizer]\n        D --> D1[在6个数据集上超越SOTA<br/>Outperforms SOTA on 6 datasets]\n        D --> D2[提升预测精度与可解释性<br/>Improves predictive accuracy & interpretability]"
    },
    {
      "title": "Sensitivity Analysis of the Consistency Assumption",
      "authors": "Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank",
      "institution": "Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University",
      "link": "https://arxiv.org/pdf/2512.21379",
      "code": null,
      "tags": [
        "causal inference",
        "consistency assumption",
        "sensitivity analysis",
        "hidden versions of treatment",
        "partial identification",
        "stable unit treatment value assumption (SUTVA)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp",
      "contributions": "1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects.",
      "summary": "This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates.",
      "mindmap": "graph TB\n        A[Sensitivity Analysis of the Consistency Assumption] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[一致性假设可能被违反/Consistency Assumption May Be Violated]\n        B1 --> B2[存在隐藏的治疗版本/Hidden Versions of Treatment Exist]\n        C --> C1[新颖的敏感性分析方法/Novel Sensitivity Analysis Method]\n        C1 --> C2[专注于隐藏版本导致的混杂/Focus on Confounding by Hidden Versions]\n        C2 --> C3[引入新的数学符号/Introduces New Mathematical Notation]\n        D --> D1[提出新的敏感性参数/Proposes New Sensitivity Parameter]\n        D1 --> D2[便于部分识别因果估计量/Facilitates Partial Identification of Causal Estimands]"
    },
    {
      "title": "Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)",
      "authors": "Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau",
      "institution": "Harvard University, Stockholm University, Malmö University, University of Catania, University of Toronto",
      "link": "https://arxiv.org/pdf/2512.21435",
      "code": null,
      "tags": [
        "spatio-temporal forecasting",
        "dynamic attention",
        "zero-inflated negative binomial",
        "elastic-net gating"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp",
      "contributions": "1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress.",
      "summary": "The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects.",
      "mindmap": "graph TB\n        A[DynAttn: Interpretable Spatio-Temporal Forecasting] --> B(核心问题/Problem: Forecasting sparse, bursty conflict fatalities)\n        A --> C(主要方法/Method: Dynamic attention, elastic-net gating, ZINB likelihood)\n        A --> D(关键结果/Results: Higher accuracy, interpretable regional dynamics)"
    },
    {
      "title": "Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding",
      "authors": "Akshansh Mishra",
      "institution": "Politecnico di Milano, AI Fab Lab",
      "link": "https://arxiv.org/pdf/2512.21344",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "molecular dynamics",
        "convolutional neural network",
        "friction stir welding",
        "explainable AI",
        "LAMMPS"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp",
      "contributions": "1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).",
      "summary": "This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.",
      "mindmap": "graph TB\n        Root[”Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模”]\n        Root --> Problem[”准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW”]\n        Root --> Method[”使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction”]\n        Root --> Results[”模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation”]"
    },
    {
      "title": "An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry",
      "authors": "Bing Cheng, Howell Tong",
      "institution": "Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text.",
      "link": "https://arxiv.org/pdf/2512.21451",
      "code": null,
      "tags": [
        "information geometry",
        "Fisher-Rao metric",
        "non-parametric",
        "G-entropy",
        "Covariate Fisher Information Matrix (cFIM)",
        "intrinsic dimensionality"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp",
      "contributions": "1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cramér-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality.",
      "summary": "This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI.",
      "mindmap": "graph TB\n        A[An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry] --> B(核心问题/Problem: Intractability of infinite-dimensional Fisher-Rao metric)\n        A --> C(主要方法/Method: Orthogonal decomposition of tangent space to derive Covariate Fisher Information Matrix (cFIM))\n        A --> D(关键结果/Results: Trace Theorem for G-entropy, link to CRLB, testable Manifold Hypothesis via cFIM rank)"
    },
    {
      "title": "Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases",
      "authors": "Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan",
      "institution": "University of California, Los Angeles (UCLA)",
      "link": "https://arxiv.org/pdf/2512.21389",
      "code": null,
      "tags": [
        "biomedical sensing and diagnostics",
        "vertical flow assay",
        "dual-mode detection",
        "neural network-based quantification",
        "multiplexed optical sensor",
        "point-of-care diagnostics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp",
      "contributions": "1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson's r > 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 µL of serum.",
      "summary": "This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples.",
      "mindmap": "graph TB\n        A[Deep learning-enhanced dual-mode multiplexed optical sensor<br>深度学习增强的双模式多重光学传感器] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Current POC tests: long turnaround, narrow range, single-analyte<br>当前POC测试：耗时长、范围窄、单分析物]\n        C[主要方法/Method<br>Dual-mode (colorimetric+chemiluminescent) xVFA + Neural Network<br>双模式(比色+化学发光)xVFA + 神经网络]\n        D[关键结果/Results<br>Simultaneous 3-analyte quant in 23 min, wide dynamic range, r>0.96<br>23分钟同步3分析物定量，宽动态范围，r>0.96]"
    },
    {
      "title": "Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications",
      "authors": "Brani Vidakovic",
      "institution": "Texas A&M University",
      "link": "https://arxiv.org/pdf/2512.21478",
      "code": null,
      "tags": [
        "quantum signal processing",
        "quantum nondecimated wavelet transform",
        "epsilon decimation",
        "Hadamard test",
        "quantum wavelet shrinkage",
        "shift invariance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp",
      "contributions": "1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing.",
      "summary": "This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction.",
      "mindmap": "graph TB\n        A[Quantum Nondecimated Wavelet Transform<br/>量子非抽取小波变换] --> B(核心问题/Problem: How to embed classical NDWT's redundancy and shift invariance into quantum computation?<br/>如何将经典NDWT的冗余性和平移不变性嵌入量子计算？)\n        A --> C(主要方法/Method: Two complementary quantum formulations.<br/>两种互补的量子形式。)\n        C --> C1(Formulation 1: Epsilon-decimated, uses controlled circular shifts & wavelet unitary.<br/>方法一：基于ε抽取，使用受控循环移位和小波酉变换。)\n        C --> C2(Formulation 2: Hadamard test, uses diagonal phase operators for interference.<br/>方法二：基于Hadamard测试，使用对角相位算子进行干涉。)\n        A --> D(关键结果/Results: Quantum NDWTs enable coherent postprocessing (e.g., shrinkage) and direct access to scalograms/spectra for applications like denoising.<br/>量子NDWT支持相干后处理并可直接获取尺度图/频谱，用于去噪等应用。)"
    },
    {
      "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models",
      "authors": "Takuro Kutsuna",
      "institution": "Toyota Central R&D Labs., Inc.",
      "link": "https://arxiv.org/pdf/2512.21593",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion models",
        "generative modeling",
        "evidence lower bound",
        "residual learning",
        "two-stage framework"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",
      "contributions": "1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.",
      "summary": "The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.",
      "mindmap": "graph TB\n        Root[”Residual Prior Diffusion (RPD) / 残差先验扩散模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details”]\n        Method --> M1[”两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model”]\n        Method --> M2[”概率模型与可处理ELBO / Probabilistic model with tractable ELBO”]\n        Results --> R1[”在合成数据上准确捕捉细节 / Accurately captures details on synthetic data”]\n        Results --> R2[”自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines”]\n        Results --> R3[”少步推理保持性能 / Maintains performance with few inference steps”]"
    },
    {
      "title": "Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine",
      "authors": "Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng",
      "institution": "Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences",
      "link": "https://arxiv.org/pdf/2512.21587",
      "code": null,
      "tags": [
        "photonic computing",
        "spatial photonic Ising machine",
        "Hadamard product",
        "amplitude-only modulation",
        "rank-free coupling",
        "incoherent light field"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp",
      "contributions": "1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation >0.98). 3. Shows the system's capability for ground-state search with &lt;0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms.",
      "summary": "This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations.",
      "mindmap": "graph TB\n        Root[”Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine<br>振幅调制空间光子伊辛机”]\n        Root --> Problem[”核心问题/Problem<br>Existing SPIMs sacrifice efficiency or scale to encode high-rank coupling and external fields.<br>现有SPIM编码高秩耦合和外场时牺牲效率或规模。”]\n        Root --> Method[”主要方法/Method<br>Reformulate Hamiltonian as sum of Hadamard products; map to incoherent light via amplitude modulators.<br>将哈密顿量重写为哈达玛积之和；通过振幅调制器映射到非相干光场。”]\n        Root --> Results[”关键结果/Results<br>797 spins, >0.98 correlation, <0.3% error rate, enables phase transition observation.<br>797个自旋，>0.98相关性，<0.3%错误率，支持相变观测。”]"
    },
    {
      "title": "Tilt Matching for Scalable Sampling and Fine-Tuning",
      "authors": "Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo",
      "institution": "Harvard University, University of Oxford, Kempner Institute, IAIFI",
      "link": "https://arxiv.org/pdf/2512.21829",
      "code": null,
      "tags": [
        "diffusion models",
        "Tilt Matching",
        "stochastic interpolants",
        "flow matching",
        "unnormalized densities",
        "fine-tuning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp",
      "contributions": "1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers.",
      "summary": "The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a \"tilted\" distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.",
      "mindmap": "graph TB\n        Root[Tilt Matching for Scalable Sampling and Fine-Tuning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Sampling from unnormalized densities and fine-tuning generative models] --> Problem_Detail[挑战/Challenges: Requires scalable, low-variance methods without reward gradients]\n        Method[主要方法/Method: Tilt Matching] --> Method_Detail1[基础/Basis: Dynamical equation relating flow matching velocity to tilted distribution]\n        Method_Detail1 --> Method_Detail2[特性/Properties: Implicitly solves stochastic optimal control, lower variance, no reward gradients needed]\n        Results[关键结果/Results: Empirical Verification] --> Results_Detail1[应用/Applications: State-of-the-art on Lennard-Jones potentials, competitive on Stable Diffusion fine-tuning]"
    },
    {
      "title": "Modeling high dimensional point clouds with the spherical cluster model",
      "authors": "Frédéric Cazals, Antoine Commaret, Louis Goldenberg",
      "institution": "Université Côte d'Azur, Inria, Ecole Polytechnique",
      "link": "https://arxiv.org/pdf/2512.21960",
      "code": null,
      "tags": [
        "clustering",
        "spherical cluster model",
        "high-dimensional median",
        "non-smooth optimization",
        "Clarke gradient",
        "stratified cell complex"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp",
      "contributions": "1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model's center behaves as a parameterized high-dimensional median.",
      "summary": "This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model's center is found to act as a robust, parameterized median.",
      "mindmap": "graph TB\n        A[Modeling high dimensional point clouds with the spherical cluster model] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[为高维点云建模/Modeling high-dimensional point clouds]\n        C --> C1[球形聚类模型/Spherical Cluster Model]\n        C --> C2[精确求解器使用Clarke梯度/Exact solver using Clarke gradient]\n        D --> D1[精确算法比BFGS快得多/Exact algorithm much faster than BFGS]\n        D --> D2[中心表现为参数化高维中位数/Center acts as parameterized high-dimensional median]"
    },
    {
      "title": "A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models",
      "authors": "John M. Mango, Ronald Katende",
      "institution": "Makerere University, Kabale University",
      "link": "https://arxiv.org/pdf/2512.22084",
      "code": null,
      "tags": [
        "dynamical systems",
        "numerical linear algebra",
        "linear conservation laws",
        "Frobenius norm",
        "orthogonal projection",
        "matrix correction",
        "data-driven models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp",
      "contributions": "1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example.",
      "summary": "This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models.",
      "mindmap": "graph TB\n        Root[”A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models”] --> Problem[”核心问题/Problem: Learned linear models violate known linear conservation laws.”]\n        Root --> Method[”主要方法/Method: Apply orthogonal projection A* = Â - C(CᵀC)⁻¹CᵀÂ to enforce CᵀA=0.”]\n        Root --> Results[”关键结果/Results: Enforces exact conservation with minimal perturbation; correction is unique and low-rank.”]"
    },
    {
      "title": "Parameter-Efficient Neural CDEs via Implicit Function Jacobians",
      "authors": "Ilya Kuleshov, Alexey Zaytsev",
      "institution": "Applied AI Institute",
      "link": "https://arxiv.org/pdf/2512.20625",
      "code": null,
      "tags": [
        "time series analysis",
        "Neural Controlled Differential Equations",
        "parameter efficiency",
        "implicit function Jacobians",
        "continuous RNN"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp",
      "contributions": "1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a \"Continuous RNN,\" aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.",
      "summary": "This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a \"Continuous RNN\" and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.",
      "mindmap": "graph LR\n    A[Parameter-Efficient Neural CDEs via Implicit Function Jacobians] --> B[核心问题/Problem: NCDEs require many parameters]\n    A --> C[主要方法/Method: Parameter-efficient formulation via implicit Jacobians, ”Continuous RNN” analogy]\n    A --> D[关键结果/Results: Achieves similar performance with far fewer parameters]"
    },
    {
      "title": "Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps",
      "authors": "Jacqueline Yau, Katherine J. Mimnaugh, Evan G. Center, Timo Ojala, Steven M. LaValle, Wenzhen Yuan, Nancy Amato, Minje Kim, Kara Federmeier",
      "institution": "University of Illinois Urbana-Champaign, University of Oulu",
      "link": "https://arxiv.org/pdf/2512.20620",
      "code": null,
      "tags": [
        "brain-computer interface",
        "EEG",
        "cybersickness",
        "interpretability maps",
        "convolutional neural networks",
        "event-related potentials"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp",
      "contributions": "1. Introduced a method using CNNs and transformers with interpretability maps (integrated gradients and class activation) to identify EEG features for cybersickness classification. 2. Identified a consistent and surprising pattern: amplitudes near the left prefrontal cortex electrode are important for cybersickness classification. 3. Proposed using the identified scalp location as a tagged feature for better real-time cybersickness classification with EEG.",
      "summary": "This paper addresses the challenge of detecting cybersickness from EEG data by using event-related potentials to isolate sickness-related brain activity from visual stimulus confounds. The authors employ trained convolutional neural networks and transformer models with interpretability maps to identify key EEG features. The main finding is that amplitudes recorded near the left prefrontal cortex are consistently important for classification, suggesting this location as a valuable feature for real-time detection.",
      "mindmap": "graph LR\n        A[Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps] --> B(核心问题/Problem: Cybersickness detection in VR using EEG is confounded by visual stimulus processing.)\n        A --> C(主要方法/Method: Use ERPs, CNNs/Transformers, and interpretability maps (integrated gradients/class activation) to analyze EEG data.)\n        A --> D(关键结果/Results: Left prefrontal cortex electrode amplitudes are consistently important for cybersickness classification.)"
    },
    {
      "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
      "authors": "Wenlong Tang",
      "institution": "Independent Researcher (No institutional affiliation inferred from provided content)",
      "link": "https://arxiv.org/pdf/2512.20629",
      "code": "https://github.com/wltang-dev/Latent-Strategy-RL-Agent",
      "tags": [
        "agent system",
        "multi-agent language systems",
        "latent strategy evolution",
        "reinforcement feedback",
        "external latent vectors",
        "dual-loop architecture"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model's parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.",
      "summary": "This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.",
      "mindmap": "graph LR\n    A[Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning] --> B[核心问题/Problem: Static semantic representations in LLMs cannot evolve with experience.]\n    A --> C[主要方法/Method: Dual-loop architecture (Behavior Loop & Language Loop) updates external latent vectors via reinforcement and reflection.]\n    A --> D[关键结果/Results: Agents develop stable, disentangled strategies; latent spaces show convergence and emergent adaptation.]"
    },
    {
      "title": "Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams",
      "authors": "Aayam Bansal, Ishaan Gangwani",
      "institution": "IEEE",
      "link": "https://arxiv.org/pdf/2512.20631",
      "code": null,
      "tags": [
        "sentiment analysis",
        "temporal drift",
        "zero-training detection",
        "transformer models",
        "social media streams",
        "model instability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp",
      "contributions": "1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.",
      "summary": "This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.",
      "mindmap": "graph LR\n    A[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --> B[核心问题/Problem: Transformer模型在动态事件期间的行为不稳定/Transformer model instability during dynamic events]\n    A --> C[主要方法/Method: 零训练检测框架与四个新指标/Zero-training detection framework with four novel metrics]\n    A --> D[关键结果/Results: 在真实数据上验证，准确率下降达23.4%，检测能力强/Validated on authentic data, 23.4% accuracy drop, strong detection capability]"
    },
    {
      "title": "Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models",
      "authors": "MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong",
      "institution": "Mayo Clinic",
      "link": "https://arxiv.org/pdf/2512.20633",
      "code": null,
      "tags": [
        "clinical prediction",
        "large language models",
        "semantic feature engineering",
        "multi-modal data integration",
        "goal-oriented knowledge curator",
        "treatment outcome prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp",
      "contributions": "1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.",
      "summary": "The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.",
      "mindmap": "graph LR\n    A[Enhancing Lung Cancer Treatment Outcome Prediction<br>增强肺癌治疗结果预测] --> B(核心问题/Problem: Sparse, heterogeneous clinical data<br>稀疏、异构的临床数据)\n    A --> C(主要方法/Method: LLMs as Goal-oriented Knowledge Curators<br>LLMs作为目标导向知识策展器)\n    A --> D(关键结果/Results: Superior AUROC 0.803, outperforms baselines<br>优异的AUROC 0.803，超越基线)"
    },
    {
      "title": "Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning",
      "authors": "Weiwei Wang",
      "institution": "Shenzhen Sunline Tech Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.20634",
      "code": null,
      "tags": [
        "llm training",
        "catastrophic forgetting",
        "spurious forgetting",
        "shallow alignment",
        "deep alignment",
        "task alignment depth"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp",
      "contributions": "1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.",
      "summary": "This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to \"spurious forgetting\" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.",
      "mindmap": "graph LR\n    A[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>虚假遗忘的实时检测与定量分析] --> B[核心问题/Problem: Catastrophic forgetting from shallow task alignment<br/>由浅层任务对齐导致的灾难性遗忘]\n    A --> C[主要方法/Method: Quantitative metrics & real-time detection for alignment depth<br/>对齐深度的量化指标与实时检测]\n    A --> D[关键结果/Results: High identification accuracy & improved robustness<br/>高识别准确率与提升的鲁棒性]"
    },
    {
      "title": "SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression",
      "authors": "Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang",
      "institution": "Minzu University of China, Shanghai Jiao Tong University, Peking University",
      "link": "https://arxiv.org/pdf/2512.20635",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "structured pruning",
        "attention head",
        "expert attention",
        "dynamic routing",
        "inference latency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee1b632660555a4b466586c0f3f7e064778c9087cde3160efa73cb5e0bf7723_w640_q70.webp",
      "contributions": "1. Proposes SHRP, a novel structured pruning framework that treats attention heads as independent experts and uses a unified Top-1 usage-driven mechanism for dynamic routing and deterministic pruning. 2. Introduces Expert Attention, a modular design with a lightweight shared expander feed-forward network to refine outputs after head selection. 3. Demonstrates significant compression on BERT-base, achieving high parameter/FLOP reduction with minimal accuracy loss, enabling practical deployment for latency-sensitive services.",
      "summary": "This paper addresses the high inference latency and memory consumption of Transformer encoders by proposing SHRP, a structured pruning framework that identifies and removes redundant attention heads. The method uses an Expert Attention module and a unified routing mechanism to compress the model while preserving accuracy. Experiments show SHRP can reduce BERT-base's parameters by 48% with 93% accuracy retained, and achieve a 4.2x throughput gain under extreme compression.",
      "mindmap": "graph LR\n    A[SHRP: Specialized Head Routing and Pruning] --> B(核心问题/Problem: Transformer编码器推理延迟高、内存消耗大/High inference latency & memory consumption of Transformer encoders)\n    A --> C(主要方法/Method: 专家注意力与动态路由/Expert Attention & dynamic routing for structured pruning)\n    A --> D(关键结果/Results: 高压缩比与精度保持/High compression ratio & accuracy preservation)"
    },
    {
      "title": "Data-Free Pruning of Self-Attention Layers in LLMs",
      "authors": "Dhananjay Saikumar, Blesson Varghese",
      "institution": "University of St Andrews",
      "link": "https://arxiv.org/pdf/2512.20636",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "attention pruning",
        "data-free pruning",
        "Gate-Norm",
        "inference acceleration",
        "attention suppression hypothesis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp",
      "contributions": "1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.",
      "summary": "The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.",
      "mindmap": "graph LR\n    A[Data-Free Pruning of Self-Attention Layers in LLMs] --> B[核心问题/Problem: LLM推理成本高，注意力层是瓶颈/High LLM inference cost, attention layers are bottleneck]\n    A --> C[主要方法/Method: 提出Gate-Norm，基于权重无数据剪枝/Propose Gate-Norm, weight-only data-free pruning]\n    A --> D[关键结果/Results: 推理速度提升1.30倍，精度损失<2%，速度快1000倍/1.30x faster inference, <2% accuracy drop, 1000x faster scoring]"
    },
    {
      "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks",
      "authors": "Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan",
      "institution": "Stanford University, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.20638",
      "code": "competency-gaps.github.io",
      "tags": [
        "llm evaluation",
        "sparse autoencoders",
        "benchmark gaps",
        "model gaps",
        "concept activations",
        "competency gaps"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp",
      "contributions": "1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.",
      "summary": "This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.",
      "mindmap": "graph LR\n        A[Uncovering Competency Gaps<br/>揭示能力差距] --> B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>问题：聚合指标掩盖模型/基准差距]\n        A --> C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>方法：使用稀疏自编码器进行概念级分解]\n        A --> D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>结果：发现非谄媚、安全方面的差距；基准过度代表服从性]"
    },
    {
      "title": "Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations",
      "authors": "Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat",
      "institution": "Vizuara AI Labs",
      "link": "https://arxiv.org/pdf/2512.20643",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Neural Ordinary Differential Equations",
        "Universal Differential Equations",
        "forecasting breakdown point",
        "n-body problem",
        "Julia"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp",
      "contributions": "1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the \"forecasting breakdown point\" to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.",
      "summary": "This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a \"forecasting breakdown point\" to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.",
      "mindmap": "graph LR\n    A[Forecasting N-Body Dynamics<br/>N体动力学预测] --> B[核心问题/Problem<br/>传统黑盒模型忽略物理定律<br/>Traditional black-box models ignore physics]\n    A --> C[主要方法/Method<br/>使用科学机器学习框架<br/>Use Scientific ML frameworks (NODEs, UDEs)]\n    A --> D[关键结果/Results<br/>UDE数据效率更高<br/>UDE is more data-efficient]"
    },
    {
      "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
      "authors": "Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam",
      "institution": "University at Buffalo, IBM Research",
      "link": "https://arxiv.org/pdf/2512.20655",
      "code": null,
      "tags": [
        "others",
        "mask optimization",
        "optical proximity correction",
        "inverse lithography technique",
        "deep learning",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp",
      "contributions": "1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.",
      "summary": "The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.",
      "mindmap": "graph LR\n    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];\n    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];\n    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];"
    },
    {
      "title": "Q-RUN: Quantum-Inspired Data Re-uploading Networks",
      "authors": "Wenbo Qiao, Shuaixian Wang, Peng Zhang, Yan Ming, Jiaming Zhao",
      "institution": "Tianjin University",
      "link": "https://arxiv.org/pdf/2512.20654",
      "code": null,
      "tags": [
        "quantum machine learning",
        "data re-uploading",
        "Fourier-expressive",
        "quantum-inspired",
        "neural network layer",
        "parameter efficiency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0762ab538e009e0b634d9be449e9107606ff0182095b377b357ae3a8796d291b_w640_q70.webp",
      "contributions": "1. Proposes Q-RUN, a novel quantum-inspired neural network layer that translates the mathematical paradigm of Data Re-uploading Quantum Circuits (DRQC) into a classical model, retaining their Fourier-expressive power without requiring quantum hardware. 2. Demonstrates that Q-RUN significantly outperforms standard fully connected layers and other state-of-the-art layers, reducing error by 1-3 orders of magnitude on certain tasks while using fewer parameters. 3. Shows that Q-RUN can serve as a versatile, drop-in replacement for fully connected layers, improving performance across a wide range of neural architectures and illustrating how quantum ML principles can enhance classical AI design.",
      "summary": "This paper introduces Q-RUN, a quantum-inspired neural network layer based on data re-uploading principles, designed to capture high-frequency functions efficiently without quantum hardware. It demonstrates superior performance and parameter efficiency compared to standard layers across various modeling tasks. The work shows how quantum machine learning concepts can guide the development of more expressive classical AI models.",
      "mindmap": "graph LR\n    A[Q-RUN: Quantum-Inspired Data Re-uploading Networks] --> B[核心问题/Problem: DRQC量子模型受限于硬件可扩展性<br>DRQC quantum models are limited by hardware scalability]\n    A --> C[主要方法/Method: 提出经典量子启发式数据重上传网络<br>Propose classical quantum-inspired data re-uploading network (Q-RUN)]\n    A --> D[关键结果/Results: 性能更优，参数更少，可作为全连接层替代<br>Superior performance, fewer parameters, drop-in replacement for FC layers]"
    },
    {
      "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering",
      "authors": "Matthew Thompson",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.20660",
      "code": null,
      "tags": [
        "agent system",
        "dual-state architecture",
        "atomic action pairs",
        "guard functions",
        "neuro-symbolic systems",
        "code generation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp",
      "contributions": "1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.",
      "summary": "This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.",
      "mindmap": "graph LR\n    A[Managing the Stochastic<br>管理随机性] --> B[Problem: LLM-based agents prone to stochastic failures<br>问题: 基于LLM的智能体易受随机性故障影响]\n    A --> C[Method: Dual-State Architecture, Atomic Action Pairs, Guard Functions<br>方法: 双态架构, 原子动作对, 守卫函数]\n    A --> D[Results: Improved success rates, architectural constraints can substitute for scale<br>结果: 成功率提升, 架构约束可替代模型规模]"
    },
    {
      "title": "Graph Neural Networks for Source Detection: A Review and Benchmark Study",
      "authors": "Martin Sterchi, Nathan Brack, Lorenz Hilfiker",
      "institution": "University of Applied Sciences and Arts Northwestern Switzerland FHNW, University of Zürich",
      "link": "https://arxiv.org/pdf/2512.20657",
      "code": null,
      "tags": [
        "graph neural networks",
        "Graph Neural Networks",
        "Epidemic Source Detection",
        "Rumor Centrality",
        "Benchmark Study",
        "Single-Source Problems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3b943e72216650945e413f1e6dcd8c0978e12972df42eff52dc2deb20fcbe59_w640_q70.webp",
      "contributions": "1. A comprehensive review of existing GNN-based methods for source detection, clarifying their settings and models. 2. Proposal of a principled GNN architecture specifically tailored for the source detection task. 3. A systematic benchmark study demonstrating that GNNs substantially outperform traditional source detection methods across various network types, and advocating for epidemic source detection as a benchmark task for evaluating GNN architectures.",
      "summary": "This paper reviews Graph Neural Network (GNN) approaches for detecting the source of an epidemic in a network, proposes a new GNN architecture for the task, and conducts a benchmark study. The experiments show that GNNs significantly outperform traditional methods like rumor centrality, establishing them as highly effective for source detection and suggesting the task as a standard benchmark for GNN evaluation.",
      "mindmap": "graph LR\n        A[Graph Neural Networks for Source Detection: A Review and Benchmark Study] --> B[核心问题/Problem: 识别网络中流行病传播的源头/Identify the source of an epidemic in a network]\n        A --> C[主要方法/Method: 回顾GNN方法并提出新的GNN架构/Review GNN methods and propose a new GNN architecture]\n        A --> D[关键结果/Results: GNN显著优于传统方法，提议作为基准任务/GNNs substantially outperform traditional methods, propose as a benchmark task]"
    },
    {
      "title": "Dominating vs. Dominated: Generative Collapse in Diffusion Models",
      "authors": "Hayeon Jeong, Jong-Seok Lee",
      "institution": "Yonsei University",
      "link": "https://arxiv.org/pdf/2512.20666",
      "code": null,
      "tags": [
        "text-to-image generation",
        "diffusion models",
        "cross-attention",
        "generative collapse",
        "multi-concept generation",
        "attention dynamics"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp",
      "contributions": "1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation & distributed head mechanisms) perspectives.",
      "summary": "This paper investigates the \"Dominant-vs-Dominated\" (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.",
      "mindmap": "graph LR\n        A[Dominating vs. Dominated<br/>支配 vs. 被支配] --> B[核心问题/Problem<br/>Multi-concept prompt generation imbalance<br/>多概念提示生成失衡];\n        A --> C[主要方法/Method<br/>Introduce DominanceBench & analyze causes<br/>引入DominanceBench并分析原因];\n        A --> D[关键结果/Results<br/>Data diversity & attention dynamics cause DvD<br/>数据多样性和注意力动态导致DvD];"
    },
    {
      "title": "Forward Only Learning for Orthogonal Neural Networks of any Depth",
      "authors": "Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen",
      "institution": "Université Paris-Dauphine - PSL, ESPCI PSL",
      "link": "https://arxiv.org/pdf/2512.20668",
      "code": "https://github.com/",
      "tags": [
        "neural network training algorithms",
        "forward-only learning",
        "orthogonal neural networks",
        "backpropagation alternative",
        "FOTON",
        "PEPITA"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp",
      "contributions": "1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs>",
      "summary": "This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.",
      "mindmap": "graph LR\n    A[Forward Only Learning for Orthogonal Neural Networks<br>前向传播学习用于正交神经网络] --> B[Problem: Backpropagation is computationally expensive<br>问题: 反向传播计算成本高]\n    A --> C[Method: FOTON - Forward-Only Training with modulated forward pass<br>方法: FOTON - 使用调制前向传播的前向训练]\n    A --> D[Results: Trains networks of any depth, outperforms PEPITA<br>结果: 可训练任意深度网络，性能优于PEPITA]"
    },
    {
      "title": "Improving Cardiac Risk Prediction Using Data Generation Techniques",
      "authors": "Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama",
      "institution": "Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela",
      "link": "https://arxiv.org/pdf/2512.20669",
      "code": null,
      "tags": [
        "generative models",
        "Conditional Variational Autoencoder",
        "synthetic data generation",
        "cardiac risk prediction",
        "data augmentation",
        "clinical records"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp",
      "contributions": "1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.",
      "summary": "This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.",
      "mindmap": "graph LR\n    A[Improving Cardiac Risk Prediction Using Data Generation Techniques] --> B(核心问题/Problem: 真实医疗数据稀缺、不完整且存在缺失值/Real-world medical data is scarce, incomplete, and has missing values)\n    A --> C(主要方法/Method: 基于条件变分自编码器的架构生成合成临床记录/CVAE-based architecture for synthetic clinical record generation)\n    A --> D(关键结果/Results: 生成的数据提高了心脏风险预测分类器的准确性/Generated data improves cardiac risk prediction classifier accuracy)"
    },
    {
      "title": "Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection",
      "authors": "Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang",
      "institution": "Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano",
      "link": "https://arxiv.org/pdf/2512.20670",
      "code": null,
      "tags": [
        "multimodal fake news detection",
        "inconsistency detection",
        "feature disentanglement",
        "conflict-consensus mechanism",
        "physics-inspired dynamics",
        "cross-modal discrepancy"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp",
      "contributions": "1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.",
      "summary": "The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.",
      "mindmap": "graph LR\n    A[Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection] --> B[核心问题/Problem: 主流一致性融合将关键跨模态差异误判为噪声，稀释了伪造证据]\n    A --> C[主要方法/Method: 提出DCCF框架，解耦事实与情感，利用物理启发的动力学主动放大矛盾]\n    A --> D[关键结果/Results: 在三个真实数据集上超越SOTA，平均准确率提升3.52%]"
    },
    {
      "title": "Revisiting the Learning Objectives of Vision-Language Reward Models",
      "authors": "Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome",
      "institution": "Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université",
      "link": "https://arxiv.org/pdf/2512.20675",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reward modeling",
        "vision-language models",
        "triplet loss",
        "Meta-World",
        "contrastive learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp",
      "contributions": "1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.",
      "summary": "This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.",
      "mindmap": "graph LR\n        A[Revisiting VLM Reward Models] --> B(核心问题/Problem: 难以比较不同奖励模型目标/Difficulty in comparing reward model objectives)\n        A --> C(主要方法/Method: 统一框架评估/Unified framework evaluation)\n        A --> D(关键结果/Results: 三元组损失更优/Triplet loss outperforms SOTA)"
    },
    {
      "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
      "authors": "Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour",
      "institution": "Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen",
      "link": "https://arxiv.org/pdf/2512.20674",
      "code": null,
      "tags": [
        "multi-modal training",
        "Low-Rank Adaptation (LoRA)",
        "parameter-efficient fine-tuning",
        "rank adaptation",
        "mobile vision language model",
        "dynamic scheduling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp",
      "contributions": "1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.",
      "summary": "This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.",
      "mindmap": "graph LR\n    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]\n    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]\n    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]"
    },
    {
      "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
      "authors": "Stefano Grassi",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.20688",
      "code": null,
      "tags": [
        "multi-agent systems",
        "Differentiable Price Mechanism",
        "Dominant Strategy Incentive Compatibility",
        "VCG-equivalent incentive",
        "Dec-POMDPs",
        "Bayesian Incentive Compatibility"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp",
      "contributions": "1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.",
      "summary": "The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.",
      "mindmap": "graph LR\n    A[Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems] --> B[核心问题/Problem: Hayekian Information Problem & Hurwiczian Incentive Problem]\n    A --> C[主要方法/Method: Differentiable Price Mechanism (DPM) & Bayesian Extension]\n    A --> D[关键结果/Results: DSIC/BIC Guarantee, Linear Scaling, 50x Faster than Model-Free RL]"
    },
    {
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "authors": "Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.20687",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical autoregressive model",
        "KV-cache optimization",
        "memory-bound inference",
        "multi-resolution context",
        "throughput-quality trade-off"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",
      "contributions": "1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.",
      "summary": "The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.",
      "mindmap": "graph LR\n    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]\n    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]\n    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]"
    },
    {
      "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
      "authors": "Omer Gazit, Yael Itzhakev, Yuval Elovici, Asaf Shabtai",
      "institution": "Ben-Gurion University of the Negev",
      "link": "https://arxiv.org/pdf/2512.20712",
      "code": null,
      "tags": [
        "adversarial machine learning",
        "adversarial attack",
        "drone detection",
        "radio frequency (RF)",
        "over-the-air (OTA)",
        "I/Q perturbation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384206c86ac50ef9b9d711e77f0b2e691640ea7831fb12c1517a40575d8c07f3_w640_q70.webp",
      "contributions": "1. The first physical adversarial attack targeting image-based object detection models for RF-based drone detection. 2. A novel method that optimizes adversarial perturbations directly in the complex baseband (I/Q) domain for over-the-air transmission. 3. Demonstration of the attack's effectiveness and hardware compatibility through both digital and physical (OTA) evaluations with multiple drone types.",
      "summary": "This paper proposes the first physical adversarial attack against RF-based drone detectors. Instead of modifying digital spectrogram images, the method generates and transmits optimized I/Q perturbation waveforms alongside legitimate drone signals. The results show these structured perturbations are compatible with standard RF hardware and reliably reduce target drone detection while maintaining detection of legitimate ones.",
      "mindmap": "graph LR\n    A[Real-World Adversarial Attacks on RF-Based Drone Detectors] --> B(核心问题/Problem: Digital RF attacks are hard to implement over-the-air)\n    A --> C(主要方法/Method: Generate & transmit I/Q perturbation waveforms)\n    A --> D(关键结果/Results: Perturbations reduce target detection, preserve others, are hardware-compatible)"
    },
    {
      "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
      "authors": "Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune",
      "institution": "Boston University, Move37 Labs",
      "link": "https://arxiv.org/pdf/2512.20732",
      "code": null,
      "tags": [
        "llm inference",
        "Finite Element Method (FEM)",
        "Code Generation",
        "LLM Benchmark",
        "Computational Mechanics",
        "Scientific Machine Learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp",
      "contributions": "1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs' ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.",
      "summary": "The paper identifies a lack of benchmarks for evaluating LLMs' scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.",
      "mindmap": "graph LR\n        A[FEM-Bench Paper] --> B[核心问题/Problem: 缺乏评估LLM生成科学物理模型代码能力的基准/Lack of benchmark for evaluating LLMs' ability to generate scientifically valid physical model code]\n        A --> C[主要方法/Method: 提出基于计算力学和有限元法的结构化基准/Proposes a structured benchmark based on computational mechanics and the Finite Element Method]\n        A --> D[关键结果/Results: 先进LLM无法可靠解决所有基准任务，为跟踪进展奠定基础/State-of-the-art LLMs cannot reliably solve all benchmark tasks, establishing a foundation for tracking progress]"
    },
    {
      "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
      "authors": "Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang",
      "institution": "Tsinghua University, Tencent Hunyuan",
      "link": "https://arxiv.org/pdf/2512.20745",
      "code": null,
      "tags": [
        "agent system",
        "tool-augmented agent",
        "agentic reinforcement learning",
        "supervised fine-tuning (SFT)",
        "request-level asynchronous rollout",
        "prefix-aware load balancing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp",
      "contributions": "1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.",
      "summary": "This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.",
      "mindmap": "graph LR\n    A[AgentMath] --> B[核心问题/Problem: LRMs are inefficient and inaccurate for complex math]\n    A --> C[主要方法/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]\n    A --> D[关键结果/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]"
    },
    {
      "title": "AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication",
      "authors": "Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur",
      "institution": "Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)",
      "link": "https://arxiv.org/pdf/2512.20739",
      "code": null,
      "tags": [
        "wireless networks",
        "Deep Reinforcement Learning (DRL)",
        "Reconfigurable Intelligent Surfaces (RIS)",
        "Energy Harvesting (EH)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp",
      "contributions": "1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.",
      "summary": "This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.",
      "mindmap": "graph LR\n    A[AI-Driven Green CRNs for 6G] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[频谱稀缺与高能耗/Spectrum Scarcity & High Energy Consumption]\n    C --> C1[AI驱动框架/AI-Driven Framework]\n    C1 --> C2[集成DRL, TL, EH, RIS/Integrates DRL, TL, EH, RIS]\n    D --> D1[节能25-30%/25-30% Energy Saving]\n    D --> D2[AUC>0.90, PDR提升/AUC>0.90, PDR Improved]"
    },
    {
      "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
      "authors": "Tony Tran, Bin Hu",
      "institution": "University of Houston",
      "link": "https://arxiv.org/pdf/2512.20746",
      "code": null,
      "tags": [
        "on-device ai",
        "neural architecture search",
        "hardware-aware search",
        "edge detection",
        "TinyML",
        "waste detection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp",
      "contributions": "1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.",
      "summary": "This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.",
      "mindmap": "graph LR\n        A[TrashDet] --> B[核心问题/Problem: 边缘设备垃圾检测<br>TinyML Constraints];\n        A --> C[主要方法/Method: 迭代硬件感知NAS<br>Iterative Hardware-aware NAS];\n        A --> D[关键结果/Results: 高效TrashDet家族<br>Efficient TrashDet Family];\n        B --> D;\n        C --> D;"
    },
    {
      "title": "Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies",
      "authors": "Diyar Altinses, Andreas Schwung",
      "institution": "South Westphalia University of Applied Sciences",
      "link": "https://arxiv.org/pdf/2512.20749",
      "code": null,
      "tags": [
        "multi-modal training",
        "Lipschitz Continuity",
        "Attention Mechanism",
        "Aggregation Methods",
        "Training Stability",
        "Multimodal Autoencoders"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp",
      "contributions": "1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method's superior performance in consistency, convergence speed, and accuracy.",
      "summary": "This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.",
      "mindmap": "graph LR\n    A[Stabilizing Multimodal Autoencoders<br/>稳定多模态自编码器] --> B(核心问题/Problem: Training Stability & Robustness<br/>训练稳定性与鲁棒性)\n    A --> C(主要方法/Method: Theoretical Lipschitz Analysis & Regularized Attention Fusion<br/>理论Lipschitz分析与正则化注意力融合)\n    A --> D(关键结果/Results: Improved Consistency, Convergence, Accuracy<br/>提升的一致性、收敛速度与精度)"
    },
    {
      "title": "Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits",
      "authors": "Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz",
      "institution": "The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside",
      "link": "https://arxiv.org/pdf/2512.20755",
      "code": null,
      "tags": [
        "others",
        "formal verification",
        "neural network robustness",
        "early exits",
        "adversarial perturbations",
        "off-the-shelf solvers"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp",
      "contributions": "1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.",
      "summary": "This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Bridging Efficiency and Safety] --> B(核心问题 / Problem<br>Verifying Early Exit Networks);\n    A --> C(主要方法 / Method<br>Tailored Robustness Property & Enhanced Algorithm);\n    A --> D(关键结果 / Results<br>Improved Verifiability & Efficiency);"
    },
    {
      "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
      "authors": "Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel",
      "institution": "University of Toronto, Vector Institute, Google DeepMind, McGill University, Mila - Quebec AI Institute, University of Cambridge, Hugging Face",
      "link": "https://arxiv.org/pdf/2512.20757",
      "code": "https://github.com/r-three/Tokenizers",
      "tags": [
        "tokenization",
        "tokenizer",
        "language models",
        "benchmark",
        "subword segmentation",
        "BPE"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp",
      "contributions": "1. Introduces TokSuite, a collection of fourteen language models that are identical except for their tokenizers, enabling isolated study of tokenizer impact. 2. Curates and releases a new benchmark designed to measure model performance under real-world text perturbations that affect tokenization. 3. Provides a robust framework that supports novel findings on the benefits and shortcomings of various popular tokenizers.",
      "summary": "This paper addresses the challenge of isolating the impact of tokenizer choice on language model behavior. It proposes TokSuite, a suite of models with different tokenizers but identical other components, along with a specialized benchmark. The work enables systematic analysis and reveals new insights into how different tokenizers affect model performance.",
      "mindmap": "graph LR\n        A[TokSuite: Measuring Tokenizer Impact] --> B[核心问题/Problem: Tokenization's role in LM performance is poorly understood]\n        A --> C[主要方法/Method: TokSuite - Identical models with different tokenizers + new benchmark]\n        A --> D[关键结果/Results: Novel findings on tokenizer benefits and shortcomings]"
    },
    {
      "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
      "authors": "Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei",
      "institution": "Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago",
      "link": "https://arxiv.org/pdf/2512.20760",
      "code": null,
      "tags": [
        "reinforcement learning",
        "RLVR",
        "causal reasoning",
        "generalization",
        "supervised fine-tuning",
        "large language models"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp",
      "contributions": "1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.",
      "summary": "This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.",
      "mindmap": "graph LR\n    A[”Generalization of RLVR Using Causal Reasoning as a Testbed<br>以因果推理为测试平台的RLVR泛化研究”] --> B[”核心问题/Problem<br>RLVR何时能实现鲁棒泛化？<br>When does RLVR yield robust generalization?”]\n    A --> C[”主要方法/Method<br>在因果图模型上实证研究RLVR与SFT<br>Empirical study of RLVR vs SFT on causal graphical models”]\n    A --> D[”关键结果/Results<br>RLVR泛化更强，但依赖模型规模与初始能力<br>RLVR yields stronger generalization but depends on model size & initial competence”]"
    },
    {
      "title": "TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform",
      "authors": "Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller",
      "institution": "Paderborn University",
      "link": "https://arxiv.org/pdf/2512.20761",
      "code": "https://huggingface.co/spaces/DAG-UPB/TS-Arena",
      "tags": [
        "others",
        "time series foundation models",
        "live forecasting",
        "pre-registration",
        "information leakage",
        "temporal split"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp",
      "contributions": "1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.",
      "summary": "The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.",
      "mindmap": "graph LR\n    A[TS-Arena Technical Report] --> B[核心问题/Problem: TSFM评估危机 / TSFM Evaluation Crisis]\n    A --> C[主要方法/Method: 预注册实时预测平台 / Pre-registered Live Forecasting Platform]\n    A --> D[关键结果/Results: 防止历史污染，真实评估泛化 / Prevents Historical Contamination, Authentic Generalization Assessment]\n    B --> E[信息泄露与数据重用 / Information Leakage & Data Reuse]\n    C --> F[实时数据流与严格时间分割 / Live Data Streams & Strict Temporal Split]\n    D --> G[可持续的基准测试基础设施 / Sustainable Benchmarking Infrastructure]"
    },
    {
      "title": "Subgroup Discovery with the Cox Model",
      "authors": "Zachary Izzo, Iain Melvin",
      "institution": "NEC Labs America",
      "link": "https://arxiv.org/pdf/2512.20762",
      "code": null,
      "tags": [
        "survival analysis",
        "subgroup discovery",
        "Cox proportional hazards model",
        "expected prediction entropy",
        "conditional rank statistics",
        "interpretable machine learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa075e21de650db93aab9430f1fc0b8df1921ae52ac2d9c0e9c3dadc8ba3c5f7_w640_q70.webp",
      "contributions": "1. Introduced two novel metrics for evaluating survival models in the context of subgroup discovery: the Expected Prediction Entropy (EPE) and the Conditional Rank Statistics (CRS). 2. Proposed eight algorithms for the Cox subgroup discovery problem, with a main algorithm that leverages both EPE and CRS and has theoretical correctness guarantees. 3. Demonstrated the effectiveness of the methods through empirical evaluation on synthetic and real data, including a case study on NASA jet engine simulation data.",
      "summary": "This paper addresses the problem of subgroup discovery for survival analysis, aiming to find interpretable data subsets where the Cox model is highly accurate. The authors propose two new evaluation metrics (EPE and CRS) and eight algorithms to solve this problem. Their methods successfully recover ground-truth subgroups and improve model fit compared to fitting a Cox model on the entire dataset, as validated on synthetic, real, and NASA case study data.",
      "mindmap": "graph LR\n    A[Subgroup Discovery with the Cox Model] --> B(核心问题/Problem: Find interpretable subsets where Cox model is accurate)\n    A --> C(主要方法/Method: Introduce EPE & CRS metrics; Propose 8 algorithms)\n    A --> D(关键结果/Results: Recovers ground-truth subgroups; Better model fit; Validated on NASA data)"
    },
    {
      "title": "Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer",
      "authors": "Jorge Sastre, Daniel Faronbi, José Miguel Alonso, Peter Traver, Javier Ibáñez, Nuria Lloret",
      "institution": "Universitat Politècnica de València, New York University",
      "link": "https://arxiv.org/pdf/2512.20777",
      "code": null,
      "tags": [
        "gpu kernels",
        "matrix exponential",
        "scaling and squaring",
        "Taylor series",
        "generative AI",
        "numerical stability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp",
      "contributions": "1. An optimized Taylor-based algorithm for matrix exponential designed for high-throughput generative AI flows. 2. A rigorous error analysis and a dynamic selection strategy for Taylor order and scaling factor to minimize computation under error tolerance. 3. Extensive numerical experiments demonstrating significant acceleration and high numerical stability compared to state-of-the-art methods.",
      "summary": "This paper proposes an optimized Taylor-based algorithm for computing the matrix exponential, a key operation in generative AI. The method improves upon classical techniques like Paterson-Stockmeyer and includes a dynamic strategy to balance accuracy and speed. Experiments show it offers significant acceleration while maintaining high numerical stability for large-scale generative modeling.",
      "mindmap": "graph LR\n        A[Improving Matrix Exponential for Generative AI Flows<br/>生成式AI流程的矩阵指数改进] --> B(Problem: Standard methods (Padé) may be inefficient for high-throughput AI<br/>问题: 标准方法对高吞吐AI效率不足)\n        A --> C(Method: Optimized Taylor-based algorithm with dynamic parameter selection<br/>方法: 优化的基于泰勒级数的算法与动态参数选择)\n        A --> D(Results: Significant acceleration & high numerical stability demonstrated<br/>结果: 显著加速并保持高数值稳定性)"
    },
    {
      "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
      "authors": "Raja Mallina, Bryar Shareef",
      "institution": "University of Nevada, Las Vegas",
      "link": "https://arxiv.org/pdf/2512.20783",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nullable prompts",
        "mixed-supervision",
        "vision-language models",
        "breast ultrasound segmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp",
      "contributions": "1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.",
      "summary": "The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.",
      "mindmap": "graph LR\n    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]\n    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]\n    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]"
    },
    {
      "title": "Symbolic regression for defect interactions in 2D materials",
      "authors": "Mikhail Lazarev, Andrey Ustyuzhanin",
      "institution": "HSE University, Institute for Functional Intelligent Materials (National University of Singapore), Constructor University",
      "link": "https://arxiv.org/pdf/2512.20785",
      "code": null,
      "tags": [
        "symbolic regression",
        "Symbolic Regression",
        "SEGVAE",
        "Graph Neural Networks",
        "2D Materials",
        "Interpretability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e61aacf1f171adb632fccf66c94fbeaf1d24724489a5e5c0d3c2a23d60bf7d16_w640_q70.webp",
      "contributions": "1. Applied the deep symbolic regression algorithm SEGVAE to model defect interactions in 2D materials. 2. Demonstrated that symbolic regression can achieve performance comparable to state-of-the-art graph neural network methods. 3. Discussed the broader applicability and advantages (e.g., interpretability) of symbolic regression methods in natural sciences.",
      "summary": "This paper applies the SEGVAE deep symbolic regression algorithm to discover analytical equations describing defect interactions in 2D materials. The results show that this interpretable method achieves performance comparable to state-of-the-art graph neural networks, highlighting its potential for scientific discovery.",
      "mindmap": "graph LR\n    A[Symbolic regression for defect interactions in 2D materials] --> B(核心问题/Problem: ML models lack interpretability for scientific data)\n    A --> C(主要方法/Method: Apply SEGVAE for symbolic regression)\n    A --> D(关键结果/Results: Comparable performance to GNNs, offers interpretability)"
    },
    {
      "title": "FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative",
      "authors": "Mohammadreza Rostami, Solmaz S. Kia",
      "institution": "University of California Irvine",
      "link": "https://arxiv.org/pdf/2512.20814",
      "code": null,
      "tags": [
        "federated learning",
        "gradient compression",
        "directional derivative",
        "privacy preservation",
        "communication efficiency",
        "low-rank projection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/026756c87a289335823e9350b3b3cf6b10435dec5873ca0fe832db069efa00b9_w640_q70.webp",
      "contributions": "1. Proposes FedMPDD, a novel FL algorithm that compresses high-dimensional gradients into low-dimensional messages via multi-projected directional derivatives, reducing uplink cost from O(d) to O(m)., 2. Provides theoretical convergence analysis showing FedMPDD achieves an O(1/√K) convergence rate, matching FedSGD, by averaging multiple projections to overcome single-projection limitations., 3. Demonstrates the method offers inherent privacy against gradient inversion attacks due to geometric properties of low-rank projections, providing a tunable privacy-utility trade-off.",
      "summary": "This paper addresses the high communication cost and privacy risks in Federated Learning. It proposes FedMPDD, which compresses client gradients by computing their directional derivatives along multiple random vectors, significantly reducing bandwidth usage while providing inherent privacy. Theoretical and experimental results show the method maintains convergence performance comparable to FedSGD and offers a tunable privacy-utility trade-off.",
      "mindmap": "graph LR\n    A[FedMPDD] --> B[核心问题/Problem: High Communication Cost & Privacy Risk in FL]\n    A --> C[主要方法/Method: Multi-Projected Directional Derivative for Gradient Compression]\n    A --> D[关键结果/Results: O(1/√K) Convergence, O(m) Communication, Inherent Privacy]"
    },
    {
      "title": "GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface",
      "authors": "Miguel Esparza, Vamshi Battal, Ali Mostafavi",
      "institution": "Texas A&M University",
      "link": "https://arxiv.org/pdf/2512.20813",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "graph neural networks",
        "XGBoost",
        "AlphaEarth embeddings",
        "contagion dynamics",
        "ensemble learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3a0fb230e974a7c6219e056bcb5e5d527990414b399512769ab5745bdc847_w640_q70.webp",
      "contributions": "1. A novel dual-specialist ensemble framework that disentangles wildfire vulnerability into distinct environmental contagion and structural fragility vectors. 2. Integration of a physics-informed Graph Neural Network (GNN) for neighborhood-scale contagion modeling with an XGBoost model for asset-level structural resilience. 3. Generation of a diagnostic risk topology enabling targeted mitigation strategies, such as vegetation management for high-connectivity clusters and structural hardening for vulnerable nodes.",
      "summary": "This paper proposes GraphFire-X, a dual-specialist ensemble framework combining a physics-informed Graph Neural Network and XGBoost to model building-scale wildfire risk at the Wildland-Urban Interface. The method disentangles environmental contagion from structural fragility, revealing that neighborhood-scale environmental pressure dominates propagation pathways while eaves are a key micro-scale vulnerability. The ensemble provides a diagnostic risk map to guide precise, proactive mitigation strategies.",
      "mindmap": "graph LR\n    A[GraphFire-X] --> B[核心问题/Problem: 传统模型无法捕捉WUI非线性蔓延动态/Traditional models fail to capture non-linear contagion dynamics at WUI]\n    A --> C[主要方法/Method: 双专家集成框架/Dual-Specialist Ensemble: 物理信息GNN(环境) + XGBoost(结构)/Physics-informed GNN (Environment) + XGBoost (Structure)]\n    A --> D[关键结果/Results: 环境压力主导蔓延路径，屋檐是主要入侵点，生成诊断风险拓扑/Environmental pressure dominates pathways, eaves are key ingress, diagnostic risk topology generated]"
    },
    {
      "title": "Defending against adversarial attacks using mixture of experts",
      "authors": "Mohammad Meymani, Roozbeh Razavi-Far",
      "institution": "University of New Brunswick",
      "link": "https://arxiv.org/pdf/2512.20821",
      "code": null,
      "tags": [
        "Adversarial Machine Learning",
        "Mixture of Experts",
        "Adversarial Training",
        "Robustness",
        "Ensemble Learning",
        "Evasion Attacks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0952838b3d0745a7f0396318a91c52e0ef0a7e273730cc1cf1a311f7bfecf079_w640_q70.webp",
      "contributions": "1. Proposes a defense system that integrates an adversarial training module within a Mixture of Experts (MoE) architecture. 2. Employs nine pre-trained experts with ResNet-18 backbones and jointly optimizes both the expert parameters and the gating mechanism during end-to-end training. 3. Demonstrates that the proposed system outperforms state-of-the-art defense systems and plain classifiers, even those with more complex architectures.",
      "summary": "This paper addresses the vulnerability of machine learning models to adversarial attacks. It proposes a defense system that combines adversarial training with a Mixture of Experts architecture, using nine pre-trained ResNet-18 models. The system is shown to be more robust than existing defenses and standard classifiers.",
      "mindmap": "graph LR\n    A[Defending against adversarial attacks using mixture of experts] --> B(核心问题/Problem: ML模型易受对抗性攻击/ML Models Vulnerable to Adversarial Attacks)\n    A --> C(主要方法/Method: 对抗训练与专家混合体结合/Adversarial Training within Mixture-of-Experts)\n    A --> D(关键结果/Results: 优于现有防御与分类器/Outperforms State-of-the-art Defenses & Classifiers)"
    },
    {
      "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions",
      "authors": "Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava",
      "institution": "Arizona State University, Brown University",
      "link": "https://arxiv.org/pdf/2512.20831",
      "code": "https://github.com/AAIR-lab/PEARL.git",
      "tags": [
        "reinforcement learning",
        "parameterized actions",
        "state abstraction",
        "action abstraction",
        "TD(λ)",
        "sample efficiency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp",
      "contributions": "1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.",
      "summary": "This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.",
      "mindmap": "graph LR\n    A[Context-Sensitive Abstractions for RL with Parameterized Actions] --> B(核心问题/Problem: RL for Parameterized Actions)\n    A --> C(主要方法/Method: Learn & Refine State/Action Abstractions)\n    A --> D(关键结果/Results: Higher Sample Efficiency for TD(λ))"
    },
    {
      "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images",
      "authors": "Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo",
      "institution": "Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.20833",
      "code": null,
      "tags": [
        "bioimage analysis",
        "multi-channel microscopy",
        "cellular morphology",
        "pre-training dataset",
        "channel-adaptive models",
        "heterogeneous data"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp",
      "contributions": "1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.",
      "summary": "This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.",
      "mindmap": "graph LR\n        A[CHAMMI-75: Pre-training Multi-channel Models] --> B[核心问题/Problem: Specialized models cannot be reused across studies]\n        A --> C[主要方法/Method: Curate CHAMMI-75, a heterogeneous multi-channel dataset]\n        A --> D[关键结果/Results: Training with CHAMMI-75 improves performance via diversity]"
    },
    {
      "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
      "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.20848",
      "code": null,
      "tags": [
        "llm inference",
        "Mixture-of-Experts",
        "Mamba-Transformer",
        "agentic reasoning",
        "sparse activation",
        "long context"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp",
      "contributions": "1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.",
      "summary": "This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.",
      "mindmap": "graph LR\n    A[Nemotron 3 Nano<br>论文标题/Paper Title] --> B[构建高效、能进行智能体推理的大模型<br>核心问题/Problem];\n    A --> C[混合MoE与Mamba-Transformer架构，稀疏激活参数<br>主要方法/Method];\n    A --> D[更高推理吞吐与精度，支持100万令牌上下文<br>关键结果/Results];"
    },
    {
      "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
      "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.20856",
      "code": null,
      "tags": [
        "llm inference",
        "Mixture-of-Experts",
        "Mamba-Transformer",
        "LatentMoE",
        "NVFP4",
        "multi-environment reinforcement learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp",
      "contributions": "1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.",
      "summary": "This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.",
      "mindmap": "graph LR\n    A[NVIDIA Nemotron 3] --> B[核心问题/Problem: Efficient and open intelligence for agentic applications]\n    A --> C[主要方法/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]\n    A --> D[关键结果/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]"
    },
    {
      "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
      "authors": "Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim",
      "institution": "University of Michigan",
      "link": "https://arxiv.org/pdf/2512.20861",
      "code": "https://github.com/pabillam/mem-efficient-blr",
      "tags": [
        "llm inference",
        "block low-rank (BLR)",
        "Triton kernels",
        "memory-bound optimization",
        "Jetson Orin Nano",
        "roofline analysis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp",
      "contributions": "1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.",
      "summary": "This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.",
      "mindmap": "graph LR\n    A[Memory-Efficient Acceleration of Block Low-Rank Foundation Models] --> B[核心问题/Problem: BLR模型多token推理存在内存墙/Multi-token inference for BLR models is memory-bound]\n    A --> C[主要方法/Method: 定制Triton内核与内存优化/Custom Triton kernels with memory optimizations]\n    A --> D[关键结果/Results: 显著加速与模型压缩/Significant speedup & model compression]"
    },
    {
      "title": "Robustness Certificates for Neural Networks against Adversarial Attacks",
      "authors": "Sara Taheri, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Majid Zamani",
      "institution": "LMU Munich, TUM Munich, University of Colorado Boulder",
      "link": "https://arxiv.org/pdf/2512.20865",
      "code": null,
      "tags": [
        "adversarial robustness",
        "barrier certificates",
        "data poisoning",
        "formal verification",
        "scenario convex program",
        "robustness certification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5a85af42ea34173637ac88fc6859bba1d4a68d3161f2fcc2567276ca7d37b5b_w640_q70.webp",
      "contributions": "1. Introduces a formal robustness certification framework by modeling gradient-based training as a discrete-time dynamical system and formulating poisoning robustness as a safety verification problem., 2. Adapts barrier certificates from control theory to derive sufficient conditions for certifying a robust radius against worst-case ℓp-norm poisoning, and makes it practical by parameterizing BCs as neural networks., 3. Derives PAC bounds via a scenario convex program to provide a confidence lower bound on the certified robustness radius, and extends the unified framework to also certify against test-time attacks.",
      "summary": "This paper addresses the lack of formal guarantees in defending neural networks against data poisoning attacks. It proposes a certification framework that models training as a dynamical system, uses barrier certificates for verification, and provides PAC-bounded robustness radii. Experiments show the approach certifies non-trivial perturbation budgets without needing prior attack knowledge.",
      "mindmap": "graph LR\n    A[Robustness Certificates for Neural Networks against Adversarial Attacks] --> B[核心问题/Problem: Lack of formal guarantees for neural networks against data poisoning attacks]\n    A --> C[主要方法/Method: Model training as dt-DS, use Barrier Certificates & Scenario Convex Program for verification]\n    A --> D[关键结果/Results: Certifies non-trivial robustness radii on MNIST/SVHN/CIFAR-10, model-agnostic, no prior attack knowledge needed]"
    },
    {
      "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
      "authors": "Jakir Hossain, Gurvinder Singh, Lukasz Ziarek, Ahmet Erdem Sarıyüce",
      "institution": "University at Buffalo",
      "link": "https://arxiv.org/pdf/2512.20872",
      "code": "https://erdemub.github.io/BCG-dataset",
      "tags": [
        "malware detection",
        "function call graphs",
        "Android malware",
        "graph-based classification",
        "APK",
        "dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87c9a8db7055364694bd0257a161b2a24a77226da1ab49504cd20679d8699222_w640_q70.webp",
      "contributions": "1. Introduces the Better Call Graphs (BCG) dataset, a new large-scale collection of unique and recent Android FCGs for malware classification., 2. Addresses limitations of existing datasets (outdated, redundant, small graphs) to prevent overfitting and enable reliable evaluation., 3. Demonstrates the necessity and value of the BCG dataset through extensive experiments with baseline classifiers.",
      "summary": "This paper addresses the lack of high-quality datasets for Android malware classification using function call graphs (FCGs). It introduces the Better Call Graphs (BCG) dataset, which contains large, unique, and recent FCGs from Android APKs. Experiments show BCG is necessary for reliable evaluation and helps overcome overfitting issues present with older datasets.",
      "mindmap": "graph LR\n    A[Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification] --> B(核心问题/Problem: Lack of large-scale, high-quality Android FCG datasets hinders malware classification research)\n    A --> C(主要方法/Method: Introduce BCG dataset with large, unique, recent Android APK FCGs)\n    A --> D(关键结果/Results: BCG enables more reliable evaluation and addresses overfitting compared to existing datasets)"
    },
    {
      "title": "Architectural Trade-offs in Small Language Models Under Compute Constraints",
      "authors": "Shivraj Singh Bhatti",
      "institution": "University of Massachusetts Amherst",
      "link": "https://arxiv.org/pdf/2512.20877",
      "code": null,
      "tags": [
        "language modeling",
        "small language models",
        "compute constraints",
        "architectural trade-offs",
        "rotary positional embeddings",
        "transformer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp",
      "contributions": "1. A systematic empirical study of architectural choices (from linear predictors to transformers) for small language models under strict compute constraints. 2. An analysis showing attention-based models are more FLOP-efficient than MLPs even at small scale, and that increasing depth/context without sufficient optimization can hurt performance. 3. An investigation revealing that techniques like Rotary Positional Embeddings (RoPE), successful in large models, do not necessarily transfer effectively to the small-model regime.",
      "summary": "This paper systematically studies how architectural choices affect small language model performance under limited compute. The method involves progressively building from linear predictors to multi-layer transformers and evaluating them on character and word-level datasets. The main conclusion is that attention is more efficient than MLPs per FLOP at small scales, but scaling depth or applying large-model techniques like RoPE can be detrimental without careful optimization.",
      "mindmap": "graph LR\n    A[Architectural Trade-offs in Small Language Models<br>小型语言模型的架构权衡] --> B[核心问题/Problem<br>How do architectural choices affect performance under compute constraints?<br>计算约束下架构选择如何影响性能？]\n    A --> C[主要方法/Method<br>Progressive architectural study from linear to transformer models<br>从线性到Transformer模型的渐进式架构研究]\n    A --> D[关键结果/Results<br>Attention > MLPs in per-FLOP efficiency; RoPE may not transfer<br>注意力机制单位FLOP效率优于MLP；RoPE可能不适用于小模型]"
    },
    {
      "title": "Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks",
      "authors": "Runqi Lin",
      "institution": "The University of Sydney",
      "link": "https://arxiv.org/pdf/2512.20893",
      "code": null,
      "tags": [
        "adversarial robustness",
        "adversarial robustness",
        "deep neural networks",
        "time-efficient",
        "red-blue adversarial framework",
        "adversarial evaluation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c027984dda35366287528822de7264568bdef6dac0b744de35e5554d0dc77b9_w640_q70.webp",
      "contributions": "1. Proposes time-efficient methods for evaluating adversarial robustness in DNNs, 2. Proposes time-efficient methods for enhancing adversarial robustness in DNNs, 3. Aims to overcome the computational intensity limitation of existing approaches for large-scale models.",
      "summary": "This thesis addresses the computational inefficiency of existing methods for evaluating and improving the adversarial robustness of deep neural networks. It proposes new time-efficient techniques for both identifying vulnerabilities (red team) and mitigating them (blue team). The main goal is to make adversarial robustness assessment and enhancement more applicable to large-scale models.",
      "mindmap": "graph LR\n    A[Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks] --> B[核心问题/Problem: 现有对抗鲁棒性评估与增强方法计算成本高 / Existing adversarial robustness evaluation and enhancement methods are computationally intensive.]\n    A --> C[主要方法/Method: 为红队（评估）和蓝队（增强）提供时间高效的方法 / Provides time-efficient methods for the red team (evaluation) and blue team (enhancement).]\n    A --> D[关键结果/Results: 旨在克服大规模模型的应用限制 / Aims to overcome the applicability limitation for large-scale models.]"
    },
    {
      "title": "From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction",
      "authors": "Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard",
      "institution": "Université Jean Monnet Saint-Étienne, CNRS, Institut d'Optique Graduate School, Laboratoire Hubert Curien, Institut Universitaire de France (IUF), Inria",
      "link": "https://arxiv.org/pdf/2512.20885",
      "code": null,
      "tags": [
        "communication & networking",
        "Graph Neural Networks",
        "Kolmogorov-Arnold Networks",
        "Symbolic Regression",
        "Attention",
        "Message Passing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96bffa46ff987475631fa980227e1a967ebbe523f541ff36441600d02001b3_w640_q70.webp",
      "contributions": "1. A heterogeneous Graph Neural Network with attention-based message passing as a strong baseline for flow delay prediction. 2. FlowKANet, a fully KAN-based GNN architecture that integrates KAN operators into message-passing and attention computation for efficiency and interpretability. 3. A symbolic distillation of FlowKANet via block-wise regression to produce lightweight, transparent closed-form surrogate models.",
      "summary": "This paper tackles flow delay prediction in communication networks by proposing a progression of models. It introduces FlowKANet, a GNN that replaces standard MLP layers with Kolmogorov-Arnold Networks (KANs) for better efficiency and interpretability, and further distills it into symbolic surrogate models. The results show that KANs offer a good efficiency-accuracy trade-off, and the symbolic surrogates enable lightweight, transparent deployment.",
      "mindmap": "graph LR\n    A[From GNNs to Symbolic Surrogates via KANs for Delay Prediction] --> B[核心问题/Problem: Accurate flow delay prediction for network optimization]\n    A --> C[主要方法/Method: Propose FlowKANet (KAN-based GNN) and distill it into symbolic surrogates]\n    A --> D[关键结果/Results: KANs balance efficiency & accuracy; surrogates enable lightweight, transparent deployment]"
    },
    {
      "title": "DiEC: Diffusion Embedded Clustering",
      "authors": "Haidong Hu",
      "institution": "Not explicitly provided in the given content.",
      "link": "https://arxiv.org/pdf/2512.20905",
      "code": null,
      "tags": [
        "deep clustering",
        "diffusion models",
        "representation selection",
        "self-training",
        "graph regularization",
        "denoising consistency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp",
      "contributions": "1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model's internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.",
      "summary": "The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.",
      "mindmap": "graph LR\n    A[DiEC: Diffusion Embedded Clustering] --> B[核心问题/Problem: Single fixed embedding ignores varying clusterability in diffusion model's internal trajectory];\n    A --> C[主要方法/Method: Two-stage search (CML & OTS) on layer×timestep, regularized self-training with denoising-consistency];\n    A --> D[关键结果/Results: Achieves competitive clustering performance on multiple benchmarks];"
    },
    {
      "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
      "authors": "Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.20920",
      "code": null,
      "tags": [
        "llm training",
        "reversible networks",
        "memory-efficient fine-tuning",
        "mixture-of-experts",
        "full-parameter fine-tuning",
        "activation recomputation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp",
      "contributions": "1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.",
      "summary": "The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model's expressive power.",
      "mindmap": "graph LR\n    A[RevFFN: Memory-Efficient Fine-Tuning] --> B[核心问题/Problem: Full fine-tuning memory overhead高]\n    A --> C[主要方法/Method: 使用可逆Transformer块/Use reversible Transformer blocks]\n    A --> D[关键结果/Results: 单GPU高效全参数微调/Efficient full fine-tuning on single GPU]"
    },
    {
      "title": "Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining",
      "authors": "Bharat Sharman, Elkafi Hassini",
      "institution": "(Inferred from author names and arXiv submission; specific institution not provided in abstract. Could be a university research group.)",
      "link": "https://arxiv.org/pdf/2512.20915",
      "code": null,
      "tags": [
        "combinatorial optimization",
        "graph features",
        "hardness prediction",
        "association rule mining",
        "maximum clique problem",
        "machine learning classification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6637eddd49fc539b03c9e2248f2656c238346a34b80db77df78bdd41cc252f_w640_q70.webp",
      "contributions": "1. Proposes GCO-HPIF, a general two-stage ML framework for predicting and explaining the hardness of graph-based combinatorial optimization problems. 2. Demonstrates the framework's effectiveness by applying it to the maximum clique problem using diverse algorithms (exact and GNN-based) and achieving high prediction accuracy with few features. 3. Introduces the use of association rule mining (FP-Growth) to generate human-interpretable explanations for the hardness predictions.",
      "summary": "This paper introduces GCO-HPIF, a machine learning framework that predicts and explains the computational hardness of graph-based combinatorial optimization problems by first classifying instances using graph features and then explaining the predictions via association rule mining. The framework was validated on a large dataset of maximum clique problem instances, achieving excellent prediction performance and generating interpretable rules. The results show the framework's potential for both accurate hardness forecasting and providing insights into problem difficulty.",
      "mindmap": "graph LR\n    A[Towards a General Framework...<br/>预测与解释图组合优化问题难度的通用框架] --> B(核心问题/Problem: Predicting computational hardness of graph-based combinatorial optimization problems<br/>预测图组合优化问题的计算难度)\n    A --> C(主要方法/Method: Two-stage ML framework (GCO-HPIF)<br/>两阶段机器学习框架)\n    C --> C1(Stage 1: Classification using graph features<br/>阶段1: 使用图特征的分类)\n    C --> C2(Stage 2: Explanation using Association Rule Mining (FP-Growth)<br/>阶段2: 使用关联规则挖掘进行解释)\n    A --> D(关键结果/Results: High prediction accuracy (F1=0.9921), interpretable rules, low error for time prediction<br/>高预测精度, 可解释规则, 低时间预测误差)"
    },
    {
      "title": "Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy",
      "authors": "Deepit Sapru",
      "institution": "University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.20932",
      "code": null,
      "tags": [
        "revenue management",
        "constrained optimization",
        "Bayesian hierarchical modeling",
        "Monte Carlo simulation",
        "price elasticity",
        "churn prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp",
      "contributions": "1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.",
      "summary": "This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.",
      "mindmap": "graph LR\n    A[Guardrailed Elasticity Pricing] --> B[核心问题/Problem: Static pricing fails to balance revenue & retention];\n    A --> C[主要方法/Method: Forecast + Elasticity + Churn model with constrained optimization];\n    A --> D[关键结果/Results: Outperforms static pricing, protects customers, enables durable growth];"
    },
    {
      "title": "A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate",
      "authors": "Yiren Shen, Juan J. Alonso",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.20941",
      "code": null,
      "tags": [
        "others",
        "graph neural network",
        "surrogate model",
        "multi-fidelity dataset",
        "scaling laws",
        "aerodynamic field prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp",
      "contributions": "1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.",
      "summary": "This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.",
      "mindmap": "graph LR\n        A[论文标题 / Paper Title<br>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws] --> B(核心问题 / Problem<br>缺乏开源多保真数据集与数据规模对模型性能影响的实证指导 / Lack of open-source multi-fidelity datasets and empirical guidelines on data scaling)\n        A --> C(主要方法 / Method<br>发布数据集并进行缩放研究 / Release dataset and conduct scaling study)\n        B --> D(关键结果 / Results<br>误差随数据量呈幂律下降 / Test error decreases with data size via power law)\n        C --> D"
    },
    {
      "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
      "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.20943",
      "code": null,
      "tags": [
        "communication & networking",
        "4D Gaussian Splatting",
        "video streaming",
        "integer linear programming",
        "pruning",
        "keyframe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
      "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
      "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
      "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
    },
    {
      "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
      "authors": "Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah",
      "institution": "Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is \"MultiMind\".",
      "link": "https://arxiv.org/pdf/2512.20950",
      "code": null,
      "tags": [
        "crosslingual information retrieval",
        "dual-encoder",
        "contrastive learning",
        "hard negative sampling",
        "data augmentation",
        "multi-source alignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp",
      "contributions": "1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.",
      "summary": "This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.",
      "mindmap": "graph LR\n        A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --> B(核心问题/Problem: Rapid spread of multilingual misinformation);\n        A --> C(主要方法/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);\n        A --> D(关键结果/Results: Improved retrieval accuracy on benchmarks);"
    },
    {
      "title": "Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations",
      "authors": "Xianjin Yang, Matthieu Darcy, Matthew Hudes, Francis J. Alexander, Gregory Eyink, Houman Owhadi",
      "institution": "Caltech",
      "link": "https://arxiv.org/pdf/2512.20956",
      "code": null,
      "tags": [
        "operator learning",
        "Gaussian processes",
        "functional renormalization group",
        "functional PDEs",
        "operator learning",
        "non-perturbative methods"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18c810a016d418e9edb4adddfcb5d19e76d457adf7e311d4c5ca1b064e1fa145_w640_q70.webp",
      "contributions": "1. Proposes a Gaussian process-based operator learning framework for solving functional PDEs directly in function space, independent of specific discretizations. 2. Demonstrates the method's application to non-perturbative functional renormalization group equations (e.g., Wetterich, Wilson-Polchinski), achieving performance equal to or better than traditional approximations like the local-potential approximation. 3. Shows the framework's flexibility in handling non-constant fields and incorporating physical priors, making it promising for studying complex field configurations like instantons.",
      "summary": "This paper introduces a Gaussian process operator learning framework to solve functional partial differential equations, specifically targeting non-perturbative functional renormalization group equations. The method directly represents functionals in function space, offering flexibility and independence from equation-specific discretizations. The results demonstrate that it matches or outperforms traditional approximations like the local-potential approximation and can handle complex, non-constant field configurations.",
      "mindmap": "graph LR\n    A[Solving Functional PDEs with Gaussian Processes] --> B(核心问题/Problem: Solving functional renormalization group equations)\n    A --> C(主要方法/Method: Gaussian process operator learning in function space)\n    A --> D(关键结果/Results: Matches or beats traditional approximations, handles non-constant fields)"
    },
    {
      "title": "ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design",
      "authors": "R Yadunandan, Nimisha Ghosh",
      "institution": "Department of Computer Science and Engineering, Shiv Nadar University Chennai",
      "link": "https://arxiv.org/pdf/2512.20958",
      "code": "https://github.com/YadunandanRaman/ReACT-Drug/",
      "tags": [
        "reinforcement learning",
        "Proximal Policy Optimization (PPO)",
        "ChemBERTa",
        "ESM-2",
        "reaction-template",
        "de novo drug design"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp",
      "contributions": "1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.",
      "summary": "This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.",
      "mindmap": "graph LR\n        A[ReACT-Drug] --> B[核心问题/Problem: Navigating vast chemical space for synthesizable, high-affinity drugs];\n        A --> C[主要方法/Method: RL + Protein Embeddings + Reaction-Template Actions];\n        A --> D[关键结果/Results: Novel, valid, synthetically accessible candidates];"
    },
    {
      "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
      "authors": "An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding",
      "institution": "University of Minnesota, University of Chicago, Cisco Research",
      "link": "https://arxiv.org/pdf/2512.20959",
      "code": null,
      "tags": [
        "automated data science",
        "agentic AI",
        "domain knowledge",
        "synthetic data",
        "large language models",
        "human-AI teaming"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp",
      "contributions": "1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.",
      "summary": "The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.",
      "mindmap": "graph LR\n    A[Can Agentic AI Match Human Data Scientists?] --> B[核心问题/Problem: Can agentic AI match human performance using domain knowledge?];\n    A --> C[主要方法/Method: Design task with latent variable in images, use synthetic insurance data];\n    A --> D[关键结果/Results: Agentic AI with generic workflow falls short; highlights need for domain-aware AI];"
    },
    {
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "authors": "Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu",
      "institution": "University of Michigan, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20963",
      "code": "https://deepthink-umich.github.io",
      "tags": [
        "diffusion models",
        "representation learning",
        "denoising autoencoder",
        "memorization detection",
        "representation steering",
        "generalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp",
      "contributions": "1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (\"spiky\" vs. \"balanced\") using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.",
      "summary": "This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to \"spiky\" representations while generalization yields \"balanced\" ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.",
      "mindmap": "graph LR\n    A[Generalization of Diffusion Models Arises with a Balanced Representation Space] --> B(核心问题/Problem: Diffusion models risk memorizing training data)\n    A --> C(主要方法/Method: Analyze via representation learning using a two-layer ReLU DAE)\n    A --> D(关键结果/Results: Memorization yields spiky representations, generalization yields balanced ones; Enables detection and editing techniques)"
    },
    {
      "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
      "authors": "Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu",
      "institution": "Fudan University, University of Oregon, Inria",
      "link": "https://arxiv.org/pdf/2512.20967",
      "code": null,
      "tags": [
        "llm training",
        "spot instance",
        "online scheduling",
        "deadline-aware",
        "LoRA",
        "integer programming"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp",
      "contributions": "1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.",
      "summary": "This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.",
      "mindmap": "graph LR\n    A[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --> B(核心问题/Problem: Expensive LLM fine-tuning with volatile spot instances)\n    A --> C(主要方法/Method: Mixed instance scheduling with prediction & online policy selection)\n    A --> D(关键结果/Results: O(√T) regret, up to 54.8% utility gain)"
    },
    {
      "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
      "authors": "Jingyang You, Hanna Kurniawati",
      "institution": "Australian National University",
      "link": "https://arxiv.org/pdf/2512.20974",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Bayesian Reinforcement Learning",
        "Meta-Reinforcement Learning",
        "Generalised Linear Models",
        "Learnable Basis Functions",
        "Variational Inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp",
      "contributions": "1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.",
      "summary": "This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.",
      "mindmap": "graph LR\n    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]\n    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]\n    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]"
    },
    {
      "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
      "authors": "Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu",
      "institution": "Lumos AI",
      "link": "https://arxiv.org/pdf/2512.20983",
      "code": null,
      "tags": [
        "llm evaluation",
        "medical conversation",
        "mistake replication",
        "benchmark creation",
        "llm judges",
        "single-shot qa"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp",
      "contributions": "1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.",
      "summary": "The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.",
      "mindmap": "graph LR\n    A[Automatic Replication of LLM Mistakes in Medical Conversations] --> B(核心问题/Problem: LLM错误难以在其他模型中复现/Mistakes hard to replicate across LLMs)\n    A --> C(主要方法/Method: MedMistake自动管道/MedMistake automatic pipeline)\n    A --> D(关键结果/Results: 发布基准并评估12个LLM/Released benchmark & evaluated 12 LLMs)\n    C --> C1(生成对话/Generate conversations)\n    C --> C2(LLM委员会评估/LLM committee evaluation)\n    C --> C3(创建单轮QA对/Create single-shot QA pairs)\n    D --> D1(MedMistake-All数据集/MedMistake-All dataset)\n    D --> D2(MedMistake-Bench验证子集/MedMistake-Bench validated subset)\n    D --> D3(GPT/Claude/Grok表现最佳/GPT/Claude/Grok performed best)"
    },
    {
      "title": "CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices",
      "authors": "Alberto. Palomo-Alonso, David Casillas-Perez, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, Sancho Salcedo-Sanz",
      "institution": "Department of Signal Processing and Communications, Universidad de Alcalá (Spain)",
      "link": "https://arxiv.org/pdf/2512.21000",
      "code": null,
      "tags": [
        "correlation matrix segmentation",
        "correlation matrices",
        "segmentation algorithms",
        "metaheuristic optimization",
        "overlapping technique",
        "window difference metric"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a9a31a49fc5fbd3e09151d607fb89a66d2e15d7c89ecd0f67b8be73bebac208_w640_q70.webp",
      "contributions": "1. Proposes CoSeNet, a novel four-layer algorithmic architecture for optimal segmentation of noisy correlation matrices. 2. Introduces a method using a heuristic algorithm to optimize the re-scaling layer parameters based on a Window Difference-based fitness metric. 3. Utilizes an overlapping technique and pre-trained ML algorithms to enhance robustness and generalizability.",
      "summary": "This paper introduces CoSeNet, a novel four-layer model for optimally segmenting correlated groups within noisy correlation matrices. The method uses an overlapping technique, pre-trained ML algorithms, and a heuristic to optimize its re-scaling parameters. The output is a clean, binary segmentation matrix, offering a balanced solution in terms of efficiency, memory, and speed.",
      "mindmap": "graph LR\n    A[CoSeNet: 相关矩阵分割] --> B[核心问题: 噪声相关矩阵中的相关段识别 / Problem: Identifying correlated segments in noisy correlation matrices]\n    A --> C[主要方法: 四层架构与启发式优化 / Method: Four-layer architecture with heuristic optimization]\n    A --> D[关键结果: 生成无噪声二值分割矩阵 / Results: Generates noise-free binary segmentation matrix]\n    C --> C1[输入层 / Input Layer]\n    C --> C2[格式化层 / Formatting Layer]\n    C --> C3[重缩放层 / Re-scaling Layer]\n    C --> C4[分割层 / Segmentation Layer]\n    C3 --> C3a[启发式参数优化 / Heuristic Parameter Optimization]\n    C4 --> C4a[重叠技术与预训练ML / Overlapping Technique & Pre-trained ML]"
    },
    {
      "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
      "authors": "Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong",
      "institution": "ByteDance Seed, Carnegie Mellon University, Columbia University",
      "link": "https://arxiv.org/pdf/2512.21010",
      "code": null,
      "tags": [
        "llm evaluation",
        "Competitive Swiss-System Dynamics",
        "Expected Win Score",
        "Failure Sensitivity Analysis",
        "Monte Carlo Simulation",
        "risk appetite"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp",
      "contributions": "1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.",
      "summary": "The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.",
      "mindmap": "graph LR\n    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]\n    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]\n    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]"
    },
    {
      "title": "Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces",
      "authors": "Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel",
      "institution": "Mercari, Inc.",
      "link": "https://arxiv.org/pdf/2512.21021",
      "code": null,
      "tags": [
        "text embeddings",
        "Matryoshka Representation Learning",
        "role-specific prefixes",
        "purchase-driven fine-tuning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp",
      "contributions": "1. Proposed a domain-aware Japanese text-embedding model fine-tuned on purchase-driven query-title pairs for C2C marketplace search. 2. Introduced the use of role-specific prefixes to model the query-item asymmetry inherent in search tasks. 3. Applied Matryoshka Representation Learning to create compact, truncation-robust embeddings that meet production latency and throughput constraints.",
      "summary": "This paper addresses the challenge of improving search relevance in noisy, user-generated C2C marketplaces by fine-tuning a Japanese text-embedding model with role-specific prefixes and Matryoshka Representation Learning. The method produces compact embeddings that are robust to truncation for efficiency. Offline and online evaluations show significant improvements in retrieval quality and business metrics, providing a practical foundation for enhanced search experiences.",
      "mindmap": "graph LR\n    A[Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces] --> B[核心问题/Problem: C2C搜索挑战<br>Short queries, Noisy listings, Production constraints]\n    A --> C[主要方法/Method: 领域感知嵌入<br>Domain-aware embeddings via fine-tuning, Role prefixes, Matryoshka Learning]\n    A --> D[关键结果/Results: 离线与在线提升<br>Offline gains, Online revenue & efficiency improvements]"
    },
    {
      "title": "Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection",
      "authors": "Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak",
      "institution": "Indian Institute of Technology (Indore, Patna)",
      "link": "https://arxiv.org/pdf/2512.21039",
      "code": null,
      "tags": [
        "misinformation detection",
        "multi-persona agent",
        "LLM-SLM synergy",
        "evidence-grounded",
        "multimodal fusion",
        "credibility fusion"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp",
      "contributions": "1. Proposed AMPEND-LS, an agentic multi-persona framework that integrates textual, visual, and contextual evidence through a structured LLM reasoning pipeline. 2. Introduced a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context to improve reliability. 3. Designed a complementary SLM classifier to mitigate LLM uncertainty and hallucinations, enhancing robustness and explainability.",
      "summary": "The paper addresses the challenge of multimodal fake news detection by proposing AMPEND-LS, a framework that synergizes LLMs and SLMs within a multi-persona agent structure to reason over diverse evidence. It demonstrates superior performance over state-of-the-art baselines in accuracy and robustness across multiple datasets. The work contributes to developing more adaptive and explainable systems for combating online misinformation.",
      "mindmap": "graph LR\n    A[AMPEND-LS] --> B[核心问题/Problem: 虚假新闻检测的挑战/Fake News Detection Challenges];\n    A --> C[主要方法/Method: 多角色智能体与证据融合/Agentic Multi-Persona & Evidence Fusion];\n    A --> D[关键结果/Results: 性能超越基线/Outperforms SOTA Baselines];\n    B --> B1[多模态内容/Multimodal Content];\n    B --> B2[领域泛化/Domain Generalization];\n    B --> B3[可解释性/Explainability];\n    C --> C1[LLM-SLM协同/LLM-SLM Synergy];\n    C --> C2[可信度融合/Credibility Fusion];\n    C --> C3[结构化推理/Structured Reasoning];\n    D --> D1[高准确率与F1/High Accuracy & F1];\n    D --> D2[强鲁棒性/Robustness];\n    D --> D3[透明推理/Transparent Reasoning];"
    },
    {
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": "Savvy Sharma, George Petrovic, Sarthak Kaushik",
      "institution": "George Brown Polytechnic",
      "link": "https://arxiv.org/pdf/2512.21048",
      "code": null,
      "tags": [
        "federated learning",
        "zero-knowledge proofs",
        "trusted execution environments",
        "blockchain",
        "medical AI",
        "verifiable aggregation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp",
      "contributions": "1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.",
      "summary": "The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.",
      "mindmap": "graph LR\n    A[zkFL-Health] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[隐私泄露与聚合器信任/Privacy Leakage & Aggregator Trust]\n    C --> C1[FL+ZKP+TEE/FL+ZKP+TEE]\n    C --> C2[链上验证/On-chain Verification]\n    D --> D1[可验证的隐私保护/Verifiable Privacy]\n    D --> D2[审计与合规/Auditability & Compliance]"
    },
    {
      "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
      "authors": "Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov",
      "institution": "Monash University, TCS Research",
      "link": "https://arxiv.org/pdf/2512.21054",
      "code": "https://github.com/kaustesseract/DexAvatar",
      "tags": [
        "3D human pose estimation",
        "3D sign language reconstruction",
        "biomechanical accuracy",
        "hand and body pose priors",
        "monocular video",
        "SMPL-X"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp",
      "contributions": "1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.",
      "summary": "The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.",
      "mindmap": "graph LR\n        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]\n        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]\n        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]"
    },
    {
      "title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics",
      "authors": "Zihan Yao, Ruoyu Wu, Tianxiang Gao",
      "institution": "DePaul University, Iowa State University",
      "link": "https://arxiv.org/pdf/2512.21075",
      "code": null,
      "tags": [
        "deep learning theory",
        "scaling laws",
        "feature learning",
        "infinite-depth limit",
        "ResNets",
        "hyperparameter transfer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp",
      "contributions": "1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.",
      "summary": "This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.",
      "mindmap": "graph LR\n    A[Understanding Scaling Laws via Feature Learning Dynamics] --> B[核心问题/Problem: Scaling laws describe success but not when/why scaling fails]\n    A --> C[主要方法/Method: Derive Neural Feature Dynamics (NFD) in infinite-width & depth limit]\n    A --> D[关键结果/Results: Explains diminishing returns, proposes depth-aware LR correction]"
    },
    {
      "title": "Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions",
      "authors": "Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick",
      "institution": "Indian Institute of Technology Indore, Indian Institute of Technology Patna",
      "link": "https://arxiv.org/pdf/2512.21076",
      "code": null,
      "tags": [
        "text classification",
        "hierarchical genre classification",
        "zero-shot semantic alignment",
        "dual-path graph convolution",
        "label co-occurrence graph",
        "blurb-refined inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp",
      "contributions": "1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.",
      "summary": "This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.",
      "mindmap": "graph LR\n    A[HiGeMine: Blurb-Refined Inference from Crowdsourced Book Reviews] --> B[核心问题/Problem: Noisy reviews & flat genre classification degrade reliability]\n    A --> C[主要方法/Method: Two-phase framework: 1. Zero-shot review filtering 2. Dual-path graph classification]\n    A --> D[关键结果/Results: Outperforms baselines on new hierarchical dataset]"
    },
    {
      "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking",
      "authors": "Enoch Hyunwook Kang",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.21080",
      "code": null,
      "tags": [
        "algorithmic fairness & evaluation",
        "field experiments",
        "A/B testing",
        "LLM personas",
        "algorithmic benchmarking",
        "information-theoretic bounds"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp",
      "contributions": "1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.",
      "summary": "The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.",
      "mindmap": "graph LR\n    A[LLM Personas as a Substitute for Field Experiments] --> B[核心问题/Problem: Field experiments are costly and slow, hindering iterative method development.]\n    A --> C[主要方法/Method: Use LLM-based persona simulation as a cheap synthetic benchmark under specific conditions.]\n    A --> D[关键结果/Results: Formal validity conditions proven; sample-size bounds derived for decision-relevance.]"
    },
    {
      "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
      "authors": "Takaya Kawakatsu",
      "institution": "Preferred Networks, Inc.",
      "link": "https://arxiv.org/pdf/2512.21083",
      "code": null,
      "tags": [
        "document analysis",
        "table recognition",
        "multi-task learning",
        "non-causal attention",
        "parallel inference",
        "hierarchical modeling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp",
      "contributions": "1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.",
      "summary": "This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.",
      "mindmap": "graph LR\n    A[Hierarchical Modeling Approach to Fast and Accurate Table Recognition] --> B(核心问题/Problem: 现有表格识别模型推理慢且有效性未充分解释/Existing models are slow and their effectiveness is not fully explained)\n    A --> C(主要方法/Method: 使用非因果注意力的多任务模型与并行推理算法/Novel multi-task model with non-causal attention & parallel inference algorithm)\n    A --> D(关键结果/Results: 在大型公开数据集上实现视觉与统计上的优越性/Superiority demonstrated visually and statistically on two large public datasets)"
    },
    {
      "title": "Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics",
      "authors": "Karim Abdelsalam, Zeyad Gamal, Ayman El-Badawy",
      "institution": "German University in Cairo",
      "link": "https://arxiv.org/pdf/2512.21081",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Dyna-Style RL",
        "SINDy",
        "TD3",
        "Model-based RL",
        "Bi-rotor Control"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp",
      "contributions": "1. Proposes a Dyna-Style RL framework that integrates SINDy for data-driven dynamics modeling with TD3 for policy learning., 2. Introduces a method to periodically inject synthetic rollouts from the learned SINDy model into the RL replay buffer to improve sample efficiency., 3. Demonstrates the framework's effectiveness on a bi-rotor system, showing superior accuracy and robustness in stabilization and trajectory tracking compared to direct model-free RL.",
      "summary": "This paper proposes a hybrid control framework combining Sparse Identification of Nonlinear Dynamics (SINDy) and Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning to efficiently control nonlinear systems. The SINDy model generates synthetic data to augment real-world training, improving sample efficiency. The method is validated on a bi-rotor system, showing better performance than direct model-free RL.",
      "mindmap": "graph LR\n    A[Dyna-Style RL Modeling and Control of Non-linear Dynamics] --> B[核心问题/Problem<br>控制复杂非线性系统<br>Controlling Complex Nonlinear Systems]\n    A --> C[主要方法/Method<br>SINDy + TD3 混合框架<br>SINDy + TD3 Hybrid Framework]\n    A --> D[关键结果/Results<br>在双旋翼系统上性能更优<br>Superior Performance on Bi-rotor]\n    B --> E[样本效率低<br>Sample Inefficiency]\n    C --> F[SINDy识别模型<br>SINDy Identifies Model]\n    C --> G[生成合成数据<br>Generates Synthetic Rollouts]\n    C --> H[TD3学习策略<br>TD3 Learns Policy]\n    D --> I[精度与鲁棒性提升<br>Improved Accuracy & Robustness]"
    },
    {
      "title": "Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends",
      "authors": "Zixiao Huang, Jixiao Yang, Sijia Li, Chi Zhang, Jinyu Chen, Chengda Xu",
      "institution": "University of Washington, Westcliff University, University of Michigan, Northeastern University, University of Virginia",
      "link": "https://arxiv.org/pdf/2512.21102",
      "code": null,
      "tags": [
        "others",
        "multi-task time series forecasting",
        "shared representation learning",
        "cloud-native systems",
        "resource contention",
        "dynamic adjustment mechanism"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9838b10e22d58d1ccca2c68a210133b154701b0c5ff14deadba418ccdbdb92f2_w640_q70.webp",
      "contributions": "1. A unified forecasting framework with a shared encoding structure for high-dimensional, multi-task time series in cloud-native backends. 2. A cross-task structural propagation module to model complex dependencies among nodes caused by resource contention and service topology changes. 3. A dynamic adjustment mechanism that regulates internal feature flows to adapt to non-stationary behaviors like sudden load shifts.",
      "summary": "This paper proposes a unified forecasting framework for high-dimensional multi-task time series in cloud-native backend systems. The method uses shared representation learning, a structural propagation module, and a dynamic adjustment mechanism to model complex dependencies and adapt to non-stationary behaviors. Experimental results show the framework achieves superior performance and provides reliable predictive capability for dynamic cloud environments.",
      "mindmap": "graph LR\n    A[Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[高维多任务时序预测/High-Dimensional Multi-Task Forecasting]\n    B --> B2[云原生后端动态环境/Cloud-Native Backend Dynamics]\n    C --> C1[共享编码结构/Shared Encoding Structure]\n    C --> C2[跨任务结构传播/Cross-Task Structural Propagation]\n    C --> C3[动态调整机制/Dynamic Adjustment Mechanism]\n    D --> D1[性能优越/Superior Performance]\n    D --> D2[可靠预测能力/Reliable Predictive Capability]"
    },
    {
      "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
      "authors": "Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea",
      "institution": "National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.21107",
      "code": null,
      "tags": [
        "content moderation",
        "semi-supervised learning",
        "data augmentation",
        "safety classifiers",
        "LLM safety",
        "prompt harmfulness"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp",
      "contributions": "1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.",
      "summary": "This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --> B[核心问题 / Problem<br>依赖大量标注数据 / Reliance on large labeled data]\n    A --> C[主要方法 / Method<br>半监督学习与任务特定增强 / SSL & Task-Specific Augmentation]\n    A --> D[关键结果 / Results<br>性能显著提升 / Significant Performance Improvement]"
    },
    {
      "title": "Semantic Refinement with LLMs for Graph Representations",
      "authors": "Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang",
      "institution": "University of Connecticut, University of Notre Dame",
      "link": "https://arxiv.org/pdf/2512.21106",
      "code": null,
      "tags": [
        "graph representation learning",
        "graph neural network",
        "large language model",
        "semantic refinement",
        "structure-semantics heterogeneity",
        "data-centric adaptation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp",
      "contributions": "1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.",
      "summary": "This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.",
      "mindmap": "graph LR\n    A[Semantic Refinement with LLMs for Graph Representations] --> B(核心问题/Problem: Graph structure-semantics heterogeneity 图的结构-语义异质性)\n    A --> C(主要方法/Method: Data-Adaptive Semantic Refinement (DAS) framework 数据自适应语义精炼框架)\n    A --> D(关键结果/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs 提升结构主导图性能，在语义丰富图上保持竞争力)"
    },
    {
      "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
      "authors": "Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong",
      "institution": "The Hong Kong University of Science and Technology, Hong Kong Observatory",
      "link": "https://arxiv.org/pdf/2512.21118",
      "code": "https://github.com/sqfoo/stldm_official",
      "tags": [
        "diffusion models",
        "precipitation nowcasting",
        "latent diffusion model",
        "spatio-temporal prediction",
        "variational autoencoder",
        "conditioning network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp",
      "contributions": "1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.",
      "summary": "The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.",
      "mindmap": "graph LR\n    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]\n    A --> C[主要方法/Method: 两阶段潜扩散模型]\n    A --> D[关键结果/Results: 性能优越，推理高效]"
    },
    {
      "title": "A Mechanistic Analysis of Transformers for Dynamical Systems",
      "authors": "Gregory Duthé, Nikolaos Evangelou, Wei Liu, Ioannis G. Kevrekidis, Eleni Chatzi",
      "institution": "ETH Zürich, Johns Hopkins University, Singapore-ETH Centre",
      "link": "https://arxiv.org/pdf/2512.21113",
      "code": null,
      "tags": [
        "dynamical systems modeling",
        "transformers",
        "attention mechanism",
        "dynamical systems",
        "representational analysis",
        "delay embedding"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad3a459d499a20015cb02bd1e25bf788476d675d70043dd137b07a4b0f35b55_w640_q70.webp",
      "contributions": "1. Provides a mechanistic interpretation of causal self-attention in Transformers as a linear, history-dependent recurrence from a dynamical systems perspective. 2. Demonstrates that the softmax attention's convexity constraint fundamentally limits the representation of certain linear dynamics, leading to oversmoothing. 3. Shows that for nonlinear partially observable systems, attention acts as an adaptive delay-embedding mechanism for state reconstruction.",
      "summary": "This paper investigates the representational capabilities of single-layer Transformers for modeling dynamical systems, interpreting attention as a history-dependent recurrence. Through linear and nonlinear case studies, it finds that softmax attention restricts representable linear dynamics but can enable state reconstruction in nonlinear systems via adaptive delay embedding. The work bridges empirical Transformer performance with classical dynamical systems theory.",
      "mindmap": "graph LR\n    A[论文标题/Paper Title: A Mechanistic Analysis of Transformers for Dynamical Systems] --> B[核心问题/Problem: Transformers as black boxes for time-series, lack of dynamical systems theory understanding];\n    A --> C[主要方法/Method: Interpret causal self-attention as linear recurrence, analyze via linear/nonlinear case studies];\n    A --> D[关键结果/Results: Softmax restricts linear dynamics (oversmoothing), attention enables nonlinear state reconstruction via delay embedding];"
    },
    {
      "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
      "authors": "Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev",
      "institution": "ETH Zurich, Snyk, INSAIT (Sofia University \"St. Kliment Ohridski\")",
      "link": "https://arxiv.org/pdf/2512.21132",
      "code": "https://github.com/eth-sri/autobaxbuilder",
      "tags": [
        "code security evaluation",
        "automated benchmarking",
        "LLM-generated code",
        "security vulnerabilities",
        "exploit generation",
        "plausibility checks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp",
      "contributions": "1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.",
      "summary": "The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.",
      "mindmap": "graph LR\n    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]\n    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]\n    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]"
    },
    {
      "title": "MODE: Multi-Objective Adaptive Coreset Selection",
      "authors": "Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui",
      "institution": "CRIL, Université d'Artois",
      "link": "https://arxiv.org/pdf/2512.21152",
      "code": null,
      "tags": [
        "others",
        "coreset selection",
        "submodular maximization",
        "data efficiency",
        "adaptive weighting",
        "multi-objective optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp",
      "contributions": "1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.",
      "summary": "The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.",
      "mindmap": "graph LR\n    A[MODE: Multi-Objective Adaptive Coreset Selection] --> B[核心问题/Problem: Static coreset selection methods cannot adapt to changing data utility during training.]\n    A --> C[主要方法/Method: Dynamic, multi-objective framework that adaptively weights selection strategies based on training phase.]\n    A --> D[关键结果/Results: (1-1/e)-approximation, O(n log n) complexity, reduced memory, interpretable insights.]"
    },
    {
      "title": "ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update",
      "authors": "Zhe Su, Giacomo Indiveri",
      "institution": "Institute of Neuroinformatics, University of Zurich and ETH Zurich",
      "link": "https://arxiv.org/pdf/2512.21153",
      "code": null,
      "tags": [
        "on-device ai",
        "spiking neural network",
        "self-supervised learning",
        "structured sparsity",
        "activity-dependent update",
        "event-driven processing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp",
      "contributions": "1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.",
      "summary": "This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.",
      "mindmap": "graph LR\n    A[ElfCore] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]\n    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]\n    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]\n    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]\n    D --> D1[功耗降低16倍/16x Lower Power]\n    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]\n    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]"
    },
    {
      "title": "BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.21165",
      "code": null,
      "tags": [
        "distributed consensus",
        "Raft",
        "election timeout",
        "contextual bandits",
        "LinUCB",
        "fault tolerance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp",
      "contributions": "1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.",
      "summary": "This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.",
      "mindmap": "graph LR\n    A[BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft] --> B(核心问题/Problem: Brittle randomized timeouts under long-tail latency & jitter)\n    A --> C(主要方法/Method: Lightweight online adaptation with contextual bandits & safe exploration)\n    A --> D(关键结果/Results: Reduces recovery/unwritable time in WAN, competitive in stable settings)"
    },
    {
      "title": "A Community-Enhanced Graph Representation Model for Link Prediction",
      "authors": "Lei Wang, Darong Lai",
      "institution": "Southeast University",
      "link": "https://arxiv.org/pdf/2512.21166",
      "code": null,
      "tags": [
        "graph representation learning",
        "Graph Neural Networks",
        "Link Prediction",
        "Community Detection",
        "Structure Enhancement"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154686865f18f6411728ca7ec41c4f9a507cd6a00a6b336c6fbfca553149d9b1_w640_q70.webp",
      "contributions": "1. Proposes a Community-Enhanced Link Prediction (CELP) framework that integrates community structure to jointly model local and global graph topology. 2. Introduces a graph enhancement technique via community-aware, confidence-guided edge completion and pruning. 3. Integrates multi-scale structural features to improve link prediction accuracy, with experimental validation showing superior performance.",
      "summary": "The paper addresses the limitation of Graph Neural Networks (GNNs) in link prediction, where they often underperform traditional heuristic methods due to over-reliance on local information. It proposes the CELP framework, which incorporates community structure to enhance the graph and capture multi-scale features. Experimental results show CELP achieves superior performance, validating the importance of community structure for accurate link prediction.",
      "mindmap": "graph LR\n        A[A Community-Enhanced Graph Representation Model for Link Prediction] --> B[核心问题/Problem: GNNs for link prediction underperform traditional heuristics due to local focus and over-smoothing.]\n        A --> C[主要方法/Method: Proposes CELP framework integrating community structure for graph enhancement and multi-scale feature learning.]\n        A --> D[关键结果/Results: Superior performance on benchmarks, validating the role of community structure.]"
    },
    {
      "title": "A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine",
      "authors": "Yogesh Kumar, Vrushank Ahire, M. A. Ganaie",
      "institution": "Indian Institute of Technology Ropar",
      "link": "https://arxiv.org/pdf/2512.21170",
      "code": null,
      "tags": [
        "machine learning for healthcare",
        "Universum learning",
        "GEPSVM",
        "EEG classification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ebfba0923a4428be5bd05b82efd0f6ff72c162a8ee710189c4ccb146c2374f3_w640_q70.webp",
      "contributions": "1. Proposes U-GEPSVM, a novel classifier integrating Universum constraints into the GEPSVM framework via a ratio-based objective function. 2. Introduces IU-GEPSVM, an improved variant with a weighted difference-based formulation for enhanced stability and independent control over class separation and Universum alignment. 3. Demonstrates the effectiveness of the proposed models for EEG seizure detection, achieving superior accuracy on the Bonn University dataset and validating improvements with rigorous statistical tests.",
      "summary": "This paper proposes two novel classifiers, U-GEPSVM and IU-GEPSVM, which combine the computational efficiency of generalized eigenvalue decomposition with Universum learning to improve EEG seizure detection. The models are designed to handle challenges like non-stationarity and limited labeled data in EEG signals. Evaluation on the Bonn University dataset shows that IU-GEPSVM outperforms baseline methods, providing an efficient and reliable solution for neurological diagnosis.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title: A Unified Framework for EEG Seizure Detection] --> B(核心问题 / Problem: EEG信号分类挑战 / EEG Signal Classification Challenges)\n    A --> C(主要方法 / Method: 提出U-GEPSVM和IU-GEPSVM / Propose U-GEPSVM and IU-GEPSVM)\n    A --> D(关键结果 / Results: IU-GEPSVM取得更高准确率 / IU-GEPSVM Achieves Higher Accuracy)\n    B --> B1(非平稳性, 低信噪比, 标记数据有限 / Non-stationarity, Low SNR, Limited Labeled Data)\n    C --> C1(结合通用集学习和广义特征值分解 / Integrates Universum Learning & GEPSVM)\n    D --> D1(O vs S: 85%峰值准确率 / O vs S: 85% Peak Accuracy)\n    D --> D2(Z vs S: 80%峰值准确率 / Z vs S: 80% Peak Accuracy)"
    },
    {
      "title": "Analytic and Variational Stability of Deep Learning Systems",
      "authors": "Ronald Katende",
      "institution": "Kabale University",
      "link": "https://arxiv.org/pdf/2512.21208",
      "code": null,
      "tags": [
        "learning theory",
        "Learning Stability Profile",
        "Lyapunov methods",
        "Clarke generalized derivatives",
        "stability exponents",
        "energy-dissipative systems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43ea5a1a8d6262f5e2056565c57706a69bbe8f558d9c82e401fa498ff92d5daa_w640_q70.webp",
      "contributions": "1. Proposes a unified analytic and variational framework for studying stability in deep learning systems via a central object called the Learning Stability Profile (LSP). 2. Proves a Fundamental Analytic Stability Theorem linking uniform boundedness of stability signatures to the existence of a dissipative Lyapunov-type energy. 3. Extends the theory to non-smooth learning systems (e.g., ReLU networks, proximal updates) using Clarke generalized derivatives and variational Lyapunov functionals.",
      "summary": "This paper proposes a unified framework to analyze the stability of deep learning systems by viewing them as coupled representation-parameter dynamics. The core method introduces a Learning Stability Profile and proves a theorem linking bounded stability signatures to a dissipative Lyapunov energy, which is extended to non-smooth cases using generalized derivatives. The main conclusion is that this provides a single, foundational dynamical description of stability that unifies and explains robustness across various architectures and optimization methods.",
      "mindmap": "graph LR\n    A[Analytic and Variational Stability of Deep Learning Systems] --> B[核心问题/Problem: 缺乏统一的深度学习稳定性数学描述 / Lack of unified mathematical description of stability in deep learning]\n    A --> C[主要方法/Method: 提出学习稳定性剖面与解析-变分框架 / Proposes Learning Stability Profile and analytic-variational framework]\n    A --> D[关键结果/Results: 证明基本稳定性定理，统一平滑与非平滑系统的稳定性分析 / Proves Fundamental Stability Theorem, unifying stability analysis for smooth and non-smooth systems]"
    },
    {
      "title": "MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models",
      "authors": "Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller",
      "institution": "École Polytechnique Fédérale de Lausanne (EPFL), University of New South Wales (UNSW), Green Dynamics, Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.21231",
      "code": null,
      "tags": [
        "reinforcement learning",
        "latent solvability",
        "mid-stage scientific training",
        "chemical reasoning",
        "rule-based rewards",
        "symbolic competence"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp",
      "contributions": "1. Identifies two necessary conditions for RL-based chemical reasoning: symbolic competence and latent chemical knowledge. 2. Proposes MiST (mid-stage scientific training), a set of techniques including data-mixing with SMILES/CIF-aware pre-processing and continued pre-training. 3. Demonstrates that MiST significantly improves latent solvability and enables RL to achieve large accuracy gains on challenging chemical tasks.",
      "summary": "This paper addresses the problem that reinforcement learning for chemical reasoning fails unless the base model already has some latent solvability. The authors propose MiST, a mid-stage training pipeline involving data pre-processing and continued pre-training, to build the necessary prerequisites. Their method substantially boosts model performance on tasks like organic reaction naming and inorganic material generation, establishing clear prerequisites for training chemical reasoning models.",
      "mindmap": "graph LR\n    A[MiST: 化学推理模型中的中期科学训练 / MiST: Mid-Stage Scientific Training for Chemical Reasoning] --> B[核心问题/Problem: RL需要潜在可解性 / RL requires latent solvability]\n    A --> C[主要方法/Method: 中期科学训练 / Mid-stage scientific training (MiST)]\n    A --> D[关键结果/Results: 准确率大幅提升 / Accuracy significantly improved]\n    B --> B1[条件: 符号能力与潜在知识 / Conditions: Symbolic competence & latent knowledge]\n    C --> C1[技术: 数据混合与持续预训练 / Techniques: Data-mixing & continued pre-training]\n    D --> D1[有机反应命名: 10.9% -> 63.9% / Organic reaction naming: 10.9% -> 63.9%]\n    D --> D2[无机材料生成: 40.6% -> 67.4% / Inorganic material generation: 40.6% -> 67.4%]"
    },
    {
      "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
      "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
      "institution": "Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments",
      "link": "https://arxiv.org/pdf/2512.21241",
      "code": "https://github.com/machanic/hard_label_attacks",
      "tags": [
        "adversarial attacks",
        "hard-label black-box attacks",
        "query efficiency",
        "ray search optimization",
        "Nesterov's Accelerated Gradient",
        "momentum-based optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp",
      "contributions": "1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.",
      "summary": "This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.",
      "mindmap": "graph LR\n    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]\n    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]\n    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]"
    },
    {
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": "Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21243",
      "code": "https://lookplangraph.github.io/",
      "tags": [
        "embodied ai",
        "scene graph",
        "vision language model",
        "dynamic planning",
        "memory graph",
        "graph augmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp",
      "contributions": "1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.",
      "summary": "The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.",
      "mindmap": "graph LR\n    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];\n    A --> C[主要方法/Method: Dynamic graph update via VLM observation];\n    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];"
    },
    {
      "title": "Assessing the Software Security Comprehension of Large Language Models",
      "authors": "Mohammed Latif Siddiq, Natalie Sekerak, Antonio Karam, Maria Leal, Arvin Islam-Gomes, Joanna C. S. Santos",
      "institution": "University of Notre Dame",
      "link": "https://arxiv.org/pdf/2512.21238",
      "code": null,
      "tags": [
        "software security assessment",
        "Bloom's Taxonomy",
        "knowledge boundary",
        "misconception patterns"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3809be1ff4cae9ad7acb47676829cd58ca3ea614efa57d9121857f85d97fc7_w640_q70.webp",
      "contributions": "1. Introduced a systematic evaluation framework using Bloom's Taxonomy to assess LLMs' software security comprehension across six cognitive levels. 2. Proposed the concept of a \"software security knowledge boundary\" to identify the highest reliable cognitive performance level for an LLM. 3. Identified and documented 51 recurring misconception patterns made by LLMs in software security tasks.",
      "summary": "This paper systematically evaluates the software security comprehension of five leading LLMs using Bloom's Taxonomy as a framework across diverse datasets. The results show that while LLMs perform well on lower-level cognitive tasks like recalling facts, their performance significantly degrades on higher-order tasks requiring reasoning and secure system creation. The study introduces a knowledge boundary to quantify reliable performance limits and identifies common misconception patterns.",
      "mindmap": "graph LR\n    A[Assessing LLM Software Security Comprehension<br/>评估LLM软件安全理解] --> B{核心问题/Problem};\n    A --> C{主要方法/Method};\n    A --> D{关键结果/Results};\n    B --> B1[LLMs' Security Expertise Unclear<br/>LLM安全专业知识不明];\n    C --> C1[Framework: Bloom's Taxonomy<br/>框架: 布鲁姆分类法];\n    C --> C2[Datasets: MCQs, Code, Courses, Case Studies<br/>数据集: 选择题, 代码, 课程, 案例];\n    D --> D1[Good on Low-Level Tasks<br/>低级任务表现好];\n    D --> D2[Poor on High-Order Reasoning<br/>高阶推理表现差];\n    D --> D3[Knowledge Boundary & Misconceptions<br/>知识边界与误解模式];"
    },
    {
      "title": "Model Merging via Multi-Teacher Knowledge Distillation",
      "authors": "Seyed Arshan Dalili, Mehrdad Mahdavi",
      "institution": "The Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.21288",
      "code": "https://github.com/arshandalili/SAMerging",
      "tags": [
        "model merging",
        "model merging",
        "knowledge distillation",
        "PAC-Bayes",
        "sharpness-aware minimization",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp",
      "contributions": "1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a \"cross-task heterogeneity\" term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.",
      "summary": "This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.",
      "mindmap": "graph LR\n    A[Model Merging via Multi-Teacher Knowledge Distillation] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[缺乏理论保证/Lack of Theoretical Guarantees]\n    B --> B2[启发式方法不稳定/Heuristic Methods are Brittle]\n    C --> C1[理论: 平坦性感知PAC-Bayes边界/Theory: Flatness-aware PAC-Bayes Bound]\n    C --> C2[框架: 多教师知识蒸馏/Framework: Multi-Teacher Knowledge Distillation]\n    C --> C3[方法: SAMerging/Method: SAMerging]\n    D --> D1[新SOTA/New SOTA]\n    D --> D2[高数据效率/High Data Efficiency]"
    },
    {
      "title": "Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering",
      "authors": "Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H.M. Abou-El-Enien",
      "institution": "Faculty of Computers and Artificial Intelligence, Cairo University",
      "link": "https://arxiv.org/pdf/2512.21301",
      "code": null,
      "tags": [
        "computational drug design",
        "metaheuristic assembly",
        "de novo drug generation",
        "transcriptomics",
        "molecular docking",
        "multi-objective optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8733d98ab11f787e5d221faf5b5b5383a6af0c23974a0e3febf13cb0aa80b4f3_w640_q70.webp",
      "contributions": "1. A novel end-to-end computational framework that integrates patient transcriptomics with de novo drug generation for personalized AML therapy. 2. Development of a reaction-first evolutionary metaheuristic algorithm for assembling novel ligands guided by target-specific structural hotspots. 3. Validation of generated drug candidates through in-silico ADMET profiling and molecular docking, demonstrating pharmacologically viable leads.",
      "summary": "This paper proposes a computational framework for personalized drug discovery in Acute Myeloid Leukemia (AML). It uses patient transcriptome data to identify biomarkers, models their 3D structures, and employs a novel metaheuristic algorithm to generate new drug-like molecules targeting these biomarkers. The results show the framework can produce viable drug candidates, offering a scalable approach for precision oncology.",
      "mindmap": "graph LR\n        A[Transcriptome-Conditioned Personalized De Novo Drug Generation] --> B[核心问题/Problem: AML分子异质性与复发率高/AML's molecular heterogeneity & high relapse]\n        A --> C[主要方法/Method: 元启发式组装与靶向过滤/Metaheuristic Assembly & Target-Driven Filtering]\n        A --> D[关键结果/Results: 生成具有药理活性的先导化合物/Generated pharmacologically viable leads]"
    },
    {
      "title": "Learning to Solve PDEs on Neural Shape Representations",
      "authors": "Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra",
      "institution": "University College London, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21311",
      "code": null,
      "tags": [
        "geometry processing",
        "neural shape representations",
        "mesh-free PDE solver",
        "geometry-conditioned operator",
        "surface PDEs",
        "differentiable pipeline"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed9eac0c574821f5dc10188c61bf1bd40ede7fca32ee8f98accf90b52993315f_w640_q70.webp",
      "contributions": "1. A novel, mesh-free formulation that learns a local update operator conditioned on neural shape attributes to solve surface PDEs directly on neural representations. 2. A method that is trained once on a single shape and generalizes across shape and topology variations, enabling fast inference without per-instance optimization. 3. The first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations, preserving differentiability.",
      "summary": "The paper addresses the mismatch between traditional mesh-based PDE solvers and modern neural shape representations by proposing a learned, mesh-free operator. This operator, trained once, solves surface PDEs directly on neural data, generalizing across shapes and preserving differentiability. The method performs comparably to classical solvers and enables the first end-to-end PDE-solving pipeline for neural surfaces.",
      "mindmap": "graph LR\n    A[Learning to Solve PDEs on Neural Shape Representations] --> B(核心问题/Problem: Mismatch between mesh-based solvers and neural shape data)\n    A --> C(主要方法/Method: Mesh-free, learned local operator conditioned on shape)\n    A --> D(关键结果/Results: Generalizes across shapes, close to FEM, enables end-to-end pipeline)"
    },
    {
      "title": "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks",
      "authors": "Roy Turgeman, Tom Tirer",
      "institution": "Bar-Ilan University",
      "link": "https://arxiv.org/pdf/2512.21315",
      "code": null,
      "tags": [
        "information theory",
        "statistical learning",
        "data processing inequality",
        "Bayes classifier",
        "low-level processing",
        "classification accuracy",
        "finite sample analysis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp",
      "contributions": "1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.",
      "summary": "This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.",
      "mindmap": "graph LR\n    A[Does the Data Processing Inequality Reflect Practice?<br/>数据加工不等式反映实践吗?] --> B(核心问题/Problem: Low-level processing is common in practice but seems to contradict the Data Processing Inequality.<br/>实践中的低级处理似乎与数据加工不等式矛盾)\n    A --> C(主要方法/Method: Theoretical study of a binary classifier + empirical validation on deep networks.<br/>二元分类器的理论研究+深度网络的实证验证)\n    A --> D(关键结果/Results: For finite samples, pre-processing can improve accuracy; gain depends on dataset properties.<br/>对于有限样本，预处理可提高精度；收益取决于数据集属性)"
    },
    {
      "title": "Parallel Token Prediction for Language Models",
      "authors": "Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt",
      "institution": "University of California, Irvine, Chan-Zuckerberg Initiative, Pyramidal AI",
      "link": "https://arxiv.org/pdf/2512.21323",
      "code": null,
      "tags": [
        "llm inference",
        "parallel token prediction",
        "speculative decoding",
        "autoregressive decoding",
        "transformer inference",
        "latency optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp",
      "contributions": "1. Proposes Parallel Token Prediction (PTP), a universal framework for parallel sequence generation that jointly predicts multiple dependent tokens in a single transformer call. 2. Proves that PTP can represent arbitrary autoregressive sequence distributions, avoiding the restrictive independence assumptions of prior multi-token prediction methods. 3. Demonstrates state-of-the-art speculative decoding performance, accepting over four tokens per step on Spec-Bench with Vicuna-7B, showing parallel long-sequence generation is feasible without losing modeling power.",
      "summary": "The paper addresses the high latency of autoregressive decoding in large language models by proposing Parallel Token Prediction (PTP), a framework that predicts multiple dependent tokens in parallel within a single transformer call. It proves PTP's universality in representing autoregressive distributions and shows it achieves superior speculative decoding performance, enabling faster text generation without sacrificing quality.",
      "mindmap": "graph LR\n    A[Parallel Token Prediction for Language Models] --> B[核心问题/Problem: Autoregressive decoding latency bottleneck]\n    A --> C[主要方法/Method: Parallel Token Prediction (PTP), joint prediction of dependent tokens]\n    A --> D[关键结果/Results: State-of-the-art speculative decoding, >4 tokens/step, universal framework]"
    },
    {
      "title": "Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation",
      "authors": "Yuan Qiu, Wolfgang Dahmen, Peng Chen",
      "institution": "Georgia Institute of Technology, University of South Carolina",
      "link": "https://arxiv.org/pdf/2512.21319",
      "code": null,
      "tags": [
        "scientific computing",
        "first-order system least squares",
        "reduced basis neural operator",
        "a posteriori error estimation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a56bc0fe5da5167f8e01f84491e1f4cb6dfb3c8d8926f4a22fe5ff594010bf_w640_q70.webp",
      "contributions": "1. Proposes a variationally correct operator learning framework using First-Order System Least-Squares (FOSLS) objectives that are provably equivalent to solution error in PDE-induced norms. 2. Introduces a Reduced Basis Neural Operator (RBNO) that predicts coefficients for a pre-computed, conforming reduced basis to ensure function space conformity and variational stability. 3. Provides a rigorous convergence analysis that decomposes the total error into discretization bias, basis truncation error, neural network approximation error, and statistical estimation errors, with the residual serving as a reliable a posteriori error estimator.",
      "summary": "This paper addresses the issue of \"variational correctness\" in neural operators for solving PDEs, where standard residual losses do not guarantee small solution errors. The authors propose a new framework using a First-Order System Least-Squares (FOSLS) loss and a Reduced Basis Neural Operator (RBNO) to ensure stability and norm equivalence. The method provides a rigorous error bound and demonstrates superior accuracy in PDE-compliant norms, with the residual acting as a reliable error estimator.",
      "mindmap": "graph LR\n    A[Variationally Correct Operator Learning<br>变分正确的算子学习] --> B[核心问题/Problem<br>Standard PDE-residual losses lack variational correctness<br>标准PDE残差损失缺乏变分正确性]\n    A --> C[主要方法/Method<br>FOSLS objective + Reduced Basis Neural Operator (RBNO)<br>FOSLS目标 + 约简基神经算子]\n    A --> D[关键结果/Results<br>Provable error equivalence & reliable a posteriori estimator<br>可证明的误差等价性 & 可靠的后验估计器]"
    },
    {
      "title": "Measuring all the noises of LLM Evals",
      "authors": "Sida Wang",
      "institution": "FAIR at Meta",
      "link": "https://arxiv.org/pdf/2512.21326",
      "code": null,
      "tags": [
        "llm inference",
        "LLM evaluation",
        "statistical noise",
        "paired analysis",
        "prediction variance",
        "data variance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp",
      "contributions": "1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the \"all-pairs paired method\" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.",
      "summary": "This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an \"all-pairs paired method\" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.",
      "mindmap": "graph LR\n    A[Measuring all the noises of LLM Evals] --> B(核心问题/Problem: LLM评估中的统计噪声/Separating signal from noise in LLM evals)\n    A --> C(主要方法/Method: 全配对分析法/All-pairs paired method)\n    A --> D(关键结果/Results: 可预测的总噪声与主导的预测噪声/Predictable total noise & dominant prediction noise)"
    },
    {
      "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
      "authors": "Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin",
      "institution": "Zhejiang University, Westlake University, University of Chicago",
      "link": "https://arxiv.org/pdf/2512.21336",
      "code": "https://github.com/LINs-lab/DenoisingEntropy",
      "tags": [
        "diffusion models",
        "Denoising Entropy",
        "Masked Diffusion Models",
        "decoding path optimization",
        "predictive uncertainty",
        "non-autoregressive generation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp",
      "contributions": "1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.",
      "summary": "The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.",
      "mindmap": "graph LR\n        A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>通过量化不确定性优化掩码扩散模型的解码路径] --> B(核心问题/Problem: MDMs生成质量对解码顺序敏感<br/>MDM output quality is sensitive to decoding order)\n        A --> C(主要方法/Method: 提出去噪熵和路径优化算法<br/>Propose Denoising Entropy & path optimization algorithms)\n        A --> D(关键结果/Results: 熵引导方法提升生成质量<br/>Entropy-guided methods improve generation quality)"
    },
    {
      "title": "Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding",
      "authors": "Stefan Volz, Martin Storath, Andreas Weinmann",
      "institution": "Technische Hochschule Würzburg-Schweinfurt",
      "link": "https://arxiv.org/pdf/2512.20682",
      "code": null,
      "tags": [
        "optimization",
        "least absolute deviations",
        "piecewise affine lower-bounding",
        "linear programming",
        "robust regression",
        "exact algorithm"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b21c32d0f735b0e43e7203101aa47a6f9a1a7a2ca1eed65c3391f51147bd5e6_w640_q70.webp",
      "contributions": "1. Proposes the Piecewise Affine Lower-Bounding (PALB) method, a novel exact algorithm for LAD line fitting. 2. Provides theoretical proof of correctness and bounds on the number of iterations for the algorithm. 3. Demonstrates empirical log-linear scaling and superior speed compared to existing LP-based and IRLS-based solvers on synthetic and real-world data.",
      "summary": "This paper addresses the computational challenge of exact Least Absolute Deviations (LAD) line fitting, a robust regression method. It proposes the PALB algorithm, which uses piecewise-affine lower bounds and a subdivision scheme to find the exact solution efficiently. The method is proven correct, shows log-linear empirical scaling, and is faster than existing practical solvers like linear programming and iteratively reweighted least squares.",
      "mindmap": "graph LR\n    A[Fast and Exact LAD Line Fitting via PALB] --> B[核心问题/Problem: LAD拟合计算复杂，现有高效算法难实现/LAD fitting is computationally involved, existing efficient algorithms are hard to implement]\n    A --> C[主要方法/Method: 使用分段仿射下界和细分方案/Use piecewise affine lower-bounding and a subdivision scheme]\n    A --> D[关键结果/Results: 算法精确、对数线性扩展、速度快于LP和IRLS/Algorithm is exact, log-linear scaling, faster than LP & IRLS]"
    },
    {
      "title": "Diffusion Models in Simulation-Based Inference: A Tutorial Review",
      "authors": "Jonas Arruda, Niels Bracher, Ullrich Köthe, Jan Hasenauer, Stefan T. Radev",
      "institution": "University of Bonn, Rensselaer Polytechnic Institute, Heidelberg University",
      "link": "https://arxiv.org/pdf/2512.20685",
      "code": null,
      "tags": [
        "diffusion models",
        "simulation-based inference",
        "score-based models",
        "parameter estimation",
        "generative modeling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/024277637e6ce2f35e35619e320a14456fb6904af22070de1a30be768a5a2475_w640_q70.webp",
      "contributions": "1. Provides a comprehensive tutorial review synthesizing recent developments on diffusion models for Simulation-Based Inference (SBI)., 2. Highlights and discusses key design choices and concepts affecting SBI performance, such as guidance, score composition, flow matching, and noise schedules., 3. Illustrates the application of diffusion models in SBI through case studies and outlines open questions for future research.",
      "summary": "This tutorial review synthesizes how diffusion models, through their score-based formulation, are used for fast and accurate parameter estimation in Simulation-Based Inference (SBI). It covers key design choices in training and inference, discusses factors affecting efficiency and accuracy, and presents case studies to illustrate the concepts.",
      "mindmap": "graph LR\n        A[Diffusion Models in SBI: A Tutorial Review] --> B[核心问题/Problem: 如何从模拟和真实数据中推断潜在参数/How to infer latent parameters from simulated and real data?];\n        A --> C[主要方法/Method: 使用基于分数的扩散模型学习条件或联合分布/Using score-based diffusion models to learn conditional or joint distributions];\n        A --> D[关键结果/Results: 综述了设计选择、概念与应用，并指出未来方向/Reviewed design choices, concepts, applications, and outlined future directions];"
    },
    {
      "title": "Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights",
      "authors": "Rommel Cortez, Bala Krishnamoorthy",
      "institution": "Washington State University",
      "link": "https://arxiv.org/pdf/2512.20811",
      "code": null,
      "tags": [
        "classification evaluation",
        "Matthews Correlation Coefficient",
        "weighted performance measure",
        "multiclass classification",
        "robustness analysis",
        "confusion matrix"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/277843f9180deb3e0b651c6e34535bdaba884d63d88920a3f85e89a8483c52d1_w640_q70.webp",
      "contributions": "1. Proposes a new weighted version of the Pearson-Matthews Correlation Coefficient (MCC) for binary and multiclass classification that is sensitive to individual observation weights. 2. Proves the robustness of the proposed weighted measures, showing that changes in weights by at most ε lead to bounded changes in the measure value (by a factor of ε for binary and ε² for multiclass). 3. Demonstrates empirically that the weighted measures can effectively distinguish classifiers based on their performance on highly weighted observations, unlike standard unweighted measures.",
      "summary": "The paper addresses the lack of performance measures for classification that account for individually weighted observations. It proposes a weighted version of the Matthews Correlation Coefficient (MCC) for binary and multiclass tasks, which is proven to be robust to weight perturbations. The results show that this new measure successfully identifies classifiers that perform well on more important, highly weighted data points.",
      "mindmap": "graph LR\n    A[Weighted MCC] --> B[核心问题/Problem: 现有分类评估指标不适用于带权重的观测值/Existing measures ignore individual observation weights]\n    A --> C[主要方法/Method: 提出加权MCC及其多分类扩展/Propose weighted MCC and multiclass extensions]\n    A --> D[关键结果/Results: 指标对权重变化鲁棒，能识别在重要样本上性能好的分类器/Measures are robust and identify classifiers good on high-weight observations]"
    },
    {
      "title": "A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations",
      "authors": "Subhamoy Chatterjee, Mausumi Dikpati",
      "institution": "Southwest Research Institute, High Altitude Observatory (NSF-NCAR)",
      "link": "https://arxiv.org/pdf/2512.20747",
      "code": null,
      "tags": [
        "astro-physics",
        "solar physics",
        "magnetohydrodynamics",
        "Physics-Informed Neural Network (PINN)",
        "MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT)",
        "solar active regions (ARs)",
        "toroidal bands (toroids)",
        "state-vector reconstruction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp",
      "contributions": "1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method's ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.",
      "summary": "This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.",
      "mindmap": "graph LR\n        A[PINNBARDS: 从全球活动区观测推导MHD状态向量 / PINNBARDS: Deriving MHD State Vectors From Global Active Regions Observations] --> B(核心问题/Problem: 表面磁图仅提供活动区分布的几何形状，无法提供初始化MHD模型所需的自洽状态向量 / Problem: Surface magnetograms only provide geometric shape of AR distribution, not the self-consistent state-vector needed to initialize MHD models.)\n        A --> C(主要方法/Method: 开发PINNBARDS，一个基于物理信息神经网络(PINN)的模拟器，使用观测到的环形带和MHD-SWT方程来推导初始状态向量 / Method: Develop PINNBARDS, a PINN-based simulator using observed toroids and MHD-SWT equations to derive the initial state-vector.)\n        A --> D(关键结果/Results: PINN收敛到物理一致解，与观测匹配；最佳参数为20-30 kG环形场和~10度带宽 / Results: PINN converges to physically consistent solutions matching observations; optimal parameters are 20-30 kG toroidal field and ~10 degree bandwidth.)"
    },
    {
      "title": "GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model",
      "authors": "Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng",
      "institution": "Nanyang Technological University, Southeast University",
      "link": "https://arxiv.org/pdf/2512.20978",
      "code": null,
      "tags": [
        "speech separation",
        "target speaker extraction",
        "generative language model",
        "coarse-to-fine",
        "exposure bias",
        "direct preference optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp",
      "contributions": "1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model's outputs with human perceptual preferences.",
      "summary": "This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.",
      "mindmap": "graph LR\n    A[GenTSE] --> B[核心问题/Problem: TSE generalization & fidelity];\n    A --> C[主要方法/Method: Two-stage generative LM, FLC, DPO];\n    A --> D[关键结果/Results: Surpasses prior LM-based systems];"
    },
    {
      "title": "Enhancing diffusion models with Gaussianization preprocessing",
      "authors": "Li Cunzhi, Louis Kang, Hideaki Shimazaki",
      "institution": "Kyoto University, RIKEN Center for Brain Science",
      "link": "https://arxiv.org/pdf/2512.21020",
      "code": null,
      "tags": [
        "diffusion models",
        "Gaussianization",
        "bifurcation",
        "sampling efficiency",
        "generative models",
        "data preprocessing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1994287172f7f096232171fd21b57b4b1d23f670ef89d86fdbadc32dceabbf46_w640_q70.webp",
      "contributions": "1. Proposes a novel Gaussianization preprocessing step for training data to align the target distribution with the initial Gaussian noise in diffusion models. 2. Aims to mitigate the bifurcation-related sampling inefficiency, particularly improving early-stage reconstruction quality. 3. Enables more stable and efficient sampling, especially beneficial for small-scale network architectures.",
      "summary": "This paper addresses the slow sampling problem in diffusion models, which is caused by a delay before trajectory bifurcation. The authors propose applying Gaussianization preprocessing to the training data to make it more closely resemble the initial Gaussian noise, simplifying the model's learning task. This method improves generation quality in the early reconstruction stages and enables more efficient sampling.",
      "mindmap": "graph LR\n    A[Enhancing diffusion models with Gaussianization preprocessing<br>增强扩散模型的Gaussianization预处理] --> B[Problem: Slow sampling due to bifurcation delay<br>问题: 分岔延迟导致采样慢]\n    A --> C[Method: Gaussianization preprocessing of training data<br>方法: 训练数据的Gaussianization预处理]\n    A --> D[Results: Improved early-stage quality & efficient sampling<br>结果: 提升早期生成质量与高效采样]"
    },
    {
      "title": "Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization",
      "authors": "Giovanni Luca Marchetti, Erin Connelly, Paul Breiding, Kathlén Kohn",
      "institution": "KTH Royal Institute of Technology, University of Osnabrück, Digital Futures",
      "link": "https://arxiv.org/pdf/2512.21029",
      "code": null,
      "tags": [
        "optimization theory",
        "Euclidean distance degree",
        "overparametrization",
        "algebraic geometry",
        "degenerate metrics",
        "critical points"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a41be3cd2e9b4a085f8c60d76c30d977b51dcb4e0c2ab2a8f0cb584bc2b6a827_w640_q70.webp",
      "contributions": "1. Relates degenerate quadratic optimization problems to nondegenerate ones via a projection, revealing the role of the projection's ramification locus. 2. Provides tools for counting the number of critical points over projective varieties in the degenerate setting. 3. Extends the theory of Euclidean distance degree (EDD) to the overparametrized regime, bridging algebraic geometry with machine learning.",
      "summary": "This paper studies the critical points of a degenerate quadratic objective over an algebraic variety, a scenario arising in overparametrized machine learning. The main method connects the degenerate problem to a nondegenerate one via a projection, highlighting the importance of the projection's ramification locus. The work extends the Euclidean distance degree framework to the degenerate setting and provides tools for analyzing optimization landscapes in overparametrized models.",
      "mindmap": "graph LR\n    A[Critical Points of Degenerate Metrics<br>退化度量临界点] --> B[核心问题/Problem<br>Overparametrized quadratic optimization<br>过参数化二次优化];\n    A --> C[主要方法/Method<br>Projection to nondegenerate problem<br>投影至非退化问题];\n    A --> D[关键结果/Results<br>Ramification locus role & EDD extension<br>分歧轨迹作用与EDD扩展];"
    },
    {
      "title": "Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments",
      "authors": "Edwin Fong, Lancelot F. James, Juho Lee",
      "institution": "The University of Hong Kong (HKU), The Hong Kong University of Science and Technology (HKUST), Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.21005",
      "code": null,
      "tags": [
        "Bayesian nonparametric models",
        "Poisson Hierarchical Indian Buffet Process",
        "sparse count data",
        "infectious disease prediction",
        "Bayesian machine learning",
        "microbiome unseen species problems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71507d2ce74cd2738ff8811f5abfd9a141d92f2c2fbc07f24c76785e9d864c9e_w640_q70.webp",
      "contributions": "1. Introduces the PHIBP, a Bayesian nonparametric model, for robustly modeling sparse count data in epidemiology. 2. Demonstrates the model's ability to borrow statistical strength from related regions to predict outbreaks in areas with zero historical cases. 3. Provides a unified framework that yields accurate predictions and meaningful epidemiological insights (e.g., alpha/beta diversity) in data-sparse environments.",
      "summary": "This paper addresses the challenge of predicting infectious disease outbreaks in regions with historically zero reported cases. It proposes using the Poisson Hierarchical Indian Buffet Process (PHIBP), a Bayesian model that leverages information from neighboring areas to handle sparse count data. The experiments show this approach provides accurate outbreak forecasts and useful epidemiological insights in data-scarce settings.",
      "mindmap": "graph LR\n    A[Learning from Neighbors with PHIBP<br>基于PHIBP向邻居学习] --> B[核心问题/Problem<br>Predicting outbreaks in regions with zero historical cases<br>预测零病例地区的疫情]\n    A --> C[主要方法/Method<br>Poisson Hierarchical Indian Buffet Process (PHIBP)<br>泊松分层印度自助餐过程]\n    A --> D[关键结果/Results<br>Accurate predictions & epidemiological insights in sparse data<br>稀疏数据下的准确预测与流行病学洞见]"
    },
    {
      "title": "Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks",
      "authors": "Andrew D. Blevins, Ian K. Quigley",
      "institution": "(Institution not explicitly stated in the provided content. Based on the authors' names and arXiv submission, it cannot be reliably inferred without full paper affiliations.)",
      "link": "https://arxiv.org/pdf/2512.20924",
      "code": null,
      "tags": [
        "cheminformatics",
        "Clever Hans",
        "shortcut learning",
        "activity prediction",
        "author-disjoint splits",
        "CHEMBL"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0714bbaf21a9c4a2f8221248ca78fbc74ff052dffe780f1607a9187bc7047174_w640_q70.webp",
      "contributions": "1. Demonstrates that machine learning models can predict the author of a molecule from its structure alone with high accuracy, revealing distinctive \"chemist style\" signals in public datasets. 2. Shows that an activity prediction model using only inferred author probabilities (and a protein ID) performs comparably to a structure-based baseline, exposing a \"Clever Hans\" failure mode where models exploit chemist intent rather than learning causal chemistry. 3. Analyzes the sources of this data leakage and proposes author-disjoint dataset splits as a mitigation strategy to decouple chemist intent from biological outcomes.",
      "summary": "The paper investigates whether machine learning models for molecular activity prediction exploit \"chemist style\" signals rather than learning causal chemistry. By training a classifier to predict molecule authors from structure and then using only the author probabilities to predict bioactivity, the authors show that models can achieve competitive performance without direct structural input. This reveals a shortcut learning problem in cheminformatics benchmarks, prompting recommendations for author-disjoint data splits to mitigate intent leakage.",
      "mindmap": "graph LR\n    A[Clever Hans in Chemistry<br>化学中的汉斯效应] --> B{核心问题/Problem};\n    A --> C{主要方法/Method};\n    A --> D{关键结果/Results};\n    B --> B1[Can models identify chemist from molecule?<br>模型能从分子识别化学家吗?];\n    B --> B2[Do models exploit intent, not chemistry?<br>模型利用意图而非化学原理吗?];\n    C --> C1[Link CHEMBL assays to authors<br>关联CHEMBL实验与作者];\n    C --> C2[Train author classifier from fingerprints<br>从指纹训练作者分类器];\n    C --> C3[Train activity model on author probabilities<br>基于作者概率训练活性模型];\n    D --> D1[Author prediction is accurate<br>作者预测准确];\n    D --> D2[Author-only model matches baseline<br>仅作者模型媲美基线];\n    D --> D3[Reveals Clever Hans failure<br>揭示汉斯效应失败模式];"
    },
    {
      "title": "Causal-driven attribution (CDA): Estimating channel influence without user-level data",
      "authors": "Georgios Filippou, Boi Mai Quach, Diana Lenghel, Arthur White, Ashish Kumar Jha",
      "institution": "Trinity College Dublin",
      "link": "https://arxiv.org/pdf/2512.21211",
      "code": null,
      "tags": [
        "causal inference",
        "PCMCI",
        "Structural Causal Model",
        "privacy-first analytics",
        "marketing attribution",
        "temporal causal discovery"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/375f379a2c16450357d40be6e36b5fe98344711a4e78be19f7e0ad3d106a71cf_w640_q70.webp",
      "contributions": "1. Proposes a Causal-Driven Attribution (CDA) framework that estimates channel influence using only aggregated impression-level data, eliminating the need for user-level path data. 2. Integrates temporal causal discovery (PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions. 3. Demonstrates the framework's accuracy and robustness on synthetic data, showing it captures cross-channel interdependencies while providing a privacy-preserving, scalable alternative to traditional models.",
      "summary": "This paper introduces Causal-Driven Attribution (CDA), a framework that uses aggregated impression data and causal inference techniques to estimate marketing channel influence without user-level tracking. It combines temporal causal discovery (PCMCI) with structural causal modeling to quantify channel contributions to conversions. The method shows strong accuracy in synthetic experiments and offers a privacy-preserving, scalable solution for attribution modeling.",
      "mindmap": "graph LR\n    A[Causal-driven attribution (CDA): Estimating channel influence without user-level data] --> B[核心问题/Problem: Attribution models rely on inaccessible user-level data due to privacy regulations.]\n    A --> C[主要方法/Method: Integrates PCMCI for temporal causal discovery and Structural Causal Model for effect estimation using aggregated data.]\n    A --> D[关键结果/Results: Achieves low relative RMSE (9.50%-24.23%), captures cross-channel effects, and provides privacy-preserving attribution.]"
    },
    {
      "title": "Autonomous Uncertainty Quantification for Computational Point-of-care Sensors",
      "authors": "Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan",
      "institution": "University of California, Los Angeles",
      "link": "https://arxiv.org/pdf/2512.21335",
      "code": null,
      "tags": [
        "on-device ai",
        "Monte Carlo dropout",
        "uncertainty quantification",
        "computational point-of-care sensors",
        "vertical flow assays",
        "neural networks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5869aed4bf3e962c5ac24490891076a87e2bc2b992a51fbf1ce76fcce3ce32b_w640_q70.webp",
      "contributions": "1. Proposed an autonomous uncertainty quantification technique for computational point-of-care diagnostics to mitigate erroneous neural network predictions. 2. Integrated a Monte Carlo dropout-based method into a diagnostic pipeline to identify and exclude high-uncertainty predictions without needing ground truth. 3. Demonstrated the method's effectiveness on a Lyme disease diagnostic platform, significantly improving sensitivity from 88.2% to 95.7% in blinded testing.",
      "summary": "This paper addresses the problem of erroneous predictions from neural networks in computational point-of-care diagnostic sensors. The authors propose an autonomous uncertainty quantification method using Monte Carlo dropout to identify and exclude unreliable predictions. Their approach, tested on a Lyme disease diagnostic platform, significantly improved diagnostic sensitivity, demonstrating enhanced robustness for neural network-driven sensing systems.",
      "mindmap": "graph LR\n    A[Autonomous Uncertainty Quantification for Computational Point-of-care Sensors] --> B[核心问题/Problem: Neural network hallucinations cause misdiagnosis in POC sensors]\n    A --> C[主要方法/Method: Integrate Monte Carlo dropout for autonomous uncertainty quantification]\n    A --> D[关键结果/Results: Diagnostic sensitivity increased from 88.2% to 95.7%]"
    },
    {
      "title": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
      "authors": "Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19696",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp",
      "contributions": "",
      "summary": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "authors": "Md Nahid Hasan Shuvo, Moinul Hossain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19711",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp",
      "contributions": "",
      "summary": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "mindmap": ""
    },
    {
      "title": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
      "authors": "Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19701",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp",
      "contributions": "",
      "summary": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
      "mindmap": ""
    },
    {
      "title": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
      "authors": "Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19716",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp",
      "contributions": "",
      "summary": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
      "mindmap": ""
    },
    {
      "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
      "authors": "James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19707",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp",
      "contributions": "",
      "summary": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
      "mindmap": ""
    },
    {
      "title": "Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches",
      "authors": "Taoran Sheng, Manfred Huber",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19713",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp",
      "contributions": "",
      "summary": "Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches",
      "mindmap": ""
    },
    {
      "title": "Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism",
      "authors": "Alessandro Casadei, Clemens Grupp, Sreyoshi Bhaduri, Lu Guo, Wilson Fung, Rohit Malshe, Raj Ratan, Ankush Pole, Arkajit Rakshit",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19722",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5fc3669c6e3b3729a8ceeab7556b8b190a71237342736f2d91f0e9e99de3dc0_w640_q70.webp",
      "contributions": "",
      "summary": "Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism",
      "mindmap": ""
    },
    {
      "title": "Per-Axis Weight Deltas for Frequent Model Updates",
      "authors": "Stefan Kuyumdzhiev, Radostin Cholakov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19720",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fced4b263f86d7aa4d12f96b832ddc3447334c926ee67499537f6d38e8e740d_w640_q70.webp",
      "contributions": "",
      "summary": "Per-Axis Weight Deltas for Frequent Model Updates",
      "mindmap": ""
    },
    {
      "title": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
      "authors": "Zhan Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19717",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp",
      "contributions": "",
      "summary": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
      "mindmap": ""
    },
    {
      "title": "Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals",
      "authors": "Vineet Yadav",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19721",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/476a71567ad37a78f3007d1e492eb010eed6a7f8bed8a187d46ae8e80465f4f5_w640_q70.webp",
      "contributions": "",
      "summary": "Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals",
      "mindmap": ""
    },
    {
      "title": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
      "authors": "Zihao Lv, Siqi Ai, Yanbin Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19719",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp",
      "contributions": "",
      "summary": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
      "mindmap": ""
    },
    {
      "title": "Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data",
      "authors": "Vasileios C. Pezoulas, Nikolaos S. Tachos, Eleni Georga, Kostas Marias, Manolis Tsiknakis, Dimitrios I. Fotiadis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19718",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp",
      "contributions": "",
      "summary": "Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data",
      "mindmap": ""
    },
    {
      "title": "Tiny, On-Device Decision Makers with the MiniConv Library",
      "authors": "Carlos Purves",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19726",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp",
      "contributions": "",
      "summary": "Tiny, On-Device Decision Makers with the MiniConv Library",
      "mindmap": ""
    },
    {
      "title": "Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking",
      "authors": "Srishti Gupta, Riccardo Balia, Daniele Angioni, Fabio Brau, Maura Pintor, Ambra Demontis, Alessandro Sebastian, Salvatore Mario Carta, Fabio Roli, Battista Biggio",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19725",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a6b157cb48d9bcd740b085c12288bed5c0e95c01a7146a5e7c2e50b7d77787d_w640_q70.webp",
      "contributions": "",
      "summary": "Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking",
      "mindmap": ""
    },
    {
      "title": "Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels",
      "authors": "Peng-Hung Tsai, Daniel Berleant",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19727",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2c9404cbdd59c5c56c246473b4a6996ca55f6051700faa6c654323d6990a290_w640_q70.webp",
      "contributions": "",
      "summary": "Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels",
      "mindmap": ""
    },
    {
      "title": "End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment",
      "authors": "Firas Bayram, Bestoun S. Ahmed, Erik Hallin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19723",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e87a73b8e332e90e3cd2839f1faa8882cdeef6cb35e274b072b141a0cabb8572_w640_q70.webp",
      "contributions": "",
      "summary": "End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment",
      "mindmap": ""
    },
    {
      "title": "Hard Negative Sample-Augmented DPO Post-Training for Small Language Models",
      "authors": "Haocheng Lu, Minjun Zhu, Henry Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19728",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp",
      "contributions": "",
      "summary": "Hard Negative Sample-Augmented DPO Post-Training for Small Language Models",
      "mindmap": ""
    },
    {
      "title": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
      "authors": "Zhonghao Yang, Cheng Luo, Daojing He, Yiming Li, Yu Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19730",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66bbe64f6f3cac5df3dfd69a018806ee7d1973ac071d8af57c13752826c622fe_w640_q70.webp",
      "contributions": "",
      "summary": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
      "mindmap": ""
    },
    {
      "title": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
      "authors": "Kosuke Ukita, Tsuyoshi Okita",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19729",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp",
      "contributions": "",
      "summary": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
      "mindmap": ""
    },
    {
      "title": "Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis",
      "authors": "Gaurav Kumar Sharma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19732",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53da428992d5e5cae28642416139ec0d148864a5c060b683ac9e143fe97079ee_w640_q70.webp",
      "contributions": "",
      "summary": "Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis",
      "mindmap": ""
    },
    {
      "title": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "authors": "Xiangzhong Luo, Weichen Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19731",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp",
      "contributions": "",
      "summary": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "mindmap": ""
    },
    {
      "title": "OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting",
      "authors": "Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19738",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp",
      "contributions": "",
      "summary": "OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting",
      "mindmap": ""
    },
    {
      "title": "The Deleuzian Representation Hypothesis",
      "authors": "Clément Cornet, Romaric Besançon, Hervé Le Borgne",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19734",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/877f862cfd6b85ad4699167cc9b9bc17de797ef85a7ede6911637da4967d6121_w640_q70.webp",
      "contributions": "",
      "summary": "The Deleuzian Representation Hypothesis",
      "mindmap": ""
    },
    {
      "title": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
      "authors": "Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19737",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp",
      "contributions": "",
      "summary": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
      "mindmap": ""
    },
    {
      "title": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
      "authors": "Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19736",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp",
      "contributions": "",
      "summary": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
      "mindmap": ""
    },
    {
      "title": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
      "authors": "Gangxiong Zhang, Yongchao Long",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19735",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffac075f9a08ac05f63c8e47026ade8aa961ca7bcc7071d314dbbcf27a110f66_w640_q70.webp",
      "contributions": "",
      "summary": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
      "mindmap": ""
    },
    {
      "title": "EdgeFlex-Transformer: Transformer Inference for Edge Devices",
      "authors": "Shoaib Mohammad, Guanqun Song, Ting Zhu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19741",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp",
      "contributions": "",
      "summary": "EdgeFlex-Transformer: Transformer Inference for Edge Devices",
      "mindmap": ""
    },
    {
      "title": "Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics",
      "authors": "Kousar Raza, Faizan Ali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19740",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f547c21d7d60b5ce4f92522c83aa7c297fc051a77b6eddf69ef405348fda9d8_w640_q70.webp",
      "contributions": "",
      "summary": "Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics",
      "mindmap": ""
    },
    {
      "title": "On-device Large Multi-modal Agent for Human Activity Recognition",
      "authors": "Md Shakhrul Iman Siam, Ishtiaque Ahmed Showmik, Guanqun Song, Ting Zhu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19742",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3cc1dde2877d131c87748e2a0e8975b217b5776f2152e724bf632e2fa6532f_w640_q70.webp",
      "contributions": "",
      "summary": "On-device Large Multi-modal Agent for Human Activity Recognition",
      "mindmap": ""
    },
    {
      "title": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
      "authors": "Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19743",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp",
      "contributions": "",
      "summary": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
      "mindmap": ""
    },
    {
      "title": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "authors": "Soumen Garai, Suman Samui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19739",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp",
      "contributions": "",
      "summary": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "mindmap": ""
    },
    {
      "title": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
      "authors": "Gustavo Coelho Haase, Paulo Henrique Dourado da Silva",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19744",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bccd562ab93d21bfa941fe354c60f5ea1f28b2e74751c8d34532405481aeeca_w640_q70.webp",
      "contributions": "",
      "summary": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
      "mindmap": ""
    },
    {
      "title": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
      "authors": "Sumin Park, Noseong Park",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19765",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp",
      "contributions": "",
      "summary": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
      "mindmap": ""
    },
    {
      "title": "Learning to Design City-scale Transit Routes",
      "authors": "Bibek Poudel, Weizi Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19767",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Design City-scale Transit Routes",
      "mindmap": ""
    },
    {
      "title": "A K-Means, Ward and DBSCAN repeatability study",
      "authors": "Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19772",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp",
      "contributions": "",
      "summary": "A K-Means, Ward and DBSCAN repeatability study",
      "mindmap": ""
    },
    {
      "title": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy",
      "authors": "Deepit Sapru",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19805",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb04e43886d4319737a1d917f74a55c539bf9fd5290a700f1e160de279d18cef_w640_q70.webp",
      "contributions": "",
      "summary": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy",
      "mindmap": ""
    },
    {
      "title": "Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling",
      "authors": "Shane X. Coffing, John Tipton, Arvind T. Mohan, Darren Engwirda",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19804",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e689d3fa5a81d3d8eba58c25d7cb4a0158b1806b3118990d514118edc1fa566_w640_q70.webp",
      "contributions": "",
      "summary": "Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling",
      "mindmap": ""
    },
    {
      "title": "UCCL-EP: Portable Expert-Parallel Communication",
      "authors": "Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19849",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",
      "contributions": "",
      "summary": "UCCL-EP: Portable Expert-Parallel Communication",
      "mindmap": ""
    },
    {
      "title": "Fine-Tuned In-Context Learners for Efficient Adaptation",
      "authors": "Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19879",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp",
      "contributions": "",
      "summary": "Fine-Tuned In-Context Learners for Efficient Adaptation",
      "mindmap": ""
    },
    {
      "title": "Detecting cyberbullying in Spanish texts through deep learning techniques",
      "authors": "Paúl Cumba-Armijos, Diego Riofrío-Luzcando, Verónica Rodríguez-Arboleda, Joe Carrión-Jumbo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19899",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp",
      "contributions": "",
      "summary": "Detecting cyberbullying in Spanish texts through deep learning techniques",
      "mindmap": ""
    },
    {
      "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "authors": "Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19920",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp",
      "contributions": "",
      "summary": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
      "authors": "Indranil Halder, Cengiz Pehlevan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19905",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp",
      "contributions": "",
      "summary": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
      "mindmap": ""
    },
    {
      "title": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
      "authors": "Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19909",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp",
      "contributions": "",
      "summary": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
      "mindmap": ""
    },
    {
      "title": "The Seismic Wavefield Common Task Framework",
      "authors": "Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19927",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59879c33166ae9f8c44c6fb1768cff1db7963674dc0f8347a443a44df80bf19_w640_q70.webp",
      "contributions": "",
      "summary": "The Seismic Wavefield Common Task Framework",
      "mindmap": ""
    },
    {
      "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
      "authors": "Samruddhi Baviskar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19935",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp",
      "contributions": "",
      "summary": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
      "mindmap": ""
    },
    {
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "authors": "Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19934",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp",
      "contributions": "",
      "summary": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "mindmap": ""
    },
    {
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19941",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp",
      "contributions": "",
      "summary": "Block-Recurrent Dynamics in Vision Transformers",
      "mindmap": ""
    },
    {
      "title": "Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis",
      "authors": "Surya Jayakumar, Kieran Sullivan, John McLaughlin, Christine O'Meara, Indrakshi Dey",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19970",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04d93c1a47d24166acae7ed5012d760c9a02de04651f40a319ded305b7aaad13_w640_q70.webp",
      "contributions": "",
      "summary": "Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis",
      "mindmap": ""
    },
    {
      "title": "Bloom Filter Encoding for Machine Learning",
      "authors": "John Cartmell, Mihaela Cardei, Ionut Cardei",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19991",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e677d5a3d466425f205097a14f528d2211f9cb2642b715b0647694a1db34a243_w640_q70.webp",
      "contributions": "",
      "summary": "Bloom Filter Encoding for Machine Learning",
      "mindmap": ""
    },
    {
      "title": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "authors": "Tamim Ahasan Rijon, Yeasin Arafath",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19989",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp",
      "contributions": "",
      "summary": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "mindmap": ""
    },
    {
      "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "authors": "Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19995",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp",
      "contributions": "",
      "summary": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "mindmap": ""
    },
    {
      "title": "Control Variate Score Matching for Diffusion Models",
      "authors": "Khaled Kahouli, Romuald Elie, Klaus-Robert Müller, Quentin Berthet, Oliver T. Unke, Arnaud Doucet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20003",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99c1fcd9cfbbb5a241296c106e6f4311b324493691659f27747af21f56a722fe_w640_q70.webp",
      "contributions": "",
      "summary": "Control Variate Score Matching for Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance",
      "authors": "Sukumar Kishanthan, Asela Hevapathige",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20006",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/767f4e4dcb2738c000b0ea66f1109ff56b696feb519040f06df4fdd1a29c343a_w640_q70.webp",
      "contributions": "",
      "summary": "Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance",
      "mindmap": ""
    },
    {
      "title": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
      "authors": "Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20004",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp",
      "contributions": "",
      "summary": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
      "mindmap": ""
    },
    {
      "title": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
      "authors": "Jiacheng You, Jingcheng Yang, Yuhang Xie, Zhongxuan Wu, Xiucheng Li, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, Xinyang Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20002",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3eef74ab05c46cb25ce1372769f7b8dc5bc12b1c381a6076e807a4bdde96f36_w640_q70.webp",
      "contributions": "",
      "summary": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
      "mindmap": ""
    },
    {
      "title": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
      "authors": "Yuan Gao, Zhenguo Dong, Xuelong Wang, Zhiqiang Wang, Yong Zhang, Shaofan Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20028",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp",
      "contributions": "",
      "summary": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
      "mindmap": ""
    },
    {
      "title": "Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems",
      "authors": "H. Li, J. Sun, Z. Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20058",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3aee744984f754345f38a1211552354aa6d648480458cd66df06732c2b2624_w640_q70.webp",
      "contributions": "",
      "summary": "Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems",
      "mindmap": ""
    },
    {
      "title": "An Optimal Policy for Learning Controllable Dynamics by Exploration",
      "authors": "Peter N. Loxley",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20053",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp",
      "contributions": "",
      "summary": "An Optimal Policy for Learning Controllable Dynamics by Exploration",
      "mindmap": ""
    },
    {
      "title": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "authors": "Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20059",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp",
      "contributions": "",
      "summary": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "mindmap": ""
    },
    {
      "title": "PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models",
      "authors": "Mingue Park, Jisung Hwang, Seungwoo Yoo, Kyeongmin Yeo, Minhyuk Sung",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20063",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp",
      "contributions": "",
      "summary": "PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models",
      "mindmap": ""
    },
    {
      "title": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection",
      "authors": "Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20086",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp",
      "contributions": "",
      "summary": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection",
      "mindmap": ""
    },
    {
      "title": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "authors": "Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20084",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp",
      "contributions": "",
      "summary": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "mindmap": ""
    },
    {
      "title": "Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning",
      "authors": "Zuo Wang, Ye Yuan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20094",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8ebe19b950699158b26962ef207ee20be9d3fd1a9e78b063e5d97cd6c18f0a_w640_q70.webp",
      "contributions": "",
      "summary": "Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning",
      "mindmap": ""
    },
    {
      "title": "Information-directed sampling for bandits: a primer",
      "authors": "Annika Hirling, Giorgio Nicoletti, Antonio Celani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20096",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp",
      "contributions": "",
      "summary": "Information-directed sampling for bandits: a primer",
      "mindmap": ""
    },
    {
      "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "authors": "Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20111",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp",
      "contributions": "",
      "summary": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "mindmap": ""
    },
    {
      "title": "Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering",
      "authors": "Yuanhao Chen, Qi Liu, Pengbin Chen, Zhongjian Qiao, Yanjie Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20115",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp",
      "contributions": "",
      "summary": "Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering",
      "mindmap": ""
    },
    {
      "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "authors": "Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20145",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp",
      "contributions": "",
      "summary": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "mindmap": ""
    },
    {
      "title": "Learning to Reason in LLMs by Expectation Maximization",
      "authors": "Junghyun Lee, Branislav Kveton, Sunav Choudhary, Subhojyoti Mukherjee, Anup Rao, Ryan A. Rossi, Alexa Siu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20169",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Reason in LLMs by Expectation Maximization",
      "mindmap": ""
    },
    {
      "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
      "authors": "Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20168",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp",
      "contributions": "",
      "summary": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
      "mindmap": ""
    },
    {
      "title": "NeuralCrop: Combining physics and machine learning for improved crop yield predictions",
      "authors": "Yunan Lin, Sebastian Bathiany, Maha Badri, Maximilian Gelbrecht, Philipp Hess, Brian Groenke, Jens Heinke, Christoph Müller, Niklas Boers",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20177",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11ac7dee2967ebac7497c5b7c4ac412a47aa4080e36cf0d94605e0611bcf487_w640_q70.webp",
      "contributions": "",
      "summary": "NeuralCrop: Combining physics and machine learning for improved crop yield predictions",
      "mindmap": ""
    },
    {
      "title": "Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning",
      "authors": "Kausthubh Manda, Raghuram Bharadwaj Diddigi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20220",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp",
      "contributions": "",
      "summary": "Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning",
      "mindmap": ""
    },
    {
      "title": "Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud",
      "authors": "Jixiao Yang, Jinyu Chen, Zixiao Huang, Chengda Xu, Chi Zhang, Sijia Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20218",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfa99879bd1a1c5083fc9e9363cb2903b42568a0726a124b591c9da1ed744c4f_w640_q70.webp",
      "contributions": "",
      "summary": "Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud",
      "mindmap": ""
    },
    {
      "title": "Adaptive Multi-task Learning for Probabilistic Load Forecasting",
      "authors": "Onintze Zaballa, Verónica Álvarez, Santiago Mazuelas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20232",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f105781b741c5e961d80700aae99a9a5f5491196f3957723a0ad4a9d0ceee4f_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Multi-task Learning for Probabilistic Load Forecasting",
      "mindmap": ""
    },
    {
      "title": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "authors": "Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20233",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp",
      "contributions": "",
      "summary": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "authors": "Xuanyu Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20249",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "mindmap": ""
    },
    {
      "title": "HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training",
      "authors": "Yuanjian Xu, Yuan Shuai, Jianing Hao, Guang Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20272",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336f6868f026371e14a0b1bb308e5d34193071d2aae6293400dd9ee01e404c90_w640_q70.webp",
      "contributions": "",
      "summary": "HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training",
      "mindmap": ""
    },
    {
      "title": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity",
      "authors": "Yuxing Gan, Ziyu Lei",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20291",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b419d6d6ce75f3399c7716ea75c97138302ae8a2a1e0c9b1547fa29a97bde7_w640_q70.webp",
      "contributions": "",
      "summary": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity",
      "mindmap": ""
    },
    {
      "title": "DeepONet-accelerated Bayesian inversion for moving boundary problems",
      "authors": "Marco A. Iglesias, Michael. E. Causon, Mikhail Y. Matveev, Andreas Endruweit, Michael .V. Tretyakov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20268",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp",
      "contributions": "",
      "summary": "DeepONet-accelerated Bayesian inversion for moving boundary problems",
      "mindmap": ""
    },
    {
      "title": "Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology",
      "authors": "Yoshihiro Maruyama",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20311",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862ac961bd55b0b8dc1a2ea1c32580f402c570473c4f0ee0635c937b87e810e8_w640_q70.webp",
      "contributions": "",
      "summary": "Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology",
      "mindmap": ""
    },
    {
      "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "authors": "Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20312",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp",
      "contributions": "",
      "summary": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning",
      "authors": "Mrinmay Sen, Subhrajit Nag",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20329",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp",
      "contributions": "",
      "summary": "FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "authors": "Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20328",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp",
      "contributions": "",
      "summary": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "mindmap": ""
    },
    {
      "title": "Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability",
      "authors": "Yoshihiro Maruyama",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20325",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d819bb4159585b74aeaf35355e16ef7a49b64ef54f670ff861804098262da1c5_w640_q70.webp",
      "contributions": "",
      "summary": "Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability",
      "mindmap": ""
    },
    {
      "title": "Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation",
      "authors": "Emilia Majerz, Witold Dzwinel, Jacek Kitowski",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20346",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp",
      "contributions": "",
      "summary": "Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation",
      "mindmap": ""
    },
    {
      "title": "Physics-guided Neural Network-based Shaft Power Prediction for Vessels",
      "authors": "Dogan Altan, Hamza Haruna Mohammed, Glenn Terje Lines, Dusica Marijan, Arnbjørn Maressa",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20348",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2fe8932d1c82e06128e6863fee4baaa3d988167af43502e32d83bf972eb356_w640_q70.webp",
      "contributions": "",
      "summary": "Physics-guided Neural Network-based Shaft Power Prediction for Vessels",
      "mindmap": ""
    },
    {
      "title": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "authors": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20363",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp",
      "contributions": "",
      "summary": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "authors": "Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20350",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp",
      "contributions": "",
      "summary": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "mindmap": ""
    },
    {
      "title": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples",
      "authors": "Xuan-An Le, Minh-Nam Tran, Son Nguyen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20403",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61964964aad2cf8cac29992fe364ccde3a687f3d0e6b42b8148ce3b0f96dee99_w640_q70.webp",
      "contributions": "",
      "summary": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples",
      "mindmap": ""
    },
    {
      "title": "GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer",
      "authors": "Corey Adams, Rishikesh Ranade, Ram Cherukuri, Sanjay Choudhry",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20399",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6966b4d62e10f76b2120afc54fcd95e1e717c9aef6840968117642fb0eb9df42_w640_q70.webp",
      "contributions": "",
      "summary": "GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer",
      "mindmap": ""
    },
    {
      "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "authors": "Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20407",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp",
      "contributions": "",
      "summary": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "mindmap": ""
    },
    {
      "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "authors": "Mihai Suteu, Ovidiu Serban",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20420",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp",
      "contributions": "",
      "summary": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "mindmap": ""
    },
    {
      "title": "Machine Learning to Predict Digital Frustration from Clickstream Data",
      "authors": "Jibin Joseph",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20438",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54b66cbccae694b64cb30bccff552dc3cd9c19b7ad622d4169444f7f1f67fdcb_w640_q70.webp",
      "contributions": "",
      "summary": "Machine Learning to Predict Digital Frustration from Clickstream Data",
      "mindmap": ""
    },
    {
      "title": "Explainable time-series forecasting with sampling-free SHAP for Transformers",
      "authors": "Matthias Hertel, Sebastian Pütz, Ralf Mikut, Veit Hagenmeyer, Benjamin Schäfer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20514",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce0ac2e96e47ae16fe2a238adc470cf7f3968e7b03d41a218d1483b207e0e532_w640_q70.webp",
      "contributions": "",
      "summary": "Explainable time-series forecasting with sampling-free SHAP for Transformers",
      "mindmap": ""
    },
    {
      "title": "Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow",
      "authors": "Tyler Clark, Christine Evers, Jonathon Hare",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20513",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp",
      "contributions": "",
      "summary": "Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow",
      "mindmap": ""
    },
    {
      "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20563",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",
      "contributions": "",
      "summary": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "mindmap": ""
    },
    {
      "title": "Improving ML Training Data with Gold-Standard Quality Metrics",
      "authors": "Leslie Barrett, Michael W. Sherman",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20577",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcda9dded34420c31f0b16ebfbeec41a79ad04607b8b4862e82f7a5504ae0ad_w640_q70.webp",
      "contributions": "",
      "summary": "Improving ML Training Data with Gold-Standard Quality Metrics",
      "mindmap": ""
    },
    {
      "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
      "authors": "Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20576",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp",
      "contributions": "",
      "summary": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "authors": "Rui Pan, Zhuofu Chen, Ravi Netravali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20573",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp",
      "contributions": "",
      "summary": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "mindmap": ""
    },
    {
      "title": "Relu and softplus neural nets as zero-sum turn-based games",
      "authors": "Stephane Gaubert, Yiannis Vlassopoulos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20582",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab02d27a8e6adea8ae6003cffa24e9fc6b3b0ee9d8f7ab828d024af5b17df0d7_w640_q70.webp",
      "contributions": "",
      "summary": "Relu and softplus neural nets as zero-sum turn-based games",
      "mindmap": ""
    },
    {
      "title": "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures",
      "authors": "Yedi Zhang, Andrew Saxe, Peter E. Latham",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20607",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e73c03581900a982dea4784fdd454e704c3d3b120fb84735e5f76640eb67bc86_w640_q70.webp",
      "contributions": "",
      "summary": "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures",
      "mindmap": ""
    },
    {
      "title": "FedPOD: the deployable units of training for federated learning",
      "authors": "Daewoon Kim, Si Young Yie, Jae Sung Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20610",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp",
      "contributions": "",
      "summary": "FedPOD: the deployable units of training for federated learning",
      "mindmap": ""
    },
    {
      "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "authors": "Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20605",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp",
      "contributions": "",
      "summary": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "mindmap": ""
    },
    {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20618",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp",
      "contributions": "",
      "summary": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "mindmap": ""
    },
    {
      "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "authors": "Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19703",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",
      "contributions": "",
      "summary": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra",
      "authors": "Federico Ottomano, Yingzhen Li, Alex M. Ganose",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19733",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1b9a0fd1ed2cc7769e92111592f07cc1a8189724bd03924f8083fce8cf7a5b_w640_q70.webp",
      "contributions": "",
      "summary": "NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra",
      "mindmap": ""
    },
    {
      "title": "Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise",
      "authors": "Joonsung Kang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19746",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5763bddf883e47ffbd39188c2d91d52b8f64bdca5101b8f91c97d4995c41b17_w640_q70.webp",
      "contributions": "",
      "summary": "Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise",
      "mindmap": ""
    },
    {
      "title": "Fundamentals of quantum Boltzmann machine learning with visible and hidden units",
      "authors": "Mark M. Wilde",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19819",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/109d3ad52a1d76b25a8f6b53d84a7219f99f5f6c4a1f4d3cfefd214d0a937f8f_w640_q70.webp",
      "contributions": "",
      "summary": "Fundamentals of quantum Boltzmann machine learning with visible and hidden units",
      "mindmap": ""
    },
    {
      "title": "Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization",
      "authors": "Habibollah Safari, Mona Bavarian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19715",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db0de246828ee3f9ea683f9e4205d984e45250d6966e0c692c62d8ded1fbecb0_w640_q70.webp",
      "contributions": "",
      "summary": "Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization",
      "mindmap": ""
    },
    {
      "title": "Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization",
      "authors": "Nikolaos Iliopoulos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19986",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f49f51d4e93dfaa76b5d85049c4b769868d3a8661c7cc58b3f721255548f39_w640_q70.webp",
      "contributions": "",
      "summary": "Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization",
      "mindmap": ""
    },
    {
      "title": "Efficient Learning of Lattice Gauge Theories with Fermions",
      "authors": "Shreya Shukla, Yukari Yamauchi, Andrey Y. Lokhov, Scott Lawrence, Abhijith Jayakumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19891",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/006eec86d07c59f9bdf92e4095e9b3bdfcd0d6d88a3ff992c9ae1bcaa42efa8d_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Learning of Lattice Gauge Theories with Fermions",
      "mindmap": ""
    },
    {
      "title": "Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function",
      "authors": "Matthew Drnevich, Stephen Jiggins, Kyle Cranmer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19913",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae42de4f7e707842fef18d1ebed0d4af2b3ff8eda273d229ca4aeaac9e765c90_w640_q70.webp",
      "contributions": "",
      "summary": "Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function",
      "mindmap": ""
    },
    {
      "title": "Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing",
      "authors": "Zhihan Huang, Ziang Niu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20007",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ac47ca7745f3ae745307a996bf9615b4c854438f5673b23f72f2db0c17c0052_w640_q70.webp",
      "contributions": "",
      "summary": "Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing",
      "mindmap": ""
    },
    {
      "title": "Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems",
      "authors": "Qiushuo Hou, Sangwoo Park, Matteo Zecchin, Yunlong Cai, Guanding Yu, Osvaldo Simeone, Tommaso Melodia",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20012",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9dd4c42fc11f6f1f9e179a396845d6d9c90f18f4b2a0968536d1bbe730aaa182_w640_q70.webp",
      "contributions": "",
      "summary": "Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems",
      "mindmap": ""
    },
    {
      "title": "GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics",
      "authors": "Suguru Shiratori, Elham Kiyani, Khemraj Shukla, George Em Karniadakis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19936",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/045354376dff81526b71609d7a09c5a83b94cf3ff400ab3fe060114398f02961_w640_q70.webp",
      "contributions": "",
      "summary": "GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics",
      "mindmap": ""
    },
    {
      "title": "Optimal Anytime-Valid Tests for Composite Nulls",
      "authors": "Shubhanshu Shekhar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20039",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6951c37e90b9cc41d18c9ac7d45b558a0fb6668f2f64b9527e2985cfe68c8bce_w640_q70.webp",
      "contributions": "",
      "summary": "Optimal Anytime-Valid Tests for Composite Nulls",
      "mindmap": ""
    },
    {
      "title": "Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models",
      "authors": "Anna R. Flowers, Christopher T. Franck, Robert B. Gramacy, Justin A. Krometis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20021",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50cf659ea746173ff874243a1ed23ec74a9a603abf5c8d8333c36a4bf579ec63_w640_q70.webp",
      "contributions": "",
      "summary": "Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models",
      "mindmap": ""
    },
    {
      "title": "Optimality-Informed Neural Networks for Solving Parametric Optimization Problems",
      "authors": "Matthias K. Hoffmann, Amine Othmane, Kathrin Flaßkamp",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20270",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e060736a8b15ff91b7a61c98f5f036e0acad7f1f487053768b143db60163709_w640_q70.webp",
      "contributions": "",
      "summary": "Optimality-Informed Neural Networks for Solving Parametric Optimization Problems",
      "mindmap": ""
    },
    {
      "title": "KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis",
      "authors": "Mebin Jose, Jisha Francis, Sudheesh Kumar Kattumannil",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20305",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp",
      "contributions": "",
      "summary": "KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis",
      "mindmap": ""
    },
    {
      "title": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
      "authors": "Samya Praharaj, Koulik Khamaru",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20368",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/912d43c0875266e42e18f18ddc2bcf11aaf3e8607a61856f82ebf3a2932bf5e1_w640_q70.webp",
      "contributions": "",
      "summary": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
      "mindmap": ""
    },
    {
      "title": "The Aligned Economic Index & The State Switching Model",
      "authors": "Ilias Aarab",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20460",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac7ffd2530ee7539902991cd05061cc86de381735b4620a6937d0821e9cb5403_w640_q70.webp",
      "contributions": "",
      "summary": "The Aligned Economic Index & The State Switching Model",
      "mindmap": ""
    },
    {
      "title": "ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification",
      "authors": "Masahiro Kato",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20523",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca45318d031a0b7e00633a6c1f048fa9feead63ea3447f2257acc9011d26b6c_w640_q70.webp",
      "contributions": "",
      "summary": "ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification",
      "mindmap": ""
    },
    {
      "title": "Over-the-Air Goal-Oriented Communications",
      "authors": "Kyriakos Stylianopoulos, Paolo Di Lorenzo, George C. Alexandropoulos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20533",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cf38466c85fbc6d90f4a4b0d12601a4d1aa3d13a53fc9c4c997edc35e26a213_w640_q70.webp",
      "contributions": "",
      "summary": "Over-the-Air Goal-Oriented Communications",
      "mindmap": ""
    },
    {
      "title": "Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention",
      "authors": "Yingzhen Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20562",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/126073a8d27aee52f27e9654f39d510e34be1f6aaec1e9c80cd8b6d23342281e_w640_q70.webp",
      "contributions": "",
      "summary": "Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention",
      "mindmap": ""
    },
    {
      "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
      "authors": "Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17911",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
      "mindmap": ""
    },
    {
      "title": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "authors": "Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17918",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp",
      "contributions": "",
      "summary": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "mindmap": ""
    },
    {
      "title": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "authors": "Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17910",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "mindmap": ""
    },
    {
      "title": "Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset",
      "authors": "Nick Rossenbach, Robin Schmitt, Tina Raissi, Simon Berger, Larissa Kleppel, Ralf Schlüter",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17915",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp",
      "contributions": "",
      "summary": "Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset",
      "mindmap": ""
    },
    {
      "title": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "authors": "Mason Youngblood",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17935",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp",
      "contributions": "",
      "summary": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "mindmap": ""
    },
    {
      "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
      "authors": "Petr Koklev",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17945",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/564ec84544e92a29e57fb456987e278879803262a1ad7f11b5331621181314ae_w640_q70.webp",
      "contributions": "",
      "summary": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
      "mindmap": ""
    },
    {
      "title": "Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States",
      "authors": "Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17934",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp",
      "contributions": "",
      "summary": "Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States",
      "mindmap": ""
    },
    {
      "title": "SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries",
      "authors": "Bin Wang, Fadi Dornaika",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17954",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp",
      "contributions": "",
      "summary": "SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries",
      "mindmap": ""
    },
    {
      "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
      "authors": "Gunho Park, Jeongin Bae, Byeongwook Kim, Baeseong park, Jiwon Ryu, Hoseung Kim, Se Jung Kwon, Dongsoo Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17970",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp",
      "contributions": "",
      "summary": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
      "mindmap": ""
    },
    {
      "title": "Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models",
      "authors": "Irina Seregina, Philippe Lalanda, German Vega",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17983",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp",
      "contributions": "",
      "summary": "Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models",
      "mindmap": ""
    },
    {
      "title": "Convolutional-neural-operator-based transfer learning for solving PDEs",
      "authors": "Peng Fan, Guofei Pang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17969",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp",
      "contributions": "",
      "summary": "Convolutional-neural-operator-based transfer learning for solving PDEs",
      "mindmap": ""
    },
    {
      "title": "MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements",
      "authors": "Ruichen Tan, Jiawei Xue, Kota Tsubouchi, Takahiro Yabe, Satish V. Ukkusuri",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17985",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a4ee269c2652d9f53e2d00acd67fb791427f4c088062380a3d842683837aff9_w640_q70.webp",
      "contributions": "",
      "summary": "MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements",
      "mindmap": ""
    },
    {
      "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "authors": "Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18014",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp",
      "contributions": "",
      "summary": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "mindmap": ""
    },
    {
      "title": "FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability",
      "authors": "S M Ruhul Kabir Howlader, Xiao Chen, Yifei Xie, Lu Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17986",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e5e22c4360871dd251f35903ee0f2af43d4bf37326ef9aa7f4dba13f94916b_w640_q70.webp",
      "contributions": "",
      "summary": "FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability",
      "mindmap": ""
    },
    {
      "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "authors": "Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18004",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp",
      "contributions": "",
      "summary": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization",
      "authors": "Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17987",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization",
      "mindmap": ""
    },
    {
      "title": "A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients",
      "authors": "Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18031",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp",
      "contributions": "",
      "summary": "A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients",
      "mindmap": ""
    },
    {
      "title": "Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models",
      "authors": "Wei Qian, Chenxu Zhao, Yangyi Li, Mengdi Huai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18035",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aaf9e957a407bb93f6576826db847a43f0675e8b765d8c6f38b7c40e06953c3_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models",
      "mindmap": ""
    },
    {
      "title": "A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations",
      "authors": "Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17984",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp",
      "contributions": "",
      "summary": "A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations",
      "mindmap": ""
    },
    {
      "title": "NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging",
      "authors": "Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18038",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp",
      "contributions": "",
      "summary": "NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging",
      "mindmap": ""
    },
    {
      "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts",
      "authors": "Roger A. Finger, Eduardo G. Cortes, Sandro J. Rigo, Gabriel de O. Ramos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18041",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp",
      "contributions": "",
      "summary": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts",
      "mindmap": ""
    },
    {
      "title": "Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics",
      "authors": "Daniel David",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18056",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12301f0dbb41071430f34a9014134b625a48eecd1e6b2f0d69403405addc6d7d_w640_q70.webp",
      "contributions": "",
      "summary": "Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics",
      "mindmap": ""
    },
    {
      "title": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "authors": "Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18057",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp",
      "contributions": "",
      "summary": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "mindmap": ""
    },
    {
      "title": "Approximation and learning with compositional tensor trains",
      "authors": "Martin Eigel, Charles Miranda, Anthony Nouy, David Sommer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18059",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91f6bb6272d1b93753642e31936e179de5889b19456b73cd0d2d98f6cb87c979_w640_q70.webp",
      "contributions": "",
      "summary": "Approximation and learning with compositional tensor trains",
      "mindmap": ""
    },
    {
      "title": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks",
      "authors": "Nina Mishra, Yonatan Naamad, Tal Wagner, Lichen Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18060",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac9385682bd6e4b07b769afe016fb50f71be5c2b6656f027de335845a16aed_w640_q70.webp",
      "contributions": "",
      "summary": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks",
      "mindmap": ""
    },
    {
      "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "authors": "Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18092",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp",
      "contributions": "",
      "summary": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "mindmap": ""
    },
    {
      "title": "From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines",
      "authors": "Kishan Kumar Ganguly, Tim Menzies",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18102",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/509a927741960d2c63062790fd8bebe6a6a22769c29f6985c39fb001039d9fab_w640_q70.webp",
      "contributions": "",
      "summary": "From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines",
      "mindmap": ""
    },
    {
      "title": "Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins",
      "authors": "Andreas E. Robertson, Samuel B. Inman, Ashley T. Lenau, Ricardo A. Lebensohn, Dongil Shin, Brad L. Boyce, Remi M. Dingreville",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18104",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4c8fb93b00e9d3df0d57fa728955a837f9e46dce9a1fe1000acf82c6b0a1f2_w640_q70.webp",
      "contributions": "",
      "summary": "Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins",
      "mindmap": ""
    },
    {
      "title": "TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates",
      "authors": "Maxmillan Ries, Sohan Seth",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18129",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a81b46d02b46e72f2027e054c761a825748caf485414255423ed130f0887b330_w640_q70.webp",
      "contributions": "",
      "summary": "TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates",
      "mindmap": ""
    },
    {
      "title": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
      "authors": "Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18133",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp",
      "contributions": "",
      "summary": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
      "mindmap": ""
    },
    {
      "title": "Learning Generalizable Neural Operators for Inverse Problems",
      "authors": "Adam J. Thorpe, Stepan Tretiakov, Dibakar Roy Sarkar, Krishna Kumar, Ufuk Topcu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18120",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e045cf5e1b4d1841db31e3e3cbb272846f23e43ba557e1455491e138f9aa045_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Generalizable Neural Operators for Inverse Problems",
      "mindmap": ""
    },
    {
      "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "authors": "Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18134",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp",
      "contributions": "",
      "summary": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "mindmap": ""
    },
    {
      "title": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "authors": "Lena Libon, Meghana Bhange, Rushabh Solanki, Elliot Creager, Ulrich Aïvodji",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18174",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp",
      "contributions": "",
      "summary": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "mindmap": ""
    },
    {
      "title": "FairExpand: Individual Fairness on Graphs with Partial Similarity Information",
      "authors": "Rebecca Salganik, Yibin Wang, Guillaume Salha-Galvan, Jian Kang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18180",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865ccd9cabcf2b700230e9c10d5db5bbd94993cb8f81acdca83db374c99156b8_w640_q70.webp",
      "contributions": "",
      "summary": "FairExpand: Individual Fairness on Graphs with Partial Similarity Information",
      "mindmap": ""
    },
    {
      "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
      "authors": "Jian Yan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18190",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp",
      "contributions": "",
      "summary": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
      "mindmap": ""
    },
    {
      "title": "When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics",
      "authors": "Yizhou Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18209",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp",
      "contributions": "",
      "summary": "When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics",
      "mindmap": ""
    },
    {
      "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "authors": "Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18215",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp",
      "contributions": "",
      "summary": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "mindmap": ""
    },
    {
      "title": "Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization",
      "authors": "Lichen Yang, Qiang Wang, Zhonghao Yang, Daojing He, Yu Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18228",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9987a54aec669e601a71ba8b0ee7d5434ba0c73d08a66e20538cf347a09669cd_w640_q70.webp",
      "contributions": "",
      "summary": "Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization",
      "mindmap": ""
    },
    {
      "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "authors": "Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18232",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp",
      "contributions": "",
      "summary": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "mindmap": ""
    },
    {
      "title": "Offline Behavioral Data Selection",
      "authors": "Shiye Lei, Zhihao Cheng, Dacheng Tao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18246",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp",
      "contributions": "",
      "summary": "Offline Behavioral Data Selection",
      "mindmap": ""
    },
    {
      "title": "Dimensionality Reduction Considered Harmful (Some of the Time)",
      "authors": "Hyeon Jeon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18230",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp",
      "contributions": "",
      "summary": "Dimensionality Reduction Considered Harmful (Some of the Time)",
      "mindmap": ""
    },
    {
      "title": "On the Convergence Rate of LoRA Gradient Descent",
      "authors": "Siqiao Mu, Diego Klabjan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18248",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8cfafc1ac3ad3a56d892c47d02b17599c178471d68de77c5fdb73874a8fc10_w640_q70.webp",
      "contributions": "",
      "summary": "On the Convergence Rate of LoRA Gradient Descent",
      "mindmap": ""
    },
    {
      "title": "LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform",
      "authors": "Lizhi Ma, Yi-Xiang Hu, Yuke Wang, Yifang Zhao, Yihui Ren, Jian-Xiang Liao, Feng Wu, Xiang-Yang Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18266",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0debec5ee9990221746ff49e0aa9313a4766307624a21ded5312ce8127844d_w640_q70.webp",
      "contributions": "",
      "summary": "LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform",
      "mindmap": ""
    },
    {
      "title": "FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation",
      "authors": "Runze You, Shi Pu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18275",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/075e8765f18ed2a89f21c4b5c0db9ea968dad78f6fb6aca6d0ea6c6135aadcde_w640_q70.webp",
      "contributions": "",
      "summary": "FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation",
      "mindmap": ""
    },
    {
      "title": "AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning",
      "authors": "Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18295",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp",
      "contributions": "",
      "summary": "AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning",
      "mindmap": ""
    },
    {
      "title": "Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings",
      "authors": "Harsh Rathva, Ojas Srivastava, Pruthwik Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18309",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp",
      "contributions": "",
      "summary": "Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings",
      "mindmap": ""
    },
    {
      "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "authors": "Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18317",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp",
      "contributions": "",
      "summary": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "mindmap": ""
    },
    {
      "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18333",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp",
      "contributions": "",
      "summary": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "mindmap": ""
    },
    {
      "title": "A two-stream network with global-local feature fusion for bone age assessment",
      "authors": "Qiong Lou, Han Yang, Fang Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18331",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp",
      "contributions": "",
      "summary": "A two-stream network with global-local feature fusion for bone age assessment",
      "mindmap": ""
    },
    {
      "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18336",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp",
      "contributions": "",
      "summary": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "mindmap": ""
    },
    {
      "title": "Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale",
      "authors": "Ansh Nagwekar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18373",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a52d6d3b760cd69f9655054febc798829d4e703cf29f84c34244775c0311631_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale",
      "mindmap": ""
    },
    {
      "title": "Neural Proofs for Sound Verification and Control of Complex Systems",
      "authors": "Alessandro Abate",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18389",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp",
      "contributions": "",
      "summary": "Neural Proofs for Sound Verification and Control of Complex Systems",
      "mindmap": ""
    },
    {
      "title": "The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?",
      "authors": "Vassilis Digalakis Jr, Christophe Pérignon, Sébastien Saurin, Flore Sentenac",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18390",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c5a36d8f48f4ca89c4a34600dea33569d95e61a50916599a5cc161e183185a_w640_q70.webp",
      "contributions": "",
      "summary": "The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?",
      "mindmap": ""
    },
    {
      "title": "Automated Mosaic Tesserae Segmentation via Deep Learning Techniques",
      "authors": "Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18406",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp",
      "contributions": "",
      "summary": "Automated Mosaic Tesserae Segmentation via Deep Learning Techniques",
      "mindmap": ""
    },
    {
      "title": "Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem",
      "authors": "Vikram Krishnamurthy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18409",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb98e65517a8f8628860576c4228301198ae634fca16d11c5f0a252aee09a842_w640_q70.webp",
      "contributions": "",
      "summary": "Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem",
      "mindmap": ""
    },
    {
      "title": "Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance",
      "authors": "Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18365",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance",
      "mindmap": ""
    },
    {
      "title": "MoE Pathfinder: Trajectory-driven Expert Pruning",
      "authors": "Xican Yang, Yuanhe Tian, Yan Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18425",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c828ca8ec3754ef48c892048aa8de845b7afe7c78f324073b3d3bb8afcdfc08_w640_q70.webp",
      "contributions": "",
      "summary": "MoE Pathfinder: Trajectory-driven Expert Pruning",
      "mindmap": ""
    },
    {
      "title": "On the Universality of Transformer Architectures; How Much Attention Is Enough?",
      "authors": "Amirreza Abbasi, Mohsen Hooshmand",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18445",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp",
      "contributions": "",
      "summary": "On the Universality of Transformer Architectures; How Much Attention Is Enough?",
      "mindmap": ""
    },
    {
      "title": "NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic",
      "authors": "Jayant Lohia",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18453",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp",
      "contributions": "",
      "summary": "NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic",
      "mindmap": ""
    },
    {
      "title": "Secret mixtures of experts inside your LLM",
      "authors": "Enric Boix-Adsera",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18452",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp",
      "contributions": "",
      "summary": "Secret mixtures of experts inside your LLM",
      "mindmap": ""
    },
    {
      "title": "Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs",
      "authors": "David Graber, Victor Armegioiu, Rebecca Buller, Siddhartha Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18454",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2eb80d49254700f260ff3967d335e1d51ecb254bd1a0020cef4b670cd5e329_w640_q70.webp",
      "contributions": "",
      "summary": "Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs",
      "mindmap": ""
    },
    {
      "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "authors": "Christopher Román Jaimes",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18462",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp",
      "contributions": "",
      "summary": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "mindmap": ""
    },
    {
      "title": "Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review",
      "authors": "Oraib Almegdadi, João Marcelino, Sarah Fakhreddine, João Manso, Nuno C. Marques",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18466",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp",
      "contributions": "",
      "summary": "Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review",
      "mindmap": ""
    },
    {
      "title": "The Geometry of Abstraction: Continual Learning via Recursive Quotienting",
      "authors": "Xin Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18471",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/946bc88c1851af1d701b7d19272c910b4e964e6d83ae04fa6fbdbf0f215f6e75_w640_q70.webp",
      "contributions": "",
      "summary": "The Geometry of Abstraction: Continual Learning via Recursive Quotienting",
      "mindmap": ""
    },
    {
      "title": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification",
      "authors": "Mykola Kuz, Ihor Lazarovych, Mykola Kozlenko, Mykola Pikuliak, Andrii Kvasniuk",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18475",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp",
      "contributions": "",
      "summary": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification",
      "mindmap": ""
    },
    {
      "title": "APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification",
      "authors": "Khaled Berkani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18473",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/917648447e1450eccba6ff12938452166cbb307d588b3036ddd72e80601da793_w640_q70.webp",
      "contributions": "",
      "summary": "APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification",
      "mindmap": ""
    },
    {
      "title": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "authors": "Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18500",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp",
      "contributions": "",
      "summary": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "mindmap": ""
    },
    {
      "title": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
      "authors": "Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18503",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp",
      "contributions": "",
      "summary": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
      "mindmap": ""
    },
    {
      "title": "Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts",
      "authors": "Hatim M. E. Geli, Islam Omar, Mona Y. Elshinawy, David W. DuBios, Lara Prehodko, Kelly H Smith, Abdel-Hameed A. Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18522",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp",
      "contributions": "",
      "summary": "Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts",
      "mindmap": ""
    },
    {
      "title": "Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study",
      "authors": "Janek Dyer, Jagdeep Ahluwalia, Javad Zarrin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18524",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08031a1b54db55f30489d768a136dda2ef1e6ae4c24a0e92ea56eefd2fef4532_w640_q70.webp",
      "contributions": "",
      "summary": "Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study",
      "mindmap": ""
    },
    {
      "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
      "authors": "S Mahmudul Hasan, Shaily Roy, Akib Jawad Nafis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18533",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp",
      "contributions": "",
      "summary": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
      "mindmap": ""
    },
    {
      "title": "Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies",
      "authors": "John Cao, Luca Furieri",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18540",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp",
      "contributions": "",
      "summary": "Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies",
      "mindmap": ""
    },
    {
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "authors": "Scott Thornton",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18542",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp",
      "contributions": "",
      "summary": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "mindmap": ""
    },
    {
      "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "authors": "Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18552",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp",
      "contributions": "",
      "summary": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "mindmap": ""
    },
    {
      "title": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
      "authors": "Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18575",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp",
      "contributions": "",
      "summary": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
      "mindmap": ""
    },
    {
      "title": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "authors": "Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18573",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp",
      "contributions": "",
      "summary": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "mindmap": ""
    },
    {
      "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "authors": "Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18583",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp",
      "contributions": "",
      "summary": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment",
      "authors": "Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18566",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17e1231f344e9051e9a9124b095324f59b3035dd2a93ac7e3bdf19d86c6eec28_w640_q70.webp",
      "contributions": "",
      "summary": "Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment",
      "mindmap": ""
    },
    {
      "title": "EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture",
      "authors": "Quanxi Zhou, Wencan Mao, Yilei Liang, Manabu Tsukada, Yunling Liu, Jon Crowcroft",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18596",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856f913f2cb31f98d005287343fc242bd36ba508c56e6d8fbbbb68c073df2a30_w640_q70.webp",
      "contributions": "",
      "summary": "EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture",
      "mindmap": ""
    },
    {
      "title": "Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows",
      "authors": "Runze Mao, Rui Zhang, Xuan Bai, Tianhao Wu, Teng Zhang, Zhenyi Chen, Minqi Lin, Bocheng Zeng, Yangchen Xu, Yingxuan Xiang, Haoze Zhang, Shubham Goswami, Pierre A. Dawe, Yifan Xu, Zhenhua An, Mengtao Yan, Xiaoyi Lu, Yi Wang, Rongbo Bai, Haobu Gao, Xiaohang Fang, Han Li, Hao Sun, Zhi X. Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18595",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54fa7968484f2312fe5e5490bcb221fdffbad35b3d30834c98660d094a86bdd2_w640_q70.webp",
      "contributions": "",
      "summary": "Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows",
      "mindmap": ""
    },
    {
      "title": "Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning",
      "authors": "Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18604",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp",
      "contributions": "",
      "summary": "Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning",
      "mindmap": ""
    },
    {
      "title": "The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss",
      "authors": "Rongyao Cai, Yuxi Wan, Kexin Zhang, Ming Jin, Hao Wang, Zhiqiang Ge, Daoyi Dong, Yong Liu, Qingsong Wen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18610",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a15caebdaadc9b61bcc30ac1fa7534d1efe4cc5ec0d9274e6b2bd1e89b75d13_w640_q70.webp",
      "contributions": "",
      "summary": "The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss",
      "mindmap": ""
    },
    {
      "title": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
      "authors": "Huiqi Deng, Qihan Ren, Zhuofan Chen, Zhenyuan Cui, Wen Shen, Peng Zhang, Hongbin Pei, Quanshi Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18607",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp",
      "contributions": "",
      "summary": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
      "mindmap": ""
    },
    {
      "title": "ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs",
      "authors": "Han-Seul Jeong, Youngjoon Park, Hyungseok Song, Woohyung Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18633",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp",
      "contributions": "",
      "summary": "ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs",
      "mindmap": ""
    },
    {
      "title": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers",
      "authors": "Ryotaro Kawata, Yujin Song, Alberto Bietti, Naoki Nishikawa, Taiji Suzuki, Samuel Vaiter, Denny Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18634",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e54fb36cb8dc15a87cb11d651f2fd0a36426f91acc811072a8e23c04544cd8_w640_q70.webp",
      "contributions": "",
      "summary": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers",
      "mindmap": ""
    },
    {
      "title": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "authors": "Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18660",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp",
      "contributions": "",
      "summary": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs",
      "authors": "Ning Lyu, Junjie Jiang, Lu Chang, Chihui Shao, Feng Chen, Chong Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18673",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c16637936bf2055d30084813c6afcafc16bc9cf66165235e01a57ad34447eb0_w640_q70.webp",
      "contributions": "",
      "summary": "Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs",
      "mindmap": ""
    },
    {
      "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "authors": "Pengchao Feng, Yao Xiao, Ziyang Ma, Zhikang Niu, Shuai Fan, Yao Li, Sheng Wang, Xie Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18699",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp",
      "contributions": "",
      "summary": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "mindmap": ""
    },
    {
      "title": "Generating Risky Samples with Conformity Constraints via Diffusion Models",
      "authors": "Han Yu, Hao Zou, Xingxuan Zhang, Zhengyi Wang, Yue He, Kehan Li, Peng Cui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18722",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccd2af87861ffba1a1a1172552bec8a5a2c9fb4db73b173801ff6c5db0f1734b_w640_q70.webp",
      "contributions": "",
      "summary": "Generating Risky Samples with Conformity Constraints via Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments",
      "authors": "Xue Yang, Michael Schukat, Junlin Lu, Patrick Mannion, Karl Mason, Enda Howley",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18670",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp",
      "contributions": "",
      "summary": "Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments",
      "mindmap": ""
    },
    {
      "title": "A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models",
      "authors": "Zhiquan Tan, Yinrong Hong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18730",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp",
      "contributions": "",
      "summary": "A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models",
      "mindmap": ""
    },
    {
      "title": "ML Inference Scheduling with Predictable Latency",
      "authors": "Haidong Zhao, Nikolaos Georgantas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18725",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a8d1548e228be441a72232fe96db7a46d1c28ad7f5a73b47e37580f3b42dadf_w640_q70.webp",
      "contributions": "",
      "summary": "ML Inference Scheduling with Predictable Latency",
      "mindmap": ""
    },
    {
      "title": "Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding",
      "authors": "Xiangrui Cai, Shaocheng Ma, Lei Cao, Jie Li, Tianyu Liu, Yilin Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18689",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp",
      "contributions": "",
      "summary": "Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding",
      "mindmap": ""
    },
    {
      "title": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation",
      "authors": "Zichuan Lin, Xiaokai Huang, Jiate Liu, Yuxuan Han, Jia Chen, Xiapeng Wu, Deheng Ye",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18737",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp",
      "contributions": "",
      "summary": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation",
      "mindmap": ""
    },
    {
      "title": "Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth",
      "authors": "Chainarong Amornbunchornvej",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18732",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp",
      "contributions": "",
      "summary": "Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth",
      "mindmap": ""
    },
    {
      "title": "Is Your Conditional Diffusion Model Actually Denoising?",
      "authors": "Daniel Pfrommer, Zehao Dou, Christopher Scarvelis, Max Simchowitz, Ali Jadbabaie",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18736",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856ef1fcb2e79aa2ab4b2a022b1c3d13580d56e426a05e9b3d88850160ea2eac_w640_q70.webp",
      "contributions": "",
      "summary": "Is Your Conditional Diffusion Model Actually Denoising?",
      "mindmap": ""
    },
    {
      "title": "Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning",
      "authors": "Minh Vu, Konstantinos Slavakis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18763",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp",
      "contributions": "",
      "summary": "Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "authors": "Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18745",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp",
      "contributions": "",
      "summary": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "mindmap": ""
    },
    {
      "title": "Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers",
      "authors": "Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18784",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp",
      "contributions": "",
      "summary": "Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers",
      "mindmap": ""
    },
    {
      "title": "Label-Informed Outlier Detection Based on Granule Density",
      "authors": "Baiyang Chen, Zhong Yuan, Dezhong Peng, Hongmei Chen, Xiaomin Song, Huiming Zheng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18774",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27f829438adaa9d5eb3ec20d666226593c7dc2a412469b585dd40c6517ab3e3_w640_q70.webp",
      "contributions": "",
      "summary": "Label-Informed Outlier Detection Based on Granule Density",
      "mindmap": ""
    },
    {
      "title": "Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection",
      "authors": "Souhail Abdelmouaiz Sadat, Mohamed Yacine Touahria Miliani, Khadidja Hab El Hames, Hamida Seba, Mohammed Haddad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18826",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp",
      "contributions": "",
      "summary": "Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection",
      "mindmap": ""
    },
    {
      "title": "Controllable Probabilistic Forecasting with Stochastic Decomposition Layers",
      "authors": "John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne II, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18815",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp",
      "contributions": "",
      "summary": "Controllable Probabilistic Forecasting with Stochastic Decomposition Layers",
      "mindmap": ""
    },
    {
      "title": "Generative Modeling through Spectral Analysis of Koopman Operator",
      "authors": "Yuanchao Xu, Fengyi Li, Masahiro Fujisawa, Youssef Marzouk, Isao Ishikawa",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18837",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Modeling through Spectral Analysis of Koopman Operator",
      "mindmap": ""
    },
    {
      "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "authors": "Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18857",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp",
      "contributions": "",
      "summary": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "mindmap": ""
    },
    {
      "title": "Application of deep learning approaches for medieval historical documents transcription",
      "authors": "Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18865",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp",
      "contributions": "",
      "summary": "Application of deep learning approaches for medieval historical documents transcription",
      "mindmap": ""
    },
    {
      "title": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
      "authors": "Gökdeniz Gülmez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18901",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp",
      "contributions": "",
      "summary": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
      "mindmap": ""
    },
    {
      "title": "The Ensemble Schr\\{ö\\}dinger Bridge filter for Nonlinear Data Assimilation",
      "authors": "Feng Bao, Hui Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18928",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b1984f127a0ea77e013fd30f9e55a2d1893fc429cb8128fbd4e89c1fefcb6_w640_q70.webp",
      "contributions": "",
      "summary": "The Ensemble Schr\\{ö\\}dinger Bridge filter for Nonlinear Data Assimilation",
      "mindmap": ""
    },
    {
      "title": "Merging of Kolmogorov-Arnold networks trained on disjoint datasets",
      "authors": "Andrew Polar, Michael Poluektov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18921",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp",
      "contributions": "",
      "summary": "Merging of Kolmogorov-Arnold networks trained on disjoint datasets",
      "mindmap": ""
    },
    {
      "title": "DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems",
      "authors": "Sarwan Ali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18932",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c243588ebf5b62b4dd8e1854c41bcdec7ca5861520351edb4b3b12455148d30_w640_q70.webp",
      "contributions": "",
      "summary": "DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems",
      "mindmap": ""
    },
    {
      "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
      "authors": "Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18934",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp",
      "contributions": "",
      "summary": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "authors": "Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18950",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "mindmap": ""
    },
    {
      "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
      "authors": "Debamita Ghosh, George K. Atia, Yue Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18957",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp",
      "contributions": "",
      "summary": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
      "mindmap": ""
    },
    {
      "title": "Learning Through Little Eyes: Attribute Discrimination Beyond Objects",
      "authors": "Patrick Batsell, Tsutsui Satoshi, Bihan Wen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18951",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e80b7c71db93c26589e4d25cfffe472d3de86a058a86caaabd4d81fb9bc2c38_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Through Little Eyes: Attribute Discrimination Beyond Objects",
      "mindmap": ""
    },
    {
      "title": "Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling",
      "authors": "Sutashu Tomonaga, Kenji Doya, Noboru Murata",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18965",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49d3230921a0fe478c29d46346dffb80acdac27624660b8dd8316094a5be88aa_w640_q70.webp",
      "contributions": "",
      "summary": "Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling",
      "mindmap": ""
    },
    {
      "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "authors": "Yizhi Wang, Linan Yue, Min-Ling Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18956",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp",
      "contributions": "",
      "summary": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "mindmap": ""
    },
    {
      "title": "Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets",
      "authors": "Baiyang Chen, Zhong Yuan, Dezhong Peng, Xiaoliang Chen, Hongmei Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18977",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f137793e37d26cff00a3715c8212e2ad3fa2487bcba70f9d1f1f23ec3f06ea9_w640_q70.webp",
      "contributions": "",
      "summary": "Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets",
      "mindmap": ""
    },
    {
      "title": "Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy",
      "authors": "Baiyang Chen, Zhong Yuan, Zheng Liu, Dezhong Peng, Yongxiang Li, Chang Liu, Guiduo Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18978",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58b9c746816307fee96e3cf245632387dbf3ca0eaff52acdf2fdc9465315f8e9_w640_q70.webp",
      "contributions": "",
      "summary": "Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy",
      "mindmap": ""
    },
    {
      "title": "OPBO: Order-Preserving Bayesian Optimization",
      "authors": "Wei Peng, Jianchen Hu, Kang Liu, Qiaozhu Zhai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18980",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b75b70c8d757c498d035de7add4278dda4866bb8a7052d89525a499956b1847_w640_q70.webp",
      "contributions": "",
      "summary": "OPBO: Order-Preserving Bayesian Optimization",
      "mindmap": ""
    },
    {
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "authors": "Kun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu, Pengfei Gu, Chenghua Lin, Paul M. Thompson, Alex Leow, Heng Huang, Lifang He, Liang Zhan, Haoteng Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18986",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp",
      "contributions": "",
      "summary": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "mindmap": ""
    },
    {
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": "Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19011",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "mindmap": ""
    },
    {
      "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
      "authors": "Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19001",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp",
      "contributions": "",
      "summary": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
      "mindmap": ""
    },
    {
      "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "authors": "Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19004",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp",
      "contributions": "",
      "summary": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "mindmap": ""
    },
    {
      "title": "The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results",
      "authors": "Konstantin Kaulen, Tobias Ladner, Stanley Bak, Christopher Brix, Hai Duong, Thomas Flinkow, Taylor T. Johnson, Lukas Koller, Edoardo Manino, ThanhVu H Nguyen, Haoze Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19007",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp",
      "contributions": "",
      "summary": "The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results",
      "mindmap": ""
    },
    {
      "title": "Optimizer Dynamics at the Edge of Stability with Differential Privacy",
      "authors": "Ayana Hussain, Ricky Fang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19019",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe869fb966bc8e432b82a891d4042c2fd2899dd38d8367f5952e66e27a733144_w640_q70.webp",
      "contributions": "",
      "summary": "Optimizer Dynamics at the Edge of Stability with Differential Privacy",
      "mindmap": ""
    },
    {
      "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "authors": "Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19020",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp",
      "contributions": "",
      "summary": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "mindmap": ""
    },
    {
      "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "authors": "Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19025",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp",
      "contributions": "",
      "summary": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "mindmap": ""
    },
    {
      "title": "Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building",
      "authors": "Liping Sun, Yucheng Guo, Siliang Lu, Zhenzhen Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19038",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac72029d8bfa1d8aa93562d8cd7cf4c8a3ea568788647c1533d43f1e70bc9335_w640_q70.webp",
      "contributions": "",
      "summary": "Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building",
      "mindmap": ""
    },
    {
      "title": "Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms",
      "authors": "Md Minhazul Islam Munna, Md Mahbubur Rahman, Jaroslav Frnda, Muhammad Shahid Anwar, Alpamis Kutlimuratov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19037",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f65bbdc2e7a521d46fdaeebb4c5a652347737b90cb0eb72dc48b2bfbbff2789_w640_q70.webp",
      "contributions": "",
      "summary": "Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms",
      "mindmap": ""
    },
    {
      "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
      "authors": "Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19027",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp",
      "contributions": "",
      "summary": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
      "mindmap": ""
    },
    {
      "title": "On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation",
      "authors": "George Vershinin, Asaf Cohen, Omer Gurewitz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19067",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/395cdf02cbfcdfcb8cd487bcc7bb40a760166e8635105ec2f48357e1a0a09bbb_w640_q70.webp",
      "contributions": "",
      "summary": "On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation",
      "mindmap": ""
    },
    {
      "title": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
      "authors": "Chi Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19061",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp",
      "contributions": "",
      "summary": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
      "mindmap": ""
    },
    {
      "title": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
      "authors": "Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19091",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp",
      "contributions": "",
      "summary": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
      "mindmap": ""
    },
    {
      "title": "Efficient Personalization of Generative Models via Optimal Experimental Design",
      "authors": "Guy Schacht, Ziyad Sheebaelhamd, Riccardo De Santi, Mojmír Mutný, Andreas Krause",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19057",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcef3113c71e842238e399994571c9492cf534afd3c06e6241febfcda1354e87_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Personalization of Generative Models via Optimal Experimental Design",
      "mindmap": ""
    },
    {
      "title": "Dual Model Deep Learning for Alzheimer Prognostication",
      "authors": "Alireza Moayedikia, Sara Fin, Uffe Kock Wiil",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19099",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57c6707a8cb9bbf9189cedc3fc04aab23c29bbec6c15b1c12163211decc70869_w640_q70.webp",
      "contributions": "",
      "summary": "Dual Model Deep Learning for Alzheimer Prognostication",
      "mindmap": ""
    },
    {
      "title": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "authors": "Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19103",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp",
      "contributions": "",
      "summary": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "mindmap": ""
    },
    {
      "title": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
      "authors": "Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19097",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp",
      "contributions": "",
      "summary": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
      "mindmap": ""
    },
    {
      "title": "A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development",
      "authors": "Yuan Fang, Fabian Waschkowski, Maximilian Reissmann, Richard D. Sandberg, Takuo Oda, Koichi Tanimoto",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19031",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6630d808b10ee83956b0b8d3975fc127ddbef17b5014ced381ff234a9441900d_w640_q70.webp",
      "contributions": "",
      "summary": "A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development",
      "mindmap": ""
    },
    {
      "title": "A Composable Channel-Adaptive Architecture for Seizure Classification",
      "authors": "Francesco Carzaniga, Michael Hersche, Kaspar Schindler, Abbas Rahimi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19123",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80ba764ee64b005cd983a9e6b724f69e3e08628a8e048027385de0b6ea62e65d_w640_q70.webp",
      "contributions": "",
      "summary": "A Composable Channel-Adaptive Architecture for Seizure Classification",
      "mindmap": ""
    },
    {
      "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
      "authors": "Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19125",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp",
      "contributions": "",
      "summary": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
      "mindmap": ""
    },
    {
      "title": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "authors": "Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19131",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp",
      "contributions": "",
      "summary": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "mindmap": ""
    },
    {
      "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "authors": "Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin, Wei Zhong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19114",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp",
      "contributions": "",
      "summary": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "mindmap": ""
    },
    {
      "title": "A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage",
      "authors": "Francis Bach",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19142",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4a93fce488552329133c465e13cb55215345c0def3d3e8109e9ab58f1388fad_w640_q70.webp",
      "contributions": "",
      "summary": "A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage",
      "mindmap": ""
    },
    {
      "title": "RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling",
      "authors": "Haoran Yang, Yinan Zhang, Wenjie Zhang, Dongxia Wang, Peiyu Liu, Yuqi Ye, Kexin Chen, Wenhai Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19147",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d3125cc08ed130beab248cba1a7473ff408ca14e82e124e77351d681c62aec5_w640_q70.webp",
      "contributions": "",
      "summary": "RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling",
      "mindmap": ""
    },
    {
      "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "authors": "Geraud Nangue Tasse, Matthew Riemer, Benjamin Rosman, Tim Klinger",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19154",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "mindmap": ""
    },
    {
      "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
      "authors": "Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19184",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp",
      "contributions": "",
      "summary": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
      "mindmap": ""
    },
    {
      "title": "Practical Quantum-Classical Feature Fusion for complex data Classification",
      "authors": "Azadeh Alavi, Fatemeh Kouchmeshki, Abdolrahman Alavi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19180",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp",
      "contributions": "",
      "summary": "Practical Quantum-Classical Feature Fusion for complex data Classification",
      "mindmap": ""
    },
    {
      "title": "Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction",
      "authors": "Leming Zhou, Zuo Wang, Zhigang Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19194",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6faf26f0cc652e06ad561a1f1b33d9cf5c741015ebbded034f635a578186206_w640_q70.webp",
      "contributions": "",
      "summary": "Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction",
      "mindmap": ""
    },
    {
      "title": "On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning",
      "authors": "Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19199",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp",
      "contributions": "",
      "summary": "On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning",
      "mindmap": ""
    },
    {
      "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "authors": "Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19190",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp",
      "contributions": "",
      "summary": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "mindmap": ""
    },
    {
      "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "authors": "Tao Zhang, Ziqian Zeng, Hao Peng, Huiping Zhuang, Cen Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19206",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp",
      "contributions": "",
      "summary": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "mindmap": ""
    },
    {
      "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "authors": "Anna-Maria Gueorguieva, Aylin Caliskan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19238",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp",
      "contributions": "",
      "summary": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "mindmap": ""
    },
    {
      "title": "Phase-space entropy at acquisition reflects downstream learnability",
      "authors": "Xiu-Cheng Wang, Jun-Jie Zhanga, Nan Cheng, Long-Gang Pang, Taijiao Du, Deyu Meng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19223",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp",
      "contributions": "",
      "summary": "Phase-space entropy at acquisition reflects downstream learnability",
      "mindmap": ""
    },
    {
      "title": "From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis",
      "authors": "Moncef Garouani, Ayah Barhrhouj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19246",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edaee8be894e0b2aabf965adc9c2da7a8499fac8b0b1b7437890bac0428efa6_w640_q70.webp",
      "contributions": "",
      "summary": "From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis",
      "mindmap": ""
    },
    {
      "title": "Regression generation adversarial network based on dual data evaluation strategy for industrial application",
      "authors": "Zesen Wang, Yonggang Li, Lijuan Lan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19232",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0ad96ff9f5740075b9f98eecc3bf3e0d3e6ae902f7528d0c14e4cef5572f55_w640_q70.webp",
      "contributions": "",
      "summary": "Regression generation adversarial network based on dual data evaluation strategy for industrial application",
      "mindmap": ""
    },
    {
      "title": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
      "authors": "Prathamesh Devadiga",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19250",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp",
      "contributions": "",
      "summary": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
      "mindmap": ""
    },
    {
      "title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "authors": "Carla Crivoi, Radu Tudor Ionescu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19253",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp",
      "contributions": "",
      "summary": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "mindmap": ""
    },
    {
      "title": "Translating Flow to Policy via Hindsight Online Imitation",
      "authors": "Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19269",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp",
      "contributions": "",
      "summary": "Translating Flow to Policy via Hindsight Online Imitation",
      "mindmap": ""
    },
    {
      "title": "GShield: Mitigating Poisoning Attacks in Federated Learning",
      "authors": "Sameera K. M., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19286",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c43d719deb3b65a2681b1c10fbb4647b0e73aa0efbe80e20c06d9f27f59d1058_w640_q70.webp",
      "contributions": "",
      "summary": "GShield: Mitigating Poisoning Attacks in Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring",
      "authors": "Keivan Faghih Niresi, Jun Qing, Mengjie Zhao, Olga Fink",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19309",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e839b4b1adb5610adcd939ece8fb2b9f58ed8b25b96c321b897b8bac816ca350_w640_q70.webp",
      "contributions": "",
      "summary": "Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring",
      "mindmap": ""
    }
  ]
}