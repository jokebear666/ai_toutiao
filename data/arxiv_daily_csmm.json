{
  "label": "cs.MM",
  "slug": "csmm",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding",
      "authors": "A V Uday Kiran Kandala",
      "institution": "Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.21698",
      "code": null,
      "tags": [
        "steganography",
        "raster domain steganography",
        "glyph perturbation",
        "deterministic rasterization",
        "multimodal embedding",
        "text-based data hiding"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp",
      "contributions": "1. Proposes a unified Glyph Perturbation Cardinality (GPC) framework for embedding heterogeneous data (text, images, audio, video) directly into the pixel space of rendered text glyphs. 2. Operates exclusively in the raster domain after font rendering, modifying bitmap pixels with minimal, visually imperceptible intensity increments for covert communication. 3. Introduces a decoding method based on re-rasterizing cover text, subtracting canonical glyph rasters, and recovering payload via pixel count analysis, leveraging deterministic raster behavior.",
      "summary": "This paper introduces a raster domain steganography framework that embeds multimodal data into text by minimally perturbing the interior pixels of rendered glyphs. The method is visually imperceptible and computationally lightweight, enabling ordinary text to serve as a covert medium for secure data embedding. It generalizes beyond traditional linguistic steganography by operating directly on the deterministic bitmap output of text rendering pipelines.",
      "mindmap": "graph TB\n        Root(”Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding”) --> Problem(”核心问题/Problem: How to embed multimodal data covertly into ordinary text?”)\n        Root --> Method(”主要方法/Method: Glyph Perturbation Cardinality (GPC) Framework”)\n        Root --> Results(”关键结果/Results: Visually imperceptible, lightweight embedding in raster domain”)\n        Problem --> P1(”传统方法局限/Limitations of linguistic & structural methods”)\n        Method --> M1(”操作于栅格化后/Operates post-rasterization”)\n        Method --> M2(”扰动字形内部像素/Perturbs interior ink pixels”)\n        Method --> M3(”基于像素基数编码/Encodes via pixel cardinality”)\n        Results --> R1(”支持多模态数据/Supports multimodal data”)\n        Results --> R2(”解码稳定可靠/Stable & decodable signal”)"
    },
    {
      "title": "Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion",
      "authors": "Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du",
      "institution": "Nanjing University of Science and Technology, Singapore Management University, University of Wisconsin-Madison, GienTech Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21863",
      "code": null,
      "tags": [
        "recommender systems",
        "frozen large video language models",
        "micro-video recommendation",
        "feature fusion",
        "intermediate hidden states",
        "dual feature fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp",
      "contributions": "1. Conducted the first systematic empirical study on integrating frozen LVLMs into micro-video recommendation, evaluating feature extraction paradigms (captions vs. hidden states) and integration strategies (replacement vs. fusion) with ID embeddings. 2. Derived three key principles: intermediate hidden states outperform captions, ID embeddings are irreplaceable (fusion > replacement), and the effectiveness of hidden states varies across layers. 3. Proposed the Dual Feature Fusion (DFF) Framework, a lightweight plug-and-play method that adaptively fuses multi-layer LVLM representations with ID embeddings, achieving state-of-the-art performance.",
      "summary": "This paper systematically studies how to best integrate frozen Large Video-Language Models (LVLMs) as feature extractors for micro-video recommendation. It finds that using intermediate decoder hidden states and fusing them with item ID embeddings is superior to using generated captions or replacing IDs. Based on these insights, the authors propose the Dual Feature Fusion (DFF) framework, which achieves state-of-the-art results on benchmark datasets.",
      "mindmap": "graph TB\n        Root(”Frozen LVLMs for Micro-Video Recommendation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LVLM集成缺乏系统评估/Lack of systematic evaluation for LVLM integration”)\n        Method --> M1(”系统实证研究/Systematic empirical study”)\n        Method --> M2(”提出DFF框架/Propose DFF Framework”)\n        M1 --> M1a(”比较特征提取范式/Compare feature extraction paradigms”)\n        M1 --> M1b(”比较ID集成策略/Compare ID integration strategies”)\n        M2 --> M2a(”自适应融合多层特征/Adaptively fuse multi-layer features”)\n        M2 --> M2b(”轻量级即插即用/Lightweight plug-and-play”)\n        Results --> R1(”中间隐藏态优于描述/Intermediate hidden states > captions”)\n        Results --> R2(”ID嵌入不可替代/Fusion > replacement”)\n        Results --> R3(”DFF实现SOTA性能/DFF achieves SOTA performance”)"
    },
    {
      "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
      "authors": "Yiquan Gao, John See",
      "institution": "Heriot-Watt University",
      "link": "https://arxiv.org/pdf/2512.21944",
      "code": null,
      "tags": [
        "low-light image enhancement",
        "Data Relativistic Uncertainty",
        "Unsupervised Learning",
        "EnlightenGAN",
        "Anime Scenery",
        "Domain Gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
      "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
      "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
    },
    {
      "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder",
      "authors": "Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "Science & Technology Research Laboratories, NHK",
      "link": "https://arxiv.org/pdf/2512.20871",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "360-degree video",
        "implicit neural representations",
        "viewport decoding",
        "spatial-temporal affine transform",
        "video compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp",
      "contributions": "1. Proposes NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of the entire panoramic frame for 360-degree videos. 2. Introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. 3. Achieves significant reductions in memory consumption (7x) and increases in decoding speed (2.5x) compared to prior work HNeRV, while improving image quality.",
      "summary": "The paper addresses the high memory usage and slow decoding of applying implicit neural representations (NeRV) to high-resolution 360-degree videos. It proposes NeRV360, a framework that integrates viewport extraction directly into the decoding process using a conditional spatial-temporal affine transform module. Experiments show NeRV360 significantly reduces memory consumption and increases decoding speed while delivering better image quality compared to prior methods.",
      "mindmap": "graph LR\n    A[NeRV360] --> B[核心问题/Problem: High memory & slow decoding for 360° NeRV]\n    A --> C[主要方法/Method: Viewport decoder with spatial-temporal affine transform]\n    A --> D[关键结果/Results: 7x memory↓, 2.5x speed↑, better quality]"
    },
    {
      "title": "MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model",
      "authors": "Haoyu Wang, Yitong Wang, Jining Wang",
      "institution": "Fudan University",
      "link": "https://arxiv.org/pdf/2512.20916",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "multimodal large language model",
        "sequential recommendation",
        "retrieval-augmented generation",
        "supervised fine-tuning",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24628be848ba62fad6bf4a863ec676471dac0625d17315b1ad3a57018467d1d9_w640_q70.webp",
      "contributions": "1. Proposes a method to use MLLMs to adaptively summarize multimodal items into concise keywords via fine-tuning with a custom reward function. 2. Integrates collaborative signals into the recommendation process by transforming them into keywords and using them as supplementary context, inspired by RAG. 3. Aligns the MLLM for multimodal sequential recommendation through supervised fine-tuning with multi-task learning, balancing performance, interpretability, and computational cost.",
      "summary": "This paper proposes MMSRARec, a method that uses a Multimodal Large Language Model (MLLM) to summarize user behavior sequences and integrate collaborative signals for sequential recommendation. The approach fine-tunes the MLLM with adaptive summarization and retrieval-augmented context to improve efficiency and interpretability. Evaluations show it effectively understands user histories for accurate recommendations.",
      "mindmap": "graph LR\n    A[MMSRARec] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有MLLM推荐方法存在局限性/Existing MLLM Rec Methods Have Limitations]\n    C --> C1[自适应多模态摘要/Adaptive Multimodal Summarization]\n    C --> C2[检索增强协同信号/Retrieval-Augmented Collaborative Signals]\n    C --> C3[监督微调与多任务学习/Supervised Fine-Tuning & Multi-Task Learning]\n    D --> D1[高效且可解释的推荐/Effective & Interpretable Recommendation]"
    },
    {
      "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
      "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.20943",
      "code": null,
      "tags": [
        "communication & networking",
        "4D Gaussian Splatting",
        "video streaming",
        "integer linear programming",
        "pruning",
        "keyframe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
      "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
      "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
      "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
    },
    {
      "title": "Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions",
      "authors": "Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick",
      "institution": "Indian Institute of Technology Indore, Indian Institute of Technology Patna",
      "link": "https://arxiv.org/pdf/2512.21076",
      "code": null,
      "tags": [
        "text classification",
        "hierarchical genre classification",
        "zero-shot semantic alignment",
        "dual-path graph convolution",
        "label co-occurrence graph",
        "blurb-refined inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp",
      "contributions": "1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.",
      "summary": "This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.",
      "mindmap": "graph LR\n    A[HiGeMine: Blurb-Refined Inference from Crowdsourced Book Reviews] --> B[核心问题/Problem: Noisy reviews & flat genre classification degrade reliability]\n    A --> C[主要方法/Method: Two-phase framework: 1. Zero-shot review filtering 2. Dual-path graph classification]\n    A --> D[关键结果/Results: Outperforms baselines on new hierarchical dataset]"
    },
    {
      "title": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "authors": "Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20059",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp",
      "contributions": "",
      "summary": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "mindmap": ""
    },
    {
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "authors": "Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20292",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp",
      "contributions": "",
      "summary": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "mindmap": ""
    },
    {
      "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "authors": "Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19703",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",
      "contributions": "",
      "summary": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation",
      "authors": "Daichi Arai, Yuichi Kondo, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20093",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/812f9579f0639bc36d83f19f057f2959cd40c919ca5879492cd5209f569425ef_w640_q70.webp",
      "contributions": "",
      "summary": "Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation",
      "mindmap": ""
    },
    {
      "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "authors": "Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17946",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",
      "contributions": "",
      "summary": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "authors": "Changxu Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18115",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp",
      "contributions": "",
      "summary": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "mindmap": ""
    },
    {
      "title": "Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation",
      "authors": "Changxu Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18122",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp",
      "contributions": "",
      "summary": "Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "authors": "Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18660",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp",
      "contributions": "",
      "summary": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "authors": "Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18804",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",
      "contributions": "",
      "summary": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "mindmap": ""
    },
    {
      "title": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "authors": "Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18809",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp",
      "contributions": "",
      "summary": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "mindmap": ""
    },
    {
      "title": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "authors": "Alina Elena Baia, Andrea Cavallaro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18864",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp",
      "contributions": "",
      "summary": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "mindmap": ""
    },
    {
      "title": "D$^\\{2\\}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
      "authors": "Junhao Xiao, Shun Feng, Zhiyu Wu, Jianjun Li, Zhiyuan Ma, Yi Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19130",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/711fd748a4cf4a217aaf71023f458a85baa17b26b01b97cc24877d0ea5e48f6e_w640_q70.webp",
      "contributions": "",
      "summary": "D$^\\{2\\}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
      "mindmap": ""
    },
    {
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": "Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19379",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp",
      "contributions": "",
      "summary": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "mindmap": ""
    },
    {
      "title": "Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse",
      "authors": "Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan",
      "institution": "University of Massachusetts Amherst, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.17108",
      "code": null,
      "tags": [
        "multi-modal inference",
        "modular reuse",
        "on-device execution",
        "model decomposition",
        "parallel execution",
        "quantization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27–33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.",
      "mindmap": ""
    },
    {
      "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
      "authors": "Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao",
      "institution": "Nanjing University",
      "link": "https://arxiv.org/pdf/2512.17319",
      "code": null,
      "tags": [
        "remote sensing",
        "multimodal evaluation",
        "RSHR-Bench",
        "adversarial filtering",
        "high-resolution imagery",
        "multimodal large language models",
        "visual question answering",
        "image captioning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.",
      "mindmap": ""
    },
    {
      "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
      "authors": "Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei",
      "institution": "University of Science and Technology of China, HiDream.ai Inc.",
      "link": "https://arxiv.org/pdf/2512.17650",
      "code": null,
      "tags": [
        "diffusion training",
        "in-context generation",
        "video diffusion",
        "latent regularization",
        "attention regularization",
        "region-constraint",
        "joint denoising"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp",
      "contributions": "",
      "summary": "The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks.",
      "mindmap": ""
    },
    {
      "title": "Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes",
      "authors": "Shrinivass Arunachalam Balasubramanian",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.15775",
      "code": null,
      "tags": [
        "others",
        "Finite Exponential Continuous State Machine (FECSM)",
        "Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA)",
        "Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU)",
        "HDBSCAN",
        "min-max normalization",
        "User Interface Change Prediction Index (UICPI)"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a dynamic web UI optimization method that uses a Finite Exponential Continuous State Machine for cross-device responsiveness assessment and a novel Quokka Nonlinear Difference Swarm Optimization Algorithm for design optimization. The core technique involves classifying user experience changes with a Bidirectional Gated Luong and Mish Recurrent Unit model. The main conclusion is that this integrated approach achieves an average fitness of 98.5632% for optimal UI design by incorporating cross-responsiveness assessment and user behavior patterns.",
      "mindmap": ""
    },
    {
      "title": "Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications",
      "authors": "Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi",
      "institution": "University of Massachusetts Amherst, Tennessee Technological University",
      "link": "https://arxiv.org/pdf/2512.15823",
      "code": null,
      "tags": [
        "multi-modal inference",
        "point cloud super-resolution",
        "attribute-based encryption",
        "downsampling",
        "upscaling"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a system to reduce latency in AR/VR streaming by downsampling and partially encrypting point cloud content at the server, then using a machine learning-based super-resolution model to reconstruct it at the client. The evaluation shows this approach effectively reduces bandwidth and encryption overhead while accurately reconstructing the original point clouds with minimal error.",
      "mindmap": ""
    },
    {
      "title": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models",
      "authors": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara",
      "institution": "University of Modena and Reggio Emilia, AMD Silo AI",
      "link": "https://arxiv.org/pdf/2512.15885",
      "code": null,
      "tags": [
        "multi-modal training",
        "self-supervised learning",
        "vision-language alignment",
        "I-JEPA",
        "JARVIS",
        "masked predictive loss",
        "frozen vision foundation models"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities.",
      "mindmap": ""
    },
    {
      "title": "A Tri-Dynamic Preprocessing Framework for UGC Video Compression",
      "authors": "Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie",
      "institution": "Peking University, Bytedance",
      "link": "https://arxiv.org/pdf/2512.16101",
      "code": null,
      "tags": [
        "others",
        "Tri-Dynamic Preprocessing",
        "adaptive factor",
        "adaptive quantization level",
        "adaptive lambda tradeoff",
        "video compression",
        "UGC",
        "deep preprocessing"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a Tri-Dynamic Preprocessing (TDP) framework for UGC video compression, which adaptively adjusts preprocessing intensity, quantization level, and the rate-distortion tradeoff. This method addresses the high variability of UGC videos, where traditional deep preprocessing fails. Experiments show the framework achieves exceptional performance on large-scale test sets.",
      "mindmap": ""
    },
    {
      "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
      "authors": "Giulia Boato, Andrea Montibeller, Edward Delp, Luisa Verdoliva, Daniele Miorandi",
      "institution": "Truebees, University of Trento, Purdue University, University of Naples Federico II",
      "link": "https://arxiv.org/pdf/2512.16614",
      "code": null,
      "tags": [
        "multi-modal inference",
        "AI forensic agents",
        "uncertainty-aware assessments",
        "detector orchestration",
        "multimedia forensics",
        "authenticity verification"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a framework for AI forensic agents that autonomously orchestrate multiple forensic detectors to verify the authenticity of multimedia content. It argues that a holistic, uncertainty-calibrated approach is necessary to address the challenges posed by generative AI, moving beyond isolated, single-purpose detectors. The main conclusion is that such explainable, uncertainty-aware agents can improve the trustworthiness and interpretability of the forensic verification process.",
      "mindmap": ""
    },
    {
      "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
      "authors": "Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu",
      "institution": "Northeastern University, LinkedIn, University of Wisconsin–Madison",
      "link": "https://arxiv.org/pdf/2512.16891",
      "code": null,
      "tags": [
        "multi-modal inference",
        "cross-layer knowledge fusion MoE",
        "VLLM",
        "world-knowledge representation",
        "token extraction",
        "layer-wise fusion"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.",
      "mindmap": ""
    },
    {
      "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
      "authors": "Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou",
      "institution": "The Chinese University of Hong Kong, Snap Inc.",
      "link": "https://arxiv.org/pdf/2512.14938",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion transformer",
        "video VAE",
        "sliding window mechanism",
        "motion-frame context",
        "latent noise injection",
        "MLLM director"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.",
      "mindmap": ""
    },
    {
      "title": "A Preprocessing Framework for Video Machine Vision under Compression",
      "authors": "Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie",
      "institution": "Peking University, Bytedance",
      "link": "https://arxiv.org/pdf/2512.15331",
      "code": null,
      "tags": [
        "others",
        "video compression",
        "neural preprocessor",
        "differentiable virtual codec",
        "rate-accuracy optimization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy.",
      "mindmap": ""
    },
    {
      "title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models",
      "authors": "Mikel Williams-Lekuona, Georgina Cosma",
      "institution": "Loughborough University",
      "link": "https://arxiv.org/pdf/2512.15372",
      "code": null,
      "tags": [
        "multi-modal inference",
        "adaptive computation",
        "early exit",
        "dual-path training",
        "image complexity classification",
        "ConvNeXt-IC"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.",
      "mindmap": ""
    },
    {
      "title": "VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics",
      "authors": "Opeyemi Bamigbade, Mark Scanlon, John Sheppard",
      "institution": "South East Technological University, University College Dublin",
      "link": "https://arxiv.org/pdf/2512.15512",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Vision Transformers (ViT)",
        "SegFormer",
        "attention mechanisms",
        "anomaly scoring",
        "hybrid framework"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics.",
      "mindmap": ""
    },
    {
      "title": "Generative Preprocessing for Image Compression with Pre-trained Diffusion Models",
      "authors": "Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang",
      "institution": "Bytedance Inc.",
      "link": "https://arxiv.org/pdf/2512.15270",
      "code": null,
      "tags": [
        "diffusion inference",
        "Consistent Score Identity Distillation (CiD)",
        "Rate-Perception (R-P) optimization",
        "Stable Diffusion 2.1",
        "parameter-efficient fine-tuning",
        "differentiable codec surrogate"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs.",
      "mindmap": ""
    }
  ]
}