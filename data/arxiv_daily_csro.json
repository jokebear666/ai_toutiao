{
  "label": "cs.RO",
  "slug": "csro",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA",
      "authors": "Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn",
      "institution": "National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)",
      "link": "https://arxiv.org/pdf/2512.22139",
      "code": "https://github.com/dll-ncai/HLS4PC",
      "tags": [
        "on-device ai",
        "FPGA",
        "HLS",
        "Point Cloud",
        "Model Compression",
        "Fixed-Point"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp",
      "contributions": "1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.",
      "summary": "This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.",
      "mindmap": "graph TB\n        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]\n        P1 --> P2[High memory/computation demand hinders real-time performance]\n        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]\n        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]\n        M2 --> M3[Creates PointMLP-Lite model]\n        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]\n        R1 --> R2[3.56x higher throughput vs. prior work]\n        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]"
    },
    {
      "title": "Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications",
      "authors": "Ndagijimana Cyprien, Mehdi Sookhak, Hosein Zarini, Chandra N Sekharan, Mohammed Atiquzzaman",
      "institution": "Texas A&M University-Corpus Christi, University of Oklahoma",
      "link": "https://arxiv.org/pdf/2512.22187",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Meta-A3C",
        "UAV-UGV deployment",
        "trajectory planning",
        "road graph",
        "Markov Decision Process"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c161f6205934be4fb52432c71ffd2a960c50e866e6e382c489af38b90ca39b44_w640_q70.webp",
      "contributions": "1. Proposes a joint UAV-UGV positioning and trajectory planning framework to guarantee optimal QoS for ground users in disaster scenarios. 2. Introduces a road graph model to constrain and direct UGV mobility according to real-world road network constraints. 3. Formulates the problem as an MDP and develops a novel Meta-A3C algorithm for rapid adaptation to new environments and dynamic conditions.",
      "summary": "This paper addresses the challenge of ensuring good QoS with minimal UAVs in disaster recovery by proposing a joint UAV-UGV deployment framework. The core method involves modeling UGV mobility with a road graph and solving the optimization problem using a novel Meta-A3C reinforcement learning algorithm. The results show that Meta-A3C outperforms baseline methods, achieving higher throughput and faster execution while meeting QoS requirements.",
      "mindmap": "graph TB\n        A[Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications] --> B(核心问题/Problem: Ensuring QoS with minimal UAVs in disaster areas)\n        A --> C(主要方法/Method: Road graph for UGV mobility & Meta-A3C for joint optimization)\n        A --> D(关键结果/Results: 13.1% higher throughput, 49% faster execution vs. baselines)"
    },
    {
      "title": "On Extending Semantic Abstraction for Efficient Search of Hidden Objects",
      "authors": "Tasha Pais, Nikhilesh Belulkar",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.22220",
      "code": null,
      "tags": [
        "object detection",
        "semantic abstraction",
        "relevancy maps",
        "3D localization",
        "hidden objects",
        "unstructured search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp",
      "contributions": "1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.",
      "summary": "This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.",
      "mindmap": "graph TB\n        Root(”On Extending Semantic Abstraction for Efficient Search of Hidden Objects”) --> Problem(”核心问题/Problem: Localizing hidden/occluded objects”)\n        Root --> Method(”主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search”)\n        Root --> Results(”关键结果/Results: Faster and accurate 3D localization vs. random search”)"
    },
    {
      "title": "Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions",
      "authors": "Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton",
      "institution": "George Mason University",
      "link": "https://arxiv.org/pdf/2512.22263",
      "code": null,
      "tags": [
        "object detection",
        "RGB-LWIR fusion",
        "multispectral imagery",
        "YOLO",
        "adaptive framework",
        "illumination conditions"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",
      "contributions": "1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.",
      "summary": "This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.",
      "mindmap": "graph TB\n        Root[Evaluating an Adaptive Multispectral Turret System] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[RGB在低光下表现差/RGB struggles in low-light]\n        Problem --> P2[热成像缺乏颜色纹理/Thermal lacks color & texture]\n        Method[主要方法/Method] --> M1[自适应RGB-LWIR融合框架/Adaptive RGB-LWIR fusion framework]\n        Method --> M2[训练33个YOLO模型/Trained 33 YOLO models]\n        Method --> M3[11种融合比例/11 fusion ratios]\n        Results[关键结果/Results] --> R1[全光模型: 92.8%置信度/Full-light model: 92.8% confidence]\n        Results --> R2[微光模型: 92.0%置信度/Dim-light model: 92.0% confidence]\n        Results --> R3[无光模型: 71.0%置信度/No-light model: 71.0% confidence]"
    },
    {
      "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
      "authors": "Wensi Huang, Shaohao Zhu, Meng Wei, Jinming Xu, Xihui Liu, Hanqing Wang, Tai Wang, Feng Zhao, Jiangmiao Pang",
      "institution": "Shanghai AI Laboratory, University of Science and Technology of China, Zhejiang University, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.22342",
      "code": "https://0309hws.github.io/VL-LN.github.io/",
      "tags": [
        "embodied navigation",
        "interactive navigation",
        "active dialog",
        "benchmark dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/963768d906e1031332e12b4d315a28062a7236f00bbf7e3b88c636a5d3422ff7_w640_q70.webp",
      "contributions": "1. Proposes the Interactive Instance Object Navigation (IION) task, which requires agents to navigate and resolve ambiguity through active dialog. 2. Introduces the VL-LN benchmark, a large-scale, automatically generated dataset with over 41k dialog-augmented trajectories for training and evaluation. 3. Demonstrates that a navigation model trained on VL-LN achieves significant improvements over baselines, validating the benchmark's effectiveness.",
      "summary": "The paper addresses the gap in embodied navigation where real-world instructions are often vague. It proposes a new task (IION) and a large-scale benchmark (VL-LN) for training and evaluating agents that can navigate and ask clarifying questions. The results show that models trained with this dialog-enabled approach significantly outperform baseline methods.",
      "mindmap": "graph TB\n        A[VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现实导航指令模糊/Real-world navigation instructions are vague]\n        C --> C1[提出IION任务与VL-LN基准/Proposes IION task & VL-LN benchmark]\n        C1 --> C2[包含大规模自动生成数据集/Includes large-scale auto-generated dataset]\n        D --> D1[模型性能显著提升/Model achieves significant improvements]"
    },
    {
      "title": "A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot",
      "authors": "Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol",
      "institution": "Mansoura University, Egypt",
      "link": "https://arxiv.org/pdf/2512.22408",
      "code": null,
      "tags": [
        "on-device ai",
        "Heterogeneous Computing",
        "ROS 2",
        "FreeRTOS",
        "PID Control",
        "AWS IoT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp",
      "contributions": "1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.",
      "summary": "This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.",
      "mindmap": "graph TB\n        Root[”A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Limitations of human-operated last-mile delivery (cost, safety, reliability)”] --> P1[”子问题/Sub-Problem<br>Need for autonomous, cost-efficient delivery robot”]\n        Method[”主要方法/Method<br>Unified multi-disciplinary approach”] --> M1[”异构计算/Heterogeneous Computing<br>RPi 5 (ROS 2) for AI & ESP32 (FreeRTOS) for control”]\n        Method --> M2[”关键技术/Key Tech<br>Low-latency comms, PID control, AWS IoT, failsafe”]\n        Results[”关键结果/Results<br>Robust, operational autonomous delivery system”] --> R1[”成果/Outcome<br>Deterministic motor control & enhanced reliability”]"
    },
    {
      "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
      "authors": "Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair",
      "institution": "Physical Intelligence, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22414",
      "code": null,
      "tags": [
        "robot learning",
        "vision-language-action models",
        "human-to-robot transfer",
        "co-training",
        "emergent capability",
        "embodiment-agnostic representations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp",
      "contributions": "1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.",
      "summary": "This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.",
      "mindmap": "graph TB\n        A[Emergence of Human to Robot Transfer in Vision-Language-Action Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Can VLA models learn from human videos for robot control?]\n        C[主要方法/Method: Simple co-training recipe on human & robot data]\n        D[关键结果/Results: Transfer emerges with scale; performance nearly doubles]"
    },
    {
      "title": "Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature",
      "authors": "Peleg Shefi, Amir Ayali, Gal A. Kaminka",
      "institution": "Bar Ilan University, Tel Aviv University",
      "link": "https://arxiv.org/pdf/2512.22448",
      "code": null,
      "tags": [
        "swarm robotics",
        "collective motion",
        "visual perception",
        "fault-tolerance",
        "intermittent locomotion",
        "distance estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456a9beab88e762dd3d3dfcafbb4d0007f77b2154cd9a7b657dcdf5df21407db_w640_q70.webp",
      "contributions": "1. A robust distance estimation method for vision-based swarms that combines perceived horizontal and vertical sizes of neighbors. 2. The introduction of intermittent locomotion as a mechanism for reliably detecting faulty peers that disrupt swarm motion. 3. A fault-avoidance strategy that is robust to errors in classifying robots as faulty, improving swarm resilience in both Avoid-Attract and Alignment-based models.",
      "summary": "This paper addresses the brittleness of artificial swarms using vision by proposing two bio-inspired mechanisms. It introduces a robust visual distance estimation method and an intermittent locomotion strategy for fault detection and avoidance. Extensive simulations show these techniques dramatically improve swarm resilience across different collective motion models.",
      "mindmap": "graph TB\n        A[Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature] --> B[核心问题/Problem: Artificial swarms are brittle with vision sensing due to ambiguities and information loss.]\n        A --> C[主要方法/Method: 1. Robust visual distance estimation. 2. Intermittent locomotion for fault detection.]\n        A --> D[关键结果/Results: Dramatic improvement in swarm resilience across Avoid-Attract and Alignment models.]"
    },
    {
      "title": "Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing",
      "authors": "Sukhyun Jeong, Yong-Hoon Choi",
      "institution": "Kwangwoon University",
      "link": "https://arxiv.org/pdf/2512.22464",
      "code": "https://github.com/jayze3736/PGR2M",
      "tags": [
        "motion generation and editing",
        "residual vector quantization (RVQ)",
        "pose code",
        "transformer",
        "text-to-motion",
        "motion editing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp",
      "contributions": "1. Proposes a hybrid motion representation (PGR²M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability.",
      "summary": "This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR²M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits.",
      "mindmap": "graph TB\n        A[Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Pose-code frameworks struggle to capture subtle temporal dynamics and high-frequency details.]\n        C[主要方法/Method: Hybrid representation (PGR²M) with pose codes and residual codes (RVQ), using a two-stage Transformer.]\n        D[关键结果/Results: Improves FID and reconstruction metrics; enables intuitive, structure-preserving edits.]"
    },
    {
      "title": "Asymmetric Friction in Geometric Locomotion",
      "authors": "Ross L. Hatton, Yousef Salaman, Shai Revzen",
      "institution": "Oregon State University (inferred from author Ross L. Hatton's affiliation)",
      "link": "https://arxiv.org/pdf/2512.22484",
      "code": null,
      "tags": [
        "geometric mechanics",
        "robotics",
        "Finsler metrics",
        "sub-Finslerian",
        "motility map",
        "asymmetric friction",
        "geometric locomotion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7325c56bd6ecd61b2e5288756589f2b41cc9256cdcdb37b465ecd8b058f30c3_w640_q70.webp",
      "contributions": "1. Extends the geometric locomotion framework from systems with symmetric (Riemannian) friction to systems with asymmetric (Finsler) friction. 2. Demonstrates that the sub-Riemannian construction of the motility map naturally generalizes to a sub-Finslerian approach. 3. Identifies system properties analogous to constraint curvature that characterize motion capabilities under asymmetric friction.",
      "summary": "This paper extends geometric models of locomotion to systems with asymmetric friction, where drag coefficients differ for forward and backward motion. The authors propose a sub-Finslerian framework to construct the system's motility map, generalizing the traditional sub-Riemannian approach. The main conclusion is that this new framework allows for the characterization of locomotion capabilities in a broader class of systems with non-reciprocal environmental interactions.",
      "mindmap": "graph TB\n        Root(”Asymmetric Friction in Geometric Locomotion”) --> Problem(”核心问题/Problem: Standard geometric locomotion models assume symmetric friction (Riemannian metrics)”)\n        Root --> Method(”主要方法/Method: Generalize framework to asymmetric friction using Finsler metrics and a sub-Finslerian approach”)\n        Root --> Results(”关键结果/Results: Identifies properties analogous to constraint curvature to characterize system motion capabilities”)"
    },
    {
      "title": "Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces",
      "authors": "Shen Changqing, Xu Bingzhou, Qi Bosong, Zhang Xiaojian, Yan Sijie, Ding Han",
      "institution": "Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22502",
      "code": null,
      "tags": [
        "computer-aided manufacturing (CAM)",
        "spiral toolpath planning",
        "scalar field optimization",
        "topology-preserving deformation",
        "conformal slit mapping",
        "boundary-conforming"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8db268a37d6b76e48aa2571b6519b562d7e55bfc3d4e8f7c07120bc4dbfc1315_w640_q70.webp",
      "contributions": "1. Proposes a strategy to enforce boundary conformity and eliminate zero-gradient singularities in scalar-field-based toolpath optimization for multiply connected surfaces. 2. Reformulates the optimization as a topology-preserving mesh deformation with boundary-synchronous updates to achieve globally optimized spacing and smooth transitions. 3. Demonstrates significant improvements in machining efficiency, scallop-height uniformity, and vibration reduction compared to a state-of-the-art method.",
      "summary": "This paper addresses the challenge of generating continuous, boundary-conforming spiral toolpaths for ball-end milling on complex freeform surfaces. The proposed method uses conformal slit mapping to create an initial singularity-free scalar field and then optimizes it via a topology-preserving mesh deformation process. Experimental results show the approach increases machining efficiency by over 14%, improves surface finish uniformity, and reduces vibrations compared to existing methods.",
      "mindmap": "graph TB\n        A[Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[挑战: 保持边界一致性并消除奇点/Challenge: Maintain boundary conformity & eliminate singularities]\n        C --> C1[使用共形狭缝映射初始化/Use conformal slit mapping for initialization]\n        C --> C2[拓扑保持网格变形优化/Topology-preserving mesh deformation optimization]\n        D --> D1[效率提升 14.24%/Efficiency improved by 14.24%]\n        D --> D2[均匀性提升 5.70%/Uniformity improved by 5.70%]\n        D --> D3[振动减少 >10%/Vibration reduced by >10%]"
    },
    {
      "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
      "authors": "Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le",
      "institution": "University of Arkansas, National University of Singapore, TU Wien, Max Planck Research School for Intelligent Systems / University of Stuttgart, University of Liverpool",
      "link": "https://arxiv.org/pdf/2512.22519",
      "code": "https://uark-aicv.github.io/OBEYED",
      "tags": [
        "robotic manipulation",
        "Vision-Language-Action (VLA)",
        "object-centric grounding",
        "geometric grounding",
        "perception module",
        "clutter robustness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7643cf1feb83697be4818d833bf42eecb6c488ed1ac9917cb58299ddd69c4cfe_w640_q70.webp",
      "contributions": "1. Proposes OBEYED-VLA, a framework that explicitly disentangles perceptual grounding from action reasoning in VLA models. 2. Introduces a perception module with a VLM-based object-centric grounding stage and a complementary geometric grounding stage to create task-conditioned, object-centric, and geometry-aware observations. 3. Demonstrates substantial real-world robustness improvements in cluttered scenarios, including handling distractors, absent targets, background changes, and unseen objects.",
      "summary": "This paper identifies that monolithic Vision-Language-Action (VLA) models suffer from poor grounding in cluttered real-world scenes. To solve this, it proposes OBEYED-VLA, which adds an explicit perception module for object-centric and geometric grounding before action prediction. The method significantly improves robustness on a real-world tabletop robot across various challenging clutter and distraction scenarios.",
      "mindmap": "graph TB\n        Root[Clutter-Resistant VLA Models<br>抗干扰VLA模型] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>VLA模型在杂乱场景中<br>感知与控制耦合，<br>导致错误执行] --> P1[过抓取/Over-grasp]\n        Problem --> P2[易受干扰/Distracted by clutter]\n        Problem --> P3[过拟合背景/Overfit to background]\n        Method[主要方法/Method<br>OBEYED-VLA框架] --> M1[感知模块/Perception Module]\n        M1 --> M1_1[基于VLM的物体感知<br>VLM-based Object Grounding]\n        M1 --> M1_2[几何感知<br>Geometric Grounding]\n        Method --> M2[动作策略微调<br>Fine-tune VLA Policy]\n        Results[关键结果/Results<br>在真实UR10e机器人上验证] --> R1[抗干扰物体鲁棒<br>Robust to distractors]\n        Results --> R2[拒绝无目标任务<br>Absent-target rejection]\n        Results --> R3[背景变化鲁棒<br>Robust to background changes]\n        Results --> R4[操作未见物体<br>Manipulate unseen objects]"
    },
    {
      "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models",
      "authors": "Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang",
      "institution": "Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22539",
      "code": "this",
      "tags": [
        "embodied ai / robot learning",
        "vision-language-action models",
        "benchmark",
        "generalization",
        "robustness",
        "structured task design"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp",
      "contributions": "1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research.",
      "summary": "This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges.",
      "mindmap": "graph TB\n        A[VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Difficult to quantitatively understand the limits and failure modes of VLAs]\n        C[主要方法/Method: Structured benchmark with orthogonal difficulty axes (Task Structure, Language, Visual) and systematic perturbations]\n        D[关键结果/Results: Revealed critical VLA limitations (memorization, asymmetric robustness); Provided open-source framework for research]"
    },
    {
      "title": "ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation",
      "authors": "Xuewei Zhang, Bailing Tian, Kai Zheng, Yulin Hui, Junjie Lu, Zhiyu Li",
      "institution": "Tianjin University",
      "link": "https://arxiv.org/pdf/2512.22575",
      "code": "https://zxw610.github.io/ParaMaP",
      "tags": [
        "motion planning",
        "Euclidean Distance Transform",
        "sampling-based model predictive control",
        "GPU parallelization",
        "collision-free planning",
        "SE(3) pose tracking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc2c02919a284bed96ccdbd82c4e1bdf169e2e161c4adb9bfc19e68ccb0dc21_w640_q70.webp",
      "contributions": "1. A parallel framework tightly integrating GPU-based Euclidean Distance Transform mapping with a sampling-based MPC planner for real-time replanning. 2. A robot-masked update mechanism for the distance field to prevent false self-collision detections during online perception. 3. Formulating motion generation as a stochastic optimization with a unified objective and a geometrically consistent SE(3) pose tracking metric for fast, accurate convergence.",
      "summary": "This paper proposes ParaMaP, a framework for real-time, collision-free motion planning in unknown environments. It combines a GPU-accelerated distance field mapping system with a parallel sampling-based model predictive control planner. The method is validated through simulations and real-world experiments on a 7-DoF manipulator, demonstrating effective high-frequency replanning.",
      "mindmap": "graph TB\n        A[ParaMaP: Parallel Mapping and Collision-free Motion Planning] --> B[核心问题/Problem: Real-time collision-free planning in unknown environments with frequent replanning]\n        A --> C[主要方法/Method: Parallel GPU framework integrating EDT-based mapping with SMPC planner]\n        A --> D[关键结果/Results: Validated on 7-DoF manipulator, enables high-frequency replanning]"
    },
    {
      "title": "Modeling of UAV Tether Aerodynamics for Real-Time Simulation",
      "authors": "Max Beffert, Andreas Zell",
      "institution": "Cognitive Systems Group, University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.22588",
      "code": null,
      "tags": [
        "robotics simulation",
        "tether aerodynamics",
        "real-time simulation",
        "catenary theory",
        "lumped mass model",
        "CasADi"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a5999cc3e2fc5ef1aa99519ee5adb890754d8b494519879e512c601ea3c9b6a_w640_q70.webp",
      "contributions": "1. Proposed an analytical tether model based on catenary theory with uniform drag for fast (&lt;1ms) real-time simulation. 2. Developed a flexible numerical tether model using segment discretization and optimization (CasADi/IPOPT) achieving real-time performance (5ms). 3. Validated both models with real-world experiments, providing a framework for offline optimization and online tasks like control and planning.",
      "summary": "This paper addresses the problem of modeling aerodynamic forces on a tether for tethered UAVs to enable continuous operation from a moving base. It proposes two complementary real-time methods: a fast analytical model and a more flexible numerical model. The work concludes that the analytical model is sufficiently accurate for most applications, while the numerical model offers higher physical fidelity when needed.",
      "mindmap": "graph TB\n        A[Modeling of UAV Tether Aerodynamics for Real-Time Simulation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[无人机续航短，系留是解决方案 / UAV short endurance, tethering is a solution]\n        B --> B2[移动基站或强风需要气动模型 / Moving base or strong wind requires aerodynamic model]\n        C --> C1[解析方法：悬链线理论，均匀阻力 / Analytical: Catenary theory, uniform drag]\n        C --> C2[数值方法：分段离散，集中质量 / Numerical: Segmented discretization, lumped masses]\n        D --> D1[解析法：<1ms，足够精确 / Analytical: <1ms, sufficiently accurate]\n        D --> D2[数值法：5ms，更高灵活性 / Numerical: 5ms, higher flexibility]\n        D --> D3[实验验证，轻量级框架 / Experimental validation, lightweight framework]"
    },
    {
      "title": "Sistema de navegación de cobertura para vehículos no holonómicos en ambientes de exterior",
      "authors": "Michelle Valenzuela, Francisco Leiva, Javier Ruiz-del-Solar",
      "institution": "Advanced Mining Technology Center, Universidad de Chile",
      "link": "https://arxiv.org/pdf/2512.22734",
      "code": null,
      "tags": [
        "mobile robotics",
        "coverage path planning",
        "coverage navigation",
        "non-holonomic vehicle",
        "obstacle recovery",
        "outdoor environments",
        "mining automation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d51f00a4abaf62f479ca3c4a4186de6c2ffb93b0fee4c3e43193c92bebff4d9_w640_q70.webp",
      "contributions": "1. Development of a coverage navigation system for non-holonomic robots in outdoor environments. 2. Incorporation of recovery behaviors to handle dynamic or unmapped obstacles, ensuring complete coverage. 3. Validation of the system in both simulated and real outdoor settings, achieving near 90% coverage.",
      "summary": "This paper presents a coverage navigation system for non-holonomic vehicles, designed for outdoor tasks like mining. The system plans routes to cover an area and includes recovery behaviors to handle unexpected obstacles. It was tested in simulation and real-world environments, achieving near 90% coverage, with plans to scale up to industrial mining vehicles.",
      "mindmap": "graph TB\n        Root[”Sistema de navegación de cobertura para vehículos no holonómicos / Coverage Navigation System for Non-Holonomic Vehicles”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Automating coverage tasks (e.g., cleaning, material handling) in mining for improved safety.”]\n        Method[”主要方法/Method: Route calculation with recovery behaviors for dynamic/unmapped obstacles.”]\n        Results[”关键结果/Results: ~90% coverage achieved in simulated/real outdoor tests; scaling to mining vehicles planned.”]"
    },
    {
      "title": "Active Constraint Learning in High Dimensions from Demonstrations",
      "authors": "Zheng Qiu, Chih-Yuan Chiu, Glen Chou",
      "institution": "Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22757",
      "code": null,
      "tags": [
        "robot learning",
        "active learning",
        "constraint inference",
        "Gaussian processes",
        "learning from demonstration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp",
      "contributions": "1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.",
      "summary": "This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.",
      "mindmap": "graph TB\n        Root(”Active Constraint Learning in High Dimensions from Demonstrations”) --> Problem(”核心问题/Problem: Data-inefficient constraint inference from demonstrations”)\n        Root --> Method(”主要方法/Method: Iterative active learning with Gaussian Processes”)\n        Root --> Results(”关键结果/Results: Outperforms baseline with sparse, informative demonstrations”)"
    },
    {
      "title": "Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems",
      "authors": "Naoki Kitamura, Yuichi Sudo, Koichi Wada",
      "institution": "The University of Osaka, Hosei University",
      "link": "https://arxiv.org/pdf/2512.22770",
      "code": null,
      "tags": [
        "distributed computing",
        "autonomous mobile robots",
        "Look-Compute-Move (LCM)",
        "computational power hierarchy",
        "finite-state robots",
        "robots with lights"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp",
      "contributions": "1. Proves that under full synchrony, the FSTA (finite-state) and LUMI (robots with lights) models coincide for two robots, showing perfect synchrony can substitute for memory and communication at this minimal scale. 2. Shows that the FSTA and FCOM (finite-communication) models are orthogonal (bidirectionally incomparable), completing the landscape of incomparability. 3. Provides the first complete and exact characterization of the computational power hierarchy for two robots across all major models and schedulers using a novel simulation-free method.",
      "summary": "This paper provides the first complete characterization of the computational power of two autonomous mobile robots across major models (OBLOT, FSTA, FCOM, LUMI) and schedulers. Using a novel simulation-free method, it reveals a landscape distinct from the general n-robot case, showing that perfect synchrony can substitute for memory and communication for two robots, and that FSTA and FCOM are orthogonal. This yields the first exact computational hierarchy for minimal robot systems.",
      "mindmap": "graph TB\n        A[Two-Robot Computational Landscape] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Two-robot computational hierarchy unresolved]\n        C --> C1[Simulation-free analysis method]\n        D --> D1[FSTA^F = LUMI^F under full sync]\n        D --> D2[FSTA and FCOM are orthogonal]\n        D --> D3[Complete landscape for two robots]"
    },
    {
      "title": "The body is not there to compute: Comment on \"Informational embodiment: Computational role of information structure in codes and robots\" by Pitti et al",
      "authors": "Matej Hoffmann",
      "institution": "Czech Technical University in Prague",
      "link": "https://arxiv.org/pdf/2512.22868",
      "code": null,
      "tags": [
        "embodied cognition",
        "robotics",
        "morphological computation",
        "embodiment",
        "information theory",
        "passive dynamic walker"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp",
      "contributions": "1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of \"morphological computation\". 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.",
      "summary": "This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of \"morphological computation\" is merely a metaphor and that the body's main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.",
      "mindmap": "graph TB\n        Root[The body is not there to compute<br>身体不是为了计算] --> Problem[核心问题/Problem<br>Is the body's primary role computational?<br>身体的主要作用是计算吗？]\n        Root --> Method[主要方法/Method<br>Conceptual critique of ”morphological computation”<br>对”形态计算”的概念性批判]\n        Root --> Results[关键结果/Results<br>Body's role is physical, not computational<br>身体的作用是物理的，而非计算的]"
    },
    {
      "title": "MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments",
      "authors": "Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao",
      "institution": "Hokkaido University, George Mason University",
      "link": "https://arxiv.org/pdf/2512.22867",
      "code": null,
      "tags": [
        "embodied navigation",
        "socially compliant navigation",
        "multimodal dataset",
        "chain-of-thought",
        "vision language models",
        "benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp",
      "contributions": "1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy.",
      "summary": "The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models.",
      "mindmap": "graph TB\n        Root[MUSON: 面向推理的多模态城市社会合规导航数据集] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有数据集缺乏显式推理监督/Lack explicit reasoning supervision]\n        Problem --> P2[动作分布高度长尾/Highly long-tailed action distribution]\n        Method[主要方法/Method] --> M1[引入MUSON数据集/Introduce MUSON dataset]\n        M1 --> M1_Sub1[五步思维链标注/Five-step Chain-of-Thought annotation]\n        M1 --> M1_Sub2[平衡的离散动作空间/Balanced discrete action space]\n        Results[关键结果/Results] --> R1[Qwen2.5-VL-3B取得最高精度/Qwen2.5-VL-3B achieves highest accuracy]\n        Results --> R2[数据集作为有效基准/Dataset serves as effective benchmark]"
    },
    {
      "title": "P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach",
      "authors": "Daqian Cao, Quan Yuan, Weibang Bai",
      "institution": "ShanghaiTech University",
      "link": "https://arxiv.org/pdf/2512.22927",
      "code": null,
      "tags": [
        "robotics",
        "inverse kinematics",
        "parallel mechanisms",
        "FABRIK",
        "workspace robustness",
        "topological decomposition"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b554beb83a556bb3fea7bd9931b48b310df4fe75825b63e9466dd5c70a26a7_w640_q70.webp",
      "contributions": "1. Proposes P-FABRIK, a novel inverse kinematics method for parallel mechanisms based on the FABRIK algorithm. 2. Introduces a new topological decomposition strategy to break down parallel mechanisms into serial sub-chains for iterative solution. 3. Demonstrates the method's generality, computational efficiency, and robustness in handling targets outside the workspace.",
      "summary": "This paper proposes P-FABRIK, a general and robust inverse kinematics method for parallel mechanisms. It adapts the FABRIK algorithm by decomposing the mechanism into serial sub-chains and iteratively revising end targets. The method is shown to be effective, efficient, and capable of handling out-of-workspace targets for various parallel mechanisms.",
      "mindmap": "graph TB\n        A[P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统几何方法依赖特定约束/Traditional geometric methods rely on specific constraints]\n        B --> B2[冗余并联机构约束复杂/Complex constraints for redundant parallel mechanisms]\n        B --> B3[目标位姿超出工作空间无解/No solution for out-of-workspace targets]\n        C --> C1[基于FABRIK算法/Based on FABRIK algorithm]\n        C --> C2[拓扑分解为串行子链/Topological decomposition into serial sub-chains]\n        C --> C3[迭代修正末端目标/Iteratively revise end targets]\n        D --> D1[适用于多种并联机构/Applicable to diverse parallel mechanisms]\n        D --> D2[计算高效/Computationally efficient]\n        D --> D3[鲁棒处理超工作空间目标/Robust to out-of-workspace targets]"
    },
    {
      "title": "PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO",
      "authors": "Mengyu Ji, Shiliang Guo, Zhengzhen Li, Jiahao Shen, Huazi Cao, Shiyu Zhao",
      "institution": "Zhejiang University, Westlake University",
      "link": "https://arxiv.org/pdf/2512.22957",
      "code": null,
      "tags": [
        "robotics control",
        "aerial manipulator",
        "prescribed performance control",
        "variable-gain extended state observer",
        "dynamic coupling",
        "motion control"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2803479d85cfb232ae59d1133082b1c37608a63bed7f68577a5675d15b2598_w640_q70.webp",
      "contributions": "1. A novel prescribed performance motion control framework (PreGME) for aerial manipulators. 2. The use of variable-gain extended state observers (ESOs) for accurate real-time estimation of rapidly varying dynamic coupling. 3. A control strategy that generates a preset error trajectory to ensure tracking errors remain within a prescribed performance envelope for high-precision control.",
      "summary": "This paper proposes PreGME, a new control framework for aerial manipulators that combines variable-gain extended state observers to estimate dynamic coupling with prescribed performance control to constrain error trajectories. The method enables high-precision control even during aggressive arm motions. Experiments, including aerial mixology and cart-pulling, validate its effectiveness under significant dynamic disturbances.",
      "mindmap": "graph TB\n        Root[”PreGME: Prescribed Performance Control of Aerial Manipulators”] --> Problem[”核心问题/Problem: Aerial manipulator dynamic coupling affects control precision”]\n        Root --> Method[”主要方法/Method: Variable-gain ESO + Prescribed performance control”]\n        Root --> Results[”关键结果/Results: High tracking performance validated by real-world experiments”]"
    },
    {
      "title": "Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives",
      "authors": "Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen",
      "institution": "Xi'an Jiaotong University, Hong Kong University of Science and Technology (Guangzhou), Chinese Academy of Sciences, Westlake University, Zhejiang University, University of Sydney, BAAI, Peking University",
      "link": "https://arxiv.org/pdf/2512.22983",
      "code": "Awesome-Robotics-Manipulation",
      "tags": [
        "robotic manipulation",
        "robotic foundation models",
        "high-level planning",
        "low-level control",
        "imitation learning",
        "reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22135ed5ae9ef66789b2dbe7f9c4c6e6a9de91ca6dd4b2025e75cfd5d4a89d02_w640_q70.webp",
      "contributions": "1. Proposes a unified algorithmic abstraction for robot manipulation, organizing approaches into high-level planning and low-level control. 2. Extends classical task planning to include reasoning over language, code, motion, affordances, and 3D representations. 3. Introduces a training-paradigm-oriented taxonomy for learning-based control, categorizing methods by input modeling, latent representation learning, and policy learning.",
      "summary": "This survey paper organizes recent learning-based approaches to robot manipulation within a unified framework of high-level planning and low-level control. It extends task planning to include multimodal reasoning and proposes a new taxonomy for learning-based control. The analysis aims to clarify the design space and identifies key challenges like scalability and safety for future robotic foundation models.",
      "mindmap": "graph TB\n        A[Embodied Robot Manipulation in the Era of Foundation Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Robot manipulation remains a central and challenging problem / 机器人操作仍是一个核心挑战]\n        C --> C1[Unified abstraction: High-level planning & Low-level control / 统一抽象：高层规划与底层控制]\n        C1 --> C2[High-level: Reasoning over language, code, motion, etc. / 高层：基于语言、代码、运动等的推理]\n        C1 --> C3[Low-level: Taxonomy for learning-based control / 底层：基于学习的控制分类法]\n        D --> D1[Clarifies design space of foundation models / 阐明基础模型的设计空间]\n        D --> D2[Identifies open challenges: scalability, safety, etc. / 指出开放挑战：可扩展性、安全性等]"
    },
    {
      "title": "Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models",
      "authors": "Saraswati Soedarmadji, Yunyue Wei, Chen Zhang, Yisong Yue, Yanan Sui",
      "institution": "Tsinghua University, California Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23077",
      "code": null,
      "tags": [
        "reinforcement learning",
        "musculoskeletal control",
        "vision-language models",
        "embodied learning",
        "reward function discovery",
        "motion representation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e43a6e8e2c1f39257ec8f88c3695335f5287e369c9a159892f3f3cd6efc7ce61_w640_q70.webp",
      "contributions": "1. Introduces MoVLR, a framework that uses Vision-Language Models (VLMs) to bridge high-level goal specification and low-level movement control for musculoskeletal systems. 2. Proposes an iterative method to explore the reward space by combining control optimization with VLM feedback, avoiding reliance on handcrafted rewards. 3. Demonstrates the framework's ability to discover and refine reward functions for complex locomotion and manipulation tasks, grounding abstract language descriptions in physical control principles.",
      "summary": "The paper addresses the challenge of designing reward functions for high-dimensional musculoskeletal control by proposing MoVLR, a framework that leverages Vision-Language Models to iteratively align control policies with high-level goals described in language. The method transforms language and vision-based assessments into structured guidance for embodied learning. The results show that VLMs can effectively ground abstract motion descriptions in the implicit principles of physiological motor control.",
      "mindmap": "graph TB\n        A[Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Reward function design for high-dimensional musculoskeletal control is challenging] --> B1[具体挑战/Challenge<br>High-level goals (e.g., ”walk upright”) are hard to translate into low-level control rewards]\n        C[主要方法/Method<br>MoVLR Framework] --> C1[关键机制/Mechanism<br>Iterative exploration of reward space via VLM feedback and control optimization]\n        D[关键结果/Results<br>VLMs can ground abstract motion in physical control principles] --> D1[应用/Application<br>Enables reward discovery for locomotion and manipulation]"
    },
    {
      "title": "APOLLO Blender: A Robotics Library for Visualization and Animation in Blender",
      "authors": "Peter Messina, Daniel Rakita",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.23103",
      "code": null,
      "tags": [
        "robotics visualization",
        "URDF",
        "Blender",
        "keyframing",
        "Python scripting",
        "3D animation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cba129e043d858f09043e81dde4891fc5ef6711639795d93574734cb37f17c5_w640_q70.webp",
      "contributions": "1. A library for importing robots and environments directly from standardized descriptions like URDF into Blender, 2. Python-based scripting tools for keyframing robot states and visual attributes, 3. Convenient generation of primitive 3D shapes for creating schematic figures and animations.",
      "summary": "This paper introduces APOLLO Blender, a lightweight Python library that simplifies creating high-quality robotics visualizations and animations within Blender by providing robotics-focused scripting tools. It bridges the gap between powerful 3D graphics software and robotics research needs, enabling researchers to generate publication-ready content without deep Blender expertise. The work demonstrates the library's utility through examples and discusses future extensions.",
      "mindmap": "graph TB\n        A[APOLLO Blender: A Robotics Library for Visualization and Animation in Blender] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Blender学习曲线陡峭/Steep Blender learning curve]\n        B --> B2[缺乏机器人专用集成/Lack of robotics-focused integrations]\n        C --> C1[导入URDF机器人/Import URDF robots]\n        C --> C2[Python脚本关键帧/Python scripting for keyframing]\n        C --> C3[生成3D图元/Generate 3D primitives]\n        D --> D1[快速创建出版物图像/Rapid creation of publication-ready images]\n        D --> D2[无需深厚Blender专业知识/No extensive Blender expertise needed]"
    },
    {
      "title": "Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models",
      "authors": "Roshan Klein-Seetharaman, Daniel Rakita",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.23135",
      "code": null,
      "tags": [
        "robotics systems",
        "URDD",
        "robot description",
        "JSON/YAML modules",
        "Bevy visualization",
        "Three.js viewer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3c8fda99c3364911696d021db2c35cc62671784fcdbf685fd62934de409e5f_w640_q70.webp",
      "contributions": "1. Introduced the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured JSON and YAML modules to reduce redundancy and improve standardization. 2. Provided an open-source toolkit with a Rust implementation to automatically generate URDDs from URDFs, including Bevy-based visualization capabilities. 3. Developed a JavaScript/Three.js viewer for web-based inspection of URDDs, enabling interactive visualization and verification across platforms.",
      "summary": "The paper addresses the problem of redundant computations and fragmented implementations in robotics due to basic robot specification files like URDF. It proposes the Universal Robot Description Directory (URDD), a modular representation that organizes richer derived information into JSON/YAML files, along with tools for automatic generation and visualization. The work concludes that URDD efficiently encapsulates extensive robot data, enabling shared standards and reducing redundancy across frameworks.",
      "mindmap": "graph TB\n        A[Beyond URDF: The Universal Robot Description Directory] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[基础机器人描述文件信息有限 / Basic robot specs have limited info]\n        B --> B2[下游应用重复计算 / Downstream apps re-derive data redundantly]\n        C --> C1[提出URDD模块化表示 / Propose URDD modular representation]\n        C --> C2[使用JSON/YAML组织信息 / Use JSON/YAML to organize info]\n        C --> C3[提供开源生成与可视化工具 / Provide open-source generation & visualization tools]\n        D --> D1[URDD高效生成且信息丰富 / URDD generated efficiently & info-rich]\n        D --> D2[支持核心子程序构建 / Enables core subroutine construction]\n        D --> D3[建立统一可扩展标准 / Establishes unified, extensible standards]"
    },
    {
      "title": "A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices",
      "authors": "Roshan Klein-Seetharama, Daniel Rakita",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.23140",
      "code": null,
      "tags": [
        "robotics",
        "self-collision matrix",
        "robot self-collision",
        "proximity query",
        "shape representation",
        "interactive visualization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea10a80ffc42162cc3aac7a557e3310d2bcf065680b6fdc22c295d40ed8b5ec4_w640_q70.webp",
      "contributions": "1. An interactive tool for generating and visualizing self-collision matrices that overcomes limitations of static tools like MoveIt Setup Assistant. 2. Support for multiple shape representations (spheres, OBBs, convex hulls, convex decompositions) and dynamic inspection/filtering. 3. Implementation in Rust with Bevy engine for high-quality visualization and export to JSON/YAML for easy integration.",
      "summary": "The paper introduces a new interactive software tool for generating and visualizing robot self-collision matrices. It supports multiple shape representations and enables dynamic inspection and refinement, leading to faster and more accurate self-collision and self-proximity queries. The tool is implemented in Rust using the Bevy game engine and outputs results in standard formats for integration into planning frameworks.",
      "mindmap": "graph TB\n        A[A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有工具静态、缺乏邻近支持、工作流繁琐/Current tools: static, lack proximity support, tedious workflow]\n        C --> C1[交互式工具支持多种形状表示与动态检查/Interactive tool with multi-shape representation & dynamic inspection]\n        C --> C2[使用Rust与Bevy引擎实现/Implemented in Rust with Bevy engine]\n        D --> D1[生成更快更准确的碰撞与邻近查询/Generates faster & more accurate collision/proximity queries]\n        D --> D2[输出JSON/YAML便于集成/Exports JSON/YAML for easy integration]"
    },
    {
      "title": "Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset",
      "authors": "Wuhao Xie, Kanji Tanaka",
      "institution": "University of Fukui",
      "link": "https://arxiv.org/pdf/2512.23141",
      "code": null,
      "tags": [
        "robot localization",
        "pole-centric descriptors",
        "contrastive learning",
        "small-scale observations",
        "landmark distinctiveness",
        "pole-at-distance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a74c5a1b74d238ad9ddaabd43dc96605a8f33839f93907c39978f07db411a7_w640_q70.webp",
      "contributions": "1. Construction of the Small Pole Landmark (SPL) dataset via an automated tracking-based pipeline for evaluating landmark identification under sparse observations. 2. Empirical comparative analysis demonstrating that Contrastive Learning (CL) induces a more robust feature space for sparse geometry than Supervised Learning (SL). 3. Systematic robustness breakdown analyzing the trade-off between observation distance and descriptor reliability, identifying effective operational ranges.",
      "summary": "This paper addresses the degradation of pole landmark identification reliability under long-distance, sparse observations in urban environments. It proposes an evaluation framework using the SPL dataset and compares Contrastive Learning (CL) and Supervised Learning (SL) paradigms, finding that CL achieves superior retrieval performance, especially in the 5-10m range, by learning more robust features.",
      "mindmap": "graph TB\n        A[Pole-centric Descriptors for Robust Robot Localization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Pole-at-Distance (PaD) observations degrade landmark identification reliability]\n        C --> C1[Establish SPL dataset via automated tracking pipeline]\n        C --> C2[Comparative analysis of Contrastive Learning vs. Supervised Learning]\n        D --> D1[CL induces more robust feature space for sparse geometry]\n        D --> D2[Superior retrieval performance in 5-10m range]"
    },
    {
      "title": "Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot",
      "authors": "Seiko Piotr Yamaguchi, Kentaro Uno, Yasumaru Fujii, Masazumi Imai, Kazuki Takada, Taku Okawara, Kazuya Yoshida",
      "institution": "Japan Aerospace Exploration Agency (JAXA), Tohoku University, Hamano Products Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.23153",
      "code": null,
      "tags": [
        "space robotics",
        "multi-limbed robot",
        "intra-vehicular activity",
        "motion planning",
        "microgravity simulation",
        "autonomous operation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/282a695eb4eaf1576b9e4ce6a2fca61f3d48e62d714d0b32d51e22f5a403fbd6_w640_q70.webp",
      "contributions": "1. A feasibility study and design for a multi-limbed intra-vehicular robot (MLIVR) to automate logistical tasks on the ISS. 2. Development and simulation of 3D motion planning for the robot's transportation capabilities in a space station environment. 3. Execution of prototype ground tests on a 2D table to validate autonomous operation in a simulated microgravity setting.",
      "summary": "This paper studies the use of an autonomous multi-limbed robot to assist astronauts with logistical tasks on the International Space Station. The method involved simulating 3D motion planning and testing a prototype on a 2D table to mimic microgravity. The results show that such tasks can be performed with minimal human intervention, enhancing operational efficiency.",
      "mindmap": "graph TB\n        Root(”Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”宇航员时间被物流任务占用/Astronaut time consumed by logistical tasks”)\n        Method --> M1(”3D运动规划模拟/3D motion planning simulation”)\n        Method --> M2(”2D平台原型测试/2D table prototype testing”)\n        Results --> R1(”验证最小人力干预的可行性/Validated feasibility of minimal human intervention”)"
    },
    {
      "title": "A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots",
      "authors": "Keigo Torii, Kentaro Uno, Shreya Santra, Kazuya Yoshida",
      "institution": "Tohoku University",
      "link": "https://arxiv.org/pdf/2512.23154",
      "code": null,
      "tags": [
        "modular robotics",
        "coupling mechanism",
        "sequential hermaphrodite",
        "shape-matching",
        "lattice-based",
        "modular self-reconfigurable robot (MSR)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e13d98b23f8f9371fe7ea6dda54625a0d3bd211a4b265a46a96f7e03a5eba13_w640_q70.webp",
      "contributions": "1. A novel shape-matching mechanical coupling mechanism that meets complex requirements for heterogeneous structural modules. 2. A design enabling controlled, sequential transitions between male and female states to facilitate single-sided operations. 3. The capability for single-sided decoupling from both the male and female sides by forcibly switching the opposite mechanism's state.",
      "summary": "This paper addresses the complex design requirements for coupling mechanisms in lattice-based modular robots, such as single-sided operation and flat uncoupled surfaces. It proposes a novel sequential hermaphrodite coupling mechanism that dynamically switches between male and female states to meet these needs. The mechanism is concluded to be applicable to various modular robot systems and tool changers.",
      "mindmap": "graph TB\n        Root[”A Sequential Hermaphrodite Coupling Mechanism<br>顺序雌雄同体耦合机制”] --> Problem[”核心问题/Problem<br>Coupling mechanisms for lattice-based modular robots need to meet multiple complex requirements (single-sided operation, flat surfaces, etc.)<br>晶格模块化机器人的耦合机制需满足多种复杂要求（单侧操作、平整表面等）”]\n        Root --> Method[”主要方法/Method<br>Propose a novel shape-matching mechanism with controlled sequential transitions between male and female states<br>提出一种新颖的形状匹配机制，可在雄性和雌性状态间进行受控顺序转换”]\n        Root --> Results[”关键结果/Results<br>Mechanism satisfies design requirements, enables single-sided coupling/decoupling from both sides<br>机制满足设计要求，可实现两侧的单侧连接/分离”]"
    },
    {
      "title": "Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control",
      "authors": "Kooktae Lee, Julian Martinez",
      "institution": "New Mexico Institute of Mining and Technology",
      "link": "https://arxiv.org/pdf/2512.23158",
      "code": null,
      "tags": [
        "multi-agent systems",
        "ergodic coverage",
        "spectral multiscale coverage",
        "stochastic perturbation",
        "gradient cancellation",
        "mean-square boundedness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d9e67e490796584c8650824889a0abbafdd39b17b5ae994eec026cc68f38_w640_q70.webp",
      "contributions": "1. Rigorously characterizes the initial conditions and symmetry-induced invariant manifolds that cause directional degeneracy (stalling/axis-constrained motion) in classical Spectral Multiscale Coverage (SMC). 2. Introduces a novel stochastic spectral control method that combines a stochastic perturbation with a contraction term to address the degeneracy. 3. Provides theoretical proofs that the proposed dynamics ensure almost-sure escape from zero-gradient manifolds while maintaining mean-square boundedness of agent trajectories.",
      "summary": "This paper addresses the problem of gradient cancellation and agent stalling in multi-agent ergodic coverage when agents start near symmetry points of a target distribution. The authors propose a stochastic spectral control method that adds perturbation and contraction to the dynamics, proving it escapes degenerate states while keeping trajectories bounded. Simulations confirm the method mitigates stalling and axis-constrained motion effectively.",
      "mindmap": "graph TB\n        Root[”Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”梯度抵消导致停滞/Gradient cancellation causes stalling”]\n        Problem --> P2[”对称点初始化问题/Initialization near symmetry points”]\n        Method --> M1[”引入随机扰动/Introduce stochastic perturbation”]\n        Method --> M2[”结合收缩项/Combine with contraction term”]\n        Results --> R1[”几乎必然逃离零梯度流形/Almost-sure escape from zero-gradient manifolds”]\n        Results --> R2[”轨迹均方有界/Trajectories mean-square bounded”]\n        Results --> R3[”缓解瞬态停滞/Mitigates transient stalling”]"
    },
    {
      "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
      "authors": "Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu",
      "institution": "NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital",
      "link": "https://arxiv.org/pdf/2512.23162",
      "code": null,
      "tags": [
        "robotic imitation learning",
        "world model",
        "vision-language-action (VLA) model",
        "inverse dynamics model",
        "surgical robotics",
        "synthetic data generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp",
      "contributions": "1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training.",
      "summary": "This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform.",
      "mindmap": "graph TB\n        A[SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling] --> B[核心问题/Problem: Data scarcity for paired video-action data in surgical robotics]\n        A --> C[主要方法/Method: Build SurgWorld world model to generate synthetic videos; Use inverse dynamics to infer pseudo-kinematics]\n        A --> D[关键结果/Results: Surgical VLA policy trained with augmented data outperforms policy trained only on real data]"
    },
    {
      "title": "A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict",
      "authors": "Qin Wang, Shanmin Pang, Jianwu Fang, Shengye Dong, Fuhao Liu, Jianru Xue, Chen Lv",
      "institution": "Xi'an Jiaotong University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.23220",
      "code": "https://github.com/i-Qin/HOCD",
      "tags": [
        "reinforcement learning",
        "cooperative driving",
        "human-machine conflict",
        "intention-aware planning",
        "authority allocation",
        "shared control"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d5e5ef9e0e93b645fd2997c604be86cc36eb4f1a7f88af7219f0cc6a908523b_w640_q70.webp",
      "contributions": "1. Proposes a Human-Oriented Cooperative Driving (HOCD) approach that minimizes human-machine conflict by prioritizing driver intention and state. 2. Designs an intention-aware trajectory planning method at the tactical level, using an intention consistency cost to align the trajectory with driver intention. 3. Develops a reinforcement learning-based control authority allocation strategy at the operational level to achieve consistency between driver state and authority allocation.",
      "summary": "This paper proposes a Human-Oriented Cooperative Driving (HOCD) approach to improve human-vehicle interaction by minimizing conflict. The method integrates intention-aware trajectory planning and a reinforcement learning-based authority allocation strategy. Simulation and human-in-the-loop experiments show the approach aligns with driver intention, ensures reasonable authority allocation, and enhances driving performance compared to other methods.",
      "mindmap": "graph TB\n        Root[”A Human-Oriented Cooperative Driving Approach<br>人机协同驾驶方法”] --> Problem[”核心问题/Problem<br>Human-machine conflict in cooperative driving<br>人机协同驾驶中的人机冲突”]\n        Root --> Method[”主要方法/Method<br>HOCD: Integrates intention & state<br>HOCD: 集成驾驶意图与状态”]\n        Method --> SubMethod1[”战术层面/Tactical Level<br>Intention-aware trajectory planning<br>意图感知轨迹规划”]\n        Method --> SubMethod2[”操作层面/Operational Level<br>RL-based authority allocation<br>基于强化学习的权限分配”]\n        Root --> Results[”关键结果/Results<br>Aligns intention, reasonable allocation, enhances performance<br>对齐意图、合理分配、提升性能”]"
    },
    {
      "title": "Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?",
      "authors": "Socratis Gkelios, Savvas D. Apostolidis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos, Athanasios Ch. Kapoutsis",
      "institution": "Democritus University of Thrace, Information Technologies Institute, Centre for Research & Technology Hellas",
      "link": "https://arxiv.org/pdf/2512.23257",
      "code": "github.com/soc12/mUDAI",
      "tags": [
        "coverage path planning",
        "UAV Swarms",
        "Multi-Agent",
        "Coverage Path Planning",
        "Remote Sensing",
        "Trajectory Optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eadfe6ac5c135ae77cf9cbeeaa979770a93369ca56f253722ad9679e402d746a_w640_q70.webp",
      "contributions": "1. Formally introduces the Fast Inspection of Scattered Regions (FISR) problem, focusing on inspecting multiple non-connected areas. 2. Proposes the multi-UAV Disjoint Areas Inspection (mUDAI) method, a two-fold optimization for image capture positions and UAV trajectories. 3. Validates the method through simulations and real-world deployments, demonstrating improved operational efficiency while maintaining data quality.",
      "summary": "This paper addresses the inefficiency of traditional coverage path planning for inspecting multiple scattered regions using UAVs. It proposes the mUDAI method, which optimizes both image capture positions and UAV trajectories to minimize time and resource use. The method is validated through simulations and real-world tests, showing it enables rapid, efficient inspections.",
      "mindmap": "graph TB\n        Root[”Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>传统覆盖路径规划(CPP)在检查多个分散区域时效率低下”]\n        Method[”主要方法/Method<br>提出mUDAI方法，双重优化图像采集位置和无人机轨迹”]\n        Results[”关键结果/Results<br>通过仿真和真实部署验证，提高了操作效率并保持了数据质量”]"
    },
    {
      "title": "PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering",
      "authors": "Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin",
      "institution": "Yuan Ze University",
      "link": "https://arxiv.org/pdf/2512.23318",
      "code": null,
      "tags": [
        "visual SLAM",
        "ORB-SLAM3",
        "YOLOv8",
        "dynamic object filtering",
        "point cloud refinement",
        "CUDA acceleration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp",
      "contributions": "1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset.",
      "summary": "This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3's robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall.",
      "mindmap": "graph TB\n        Root[PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: vSLAM accuracy compromised by dynamic objects]\n        Method[主要方法/Method: ORB-SLAM3 + YOLOv8 segmentation + multi-stage point cloud filtering]\n        Results[关键结果/Results: Mixed performance, notable improvement in specific sequences (e.g., Seq04)]"
    },
    {
      "title": "Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants",
      "authors": "Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin",
      "institution": "Yuan Ze University",
      "link": "https://arxiv.org/pdf/2512.23312",
      "code": null,
      "tags": [
        "explainable ai (xai)",
        "inverse kinematics",
        "shapley additive explanations (SHAP)",
        "InterpretML",
        "obstacle avoidance",
        "neural network"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp",
      "contributions": "1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.",
      "summary": "This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.",
      "mindmap": "graph TB\n        A[”Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation<br>可解释神经逆运动学用于障碍物感知机器人操作”] --> B\n        A --> C\n        A --> D\n        B[”核心问题/Problem<br>Opaque neural IK models lack transparency and safety for responsible AI.<br>黑盒神经IK模型缺乏透明度与安全性”] --> B1[”挑战/Challenges<br>Debugging failures, safety certification”]\n        C[”主要方法/Method<br>XAI workflow integrating SHAP and physics simulation.<br>集成SHAP与物理仿真的XAI工作流”] --> C1[”模型/Variants<br>Improved IKNet, Focused IKNet”]\n        C --> C2[”工具/Tools<br>SHAP, InterpretML, Simulator”]\n        D[”关键结果/Results<br>Balanced feature attribution correlates with wider safety margins.<br>均衡的特征归因与更宽的安全裕度相关”] --> D1[”结论/Conclusion<br>XAI guides architectural refinement for trustworthy IK.<br>XAI指导可信IK的架构改进”]"
    },
    {
      "title": "Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains",
      "authors": "Simay Atasoy Bingöl, Tobias Töpfer, Sven Kosub, Heiko Hamann, Andreagiovanni Reina",
      "institution": "Universität Konstanz, Max Planck Institute of Animal Behavior",
      "link": "https://arxiv.org/pdf/2512.23431",
      "code": null,
      "tags": [
        "multi-agent systems",
        "task allocation",
        "swarm robotics",
        "scalability functions",
        "marginal gains",
        "collective decision-making"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65eaf636a462023cd662333c71ac9a0b2b73588d8278f1b001d8292a37ecc630_w640_q70.webp",
      "contributions": "1. A computationally efficient algorithm for optimal agent allocation based on marginal performance gains. 2. The algorithm handles tasks with concave scalability functions, including linear, saturating, and retrograde scaling. 3. Validation of the algorithm in a simulated robot swarm performing collective decision-making tasks with varying difficulty.",
      "summary": "The paper addresses the problem of optimally allocating a finite number of agents across multiple tasks where performance scales differently. It proposes an efficient algorithm based on marginal gains to handle concave scalability functions, including retrograde scaling where too many agents degrade performance. The method is validated in robot swarm simulations for collective decision-making, showing its utility for future multi-robot systems.",
      "mindmap": "graph TB\n        A[Optimal Scalability-Aware Allocation of Swarm Robots<br>机器人集群的可扩展性感知最优分配] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[有限智能体分配到多个任务<br>Limited agents to multiple tasks]\n        B --> B2[性能随智能体数量非线性变化<br>Nonlinear performance scaling]\n        B --> B3[暴力搜索不可行<br>Brute-force infeasible]\n        C --> C1[基于边际性能增益的算法<br>Algorithm based on marginal gains]\n        C --> C2[处理凹可扩展性函数<br>Handles concave scalability functions]\n        D --> D1[在机器人集群决策中验证<br>Validated in robot swarm decision-making]\n        D --> D2[算法有效分配机器人<br>Algorithm useful for allocation]"
    },
    {
      "title": "Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing",
      "authors": "Manuel Franco-Vivo",
      "institution": "University of Bristol",
      "link": "https://arxiv.org/pdf/2512.23445",
      "code": null,
      "tags": [
        "multi-agent systems",
        "Model Predictive Control",
        "Coverage-Based Testing",
        "Edge-Case Exploration",
        "Multi-Agent Simulation",
        "Behaviour Coverage"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp",
      "contributions": "1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics.",
      "summary": "This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks.",
      "mindmap": "graph TB\n    A[Assessing Behaviour Coverage in a Multi-Agent System Simulation for Autonomous Vehicle Testing] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[如何全面评估自动驾驶系统在模拟环境中的行为覆盖度？/How to comprehensively evaluate behaviour coverage of AV systems in simulation?]\n    C --> C1[定义场景与交互，提出MPC行人智能体/Define scenarios & interactions, propose MPC pedestrian agent]\n    D --> D1[行为覆盖度对验证系统有效性至关重要/Behaviour coverage is crucial for validating system effectiveness]"
    },
    {
      "title": "Theory of Mind for Explainable Human-Robot Interaction",
      "authors": "Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter",
      "institution": "University of Hamburg",
      "link": "https://arxiv.org/pdf/2512.23482",
      "code": null,
      "tags": [
        "human-robot interaction",
        "Theory of Mind",
        "Explainable AI",
        "XAI evaluation",
        "human-centered explanation",
        "VXAI framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp",
      "contributions": "1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot's actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.",
      "summary": "This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM's user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.",
      "mindmap": "graph TB\n        Root(”Theory of Mind for Explainable Human-Robot Interaction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”ToM解释与机器人内部推理不一致/ToM explanations may not match robot's internal reasoning”)\n        Problem --> P2(”XAI缺乏以用户为中心的解释/XAI lacks user-centered explanations”)\n        Method --> M1(”将ToM视为XAI的一种形式/Consider ToM as a form of XAI”)\n        Method --> M2(”在XAI框架内整合ToM原则/Integrate ToM principles within XAI frameworks”)\n        Results --> R1(”提出视角转变，优先考虑用户需求/Proposed shift in perspective to prioritize user's needs”)\n        Results --> R2(”为使用VXAI等框架评估ToM奠定基础/Laid foundation for evaluating ToM using frameworks like VXAI”)"
    },
    {
      "title": "Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery",
      "authors": "Mehdi Heydari Shahna",
      "institution": "Based on the author name \"Mehdi Heydari Shahna\", a common affiliation for this research area would be a university with a strong robotics program, such as Aalto University, Tampere University, or the University of Oulu in Finland. However, the provided text does not explicitly state the institution. A reasonable inference from the context of heavy-duty machinery and robotics in Europe could be \"Tampere University\" or \"Aalto University\", but without explicit data, the safest answer is \"Not explicitly stated in provided content\".",
      "link": "https://arxiv.org/pdf/2512.23505",
      "code": null,
      "tags": [
        "robust control",
        "learning-based control",
        "robust control",
        "deep learning",
        "safety guarantees",
        "heavy-duty machinery",
        "hierarchical control"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f78ddebe7bd223957b3334fed0e1b9489d7be34c9c804202f8cfb9e774548a_w640_q70.webp",
      "contributions": "1. A generic modular control framework for electrified heavy-duty mobile machines that is energy-source independent and simplifies design. 2. Hierarchical control policies that integrate AI/learning strategies while providing formal guarantees for safety, performance, and stability. 3. Methods to interpret and verify black-box learning components to ensure compliance with international safety standards.",
      "summary": "This dissertation addresses the challenge of safely integrating AI into heavy-duty mobile machines undergoing electrification. It proposes a robust, modular control framework that combines deep learning with formal guarantees for performance and stability. The framework is validated through multiple case studies, advancing control methods for safer and more reliable robotic systems in heavy industry.",
      "mindmap": "graph TB\n        A[Robust Deep Learning Control for Heavy-Duty Machinery] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Two Transitions for HDMMs<br>1. 柴油液压到电动<br>Diesel-Hydraulic to Electric<br>2. 人工监督到自主化<br>Human Supervision to Autonomy<br>关键挑战: 安全与可靠性<br>Key Challenge: Safety & Reliability]\n        C[主要方法/Method<br>提出一个控制框架<br>Proposes a Control Framework<br>1. 通用模块化设计<br>Generic Modular Design<br>2. 分层策略整合AI<br>Hierarchical Policy Integrating AI<br>3. 保证性能与稳定性<br>Guarantees Performance & Stability]\n        D[关键结果/Results<br>框架得到验证<br>Framework Validated<br>通过三个案例研究<br>Via Three Case Studies<br>产出五篇论文<br>Five Publications<br>支持两大转型<br>Supports Both Transitions]"
    },
    {
      "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
      "authors": "Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo",
      "institution": "Agibot Research",
      "link": "https://arxiv.org/pdf/2512.23541",
      "code": "https://act2goal.github.io/",
      "tags": [
        "reinforcement learning",
        "goal-conditioned policy",
        "world model",
        "multi-scale temporal hashing",
        "hindsight goal relabeling",
        "LoRA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp",
      "contributions": "1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.",
      "summary": "This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.",
      "mindmap": "graph TB\n        A[Act2Goal: From World Model To General Goal-conditioned Policy] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有目标条件策略在长视野操作中表现不佳/Existing goal-conditioned policies struggle with long-horizon manipulation]\n        C --> C1[集成目标条件视觉世界模型与多尺度时序控制/Integrates goal-conditioned visual world model with multi-scale temporal control]\n        C --> C2[引入多尺度时序哈希(MSTH)分解轨迹/Introduces Multi-Scale Temporal Hashing (MSTH) to decompose trajectory]\n        D --> D1[零样本泛化能力强/Strong zero-shot generalization]\n        D --> D2[通过在线自适应显著提升成功率/Improves success rates significantly via online adaptation]"
    },
    {
      "title": "Soft Robotic Technological Probe for Speculative Fashion Futures",
      "authors": "Amy Ingold, Loong Yi Lee, Richard Suphapol Diteesawat, Ajmal Roshan, Yael Zekaria, Edith-Clare Hall, Enrico Werner, Nahian Rahman, Elaine Czech, Jonathan Rossiter",
      "institution": "University of Bristol",
      "link": "https://arxiv.org/pdf/2512.23570",
      "code": null,
      "tags": [
        "human-robot interaction",
        "soft robotics",
        "wearable technology",
        "speculative design",
        "pneumatic actuation",
        "technological probe"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/961ff66eaf860636f7951016c0465ba6020b516e266f501287c6b78b23a9fafa_w640_q70.webp",
      "contributions": "1. The design and fabrication of \"Sumbrella,\" a novel soft robotic garment integrating origami-inspired bistable units, fabric pneumatic actuators, and computer vision. 2. The use of Sumbrella as a technological probe in a focus group study to explore public interpretation, interaction, and ethical concerns regarding future soft robotic wearables. 3. The contribution of key considerations for HRI, including kinesic communication, social dynamics, and ethical guidelines, and a reflection on the value of speculative design for evaluating social acceptability.",
      "summary": "This paper presents \"Sumbrella,\" a soft robotic garment designed as a speculative fashion probe to explore the social implications of wearable robotics. Through a focus group study, the authors used the prototype to gather insights on how people imagine future interactions with such technology, revealing both expressive potential and significant ethical concerns. The work contributes design considerations and a methodological reflection on using speculative design in Human-Robot Interaction research to address social meaning alongside functionality.",
      "mindmap": "graph TB\n        A[Soft Robotic Technological Probe for Speculative Fashion Futures] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[新兴可穿戴机器人需兼顾功能与社会意义/Emerging wearable robotics demand design addressing function and social meaning]\n        C --> C1[设计并制造Sumbrella软体机器人服装/Design and fabricate Sumbrella soft robotic garment]\n        C --> C2[作为技术探针进行焦点小组研究/Use as a technological probe in a focus group study]\n        D --> D1[引发对表达潜力与伦理风险的丰富讨论/Surfaced discussions on expressive potential and ethical risks]\n        D --> D2[为HRI贡献设计考量与伦理指南/Contributed HRI design considerations and ethical guidelines]\n        D --> D3[反思推测性设计方法的价值/Reflected on the value of speculative design]"
    },
    {
      "title": "Unsupervised Learning for Detection of Rare Driving Scenarios",
      "authors": "Dat Le, Thomas Manhardt, Moritz Venator, Johannes Betz",
      "institution": "Technical University of Munich (TUM), CARIAD SE",
      "link": "https://arxiv.org/pdf/2512.23585",
      "code": null,
      "tags": [
        "anomaly detection",
        "Deep Isolation Forest",
        "t-SNE",
        "naturalistic driving data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d1396541ab0766bc09a3986c6b01acba4cc137b0167e70049e1da565e969a3_w640_q70.webp",
      "contributions": "1. Proposes an unsupervised learning framework using Deep Isolation Forest for detecting rare driving scenarios without labeled data. 2. Introduces a preprocessing pipeline that extracts structured statistical features from perception data using sliding windows. 3. Incorporates t-SNE for dimensionality reduction and visualization to improve the interpretability of detected anomalies.",
      "summary": "This paper addresses the challenge of detecting rare and hazardous driving scenarios for autonomous systems. It proposes an unsupervised framework using Deep Isolation Forest on naturalistic driving data, which effectively identifies anomalies without relying on labeled datasets. The method provides a scalable solution, though it depends on proxy ground truth and manually defined features.",
      "mindmap": "graph TB\n        Root[”Unsupervised Learning for Detection of Rare Driving Scenarios”] --> Problem[”核心问题/Problem: Detecting rare and hazardous driving scenarios for autonomous systems”]\n        Root --> Method[”主要方法/Method: Unsupervised framework using Deep Isolation Forest and t-SNE on naturalistic driving data”]\n        Root --> Results[”关键结果/Results: Effectively identifies rare scenarios, offers a scalable anomaly detection solution”]"
    },
    {
      "title": "A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems",
      "authors": "Nikolai Beving, Jonas Marxen, Steffen Mueller, Johannes Betz",
      "institution": "Technical University of Munich, Technical University of Berlin",
      "link": "https://arxiv.org/pdf/2512.23593",
      "code": null,
      "tags": [
        "control systems",
        "Kalman Filter",
        "Disturbance Observer",
        "Steer-by-Wire",
        "Driver Impedance",
        "Torque Estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae20fc48cc0d99b531523c76958cd606e506aeb5f6e913a69a693c9835f5d2eb_w640_q70.webp",
      "contributions": "1. Designed a Kalman filter-based disturbance observer to estimate high-frequency driver torque without using direct torque sensors. 2. Modeled the driver's passive torque as an extended state using a PT1-lag approximation within both linear and nonlinear system models. 3. Demonstrated that a nonlinear extended Kalman Filter outperforms a linear one in handling frictional nonlinearities during static-to-dynamic friction transitions.",
      "summary": "This paper addresses the problem of high-frequency driver-induced disturbances in Steer-by-Wire systems. It proposes a Kalman filter-based disturbance observer that estimates driver torque using only motor state measurements, eliminating the need for costly direct torque sensors. The method was validated via simulation, showing accurate disturbance reconstruction with minimal delay (~14ms) and improved performance using a nonlinear extended Kalman filter for handling friction.",
      "mindmap": "graph TB\n        Root[A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: SbW系统易受高频驾驶员扭矩干扰/SbW systems susceptible to high-frequency driver torque disturbances]\n        Method[主要方法/Method: 基于卡尔曼滤波的扰动观测器/Kalman filter-based disturbance observer]\n        Results[关键结果/Results: 准确重构干扰，延迟约14ms，非线性EKF性能更优/Accurately reconstructs disturbances, ~14ms delay, nonlinear EKF performs better]"
    },
    {
      "title": "Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces",
      "authors": "Christoph Willibald, Lugh Martensen, Thomas Eiband, Dongheui Lee",
      "institution": "German Aerospace Center (DLR), University of Lübeck, TU Wien",
      "link": "https://arxiv.org/pdf/2512.23616",
      "code": null,
      "tags": [
        "human-robot interaction",
        "surface segmentation",
        "mixed reality interface",
        "task-centric programming",
        "robot trajectory generation",
        "user study"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a561f172d18a77b56aecf8017c7bffbc65ae8d01334f01ff6a1aa8ff7c050bf3_w640_q70.webp",
      "contributions": "1. A novel interactive robot programming approach using task-centric workflows for non-experts. 2. A new surface segmentation algorithm that incorporates human input and provides continuous visual feedback for iterative refinement. 3. An optimal mixed reality interface design, validated through user studies, that reduces workload and improves usability for surface finishing tasks.",
      "summary": "This paper addresses the barrier of complex robot programming for surface finishing tasks in small-scale manufacturing. It proposes a task-centric mixed reality interface where non-experts can intuitively program a robot by interactively segmenting workpiece surfaces and receiving visual feedback, with the system then generating the corresponding robot trajectory. User studies showed that this approach significantly reduces user workload and improves usability for effective task programming.",
      "mindmap": "graph TB\n        A[Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 机器人编程设置复杂，阻碍在中小企业中部署/Robot programming setup is complex, hindering deployment in SMEs]\n        C[主要方法/Method: 面向任务的混合现实界面与交互式表面分割/Task-centric MR interface with interactive surface segmentation]\n        D[关键结果/Results: 降低用户工作负荷，提高可用性/Reduced user workload, improved usability]"
    },
    {
      "title": "The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors",
      "authors": "Antonio Franchi",
      "institution": "Based on the author name \"Antonio Franchi\", the institution is not explicitly stated in the provided text. Common affiliations for this author include CNRS (Centre national de la recherche scientifique) and LAAS (Laboratoire d'analyse et d'architecture des systèmes) in Toulouse, France.",
      "link": "https://arxiv.org/pdf/2512.23619",
      "code": null,
      "tags": [
        "aerial robotics",
        "control systems",
        "geometric optimization",
        "fully-actuated multirotors",
        "topological optimization",
        "isotropy metric",
        "phase locking",
        "N-5 scaling law"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/867b87b0001ff54cd94ab728db5acbc6219646d0a24ca04d8913e9c5baa68162_w640_q70.webp",
      "contributions": "1. Formulates the geometric design of fully-actuated multirotors as a topological optimization problem on the product manifold of Projective Lines, moving beyond parametric optimization. 2. Discovers and formulates the \"N-5 Scaling Law\", an empirical relationship describing the topology of optimal configurations for regular polyhedral chassis. 3. Identifies a design redundancy enabling optimality-preserving morphing, allowing continuous vehicle reconfiguration without loss of isotropic control authority.",
      "summary": "This paper investigates the optimal geometric design of fully-actuated multirotor drones. It formulates the problem as a topological optimization on a manifold and discovers that for symmetric chassis, the optimal solutions form continuous curves following an \"N-5 Scaling Law\". This reveals a design redundancy that allows the vehicle to morph its shape while maintaining optimal control performance.",
      "mindmap": "graph TB\n        Root[”The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors”] --> Problem[”核心问题/Problem: Conventional parametric optimization for multirotor design misses the intrinsic structure of the solution space.”]\n        Root --> Method[”主要方法/Method: Formulate design on manifold (RP²)^N, minimize Log-Volume isotropy metric.”]\n        Root --> Results[”关键结果/Results: Discovers N-5 Scaling Law & topology enabling optimality-preserving morphing.”]"
    },
    {
      "title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion",
      "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang",
      "institution": "Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23649",
      "code": null,
      "tags": [
        "robot learning",
        "video-to-locomotion",
        "visual motion intent",
        "diffusion policy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp",
      "contributions": "1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework's effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates.",
      "summary": "This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines.",
      "mindmap": "graph TB\n        A[RoboMirror: Video to Humanoid Locomotion] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[视觉理解与控制存在鸿沟/Gap between visual understanding and control]\n        B --> B2[现有方法缺乏真正的视觉理解/Existing methods lack genuine visual understanding]\n        C --> C1[利用VLM提取视觉运动意图/Use VLMs to distill visual motion intents]\n        C --> C2[基于扩散的策略生成动作/Diffusion-based policy generates actions]\n        C --> C3[无需姿态重建或重定向/No explicit pose reconstruction or retargeting]\n        D --> D1[支持第一与第三人称视频控制/Supports egocentric & third-person control]\n        D --> D2[降低80%控制延迟/Reduces control latency by 80%]\n        D --> D3[任务成功率提升3.7%/3.7% higher task success rate]"
    },
    {
      "title": "Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control",
      "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang",
      "institution": "BAAI (Beijing Academy of Artificial Intelligence), University of Sydney, Harbin Institute of Technology, Hong Kong University of Science and Technology, Shanghai Jiao Tong University, Peking University",
      "link": "https://arxiv.org/pdf/2512.23650",
      "code": null,
      "tags": [
        "robot learning",
        "audio-to-locomotion",
        "diffusion policy",
        "retargeting-free"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384b6201b72bb7358f39e97637b20f414d5429fd79013e12be43d148e4e91c65_w640_q70.webp",
      "contributions": "1. Proposes RoboPerform, the first unified framework for directly generating music-driven dance and speech-driven co-speech gestures from audio for humanoid robots. 2. Introduces a novel \"motion = content + style\" principle, treating audio as implicit style signals to eliminate the need for explicit motion reconstruction and retargeting. 3. Designs a policy architecture integrating a ResMoE teacher for diverse motion patterns and a diffusion-based student for audio style injection, ensuring low latency and high fidelity.",
      "summary": "This paper addresses the lack of expressive, audio-reactive locomotion in humanoid robots by proposing RoboPerform, a unified framework that directly generates dance and co-speech gestures from audio without explicit motion reconstruction. The method treats audio as a style signal and uses a teacher-student policy with diffusion for style injection. Experiments show the approach achieves physically plausible and audio-aligned motions, enabling responsive robot performance.",
      "mindmap": "graph TB\n        A[Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Robots lack expressive, audio-reactive locomotion] --> B1[当前方法局限/Limitations<br>Predefined motions, retargeting errors, high latency]\n        C[主要方法/Method<br>RoboPerform: Unified audio-to-locomotion framework] --> C1[核心原理/Core Principle<br>Motion = Content + Style] --> C2[技术架构/Architecture<br>ResMoE Teacher + Diffusion Student]\n        D[关键结果/Results<br>Promising physical plausibility and audio alignment] --> D1[优势/Advantages<br>Low latency, high fidelity, retargeting-free]"
    },
    {
      "title": "The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation",
      "authors": "Mohammed Baziyad, Manal Al Shohna, Tamer Rabie",
      "institution": "University of Sharjah",
      "link": "https://arxiv.org/pdf/2512.23672",
      "code": null,
      "tags": [
        "robot navigation",
        "Artificial Potential Field",
        "local minima",
        "path planning",
        "backfilling mechanism",
        "ramp-based enhancement"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/102dc24d2d8cfbc8e6d1d7ea2397f8d21537a37a1ed1946ffbedecc40d34a0a3_w640_q70.webp",
      "contributions": "1. Proposed the novel \"Bulldozer\" technique, which introduces a backfilling mechanism to systematically identify and eliminate local minima regions in APF-based navigation by increasing their potential values. 2. Incorporated a ramp-based enhancement to assist robots in escaping trap areas when they start within a local minimum, improving robustness. 3. Provided comprehensive experimental validation on a physical mobile robot, demonstrating superior execution speed and competitive path quality compared to standard APF, adaptive APF, A*, PRM, and RRT.",
      "summary": "This paper addresses the local minima trap problem in Artificial Potential Field (APF) based robot navigation by proposing the \"Bulldozer\" technique. The method uses a backfilling mechanism to eliminate local minima and a ramp-based enhancement to escape traps, preserving APF's advantages. Experimental results show it effectively solves the local minima issue while achieving fast execution and good path quality suitable for real-world use.",
      "mindmap": "graph TB\n        A[The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: APF方法的局部极小值陷阱/Local Minima Trap in APF]\n        C[主要方法/Method: 推土机技术：回填机制与斜坡增强/Bulldozer Technique: Backfilling & Ramp Enhancement]\n        D[关键结果/Results: 有效解决局部极小值，执行速度快，路径质量好/Resolves Local Minima, Fast Execution, Good Path Quality]"
    },
    {
      "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
      "authors": "Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wang, Yuheng Ji, Cheng Chi, Yaoxu Lyu, Zhongxia Zhao, Xiansheng Chen, Peterson Co, Shaoxuan Xie, Guocai Yao, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang",
      "institution": "Peking University, Beijing Academy of Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.23703",
      "code": "https://robo-dopamine.github.io",
      "tags": [
        "reinforcement learning",
        "Process Reward Model",
        "Policy-Invariant Reward Shaping",
        "Multi-Perspective Reward Fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbdde8d7dea23f224b530580752926db8c72c9f5768172278573c890a3c6b0c6_w640_q70.webp",
      "contributions": "1. Introduces Dopamine-Reward, a method for learning a step-aware, general-purpose process reward model (GRM) from multi-view inputs to overcome perceptual limitations. 2. Proposes a theoretically-sound Policy-Invariant Reward Shaping method within the Dopamine-RL framework to enable efficient policy learning without altering the optimal policy. 3. Demonstrates high efficiency and generalization, where a one-shot adapted GRM enables policy learning to achieve 95% success with only 150 online rollouts.",
      "summary": "The paper addresses the challenge of designing effective reward functions for real-world robotic RL by introducing Robo-Dopamine. It proposes a general, step-aware reward model trained on a large dataset and a robust policy learning framework with theoretically-sound reward shaping. Experiments show the approach achieves state-of-the-art reward accuracy and significantly improves policy learning efficiency with strong generalization.",
      "mindmap": "graph TB\n        A[Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: RL reward design is hard; existing PRMs lack step-awareness & use single-view, reward shaping is unsound]\n        C[主要方法/Method: Dopamine-Reward (GRM with Step-wise Discretization & Multi-Perspective Fusion) & Dopamine-RL (Policy-Invariant Reward Shaping)]\n        D[关键结果/Results: SOTA reward accuracy; High policy learning efficiency (95% success with 150 rollouts); Strong generalization]"
    },
    {
      "title": "Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks",
      "authors": "Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li",
      "institution": "University of Electronic Science and Technology of China, North China University of Technology",
      "link": "https://arxiv.org/pdf/2512.21375",
      "code": null,
      "tags": [
        "robotic perception and planning",
        "Interfered Fluid Dynamical System (IFDS)",
        "Model Predictive Control (MPC)",
        "Dynamic Flight Altitude Adjustment (DFAA)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp",
      "contributions": "1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.",
      "summary": "This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.",
      "mindmap": "graph TB\n    A[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --> B\n    A --> C\n    A --> D\n    B[核心问题/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]\n    C[主要方法/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]\n    D[关键结果/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]"
    },
    {
      "title": "Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction",
      "authors": "Rahul Moorthy Mahesh, Oguzhan Goktug Poyrazoglu, Yukang Cao, Volkan Isler",
      "institution": "University of Minnesota (inferred from author \"Volkan Isler\")",
      "link": "https://arxiv.org/pdf/2512.21398",
      "code": null,
      "tags": [
        "robot navigation",
        "language-conditioned planning",
        "map forecasting",
        "Log-MPPI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp",
      "contributions": "1. Introduces PaceForecaster, a novel architecture that integrates co-pilot language instructions into local motion planning. 2. Predicts a forecasted map (Level-2) of occluded areas and an instruction-conditioned subgoal within it. 3. Demonstrates a 36% improvement in navigation performance by integrating PaceForecaster with a Log-MPPI controller.",
      "summary": "This paper addresses the speed-safety trade-off in robot navigation in occluded environments by introducing PaceForecaster, a method that uses language instructions to forecast occluded map regions and generate conditioned subgoals. Integrating this with a Log-MPPI controller allows for more decisive and goal-directed planning. The approach improves navigation performance by 36% over a baseline that uses only the local sensor map.",
      "mindmap": "graph TB\n        A[Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[安全与速度的权衡<br/>Safety-Speed Trade-off]\n        B1 --> B2[遮挡导致的不确定性<br/>Uncertainty from Occlusions]\n        C --> C1[PaceForecaster 架构<br/>PaceForecaster Architecture]\n        C1 --> C2[输入: 传感器足迹与指令<br/>Input: Sensor Footprint & Instruction]\n        C2 --> C3[输出: 预测地图与子目标<br/>Output: Forecasted Map & Subgoal]\n        C3 --> C4[集成 Log-MPPI 控制器<br/>Integrated with Log-MPPI Controller]\n        D --> D1[性能提升 36%<br/>36% Performance Improvement]\n        D1 --> D2[超越仅使用局部地图的基线<br/>Over Local-Map-Only Baseline]"
    },
    {
      "title": "Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments",
      "authors": "Hang Zhou, Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang, Xiaowei Shi, Xiaopeng",
      "institution": "University of Wisconsin-Madison, University of Illinois Urbana-Champaign, University of Wisconsin-Milwaukee",
      "link": "https://arxiv.org/pdf/2512.21425",
      "code": "https://github.com/CATS-Lab/UAM-FD",
      "tags": [
        "Transportation Systems",
        "Robotics",
        "Fundamental Diagram",
        "Urban Air Mobility",
        "Traffic Flow Theory",
        "Drone Control",
        "Physical Experiments"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp",
      "contributions": "1. Proposes a novel framework integrating theory and physical experiments to construct a Fundamental Diagram for Urban Air Mobility traffic. 2. Develops and validates the first UAM Fundamental Diagram using real-world physical test data from a reduced-scale drone testbed. 3. Creates and releases the UAMTra2Flow dataset containing simulation and physical test trajectory data for UAM traffic analysis.",
      "summary": "This study addresses the lack of understanding of Urban Air Mobility (UAM) traffic flow by proposing a framework to construct its Fundamental Diagram (FD) through theoretical modeling and physical experiments using drones. The results show that classical ground traffic FD structures are applicable to UAM, but physical experiments reveal deviations from simulation, underscoring the need for experimental validation. The findings and a public dataset provide practical insights for future UAM system design.",
      "mindmap": "graph TB\n    Root(”Developing a Fundamental Diagram for Urban Air Mobility Traffic Flow / 构建城市空中交通基本图”)\n    Root --> Problem(”UAM交通流特性未知 / UAM Traffic Flow Poorly Understood”)\n    Root --> Method(”理论分析+物理实验框架 / Theory + Physical Experiment Framework”)\n    Root --> Results(”经典FD结构适用，实验验证关键 / Classical FD Applicable, Validation Crucial”)"
    },
    {
      "title": "EVE: A Generator-Verifier System for Generative Policies",
      "authors": "Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira",
      "institution": "Georgia Institute of Technology, Toyota Research Institute, Symbotic Inc.",
      "link": "https://arxiv.org/pdf/2512.21430",
      "code": null,
      "tags": [
        "robot learning",
        "generative policies",
        "test-time compute",
        "vision-language models",
        "verifier agents",
        "embodied control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp",
      "contributions": "1. Proposes EVE, a modular generator-verifier framework that improves pretrained generative visuomotor policies at test time without additional training. 2. Introduces a system of multiple zero-shot VLM-based verifier agents that propose action refinements, coupled with an action incorporator to fuse these suggestions. 3. Provides a systematic analysis and practical guidelines for designing scalable generator-verifier systems for embodied control through extensive ablations.",
      "summary": "The paper addresses the problem that generative visuomotor policies degrade under distribution shifts and lack recovery capabilities. It proposes EVE, a framework that uses additional inference-time compute to refine a frozen base policy's actions via multiple zero-shot VLM verifiers. The method consistently improves task success rates across diverse manipulation tasks without any retraining.",
      "mindmap": "graph TB\n        Root[EVE: A Generator-Verifier System for Generative Policies] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Generative policies degrade under distribution shifts and lack recovery.] --> ProblemDetail[问题细节/Problem Detail: Costly to finetune; limited test-time robustness.]\n        Method[主要方法/Method: EVE framework] --> MethodDetail1[方法细节/Method Detail 1: Wraps frozen base policy with zero-shot VLM verifiers.]\n        Method --> MethodDetail2[方法细节/Method Detail 2: Verifiers propose action refinements.]\n        Method --> MethodDetail3[方法细节/Method Detail 3: Action incorporator fuses verifier outputs.]\n        Results[关键结果/Results: Boosts performance without training.] --> ResultsDetail[结果细节/Results Detail: Improves task success rates across diverse manipulation tasks.]"
    },
    {
      "title": "Planetary Terrain Datasets and Benchmarks for Rover Path Planning",
      "authors": "Marvin Chancán, Avijit Banerjee, George Nikolakopoulos",
      "institution": "Luleå University of Technology",
      "link": "https://arxiv.org/pdf/2512.21438",
      "code": "https://github.com/mchancan/PlanetaryPathBench",
      "tags": [
        "path planning",
        "global path planning",
        "digital terrain models",
        "autonomous navigation",
        "rover exploration",
        "benchmark datasets"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp",
      "contributions": "1. Proposes the first two large, space mission-derived planetary benchmark datasets for rover path planning (MarsPlanBench and MoonPlanBench). 2. Establishes a unified framework for evaluating both classical and learning-based path planning algorithms on these planetary terrains. 3. Provides new empirical insights into algorithm performance, showing classical methods achieve high success rates on challenging terrains while learning-based methods struggle to generalize.",
      "summary": "This paper addresses the lack of standardized planetary datasets and benchmarks for rover path planning by introducing MarsPlanBench and MoonPlanBench, two large datasets derived from high-resolution digital terrain models of Mars and the Moon. The authors evaluate classical and learning-based path planning algorithms in a unified framework on these new benchmarks. The key finding is that classical algorithms achieve very high success rates (up to 100%) on challenging planetary terrains, explaining their practical use by agencies like NASA, while learning-based models still face generalization difficulties in these domains.",
      "mindmap": "graph TB\n    A[Planetary Terrain Datasets and Benchmarks for Rover Path Planning] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[缺乏用于路径规划的行星数据集与基准/Lack of planetary datasets & benchmarks for path planning]\n    C --> C1[提出火星与月球基准数据集/Propose MarsPlanBench & MoonPlanBench datasets]\n    C --> C2[建立统一评估框架/Establish unified evaluation framework]\n    D --> D1[经典算法成功率高达100%/Classical algorithms achieve up to 100% success rate]\n    D --> D2[学习模型泛化困难/Learning-based models struggle to generalize]"
    },
    {
      "title": "Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment",
      "authors": "Siddhartha Upadhyay, Ratnangshu Das, Pushpak Jagtap",
      "institution": "Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science (IISc), Bengaluru",
      "link": "https://arxiv.org/pdf/2512.21497",
      "code": null,
      "tags": [
        "control theory",
        "spatiotemporal tube",
        "probabilistic safety",
        "real-time control",
        "uncertain dynamic environment",
        "reach-avoid-stay"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp",
      "contributions": "1. Extension of the Spatiotemporal Tube (STT) framework to handle Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in environments with time-varying uncertain obstacles. 2. Development of a real-time tube synthesis procedure that provides formal probabilistic safety guarantees. 3. Derivation of a closed-form, model-free, approximation-free, and optimization-free control law that confines the system within the tube for efficient real-time execution.",
      "summary": "This paper proposes an extension of the Spatiotemporal Tube framework to solve Probabilistic Temporal Reach-Avoid-Stay tasks in uncertain dynamic environments. The method synthesizes a time-varying safe tube online and provides a closed-form control law to keep the system inside it, offering formal probabilistic safety and task completion guarantees. The approach is validated through simulations and hardware experiments on various robotic platforms.",
      "mindmap": "graph TB\n        A[Paper Title: Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task] --> B(核心问题/Problem: Probabilistic Temporal Reach-Avoid-Stay in uncertain dynamic environments)\n        A --> C(主要方法/Method: Real-time Spatiotemporal Tube synthesis with closed-form control)\n        A --> D(关键结果/Results: Formal probabilistic safety guarantees & efficient real-time execution)"
    },
    {
      "title": "A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming",
      "authors": "Congrui Bai, Zhenting Du, Weibang Bai",
      "institution": "ShanghaiTech University, King's College London",
      "link": "https://arxiv.org/pdf/2512.21534",
      "code": null,
      "tags": [
        "soft robotics",
        "variable stiffness actuators",
        "electrostatic layer jamming",
        "helical winding",
        "variable stiffness robotic finger",
        "voltage-driven control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp",
      "contributions": "1. Proposes a novel Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) mechanism for variable stiffness., 2. Demonstrates that the helical configuration provides exponentially greater stiffness adjustment and a reduced footprint compared to conventional planar designs., 3. Develops and validates a functional robotic finger prototype that confirms the feasibility of voltage-driven stiffness modulation.",
      "summary": "This paper introduces a novel variable stiffness mechanism for robotics called Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ). It uses a helical configuration and electrostatic attraction to achieve tunable stiffness, offering superior performance and a smaller footprint than traditional planar designs. The work is validated through experiments and a functional robotic finger prototype, confirming its feasibility.",
      "mindmap": "graph TB\n        A[A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming] --> B(核心问题/Problem: Robotic joints need variable stiffness for adaptability and safety.)\n        A --> C(主要方法/Method: Proposes HWS-ELJ, using helical winding and electrostatic attraction for tunable stiffness.)\n        A --> D(关键结果/Results: HWS-ELJ shows superior stiffness enhancement and smaller footprint; a functional robotic finger prototype validates feasibility.)"
    },
    {
      "title": "World-Coordinate Human Motion Retargeting via SAM 3D Body",
      "authors": "Zhangzheng Tum, Kailun Su, Shaolong Zhu, Yukun Zheng",
      "institution": "Dalian University of Technology, Shenzhen University, Harbin Institute of Technology, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.21573",
      "code": null,
      "tags": [
        "human motion capture and retargeting",
        "SAM 3D Body",
        "Momentum HumanRig",
        "contact-aware optimization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp",
      "contributions": "1. Proposes a lightweight framework using a frozen SAM 3D Body backbone and Momentum HumanRig representation for world-coordinate human motion recovery from monocular video. 2. Introduces temporal consistency enforcement via identity/scale locking and efficient sliding-window smoothing in a low-dimensional latent space. 3. Recovers physically plausible global root trajectories using a differentiable soft foot-ground contact model and contact-aware optimization for reliable robot retargeting.",
      "summary": "This paper proposes a lightweight framework for recovering world-coordinate human motion from monocular video and retargeting it to a humanoid robot. The method leverages SAM 3D Body as a frozen backbone, enforces temporal consistency, smooths predictions, and uses a contact model for plausible global trajectories. Results show the method produces stable, robot-ready motion from monocular input.",
      "mindmap": "graph TB\n        A[World-Coordinate Human Motion Retargeting via SAM 3D Body] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>从单目视频恢复世界坐标系人体运动并重定向到机器人]\n        C[主要方法/Method<br>轻量级框架，使用冻结的SAM 3D Body，MHR表示，时序一致性约束，滑动窗口平滑，接触感知全局优化]\n        D[关键结果/Results<br>稳定的世界轨迹，可靠的机器人重定向，从单目输入生成机器人就绪的运动]"
    },
    {
      "title": "SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration",
      "authors": "Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang",
      "institution": "Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University",
      "link": "https://arxiv.org/pdf/2512.21618",
      "code": null,
      "tags": [
        "novel view synthesis",
        "diffusion models",
        "3D Gaussian Splatting",
        "auto-regressive restoration",
        "context-aware inpainting",
        "view synthesis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",
      "contributions": "1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.",
      "summary": "SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.",
      "mindmap": "graph TB\n        A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --> B[核心问题/Problem: 现有方法难以同时实现高保真渲染和交互式交通编辑 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n        A --> C[主要方法/Method: 提出对称自回归在线修复范式与免训练协调机制 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n        A --> D[关键结果/Results: 在新视角增强和3D车辆插入中实现最先进性能 / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"
    },
    {
      "title": "AstraNav-Memory: Contexts Compression for Long Memory",
      "authors": "Botao Ren, Junjun Hu, Xinda Xue, Minghua Luo, Jintao Chen, Haochen Bai, Liangliang You, Mu Xu",
      "institution": "Alibaba Group (Amap), Tsinghua University, Peking University",
      "link": "https://arxiv.org/pdf/2512.21627",
      "code": "https://astra-amap.github.io/AstraNav-Memory.github.io/",
      "tags": [
        "memory & caching",
        "visual context compression",
        "image-centric memory",
        "lifelong embodied navigation",
        "DINOv3",
        "Qwen2.5-VL"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp",
      "contributions": "1. Proposed an image-centric memory framework for lifelong embodied navigation that uses an efficient visual context compression module to achieve long-term implicit memory. 2. Introduced a configurable visual tokenizer built on a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, enabling high compression rates (e.g., 16x) to expand context capacity. 3. Demonstrated state-of-the-art navigation performance on benchmarks (GOAT-Bench, HM3D-OVON), showing improved exploration in unfamiliar environments and shorter paths in familiar ones, with ablation studies validating the efficiency-accuracy trade-off.",
      "summary": "This paper addresses the challenge of building long-term memory for lifelong embodied navigation. It proposes AstraNav-Memory, an image-centric framework that compresses visual contexts using a configurable tokenizer to efficiently store hundreds of images, coupled with a Qwen2.5-VL-based navigation policy. The method achieves state-of-the-art performance, balancing efficiency and accuracy for scalable lifelong agents.",
      "mindmap": "graph TB\n        A[AstraNav-Memory: Contexts Compression for Long Memory] --> B[核心问题/Problem: Lifelong embodied navigation requires efficient long-term spatial-semantic memory. Object-centric methods are limited by robustness and scalability.]\n        A --> C[主要方法/Method: Propose an image-centric memory framework with a visual context compression module (ViT+DINOv3+PixelUnshuffle) coupled with a Qwen2.5-VL navigation policy.]\n        A --> D[关键结果/Results: Achieves SOTA on GOAT-Bench and HM3D-OVON. Improves exploration in unfamiliar environments and shortens paths in familiar ones. Moderate compression offers best balance.]"
    },
    {
      "title": "Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning",
      "authors": "Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee",
      "institution": "Kyungpook National University, Kumoh National Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21654",
      "code": null,
      "tags": [
        "swarm intelligence",
        "Ant Colony Optimization",
        "structural prior",
        "load-aware objective",
        "overlap suppression",
        "multi-robot path planning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp",
      "contributions": "1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.",
      "summary": "This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.",
      "mindmap": "graph TB\n        A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. 传统方法难以扩展/Traditional methods struggle to scale.]\n        C[主要方法/Method: A structure-induced ACO framework. 利用结构先验、负载感知目标和重叠抑制/Uses structural prior, load-aware objective, and overlap suppression.]\n        D[关键结果/Results: Improves route compactness, stability, and workload distribution. 提供可扩展的框架/Provides a scalable framework.]"
    },
    {
      "title": "MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning",
      "authors": "Zishuo Wang, Xinyu Zhang, Zhuonan Liu, Tomohito Kawabata, Daeun Song, Xuesu Xiao, Ling Xiao",
      "institution": "Hokkaido University, George Mason University",
      "link": "https://arxiv.org/pdf/2512.21722",
      "code": null,
      "tags": [
        "socially compliant navigation",
        "meta-cognitive prompt",
        "vision language model",
        "multi-action generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp",
      "contributions": "1. Proposes MAction-SocialNav, a vision language model for socially compliant navigation that generates multiple plausible actions to handle real-world ambiguity. 2. Introduces a novel meta-cognitive prompt (MCP) method to enhance the model's reasoning capability. 3. Curates a new multi-action socially compliant navigation dataset with diverse conditions and dual human annotations, and designs five evaluation metrics.",
      "summary": "This paper addresses the problem of action ambiguity in socially compliant robot navigation by proposing MAction-SocialNav, an efficient vision language model that generates multiple plausible actions per scenario using a novel meta-cognitive prompt tuning method. The method is evaluated on a newly curated dataset and shows superior decision quality, safety alignment, and real-time efficiency compared to large language models like GPT-4o and Claude.",
      "mindmap": "graph TB\n        Root[”MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning”] --> Problem[”核心问题/Problem: Social norms are ambiguous; multiple actions may be acceptable, but existing methods assume a single correct action.”]\n        Root --> Method[”主要方法/Method: Proposes an efficient VLM with a novel meta-cognitive prompt (MCP) to generate multiple plausible actions.”]\n        Root --> Results[”关键结果/Results: Achieves higher decision quality and safety than GPT-4o/Claude while maintaining real-time efficiency.”]"
    },
    {
      "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks",
      "authors": "Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21723",
      "code": null,
      "tags": [
        "agent system",
        "embodied agent",
        "hierarchical planning",
        "large language model",
        "household tasks",
        "open source LLM"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",
      "contributions": "1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.",
      "summary": "The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.",
      "mindmap": "graph TB\n        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"
    },
    {
      "title": "MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction",
      "authors": "Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida",
      "institution": "Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University",
      "link": "https://arxiv.org/pdf/2512.21853",
      "code": null,
      "tags": [
        "space robotics",
        "modular robot",
        "reconfigurable robot",
        "lunar construction",
        "field demonstration",
        "connector design"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp",
      "contributions": "1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.",
      "summary": "This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.",
      "mindmap": "graph TB\n        A[MoonBot: 面向月球基地建设的模块化按需可重构机器人] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[月球探索与基地建设需求 / Lunar Exploration & Base Construction Needs]\n        C --> C1[模块化可重构机器人系统 / Modular & Reconfigurable Robotic System]\n        C --> C2[概念验证与现场演示 / Proof-of-Concept & Field Demonstration]\n        D --> D1[成功执行模拟任务 / Successfully Executed Simulated Tasks]\n        D --> D2[总结了连接器设计等经验教训 / Summarized Lessons (e.g., Connector Design)]"
    },
    {
      "title": "Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking",
      "authors": "Kenta Iizuka, Akiyoshi Uchida, Kentaro Uno, Kazuya Yoshida",
      "institution": "Tohoku University",
      "link": "https://arxiv.org/pdf/2512.21882",
      "code": null,
      "tags": [
        "space robotics",
        "trajectory planning",
        "nonlinear optimization",
        "dynamic keep-out sphere",
        "ON/OFF thrusters",
        "rendezvous and docking"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp",
      "contributions": "1. A trajectory planning method based on nonlinear optimization for close-range rendezvous with a tumbling target. 2. The introduction of a dynamic keep-out sphere that adapts to approach conditions for safer access. 3. A control strategy to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.",
      "summary": "This paper addresses the challenge of safely approaching a tumbling space debris object for capture. It proposes a trajectory planning method using nonlinear optimization with a dynamic safety boundary and a control strategy for ON/OFF thrusters. The method enables closer and safer access as a preliminary step for robotic capture.",
      "mindmap": "graph TB\n        Root(”Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”安全接近翻滚目标/Safely approaching a tumbling target”)\n        Method --> M1(”非线性优化轨迹规划/Nonlinear optimization-based trajectory planning”)\n        Method --> M2(”动态禁入球体/Dynamic keep-out sphere”)\n        Method --> M3(”ON/OFF推进器控制/ON/OFF thruster control strategy”)\n        Results --> R1(”更近更安全的访问/Closer and safer access”)\n        Results --> R2(”考虑实际约束/Practical implementation considered”)"
    },
    {
      "title": "Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications",
      "authors": "Akiyoshi Uchida, Antonine Richard, Kentaro Uno, Miguel Olivares-Mendez, Kazuya Yoshida",
      "institution": "Tohoku University, University of Luxembourg",
      "link": "https://arxiv.org/pdf/2512.21886",
      "code": null,
      "tags": [
        "robotics",
        "dynamics and control",
        "inertia parameter estimation",
        "online identification",
        "momentum conservation",
        "floating-base robots",
        "space robotics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp",
      "contributions": "1. Extended an existing online inertia identification method to be applicable to floating-base robots by incorporating momentum conservation. 2. Validated the proposed method through numerical simulations for space applications like on-orbit servicing. 3. Demonstrated accurate estimation of unknown object inertia parameters during manipulation in a simulated microgravity environment.",
      "summary": "This paper addresses the problem of estimating the inertia parameters of an unknown object grasped by a manipulator on a free-floating space robot. The authors extend an existing online identification method by incorporating momentum conservation to handle the floating base. Numerical simulations validate the method, showing accurate identification and highlighting its potential for on-orbit servicing missions.",
      "mindmap": "graph TB\n    Root(Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications) --> Problem(核心问题/Problem)\n    Root --> Method(主要方法/Method)\n    Root --> Results(关键结果/Results)\n    Problem --> P1(估计未知被抓取物体的惯性参数/Estimate inertia parameters of unknown grasped object)\n    Problem --> P2(面向自由漂浮基座的空间机器人/For floating-base space robots)\n    Method --> M1(应用并扩展在线识别方法/Apply and extend online identification method)\n    Method --> M2(结合动量守恒定律/Incorporate momentum conservation)\n    Results --> R1(通过数值模拟验证/Validated via numerical simulation)\n    Results --> R2(参数识别准确/Accurate parameter identification)\n    Results --> R3(适用于在轨服务/Applicable to on-orbit servicing)"
    },
    {
      "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space",
      "authors": "Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21887",
      "code": null,
      "tags": [
        "visual navigation",
        "world model",
        "future frame projection",
        "4-dof uav",
        "long-horizon visual generation",
        "aerial navigation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp",
      "contributions": "1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.",
      "summary": "This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.",
      "mindmap": "graph TB\n        A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[UAV导航缺乏高层语义规划能力/UAV navigation lacks high-level semantic planning]\n        B --> B2[现有模型难以处理复杂动作空间与长距离视觉生成/Existing models struggle with complex action space & long-horizon visual generation]\n        C --> C1[提出ANWM世界模型/Propose ANWM world model]\n        C --> C2[引入未来帧投影模块/Introduce Future Frame Projection module]\n        D --> D1[长距离视觉预测性能显著提升/Significantly outperforms in long-distance visual forecasting]\n        D --> D2[提高大规模环境导航成功率/Improves UAV navigation success rates in large-scale environments]"
    },
    {
      "title": "Flexible Multitask Learning with Factorized Diffusion Policy",
      "authors": "Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du",
      "institution": "University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University",
      "link": "https://arxiv.org/pdf/2512.21898",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion policy",
        "modular architecture",
        "multitask learning",
        "imitation learning",
        "mixture-of-experts"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp",
      "contributions": "1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.",
      "summary": "This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.",
      "mindmap": "graph TB\n        A[Flexible Multitask Learning with Factorized Diffusion Policy] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[机器人多任务学习/Robot Multitask Learning]\n        B1 --> B2[动作分布复杂多模态/Action Distribution Highly Multimodal]\n        B2 --> B3[单体模型欠拟合与不灵活/Monolithic Models Underfit & Inflexible]\n        C --> C1[因子化扩散策略/Factorized Diffusion Policy (FDP)]\n        C1 --> C2[模块化扩散专家/Modular Diffusion Experts]\n        C2 --> C3[基于观察的路由器/Observation-Conditioned Router]\n        C3 --> C4[连续分数聚合/Continuous Score Aggregation]\n        D --> D1[性能超越基线/Outperforms Baselines]\n        D1 --> D2[仿真与真实机器人验证/Simulation & Real-World Validation]\n        D2 --> D3[支持灵活策略适应/Enables Flexible Policy Adaptation]"
    },
    {
      "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision",
      "authors": "Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang",
      "institution": "Peking University, The University of Hong Kong, Institute of Automation, Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.21970",
      "code": "https://shengliangd.github.io/StereoVLA-Webpage",
      "tags": [
        "robotic vision",
        "stereo vision",
        "vision-language-action models",
        "geometric-semantic fusion",
        "depth estimation",
        "robotic manipulation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp",
      "contributions": "1. Proposed StereoVLA, a novel Vision-Language-Action model that leverages stereo vision for enhanced spatial perception. 2. Introduced a Geometric-Semantic Feature Extraction module to fuse geometric cues from stereo differences with semantic features from a monocular view. 3. Designed an auxiliary Interaction-Region Depth Estimation task to improve spatial understanding and accelerate model convergence.",
      "summary": "This paper addresses the limitation of single-view input in Vision-Language-Action (VLA) models for robotic manipulation by introducing StereoVLA, which utilizes stereo vision. The core method involves a novel module to extract and fuse geometric and semantic features, along with an auxiliary depth estimation task. Experiments show the model significantly outperforms baselines in stereo-based tasks and is robust to camera pose variations.",
      "mindmap": "graph TB\n        A[StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(单目VLA模型缺乏精确的几何感知/Single-view VLAs lack accurate geometry perception)\n        C --> C1(提出StereoVLA模型/Propose StereoVLA model)\n        C1 --> C2(几何-语义特征提取模块/Geometric-Semantic Feature Extraction)\n        C2 --> C3(从立体视图提取几何特征/Extract geometric features from stereo views)\n        C2 --> C4(从单目视图提取语义特征/Extract semantic features from monocular view)\n        C1 --> C5(辅助交互区域深度估计任务/Auxiliary Interaction-Region Depth Estimation task)\n        D --> D1(在立体设置下大幅超越基线/Large margin outperforms baselines under stereo setting)\n        D --> D2(对相机位姿变化具有强鲁棒性/Strong robustness to camera pose variations)"
    },
    {
      "title": "Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation",
      "authors": "Saksham Gupta, Sarthak Mishra, Arshad Ayub, Kamran Farooque, Spandan Roy, Babita Gupta",
      "institution": "International Institute of Information Technology Hyderabad (IIIT-H), All India Institute of Medical Sciences, New Delhi (AIIMS)",
      "link": "https://arxiv.org/pdf/2512.21983",
      "code": null,
      "tags": [
        "medical robotics",
        "robotic intubation",
        "closed-loop control",
        "shape sensing",
        "depth estimation",
        "human-in-the-loop"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp",
      "contributions": "1. A compact, integrated robotic platform (BRIS) for fiberoptic-guided intubation, featuring a steerable bronchoscope, an independent tube advancement mechanism, and a camera-augmented mouthpiece. 2. A learning-enabled closed-loop control framework that uses real-time shape sensing to map joystick inputs to precise bronchoscope tip motion, ensuring stable teleoperation despite tendon nonlinearities. 3. The use of monocular endoscopic depth estimation to classify airway regions and provide anatomy-aware guidance for safe endotracheal tube positioning relative to the carina.",
      "summary": "This paper presents the Bab Sak Robotic Intubation System (BRIS), a human-in-the-loop platform designed to assist with safe fiberoptic endotracheal intubation. It integrates a learning-enabled control framework for stable scope navigation and uses monocular depth estimation for anatomy-aware tube placement guidance. The system was validated on airway mannequins, demonstrating reliable performance as a step toward safer and more consistent robotic airway management.",
      "mindmap": "graph TB\n        A[BRIS: 安全光纤插管系统 / BRIS: Safe Fiberoptic Intubation System] --> B[核心问题/Problem: 插管技术难度高，现有系统不完善 / Problem: Intubation is difficult, existing systems are limited]\n        A --> C[主要方法/Method: 集成机器人平台与学习控制框架 / Method: Integrated robotic platform with learning-enabled control]\n        A --> D[关键结果/Results: 在人体模型上验证可靠 / Results: Validated as reliable on mannequins]\n        B --> B1[并发症风险高 / High risk of complications]\n        B --> B2[缺乏集成管推进与深度验证 / Lacks integrated tube control & depth verification]\n        C --> C1[闭环控制与形状感知 / Closed-loop control & shape sensing]\n        C --> C2[单目深度估计引导 / Monocular depth estimation guidance]\n        D --> D1[可靠导航 / Reliable navigation]\n        D --> D2[可控置管 / Controlled tube placement]"
    },
    {
      "title": "Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search",
      "authors": "Jan Mikula, Miroslav Kulich",
      "institution": "Czech Technical University in Prague",
      "link": "https://arxiv.org/pdf/2512.20711",
      "code": null,
      "tags": [
        "robotics",
        "anytime metaheuristic",
        "minimum latency problem",
        "global route optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c34e933b17864fec081613deab6bcbb317d431663dc9f166a2bd0b677d770a42_w640_q70.webp",
      "contributions": "1. Introduces Milaps, a model-based solution framework for Expected-Time Mobile Search (ETS) that integrates novel auxiliary objectives. 2. Adapts a recent anytime metaheuristic for the traveling deliveryman problem to optimize global routes under tight runtime constraints. 3. Presents evaluations on a novel large-scale dataset demonstrating superior trade-offs between solution quality and runtime compared to state-of-the-art baselines.",
      "summary": "This paper addresses the global route optimization problem for Expected-Time Mobile Search (ETS) in continuous environments. It proposes the Milaps framework, which uses minimum latency problems and an adapted anytime metaheuristic to efficiently plan search routes. The method shows better performance and runtime trade-offs than existing approaches on a new large-scale dataset.",
      "mindmap": "graph LR\n    A[Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search] --> B(核心问题/Problem: Global route optimization for ETS is intractable due to continuous environment and constraints)\n    A --> C(主要方法/Method: Introduces Milaps framework using minimum latency problems and an anytime metaheuristic)\n    A --> D(关键结果/Results: Superior trade-offs between solution quality and runtime on a novel dataset)"
    },
    {
      "title": "Fixed-time control with prescribed performance for path following of underwater gliders",
      "authors": "Hanzhi Yang, Nina Mahmoudian",
      "institution": "Michigan Technological University",
      "link": "https://arxiv.org/pdf/2512.20748",
      "code": null,
      "tags": [
        "control systems",
        "fixed-time control",
        "prescribed performance",
        "sliding mode disturbance observer",
        "underwater gliders",
        "path following"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e021d16f533eb36c3285d4b072d1ffbb64723643d1f22973c4807f8c99eff3d2_w640_q70.webp",
      "contributions": "1. A novel fixed-time prescribed performance controller ensuring tracking errors converge within a fixed time independent of initial conditions. 2. A fixed-time sliding mode disturbance observer for exact finite-time estimation of model uncertainties and environmental disturbances. 3. An integrated control scheme with iLOS guidance enabling robust and accurate 3D path following for underwater gliders.",
      "summary": "This paper proposes a fixed-time prescribed performance control scheme for 3D path following of underwater gliders to handle model uncertainties and environmental disturbances. The method integrates a finite-time performance function with a fixed-time sliding mode disturbance observer and an iLOS guidance law. Simulation results show it outperforms conventional methods in tracking accuracy, convergence speed, and control smoothness.",
      "mindmap": "graph LR\n    A[Fixed-time control with prescribed performance for path following of underwater gliders] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[强洋流与湍流下的水下滑翔机导航安全/Navigation safety of underwater gliders in strong currents and turbulence]\n    C --> C1[固定时间规定性能控制/Fixed-time prescribed performance control]\n    C --> C2[固定时间滑模扰动观测器/Fixed-time sliding mode disturbance observer]\n    C --> C3[集成iLOS制导律/Integrated iLOS guidance law]\n    D --> D1[跟踪误差在固定时间内收敛/Tracking errors converge within a fixed time]\n    D --> D2[仿真中优于传统方法/Outperforms conventional methods in simulations]"
    },
    {
      "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
      "authors": "Moshe Rafaeli Shimron, Vadim Indelman",
      "institution": "Technion - Israel Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20778",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Dec-POMDP",
        "belief inconsistency",
        "limited communication",
        "action consistency",
        "multi-agent planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4538ed8602d7a249e385e29bfc0423ed3f6682a9ac338c82e10822f10bd96df8_w640_q70.webp",
      "contributions": "1. A novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies among agents. 2. Provides probabilistic guarantees for both action consistency and performance relative to a fully-communicating baseline. 3. Introduces a mechanism to selectively trigger communication only when necessary and addresses the decision of whether to share data after action selection to improve inference.",
      "summary": "The paper addresses the problem of multi-agent decision-making under uncertainty when agents have inconsistent beliefs due to limited communication. It proposes a new decentralized framework for Dec-POMDPs that provides performance and action consistency guarantees while minimizing communication. Simulation results demonstrate that the approach outperforms existing state-of-the-art algorithms.",
      "mindmap": "graph LR\n    A[Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs] --> B(核心问题/Problem: Belief Inconsistency & Limited Communication)\n    A --> C(主要方法/Method: Novel Decentralized Framework with Guarantees)\n    A --> D(关键结果/Results: Outperforms SOTA Algorithms)"
    },
    {
      "title": "A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets",
      "authors": "Tanmay P. Patel, Erica L. Tevere, Erik H. Kramer, Rudranarayan M. Mukherjee",
      "institution": "Jet Propulsion Laboratory, California Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20769",
      "code": null,
      "tags": [
        "robotic vision",
        "Extended Kalman Filter",
        "receding-horizon planner",
        "vision-based interception"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28efc9a9f673cd478e9eafde765d4b0dcce3f7e02da154907e1ffc26368fd6d0_w640_q70.webp",
      "contributions": "1. A streamlined, general-purpose framework for autonomous interception adaptable to robots with varying dynamics (UAV, rover, spacecraft). 2. A comprehensive study of the interception problem under conditions of limited observability (partial FOV, dropouts, occlusions) and no global localization. 3. Integration of a relative pose estimator (EKF), a history-conditioned motion predictor, and a real-time receding-horizon convex planner for robust performance.",
      "summary": "This paper presents a general framework for robots to autonomously intercept moving targets using only a monocular camera, without needing global positioning. The method combines relative pose estimation, target motion prediction, and real-time path planning. Experiments on aerial, ground, and spacecraft platforms show the approach is robust, generalizable, and runs efficiently on embedded hardware.",
      "mindmap": "graph LR\n    A[A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets] --> B(核心问题/Problem: 在有限观测与无全局定位下拦截非合作动态目标/Intercept non-cooperative dynamic targets under limited observability & no global localization)\n    A --> C(主要方法/Method: 视觉框架集成EKF、运动预测器与滚动时域规划器/Vision framework integrates EKF, motion predictor & receding-horizon planner)\n    A --> D(关键结果/Results: 在多种机器人平台上验证了鲁棒性、通用性与计算效率/Validated robustness, generalizability & computational efficiency across diverse robotic platforms)"
    },
    {
      "title": "YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion",
      "authors": "Parag Khanna, Karen Jane Dsouza, Chunyu Wang, Mårten Björkman, Christian Smith",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20847",
      "code": null,
      "tags": [
        "human-robot interaction",
        "motion capture",
        "dataset",
        "handover",
        "weight adaptation",
        "YCB dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af98ffe543cfd02847143e303696cbfb3dbc3991e06d43e1e59dda76a0ca8a49_w640_q70.webp",
      "contributions": "1. Introduces the novel YCB-Handovers dataset, capturing 2771 human-human handover motions with varied object weights., 2. Provides an analysis of the impact of object weight on human reaching motion during handovers., 3. Bridges a gap in human-robot collaboration research by enabling data-driven, human-inspired models for weight-sensitive robotic motion planning.",
      "summary": "This paper introduces the YCB-Handovers dataset, a motion capture dataset of human-human handovers with objects of varying weights. The dataset is built upon the YCB object set and aims to provide insights for developing intuitive, weight-adaptive robotic handover motions. The analysis shows that object weight significantly impacts human reaching motion, which can inform more natural and safe robotic handover behaviors.",
      "mindmap": "graph LR\n    A[YCB-Handovers Dataset] --> B(核心问题/Problem: Lack of data on weight impact in handovers for HRI);\n    A --> C(主要方法/Method: Capture human-human handover motions with varied weights);\n    A --> D(关键结果/Results: Dataset & analysis for weight-adaptive robotic motion);"
    },
    {
      "title": "Early warning signals for loss of control",
      "authors": "Jasper J. van Beers, Marten Scheffer, Prashant Solanki, Ingrid A. van de Leemput, Egbert H. van Nes, Coen C. de Visser",
      "institution": "Delft University of Technology, Wageningen University & Research",
      "link": "https://arxiv.org/pdf/2512.20868",
      "code": null,
      "tags": [
        "control theory",
        "resilience engineering",
        "critical slowing down",
        "dynamical indicators",
        "loss of control",
        "early warning signals",
        "feedback systems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/768d446a196a0959349c5e0870e0edad71ddcadf445696f4881d31eeed1d3098_w640_q70.webp",
      "contributions": "1. Proposes a model-free, holistic safety monitor for feedback systems based on dynamical indicators of resilience. 2. Demonstrates the application of the generic phenomenon of \"critical slowing down\" to engineered systems for predicting instability. 3. Validates the approach using drones and suggests broad applicability to reactors, aircraft, and self-driving cars.",
      "summary": "This paper addresses the problem of detecting impending instability (loss of control) in feedback systems, which traditional model-based methods may miss when the system is damaged. It proposes a model-free monitoring method based on the generic phenomenon of \"critical slowing down,\" using it as an early warning signal. The core conclusion is that these dynamical indicators can provide real-time warnings and guide resilient design for a wide class of controlled systems.",
      "mindmap": "graph LR\n    A[Early warning signals for loss of control] --> B[核心问题/Problem: Model-based control fails under system damage]\n    A --> C[主要方法/Method: Model-free monitor using critical slowing down]\n    A --> D[关键结果/Results: Early warning signals validated, applicable to diverse systems]"
    },
    {
      "title": "Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task",
      "authors": "Kanata Suzuki, Shota Shimizu, Tetsuya Ogata",
      "institution": "Not specified in provided content.",
      "link": "https://arxiv.org/pdf/2512.20876",
      "code": null,
      "tags": [
        "robot learning",
        "vision language model",
        "proprioception",
        "task captioning",
        "subtask segmentation",
        "imitation learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0102ecac047d1a1a9b2e14bec98cdce3bc34442aa52215ec414dfed2159bebcd_w640_q70.webp",
      "contributions": "1. Proposes a method to enhance Vision Language Models (VLMs) by incorporating low-level robot motion data (proprioception) for video understanding. 2. Introduces a two-stage captioning approach that generates individual \"scene\" captions and then summarizes them into a full task caption. 3. Performs subtask segmentation by comparing text embeddings of image captions, leveraging the enhanced understanding from motion data.",
      "summary": "This paper addresses the challenge of enabling Vision Language Models (VLMs) to understand robot motion by incorporating proprioceptive data (joint and end-effector states). The proposed method uses this data to generate detailed task captions and perform subtask segmentation from robot task videos. Simulator experiments validate that providing motion information enhances the VLM's performance in these tasks, which can improve robot imitation learning.",
      "mindmap": "graph LR\n    A[Proprioception Enhances VLM in Robot Tasks] --> B(核心问题/Problem: VLMs lack understanding of robot motion from offline data)\n    A --> C(主要方法/Method: Input joint/end-effector states to VLM for caption generation & segmentation)\n    A --> D(关键结果/Results: Enhanced captioning and segmentation performance validated in simulation)"
    },
    {
      "title": "Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms",
      "authors": "Yiding Nie, Dongliang Fan, Jiatai Huang, Chunyu Liu, Jian S. Dai",
      "institution": "Southern University of Science and Technology (Shenzhen, China)",
      "link": "https://arxiv.org/pdf/2512.20888",
      "code": null,
      "tags": [
        "soft robotics",
        "tactile sensing",
        "stretchable optical sensor",
        "continuous spectral-filtering",
        "parallel mechanism trajectory tracking",
        "high spatial resolution",
        "force-position decoupling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24531da8de35dd40b0bb2f80d03204b5835c32af6ae9054191d3d61ee21d03e7_w640_q70.webp",
      "contributions": "1. Proposed a novel stretchable tactile sensor based on a continuous spectral-filtering principle, enabling super-high spatial and force resolution. 2. Demonstrated the sensor's high linearity and robustness under stretching, bending, piercing, and cutting, with design scalability. 3. Integrated the sensor into a planar parallel mechanism for real-time, high-precision trajectory tracking with a rotational resolution of 0.02°.",
      "summary": "The paper addresses the challenge of achieving high spatial resolution and decoupling in stretchable tactile sensors. It proposes a new sensor based on a continuous spectral-filtering principle, which achieves high linearity and resolution even under deformation. The sensor's performance is validated by its successful integration into a parallel mechanism for precise real-time trajectory tracking.",
      "mindmap": "graph LR\n    A[Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms] --> B[核心问题/Problem: 高空间分辨率与自解耦能力/High Spatial Resolution & Self-Decoupling]\n    A --> C[主要方法/Method: 连续光谱滤波原理/Continuous Spectral-Filtering Principle]\n    A --> D[关键结果/Results: 高线性度与分辨率，实时轨迹跟踪/High Linearity & Resolution, Real-time Trajectory Tracking]"
    },
    {
      "title": "Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality",
      "authors": "Baoshan Song, Matthew Giamou, Penggao Yan, Chunxi Xia, Li-Ta Hsu",
      "institution": "The Hong Kong Polytechnic University (inferred from author \"Li-Ta Hsu, Senior member, IEEE\" and common affiliation patterns; other authors' affiliations not explicitly stated in provided text)",
      "link": "https://arxiv.org/pdf/2512.20931",
      "code": "https://github.com/Baoshan-Song/Certifiable-Doppler-alignment",
      "tags": [
        "sensor fusion and navigation",
        "GNSS",
        "frame alignment",
        "convex relaxation",
        "Lagrangian duality",
        "certifiable optimality"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c8f1d855f3e36da72d29a8f11fc4cb03d3b1deecf237451917efbbef693d9b58_w640_q70.webp",
      "contributions": "1. Formulates the GNSS-local frame alignment as a nonconvex QCQP and develops a convex relaxation via Lagrangian duality to achieve global optimality. 2. Performs relaxation tightness and observability analysis to derive criteria for certifiably optimal solutions. 3. Demonstrates robust performance with minimal satellite data (e.g., 2 satellites with Doppler measurements) where prior methods fail.",
      "summary": "This paper addresses the problem of aligning a local sensor frame with a global GNSS frame, which is prone to local minima. The authors propose a certifiable, globally optimal solver by transforming raw GNSS measurements into a convexly relaxed Lagrangian dual problem. Experiments show the method reliably finds optimal solutions even in GNSS-degraded environments with few satellites, outperforming existing techniques.",
      "mindmap": "graph LR\n    A[Certifiable Alignment of GNSS and Local Frames<br/>GNSS与局部坐标系的可认证对齐] --> B(核心问题/Problem: Frame alignment suffers from local minima & satellite dependency<br/>坐标系对齐存在局部最优和卫星依赖问题)\n    A --> C(主要方法/Method: Convex relaxation via Lagrangian duality for certifiable global optimum<br/>通过拉格朗日对偶进行凸松弛以获得可认证的全局最优解)\n    A --> D(关键结果/Results: Works with only 2 satellites, outperforms VOBA & GVINS<br/>仅需2颗卫星即可工作，性能优于VOBA和GVINS)"
    },
    {
      "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
      "authors": "Shuhao Ye, Sitong Mao, Yuxiang Cui, Xuan Yu, Shichao Zhai, Wen Chen, Shunbo Zhou, Rong Xiong, Yue Wang",
      "institution": "Zhejiang University, Huawei Technologies Co., Ltd, Zhejiang Humanoid Robot Innovation Center",
      "link": "https://arxiv.org/pdf/2512.20940",
      "code": "https://github.com/Cepillar/ETP-R1",
      "tags": [
        "vision-language navigation",
        "topological map",
        "reinforcement fine-tuning",
        "GRPO",
        "VLN-CE",
        "Gemini API"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8029b317714b2a8a459b881eeaba1bf8cc335350a443fb795dfc190d0ca8b4_w640_q70.webp",
      "contributions": "1. Constructed a large-scale, high-quality pretraining dataset for topological trajectories using the Gemini API. 2. Introduced a three-stage training paradigm that unifies data from R2R and RxR tasks for joint pretraining. 3. First application of closed-loop, online reinforcement fine-tuning (RFT) to a graph-based VLN-CE model using the GRPO algorithm.",
      "summary": "This paper proposes ETP-R1, a framework that enhances graph-based Vision-Language Navigation in Continuous Environments (VLN-CE) by scaling up data and applying reinforcement fine-tuning. It builds a large pretraining dataset with Gemini, uses a three-stage training process, and applies closed-loop RFT with GRPO. The method achieves state-of-the-art results on R2R-CE and RxR-CE benchmarks.",
      "mindmap": "graph LR\n    A[ETP-R1] --> B[核心问题/Problem: Graph-based VLN-CE methods lag behind LVLM methods in data scaling and training paradigms.]\n    A --> C[主要方法/Method: Scale data via Gemini API, joint pretraining, and apply closed-loop RFT with GRPO.]\n    A --> D[关键结果/Results: Achieves SOTA on R2R-CE and RxR-CE benchmarks.]"
    },
    {
      "title": "From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection",
      "authors": "Jiangen He, Wanqi Zhang, Jessica Barfield",
      "institution": "The University of Tennessee, University of Kentucky",
      "link": "https://arxiv.org/pdf/2512.20951",
      "code": null,
      "tags": [
        "human-robot interaction",
        "racial bias",
        "occupational stereotypes",
        "stereotype priming",
        "skin tone discrimination",
        "anthropomorphism"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e05e8cac9fadcb40ef8a6f45c93ec8405326df2cb55fbb423081a31a9baebd6a_w640_q70.webp",
      "contributions": "1. Demonstrated that human occupational biases and skin-tone-based discrimination directly transfer to robot selection decisions. 2. Revealed distinct, context-dependent patterns of robot preference, with lighter-skinned agents favored in healthcare/education and darker-toned agents in construction/athletics. 3. Showed that exposure to human professionals of specific races can prime and systematically shift subsequent robot preferences in stereotype-consistent directions.",
      "summary": "This paper investigates how societal biases influence human decisions when selecting robots for professional roles. Through two experiments with over 1000 participants, the study found that preferences for robots with different skin tones vary by occupational context and can be primed by exposure to human racial stereotypes. The main conclusion is that robotic deployment risks perpetuating existing social inequalities by inheriting human biases from human-human evaluation contexts.",
      "mindmap": "graph LR\n    A[From Human Bias to Robot Choice<br>从人类偏见到机器人选择] --> B[核心问题/Problem<br>How societal biases influence robot selection<br>社会偏见如何影响机器人选择];\n    A --> C[主要方法/Method<br>Two experiments (N=1038) across four occupational contexts<br>两个实验，涵盖四种职业场景];\n    A --> D[关键结果/Results<br>Bias transfer & context-dependent preferences<br>偏见转移与情境依赖的偏好];"
    },
    {
      "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
      "authors": "Jingyang You, Hanna Kurniawati",
      "institution": "Australian National University",
      "link": "https://arxiv.org/pdf/2512.20974",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Bayesian Reinforcement Learning",
        "Meta-Reinforcement Learning",
        "Generalised Linear Models",
        "Learnable Basis Functions",
        "Variational Inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp",
      "contributions": "1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.",
      "summary": "This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.",
      "mindmap": "graph LR\n    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]\n    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]\n    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]"
    },
    {
      "title": "Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation",
      "authors": "Tian-Ao Ren, Jorge Garcia, Seongheon Hong, Jared Grinberg, Hojung Choi, Julia Di, Hao Li, Dmitry Grinberg, Mark R. Cutkosky",
      "institution": "Stanford University, Symbiokinetics Inc",
      "link": "https://arxiv.org/pdf/2512.20992",
      "code": null,
      "tags": [
        "robotic sensing and manipulation",
        "multimodal sensing",
        "tactile imaging",
        "force-torque sensor",
        "robotic palpation",
        "subsurface feature detection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f076e443ea0d5bb35330eb055b3d2af44e063a02a9d227e1f108a9bf46d7b4a_w640_q70.webp",
      "contributions": "1. Introduces PhysioVisionFT (PVFT), a compact multimodal sensor integrating a high-resolution vision-based tactile dome with a 6-axis force-torque sensor for robust physiotherapy-scale interactions. 2. Demonstrates through experiments on silicone phantoms that tactile images reveal clear structural details of subsurface features where force signals alone are ambiguous. 3. Shows that combining tactile and force modalities enables robust subsurface feature detection while maintaining safe, controlled robotic palpation.",
      "summary": "This paper addresses the problem of unreliable subsurface feature detection in robotic palpation using force sensing alone. The authors propose a compact multimodal sensor, PhysioVisionFT, which combines vision-based tactile imaging with a 6-axis force-torque sensor. Preliminary results show this combined approach enables robust detection of subtle subsurface features while maintaining safe force control for physiotherapy applications.",
      "mindmap": "graph LR\n    A[Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection] --> B(核心问题/Problem: Force sensing is ambiguous for subsurface features in soft tissue)\n    A --> C(主要方法/Method: Integrate vision-based tactile imaging with force-torque sensor)\n    A --> D(关键结果/Results: Tactile images reveal clear structural details; multimodal sensing enables robust detection and control)"
    },
    {
      "title": "Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction",
      "authors": "Cheng-Yu Kuo, Hirofumi Shin, Takamitsu Matsubara",
      "institution": "Nara Institute of Science and Technology, Honda R&D Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21043",
      "code": null,
      "tags": [
        "reinforcement learning",
        "tactile sensing",
        "model-based reinforcement learning",
        "energy abstraction",
        "grasping force control",
        "slip detection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e3063d1f417e52ff4adde8d018be97bb4b3da39e8fbdcf6f72fad61691585a2_w640_q70.webp",
      "contributions": "1. Proposed a novel physics-informed energy abstraction that models an object as a virtual energy container, using the inconsistency between applied power and retained energy as a physically grounded signal for slip-aware stability inference. 2. Developed a model-based learning and planning framework that efficiently models energy dynamics from tactile sensing and performs real-time grasping force optimization using probabilistic Model Predictive Control (pMPC). 3. Demonstrated that the method can learn effective grasping force control from scratch within minutes, reducing slippage and extending grasp duration across diverse scenarios without external sensing or prior object knowledge, validated in both simulation and hardware.",
      "summary": "This paper addresses the challenge of preventing object slippage during robotic manipulation by proposing a tactile-based, learning-driven force control method. The core idea is to abstract the object as an energy container and use a model-based reinforcement learning framework to learn energy-flow dynamics from touch, enabling real-time force optimization. Experiments show the method can learn quickly from scratch and effectively reduce slippage without relying on external vision or prior object models.",
      "mindmap": "graph LR\n    A[论文标题/Paper Title<br>Tracing Energy Flow] --> B[核心问题/Problem<br>动态交互中防滑抓取/Dynamic Grasping with Slippage]\n    A --> C[主要方法/Method<br>触觉能量抽象与模型学习/Tactile Energy Abstraction & Model Learning]\n    A --> D[关键结果/Results<br>快速学习与有效防滑/Fast Learning & Effective Anti-Slip]"
    },
    {
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "authors": "Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.21065",
      "code": null,
      "tags": [
        "robotic grasping",
        "language-guided grasping",
        "cross-modal fusion",
        "coarse-to-fine learning",
        "CLIP embeddings",
        "dynamic convolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical cross-modal fusion pipeline that progressively injects linguistic cues into visual feature reconstruction for fine-grained visual-semantic alignment., 2. Introduces a language-conditioned dynamic convolution head (LDCH) that adaptively mixes convolution experts based on sentence-level features for instruction-adaptive predictions., 3. Presents a final refinement module to enhance grasp consistency and robustness in complex scenes, validated on real robotic platforms.",
      "summary": "This paper proposes LGGD, a language-guided grasp detection method using a coarse-to-fine learning paradigm with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head. It achieves superior performance on benchmark datasets and demonstrates effective real-world robotic manipulation.",
      "mindmap": "graph LR\n    A[Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation] --> B[核心问题/Problem: Weak alignment between language instructions and visual grasp reasoning in cluttered scenes.]\n    A --> C[主要方法/Method: Coarse-to-fine learning with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head (LDCH).]\n    A --> D[关键结果/Results: Outperforms existing methods, shows strong generalization, and is effective on a real robot.]"
    },
    {
      "title": "Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning",
      "authors": "Shlok Deshmukh, Javier Alonso-Mora, Sihao Sun",
      "institution": "Delft University of Technology",
      "link": "https://arxiv.org/pdf/2512.21085",
      "code": null,
      "tags": [
        "reinforcement learning",
        "aerial manipulation",
        "underactuated system",
        "proximal policy optimization (PPO)",
        "incremental nonlinear dynamic inversion (INDI)",
        "sim-to-real transfer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp",
      "contributions": "1. Proposes a reinforcement learning-based whole-body control strategy for a lightweight, underactuated aerial manipulator (DSAM) to achieve full 6-DoF end-effector pose control. 2. Employs a hybrid control architecture where a PPO agent generates high-level feedforward commands, tracked by an INDI attitude controller and a PID joint controller for robustness. 3. Demonstrates robust real-world performance with centimeter/degree-level accuracy under external disturbances like heavy loads and pushing, bridging the sim-to-real gap.",
      "summary": "This paper addresses the control challenge of a lightweight, underactuated aerial manipulator. It uses reinforcement learning (PPO) to train a controller in simulation, which generates commands for a hybrid low-level controller (INDI+PID). Flight experiments show the method achieves precise and robust end-effector pose control, enabling contact-rich tasks with a simple platform.",
      "mindmap": "graph LR\n    A[Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning] --> B(核心问题/Problem: Lightweight, underactuated aerial manipulator control under disturbances)\n    A --> C(主要方法/Method: RL (PPO) for feedforward commands + INDI/PID low-level tracking)\n    A --> D(关键结果/Results: Centimeter/degree-level accuracy, robust to loads/pushing)"
    },
    {
      "title": "Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives",
      "authors": "Chen Liang, Daniel Rakita",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.21109",
      "code": "https://github.com/chen-dylan-liang/mujoco",
      "tags": [
        "others",
        "Model Predictive Control",
        "MuJoCo",
        "Web of Affine Spaces",
        "finite differencing",
        "iLQG"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ada8447964fa0e64d90a6328db6ae0e8cb32dd7609195104d67ce039de9ad6b_w640_q70.webp",
      "contributions": "1. Introduced Web of Affine Spaces (WASP) derivatives as a drop-in replacement for finite differencing in the MuJoCo MPC (MJPC) library to compute derivatives more efficiently and robustly. 2. Demonstrated that WASP integrates seamlessly across diverse robotic tasks, achieves up to 2x speedup over finite differencing, and outperforms stochastic sampling-based planners in MJPC. 3. Released an open-source implementation of MJPC with WASP derivatives fully integrated to support adoption and future research.",
      "summary": "This paper addresses the computational bottleneck of using finite differencing for derivative calculations in MuJoCo-based Model Predictive Control (MPC). The authors propose using Web of Affine Spaces (WASP) derivatives within the MJPC library, which reuses prior derivative information to accelerate and stabilize computations. The method is shown to be robust, integrate seamlessly, and achieve up to a 2x speedup compared to finite differencing while also outperforming sampling-based planners.",
      "mindmap": "graph LR\n        A[Robust and Efficient MuJoCo-based MPC via WASP Derivatives] --> B[核心问题/Problem: Finite Differencing in MJPC is slow and a bottleneck for real-time MPC]\n        A --> C[主要方法/Method: Use Web of Affine Spaces (WASP) derivatives as a drop-in replacement in MJPC]\n        A --> D[关键结果/Results: Up to 2x speedup, robust performance, outperforms sampling planners]"
    },
    {
      "title": "SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation",
      "authors": "Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson",
      "institution": "KTH Royal Institute of Technology, Nanyang Technological University, iFLYTEK Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21133",
      "code": null,
      "tags": [
        "others",
        "sparse graph learning",
        "traffic scene representation",
        "multi-agent trajectory generation",
        "graph neural networks",
        "efficient inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3548cf1020ee1cd5d02c5277e19e837d708b6ef2a3068cbeae46b37bcc824c7e_w640_q70.webp",
      "contributions": "1. Proposes a topology-guided sparse graph construction method that uses lane graph structure instead of distance thresholds to create efficient and interpretable scene graphs. 2. Introduces a lightweight graph encoder for efficiently aggregating agent-map and agent-agent interactions, leading to compact scene representations. 3. Demonstrates superior scalability and efficiency, capable of generating trajectories for thousands of agents and lanes with low latency and GPU memory usage.",
      "summary": "The paper proposes SparScene, a sparse graph learning framework for efficient traffic scene representation in multi-agent trajectory generation. It constructs sparse, topology-aware graphs between agents and lanes and uses a lightweight encoder, achieving competitive performance on the Waymo dataset with significantly improved inference speed and scalability for large scenes.",
      "mindmap": "graph LR\n    A[SparScene] --> B[核心问题/Problem: 密集图效率低，限制大规模场景扩展/Dense graphs are inefficient, limiting scalability to large scenes]\n    A --> C[主要方法/Method: 基于车道拓扑的稀疏图学习/Sparse graph learning guided by lane topology]\n    A --> D[关键结果/Results: 高推理效率与可扩展性/High inference efficiency and scalability]"
    },
    {
      "title": "Flocking phase transition and threat responses in bio-inspired autonomous drone swarms",
      "authors": "Matthieu Verdoucq, Dari Trendafilov, Clément Sire, Ramón Escobedo, Guy Theraulaz, Gautier Hattenberger",
      "institution": "École Nationale de l'Aviation Civile (ENAC), Centre National de la Recherche Scientifique (CNRS), Université de Toulouse",
      "link": "https://arxiv.org/pdf/2512.21196",
      "code": null,
      "tags": [
        "swarm robotics",
        "flocking algorithm",
        "phase transition",
        "collective behavior",
        "drone swarm",
        "bio-inspired"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbbec1c0bb405202136a9915b066a8dfdef5d5347b2c64b21f6dedcc0e2f35e_w640_q70.webp",
      "contributions": "1. Developed a minimal, bio-inspired 3D flocking algorithm for drones based solely on local alignment and attraction cues. 2. Mapped a phase diagram by tuning interaction gains, identifying a critical transition region that maximizes collective responsiveness and reorganization capacity. 3. Demonstrated through outdoor experiments and simulations that operating near this critical transition enhances the swarm's resilience and ability to perform rapid collective maneuvers in response to threats.",
      "summary": "This paper presents a bio-inspired flocking algorithm for drone swarms using minimal local interaction rules. By tuning the alignment and attraction gains, the authors identify a critical phase transition region where the swarm's responsiveness peaks. Outdoor experiments with ten drones show that operating near this transition enables rapid, resilient collective responses to external disturbances like intruders.",
      "mindmap": "graph LR\n    A[Flocking phase transition and threat responses in bio-inspired autonomous drone swarms] --> B(核心问题/Problem: How to design resilient and responsive autonomous drone swarms?);\n    A --> C(主要方法/Method: Bio-inspired 3D flocking algorithm with minimal local interactions);\n    A --> D(关键结果/Results: Phase diagram with critical transition region enhances swarm responsiveness and resilience);"
    },
    {
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": "Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu",
      "institution": "Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.21201",
      "code": "https://heyu322.github.io/Schrodinger-Navigator.github.io/",
      "tags": [
        "robot navigation",
        "zero-shot object navigation",
        "trajectory-conditioned 3D imagination",
        "occlusion-aware planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp",
      "contributions": "1. Proposed Schrödinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.",
      "summary": "The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.",
      "mindmap": "graph LR\n        A[Schrödinger's Navigator] --> B[核心问题/Problem: ZSON struggles with occlusions & uncertainty]\n        A --> C[主要方法/Method: Trajectory-conditioned 3D imagination of futures]\n        A --> D[关键结果/Results: Outperforms baselines on real robot]"
    },
    {
      "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
      "authors": "Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu",
      "institution": "Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University",
      "link": "https://arxiv.org/pdf/2512.21220",
      "code": null,
      "tags": [
        "agent system",
        "runtime safety guardrail",
        "executable safety logic",
        "hybrid reasoning",
        "temporal safety predicate",
        "context-aware safety predicate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp",
      "contributions": "1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.",
      "summary": "The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.",
      "mindmap": "graph LR\n        A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --> B[核心问题/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]\n        A --> C[主要方法/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]\n        A --> D[关键结果/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]"
    },
    {
      "title": "Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3",
      "authors": "Muhtadin, Faris Rafi Pramana, Dion Hayu Fandiantoro, Moh Ismarintan Zazuli, Atar Fuady Babgei",
      "institution": "Institut Teknologi Sepuluh Nopember, Kumamoto University, Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21219",
      "code": null,
      "tags": [
        "robotics",
        "Center of Pressure",
        "PID control",
        "load cell",
        "ESP32-C3",
        "wireless feedback"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbae0a88bb642f47494a4135950d07297e352dc45ffe7383ebb0a32e065f48fe_w640_q70.webp",
      "contributions": "1. Designed a wireless embedded balance system using ESP32-C3 and custom foot units with load cells to estimate Center of Pressure (CoP) in real-time, reducing wiring complexity. 2. Implemented a PID control strategy that uses CoP feedback to adjust torso, hip, and ankle roll joints for stability on uneven surfaces. 3. Demonstrated high sensor precision (14.8g avg error) and a 100% success rate in balance maintenance during single-leg lifting tasks on a 3-degree incline.",
      "summary": "This paper proposes a wireless balance control system for a humanoid robot using load cells and an ESP32-C3 to estimate the Center of Pressure (CoP). The CoP data is sent wirelessly to a main controller which uses a PID strategy to adjust joints for stability. The system achieved a 100% success rate in balance tests, validating wireless CoP feedback for enhancing robot stability without restricting movement.",
      "mindmap": "graph LR\n    A[Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3] --> B[核心问题/Problem: Humanoid robot stability during single-support phase, especially on uneven surfaces]\n    A --> C[主要方法/Method: Wireless CoP estimation using load cells & ESP32-C3, PID control for joint adjustment]\n    A --> D[关键结果/Results: 14.8g sensor error, 100% balance success rate at 3-degree inclination]"
    },
    {
      "title": "Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot",
      "authors": "Shuhan Zhang, Tin Lun Lam",
      "institution": "The Chinese University of Hong Kong, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.21226",
      "code": null,
      "tags": [
        "robot localization",
        "ArUco marker recognition",
        "optical flow analysis",
        "IMU data fusion",
        "modular robot",
        "relative localization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a70ca38b5cef5867dcae52114cc8fc7247f94a7a882811060f4c9a4a7e811c_w640_q70.webp",
      "contributions": "1. Design and implementation of a relative localization system specifically for the SnailBot modular robot platform. 2. A unified sensor fusion framework integrating ArUco marker recognition, optical flow analysis, and IMU data processing. 3. A rule-based fusion strategy that ensures robust and accurate real-time positioning in dynamic scenarios.",
      "summary": "This paper addresses the relative localization challenge for collaborative modular robots by proposing a system that fuses data from ArUco markers, optical flow, and an IMU. The rule-based fusion method prioritizes reliable sensor inputs to achieve accurate positioning. Experimental results validate the system's effectiveness and potential for scalable deployment in modular robotic systems.",
      "mindmap": "graph LR\n    A[Relative Localization System Design for SnailBot] --> B(核心问题/Problem: 模块化机器人相对定位/Relative Localization for Modular Robots)\n    A --> C(主要方法/Method: 多传感器融合框架/Multi-sensor Fusion Framework)\n    A --> D(关键结果/Results: 实时鲁棒定位/Real-time Robust Localization)\n    C --> E(ArUco标记识别/ArUco Marker Recognition)\n    C --> F(光流分析/Optical Flow Analysis)\n    C --> G(IMU数据处理/IMU Data Processing)"
    },
    {
      "title": "UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer",
      "authors": "Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu",
      "institution": "Peking University, BeingBeyond",
      "link": "https://arxiv.org/pdf/2512.21233",
      "code": "https://beingbeyond.github.io/UniTacHand/",
      "tags": [
        "robotic manipulation",
        "tactile sensing",
        "representation learning",
        "contrastive learning",
        "skill transfer",
        "dexterous manipulation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22dcfd29747b902c665565a7fffc2c6af468e1f6351afb8f649a1b003f734166_w640_q70.webp",
      "contributions": "1. Proposes a unified spatio-tactile representation (UniTacHand) that projects tactile signals from human hands (via gloves) and robotic hands onto a common 2D surface space using the MANO model. 2. Introduces a contrastive learning method to align human and robotic tactile data into a unified latent space, requiring only 10 minutes of paired data. 3. Enables zero-shot tactile-based policy transfer from human demonstrations to a real robot, improving performance and data efficiency compared to using only robotic data.",
      "summary": "This paper addresses the challenge of transferring dexterous manipulation skills from humans to robots using tactile sensing. It proposes UniTacHand, a method that unifies human and robotic tactile data onto a common spatial representation and aligns them via contrastive learning. This enables zero-shot policy transfer to robots and improves learning efficiency by leveraging low-cost human data.",
      "mindmap": "graph LR\n        A[UniTacHand] --> B[核心问题/Problem: 触觉数据难以对齐/Human-Robot Tactile Data Misalignment]\n        A --> C[主要方法/Method: 统一表示与对比学习/Unified Representation & Contrastive Learning]\n        A --> D[关键结果/Results: 零样本技能迁移与高效学习/Zero-shot Transfer & Efficient Learning]"
    },
    {
      "title": "RoboCade: Gamifying Robot Data Collection",
      "authors": "Suvir Mirchandani, Mia Tang, Jiafei Duan, Jubayer Ibn Hamid, Michael Cho, Dorsa Sadigh",
      "institution": "Stanford University, University of Washington, FrodoBots",
      "link": "https://arxiv.org/pdf/2512.21235",
      "code": "robocade.github.io",
      "tags": [
        "imitation learning",
        "gamification",
        "robot teleoperation",
        "demonstration dataset",
        "co-training",
        "user engagement"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b1541369e8ca95c4d5be308bf96fdc75adbb1d8b4932ac8966850ffa2c0eeff_w640_q70.webp",
      "contributions": "1. Developed RoboCade, a gamified remote teleoperation platform for scalable robot demonstration data collection. 2. Proposed design principles for constructing gamified tasks that align with useful downstream robot manipulation tasks. 3. Demonstrated that data collected via the gamified platform improves robot policy success rates and is rated as more enjoyable by novice users.",
      "summary": "The paper addresses the scalability challenge of collecting robot demonstration data by introducing RoboCade, a gamified remote teleoperation platform. The platform incorporates game-like elements in its interface and task design to make data collection more accessible and engaging for general users. The results show that policies co-trained with this gamified data achieve higher success rates on target tasks, and users find the platform significantly more enjoyable than a standard non-gamified system.",
      "mindmap": "graph LR\n    A[RoboCade: Gamifying Robot Data Collection] --> B[核心问题/Problem: 模仿学习数据收集成本高、过程乏味、规模受限]\n    A --> C[主要方法/Method: 开发游戏化远程遥操作平台，集成视觉反馈、进度条、排行榜等元素]\n    A --> D[关键结果/Results: 提升下游策略成功率(+16-56%)，用户愉悦度提升(+24%)]"
    },
    {
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": "Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21243",
      "code": "https://lookplangraph.github.io/",
      "tags": [
        "embodied ai",
        "scene graph",
        "vision language model",
        "dynamic planning",
        "memory graph",
        "graph augmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp",
      "contributions": "1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.",
      "summary": "The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.",
      "mindmap": "graph LR\n    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];\n    A --> C[主要方法/Method: Dynamic graph update via VLM observation];\n    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];"
    },
    {
      "title": "Quadrupped-Legged Robot Movement Plan Generation using Large Language Model",
      "authors": "Muhtadin, Vincentius Gusti Putu A. B. M., Ahmad Zaini, Mauridhi Hery Purnomo, I Ketut Eddy Purnama, Chastine Fatichah",
      "institution": "Institut Teknologi Sepuluh Nopember (ITS)",
      "link": "https://arxiv.org/pdf/2512.21293",
      "code": null,
      "tags": [
        "agent system",
        "Large Language Model",
        "quadruped robot",
        "ROS navigation",
        "sensor fusion",
        "offloaded inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70190e79023c7f7dad391e0e2059aa027ac578d23c266728acc6d3e205234b01_w640_q70.webp",
      "contributions": "1. A novel distributed control architecture that offloads LLM-based high-level planning to an external server to overcome the computational constraints of a lightweight quadruped robot platform. 2. A system that grounds natural language instructions into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, Odometry). 3. Experimental validation in a structured indoor environment demonstrating over 90% aggregate success rate, proving the feasibility of the offloaded LLM planning approach for real-world deployment.",
      "summary": "This paper proposes a distributed control framework that uses a Large Language Model (LLM) to generate navigation plans for a quadruped robot from natural language commands. The computationally intensive LLM inference is offloaded to an external server, while the robot executes the plans locally using ROS and sensor data. Experiments in indoor environments showed the system is robust, achieving over 90% success rate and validating the approach for intuitive robot control.",
      "mindmap": "graph LR\n    A[Quadrupped-Legged Robot Movement Plan Generation using Large Language Model] --> B(核心问题/Problem: High barrier to entry for robot control)\n    A --> C(主要方法/Method: Offloaded LLM planning + ROS navigation with sensor fusion)\n    A --> D(关键结果/Results: >90% success rate in indoor navigation)"
    },
    {
      "title": "Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study",
      "authors": "Andrew Stirling, Mykola Lukashchuk, Dmitry Bagaev, Wouter Kouw, James R. Forbes",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19855",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9016f998c96e245ed8aaa845a90618946ec0f9528b50b0585f4ee2e13dca70ac_w640_q70.webp",
      "contributions": "",
      "summary": "Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study",
      "mindmap": ""
    },
    {
      "title": "A Class of Axis-Angle Attitude Control Laws for Rotational Systems",
      "authors": "Francisco M. F. R. Gonçalves, Ryan M. Bena, Néstor O. Pérez-Arancibia",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19846",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/052f19775c0d61e53c6124e8478d4e7ee400cc921f19dbc0a3ec62a387c79f86_w640_q70.webp",
      "contributions": "",
      "summary": "A Class of Axis-Angle Attitude Control Laws for Rotational Systems",
      "mindmap": ""
    },
    {
      "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "authors": "Sujan Warnakulasooriya, Andreas Willig, Xiaobing Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19914",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp",
      "contributions": "",
      "summary": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "mindmap": ""
    },
    {
      "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
      "authors": "Sangoh Lee, Sangwoo Mo, Wook-Shin Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20014",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp",
      "contributions": "",
      "summary": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
      "mindmap": ""
    },
    {
      "title": "Learning Skills from Action-Free Videos",
      "authors": "Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20052",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Skills from Action-Free Videos",
      "mindmap": ""
    },
    {
      "title": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing",
      "authors": "Wenzhao Wu, Yahui Tang, Mingfei Cheng, Wenbing Tang, Yuan Zhou, Yang Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20083",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2f6a93b0ca9601c38b73ad3e8c062333303a8f9538415f2546b2a923116462a_w640_q70.webp",
      "contributions": "",
      "summary": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing",
      "mindmap": ""
    },
    {
      "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "authors": "Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20148",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "mindmap": ""
    },
    {
      "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
      "authors": "Xiaofan Wang, Xingyu Gao, Jianlong Fu, Zuolei Li, Dean Fortier, Galen Mullins, Andrey Kolobov, Baining Guo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20166",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cca5d5cc494ac98e7e5083e1b8aeea89e51f1de0242a845197c5e3b3df56b17_w640_q70.webp",
      "contributions": "",
      "summary": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "authors": "Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20188",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas",
      "authors": "Qijun Qin, Ziqi Zhang, Yihan Zhong, Feng Huang, Xikun Liu, Runzhi Hu, Hang Chen, Wei Hu, Dongzhe Su, Jun Zhang, Hoi-Fung Ng, Weisong Wen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20224",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eafa66d44e2633f1e1623b3bfe8d87a949ce8da88e1e0430473ab41769747405_w640_q70.webp",
      "contributions": "",
      "summary": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas",
      "mindmap": ""
    },
    {
      "title": "Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation",
      "authors": "Imtiaz Ur Rehman, Moussa Labbadi, Amine Abadi, Lew Lew Yan Voon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20229",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcb428fb736e259aa4637b3927a792e0cf3fb0c6c2182598360d56c1081362b_w640_q70.webp",
      "contributions": "",
      "summary": "Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation",
      "mindmap": ""
    },
    {
      "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
      "authors": "Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20276",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp",
      "contributions": "",
      "summary": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
      "mindmap": ""
    },
    {
      "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "authors": "Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20299",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp",
      "contributions": "",
      "summary": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "mindmap": ""
    },
    {
      "title": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots",
      "authors": "Katsu Uchiyama, Ryuma Niiyama",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20322",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/358991788a6fb301b2e9eea033e19d4fd9eecadc34cda99479fccc3d85fd1601_w640_q70.webp",
      "contributions": "",
      "summary": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots",
      "mindmap": ""
    },
    {
      "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration",
      "authors": "Hao Wei, Peiji Wang, Qianhao Wang, Tong Qin, Fei Gao, Yulin Si",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20355",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b1319141213b2b28f550b4d7cf9685c47d5f6e12e3ee5a934bec5a817f95189_w640_q70.webp",
      "contributions": "",
      "summary": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration",
      "mindmap": ""
    },
    {
      "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing",
      "authors": "Maulana Bisyir Azhari, Donghun Han, Je In You, Sungjun Park, David Hyunchul Shim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20475",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47bd82e4c97fdc34a8d98cf68ba2bc04d4ee3480b229430db92dd47b53b90e1_w640_q70.webp",
      "contributions": "",
      "summary": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing",
      "mindmap": ""
    },
    {
      "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20563",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",
      "contributions": "",
      "summary": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "mindmap": ""
    },
    {
      "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing",
      "authors": "Changyi Lin, Boda Huo, Mingyang Yu, Emily Ruppel, Bingqing Chen, Jonathan Francis, Ding Zhao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20591",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e6d8b8d3a87a06671a80e30688f46eb142134f0545d66682f305f4219edec9_w640_q70.webp",
      "contributions": "",
      "summary": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing",
      "mindmap": ""
    },
    {
      "title": "Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator",
      "authors": "Chanchan Xu, Shuai Dong, Xiaojie Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20342",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373ffac6f0235699aed45ecfc76ef062a0f91e5868ec23a1feee94ffde2e1eed_w640_q70.webp",
      "contributions": "",
      "summary": "Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator",
      "mindmap": ""
    },
    {
      "title": "Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms",
      "authors": "Georg Schildbach",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20391",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1dffba56daf9a83e73971cbad3df8a558f4f8bd39358567cc9675935db77fa4_w640_q70.webp",
      "contributions": "",
      "summary": "Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms",
      "mindmap": ""
    },
    {
      "title": "Untethered thin dielectric elastomer actuated soft robot",
      "authors": "Xi Wang, Jing Liu, Siqian Li, Hengtai Dai, Jung-Che Chang, Adam Rushworth, Xin Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17940",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/147e184fb3d08f337e1d702b64cecb4a8209fe095f79478259483975e982eb39_w640_q70.webp",
      "contributions": "",
      "summary": "Untethered thin dielectric elastomer actuated soft robot",
      "mindmap": ""
    },
    {
      "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "authors": "Farida Mohsen, Ali Safa",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17958",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp",
      "contributions": "",
      "summary": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "mindmap": ""
    },
    {
      "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "authors": "Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18007",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp",
      "contributions": "",
      "summary": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "mindmap": ""
    },
    {
      "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "authors": "Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18028",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp",
      "contributions": "",
      "summary": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "mindmap": ""
    },
    {
      "title": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots",
      "authors": "Jacqueline Borgstedt, Jake Bhattacharyya, Matteo Iovino, Frank E. Pollick, Stephen Brewster",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18032",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp",
      "contributions": "",
      "summary": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots",
      "mindmap": ""
    },
    {
      "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
      "authors": "Qianwei Wang, Bowen Li, Zhanpeng Luo, Yifan Xu, Alexander Gray, Tom Silver, Sebastian Scherer, Katia Sycara, Yaqi Xie",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17992",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a6ef8216b049bb762fabd2e4dcf908ec9b4a2cc0dfdfed0068704f85aa89e8a_w640_q70.webp",
      "contributions": "",
      "summary": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
      "mindmap": ""
    },
    {
      "title": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications",
      "authors": "Nidhi Malhotra, Amber K. Rothe, Revanth Konda, Jaydev P. Desai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18048",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb51c5c2d914d049ca0f0ea5fe80a670a7185b2b3b169a2816bc271d98f67cde_w640_q70.webp",
      "contributions": "",
      "summary": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications",
      "mindmap": ""
    },
    {
      "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning",
      "authors": "Juo-Tung Chen, XinHao Chen, Ji Woong Kim, Paul Maria Scheikl, Richard Jaepyeong Cha, Axel Krieger",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18068",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afad84fb7ac4ae6f6bdc263133a9e2d4acce99d04476e05fcc665011a0948574_w640_q70.webp",
      "contributions": "",
      "summary": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning",
      "mindmap": ""
    },
    {
      "title": "Towards Autonomous Navigation in Endovascular Interventions",
      "authors": "Tudor Jianu, Anh Nguyen, Sebastiano Fichera, Pierre Berthet-Rayne",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18081",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Autonomous Navigation in Endovascular Interventions",
      "mindmap": ""
    },
    {
      "title": "On Swarm Leader Identification using Probing Policies",
      "authors": "Stergios E. Bachoumas, Panagiotis Artemiadis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18146",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp",
      "contributions": "",
      "summary": "On Swarm Leader Identification using Probing Policies",
      "mindmap": ""
    },
    {
      "title": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination",
      "authors": "Trevor Stepp, Parthan Olikkal, Ramana Vinjamuri, Rajasekhar Anguluri",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18206",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16675ffdd8bab8dfbdd7e63041cd5c55425ee187db948a8a16692b6b569f0516_w640_q70.webp",
      "contributions": "",
      "summary": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination",
      "mindmap": ""
    },
    {
      "title": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization",
      "authors": "Wu-Te Yang, Masayoshi Tomizuka",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18213",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dc328a115226b3e0e0e21126d10a6edacc4f54483536e9b10d3091f0de254b_w640_q70.webp",
      "contributions": "",
      "summary": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization",
      "mindmap": ""
    },
    {
      "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning",
      "authors": "Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18211",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp",
      "contributions": "",
      "summary": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning",
      "mindmap": ""
    },
    {
      "title": "Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction",
      "authors": "Shahram Najam Syed, Yitian Hu, Yuchao Yao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18237",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp",
      "contributions": "",
      "summary": "Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction",
      "mindmap": ""
    },
    {
      "title": "On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region",
      "authors": "Si Wei Feng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18268",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d836c87e7156449cac14d8856f6ca606bb4ddeea577e006c464614eac364fe2d_w640_q70.webp",
      "contributions": "",
      "summary": "On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region",
      "mindmap": ""
    },
    {
      "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18333",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp",
      "contributions": "",
      "summary": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "mindmap": ""
    },
    {
      "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18336",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp",
      "contributions": "",
      "summary": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "mindmap": ""
    },
    {
      "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation",
      "authors": "Yihang Zhu, Weiqing Wang, Shijie Wu, Ye Shi, Jingya Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18368",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/004564326e17c1e67b90e1c7ff4c2148031c0e38d68983ac3a5150236f6fa495_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
      "authors": "Yulu Wu, Jiujun Cheng, Haowen Wang, Dengyang Suo, Pei Ren, Qichao Mao, Shangce Gao, Yakun Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18396",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336a4848582e30bb097fa913acf8a86a750fe8f43097ffae12c485a2ff45723d_w640_q70.webp",
      "contributions": "",
      "summary": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
      "mindmap": ""
    },
    {
      "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark",
      "authors": "Dmytro Kuzmenko, Nadiya Shvai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18474",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp",
      "contributions": "",
      "summary": "When Robots Say No: The Empathic Ethical Disobedience Benchmark",
      "mindmap": ""
    },
    {
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "authors": "Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18477",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp",
      "contributions": "",
      "summary": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators",
      "authors": "Erdao Liang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18537",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c63a2c59e4cb34d27c44febba963f6d35bc48277e311a14bca062e28fbe5aee_w640_q70.webp",
      "contributions": "",
      "summary": "Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators",
      "mindmap": ""
    },
    {
      "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "authors": "Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18583",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp",
      "contributions": "",
      "summary": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "authors": "Zhenhao Zhou, Dan Negrut",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18619",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp",
      "contributions": "",
      "summary": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "mindmap": ""
    },
    {
      "title": "Offline Reinforcement Learning for End-to-End Autonomous Driving",
      "authors": "Chihiro Noguchi, Takaki Yamamoto",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18662",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp",
      "contributions": "",
      "summary": "Offline Reinforcement Learning for End-to-End Autonomous Driving",
      "mindmap": ""
    },
    {
      "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "authors": "Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18640",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp",
      "contributions": "",
      "summary": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "mindmap": ""
    },
    {
      "title": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles",
      "authors": "Cailin Lei, Haiyang Wu, Yuxiong Ji, Xiaoyu Cai, Yuchuan Du",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18703",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp",
      "contributions": "",
      "summary": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles",
      "mindmap": ""
    },
    {
      "title": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics",
      "authors": "Maozeng Zhang, Ke Shi, Huijun Li, Tongshu Chen, Jiejun Yan, Aiguo Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18712",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/788f7143f635c52e41543f64d13fef2124889bb6d62b5ae4656ad27e5705081b_w640_q70.webp",
      "contributions": "",
      "summary": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics",
      "mindmap": ""
    },
    {
      "title": "Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes",
      "authors": "Jingjia Teng, Yang Li, Jianqiang Wang, Yingbai Hu, Songyuan Tang, Manjiang Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18836",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a54132191e50063831023e6eae9aa9357ab29b75e7078d8784bd1328c2d0f0_w640_q70.webp",
      "contributions": "",
      "summary": "Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes",
      "mindmap": ""
    },
    {
      "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement",
      "authors": "Feeza Khan Khanzada, Jaerock Kwon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18850",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp",
      "contributions": "",
      "summary": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement",
      "mindmap": ""
    },
    {
      "title": "Construction and deformation of P-hedra using control polylines",
      "authors": "Georg Nawratil",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18869",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c054a6aa06f719232431f4b3e82e7945ca7f3f642dc9e2322fdc9185878ba5_w640_q70.webp",
      "contributions": "",
      "summary": "Construction and deformation of P-hedra using control polylines",
      "mindmap": ""
    },
    {
      "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction",
      "authors": "Tianyuan Liu, Richard Dazeley, Benjamin Champion, Akan Cosgun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18922",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb36c038274937dd40786c8ac78288222c12acc29b7aee73573c5a6331a6145_w640_q70.webp",
      "contributions": "",
      "summary": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction",
      "mindmap": ""
    },
    {
      "title": "Point What You Mean: Visually Grounded Instruction Policy",
      "authors": "Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18933",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp",
      "contributions": "",
      "summary": "Point What You Mean: Visually Grounded Instruction Policy",
      "mindmap": ""
    },
    {
      "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
      "authors": "Yadong Liu, Jianwei Liu, He Liang, Dimitrios Kanoulas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18938",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp",
      "contributions": "",
      "summary": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
      "mindmap": ""
    },
    {
      "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners",
      "authors": "Yanding Yang, Weitao Zhou, Jinhai Wang, Xiaomin Guo, Junze Wen, Xiaolong Liu, Lang Ding, Zheng Fu, Jinyu Miao, Kun Jiang, Diange Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18988",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b25603fac1726ddbcf4804fa919bd28a3ae6d25cea4e62760e5e98ac335e570_w640_q70.webp",
      "contributions": "",
      "summary": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners",
      "mindmap": ""
    },
    {
      "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "authors": "Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18987",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp",
      "contributions": "",
      "summary": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "mindmap": ""
    },
    {
      "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "authors": "Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19021",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp",
      "contributions": "",
      "summary": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "mindmap": ""
    },
    {
      "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
      "authors": "Chao Yang, Yingkai Sun, Peng Ye, Xin Chen, Chong Yu, Tao Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19043",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7fe11e8953d7da559e3d75bda7b4d386de464da3c906331d4f0b2c94d096ac4_w640_q70.webp",
      "contributions": "",
      "summary": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
      "mindmap": ""
    },
    {
      "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "authors": "Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19024",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp",
      "contributions": "",
      "summary": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "mindmap": ""
    },
    {
      "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
      "authors": "Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19083",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp",
      "contributions": "",
      "summary": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
      "mindmap": ""
    },
    {
      "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "authors": "Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19133",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp",
      "contributions": "",
      "summary": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "mindmap": ""
    },
    {
      "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
      "authors": "Jose Gustavo Buenaventura Carreon, Floris Erich, Roman Mykhailyshyn, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19148",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956d6e879c4f50e7adb31c9c246b0ef696f20c3d25b5e10073242ec049305b97_w640_q70.webp",
      "contributions": "",
      "summary": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
      "mindmap": ""
    },
    {
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "authors": "Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19178",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp",
      "contributions": "",
      "summary": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "mindmap": ""
    },
    {
      "title": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements",
      "authors": "Tarek Bouazza, Alessandro Melis, Soulaimane Berkane, Robert Mahony, Tarek Hamel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19245",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp",
      "contributions": "",
      "summary": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements",
      "mindmap": ""
    },
    {
      "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization",
      "authors": "Zhaoyang Liu, Weitao Zhou, Junze Wen, Cheng Jing, Qian Cheng, Kun Jiang, Diange Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19270",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956fe529e5541a0272de8e8183c92a6a5f010136fae9741d64e0049d6c4ea14c_w640_q70.webp",
      "contributions": "",
      "summary": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization",
      "mindmap": ""
    },
    {
      "title": "Translating Flow to Policy via Hindsight Online Imitation",
      "authors": "Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19269",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp",
      "contributions": "",
      "summary": "Translating Flow to Policy via Hindsight Online Imitation",
      "mindmap": ""
    },
    {
      "title": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems",
      "authors": "Longxiang Shao, Ulrich Dahmen, Juergen Rossmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19289",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd64634c88e8117da55124a532a6234cc3d8703ceb30084a311c5e28bb6ba516_w640_q70.webp",
      "contributions": "",
      "summary": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems",
      "mindmap": ""
    },
    {
      "title": "OMP: One-step Meanflow Policy with Directional Alignment",
      "authors": "Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19347",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bebf7d7dda43e94f094030d3f33a13922da836a13a10a2f445b742b579a24684_w640_q70.webp",
      "contributions": "",
      "summary": "OMP: One-step Meanflow Policy with Directional Alignment",
      "mindmap": ""
    },
    {
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "authors": "Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19390",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp",
      "contributions": "",
      "summary": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework",
      "authors": "Philipp L. Kinon, Simon R. Eugster, Peter Betsch",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19408",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2011f29ba413592cebb090b3281d18ea6b552fbd91b93ffc64d7c6a5c901ca5_w640_q70.webp",
      "contributions": "",
      "summary": "Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework",
      "mindmap": ""
    },
    {
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": "Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19402",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp",
      "contributions": "",
      "summary": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "mindmap": ""
    },
    {
      "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "authors": "Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19451",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp",
      "contributions": "",
      "summary": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "mindmap": ""
    },
    {
      "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
      "authors": "Zhenglong Guo, Yiming Zhao, Feng Jiang, Heng Jin, Zongbao Feng, Jianbin Zhou, Siyuan Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19453",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf7556ca61a6911a607e95a1a25dcd238b160d37af7403e1dcec7e54c013afa6_w640_q70.webp",
      "contributions": "",
      "summary": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
      "mindmap": ""
    },
    {
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": "Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19562",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp",
      "contributions": "",
      "summary": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "authors": "Yanliang Huang, Xia Yan, Peiran Yin, Zhenduo Zhang, Zeyan Shao, Youran Wang, Haoliang Huang, Matthias Althoff",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19564",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp",
      "contributions": "",
      "summary": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "mindmap": ""
    },
    {
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": "Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19576",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp",
      "contributions": "",
      "summary": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "mindmap": ""
    },
    {
      "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry",
      "authors": "Carlos Pérez-Ruiz, Joan Solà",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19567",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca49f17edae17bf6ccc3e14c30cbfe1362d605146c63dbbff4a97cc1a226407_w640_q70.webp",
      "contributions": "",
      "summary": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry",
      "mindmap": ""
    },
    {
      "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
      "authors": "Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19583",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
      "mindmap": ""
    },
    {
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": "Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19629",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp",
      "contributions": "",
      "summary": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "mindmap": ""
    },
    {
      "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "authors": "Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19684",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp",
      "contributions": "",
      "summary": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "mindmap": ""
    },
    {
      "title": "Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics",
      "authors": "Sun-Hyun Youn",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18320",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb9a9476c629b61f7574cea9ece28357bfe67bff74e75f1e094face2582eef9c_w640_q70.webp",
      "contributions": "",
      "summary": "Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics",
      "mindmap": ""
    },
    {
      "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
      "authors": "Devi Yuliarti, Ravi Prakash, Hiu Ching Cheung, Amy Strong, Patrick J. Codd, Shan Lin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19010",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a46f8c0e00e5fc7cc3fa225c9fc8a67e630549137470ebd50de3e6bd418c8a5b_w640_q70.webp",
      "contributions": "",
      "summary": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
      "mindmap": ""
    },
    {
      "title": "Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making",
      "authors": "Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman",
      "institution": "Toyota Research Institute",
      "link": "https://arxiv.org/pdf/2512.17091",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reinforcement learning",
        "model predictive control",
        "MPPI",
        "hierarchical planning",
        "adaptive sampling"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.",
      "mindmap": ""
    },
    {
      "title": "DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations",
      "authors": "Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz",
      "institution": "Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.17129",
      "code": null,
      "tags": [
        "others",
        "differentiable simulation",
        "graph neural network",
        "SE(3)-equivariance",
        "attention mechanism",
        "3D Zernike polynomials",
        "shape-matching loss",
        "implicit differentiation",
        "bilevel optimization"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues.",
      "mindmap": ""
    },
    {
      "title": "Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors",
      "authors": "Maher Mesto, Francisco Cruz",
      "institution": "University of New South Wales, Universidad Central de Chile",
      "link": "https://arxiv.org/pdf/2512.17180",
      "code": null,
      "tags": [
        "reinforcement learning",
        "interactive reinforcement learning",
        "multi-teacher learning",
        "Q-learning",
        "teacher selection",
        "concept drift"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.",
      "mindmap": ""
    },
    {
      "title": "Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines",
      "authors": "Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang",
      "institution": "Peking University",
      "link": "https://arxiv.org/pdf/2512.17215",
      "code": null,
      "tags": [
        "robotics and navigation",
        "extended kalman filter",
        "inertial measurement unit",
        "wheel odometer",
        "dead reckoning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a dead reckoning method for a self-propelled pipeline robot, using an IMU for initial attitude estimation, refining it with an Extended Kalman Filter, and combining it with wheel odometer data for localization. The method was tested in a rectangular loop pipeline, and the results verified the effectiveness of the proposed algorithm for navigating complex three-dimensional pipelines.",
      "mindmap": ""
    },
    {
      "title": "TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data",
      "authors": "Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Science and Technology Beijing; Chongqing Chang'an Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.17370",
      "code": null,
      "tags": [
        "post-training",
        "imitation learning",
        "dataset aggregation",
        "direct preference optimization",
        "closed-loop evaluation",
        "expert takeover data"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes TakeAD, a framework that fine-tunes a pre-trained imitation learning policy for autonomous driving using expert takeover data. The method combines iterative Dataset Aggregation (DAgger) for imitation with Direct Preference Optimization (DPO) for preference alignment to improve closed-loop performance. Experiments show it effectively mitigates the open-loop gap and outperforms pure imitation learning methods.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry",
      "authors": "Ufuk Asil, Efendi Nasibov",
      "institution": "Dokuz Eylul University",
      "link": "https://arxiv.org/pdf/2512.17505",
      "code": null,
      "tags": [
        "sensor fusion and filtering",
        "Error-State Extended Kalman Filter",
        "Scaled Unscented Kalman Filter",
        "visual-inertial odometry",
        "quaternion estimation",
        "adaptive covariance",
        "loosely coupled architecture"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments.",
      "mindmap": ""
    },
    {
      "title": "Learning Safe Autonomous Driving Policies Using Predictive Safety Representations",
      "authors": "Mahesh Keswani, Raunak Bhattacharyya",
      "institution": "Indian Institute of Technology Delhi",
      "link": "https://arxiv.org/pdf/2512.17586",
      "code": null,
      "tags": [
        "safe reinforcement learning",
        "safe reinforcement learning",
        "predictive safety representations",
        "constrained markov decision processes",
        "waymo open motion dataset",
        "nuplan",
        "srpl"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp",
      "contributions": "",
      "summary": "This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.",
      "mindmap": ""
    },
    {
      "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
      "authors": "Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.17661",
      "code": null,
      "tags": [
        "diffusion inference",
        "video diffusion",
        "autoregressive generation",
        "masked inverse dynamics model",
        "closed-loop control",
        "cross-embodiment pre-training",
        "KV cache"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines.",
      "mindmap": ""
    },
    {
      "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes",
      "authors": "Carlos Vélez García, Miguel Cazorla, Jorge Pomares",
      "institution": "INESCOP, University of Alicante",
      "link": "https://arxiv.org/pdf/2512.17846",
      "code": null,
      "tags": [
        "reinforcement learning",
        "energy-based models",
        "gradient-based refinement",
        "hindsight goal relabeling",
        "latent-space planning"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data.",
      "mindmap": ""
    },
    {
      "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
      "authors": "Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper",
      "institution": "Robotics and AI Institute",
      "link": "https://arxiv.org/pdf/2512.17853",
      "code": null,
      "tags": [
        "multi-modal training",
        "ViPR",
        "ViPR-Eureka",
        "ViPR-RL",
        "behavior cloning",
        "VLM-in-the-loop Parallel Refinement",
        "LLM-guided contact sampling",
        "sim-to-real transfer",
        "GPU simulation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp",
      "contributions": "",
      "summary": "The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks.",
      "mindmap": ""
    },
    {
      "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
      "authors": "Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany",
      "institution": "Technion, MIT, NVIDIA, University of Toronto, Vector Institute",
      "link": "https://arxiv.org/pdf/2512.17897",
      "code": null,
      "tags": [
        "multi-modal inference",
        "diffusion model",
        "bird's-eye-view",
        "radar cross section",
        "Doppler",
        "point cloud generation",
        "foundation models"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp",
      "contributions": "",
      "summary": "RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.",
      "mindmap": ""
    },
    {
      "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
      "authors": "Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa",
      "institution": "UC Berkeley, Sony Group Corporation, Meta",
      "link": "https://arxiv.org/pdf/2512.17900",
      "code": null,
      "tags": [
        "multi-agent motion generation",
        "diffusion forcing",
        "autoregressive diffusion",
        "transformer",
        "multi-agent interaction",
        "denoising"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents.",
      "mindmap": ""
    },
    {
      "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying",
      "authors": "Shaun Baek, Sam Liu, Joseph Ukpong",
      "institution": "Emory University",
      "link": "https://arxiv.org/pdf/2512.15776",
      "code": null,
      "tags": [
        "embodied ai",
        "Asymmetric Assistive Reasoning",
        "active querying",
        "Pull-based protocol",
        "Privileged Information Bias",
        "AI2-THOR",
        "Leader-Follower dyad"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes an Asymmetric Assistive Reasoning framework in AI2-THOR to study how a knowledgeable \"Leader\" agent can guide a sensor-limited \"Follower,\" finding that standard \"Push-based\" instruction fails due to Privileged Information Bias. It demonstrates that a \"Pull-based\" active querying protocol, where the Follower requests clarifications, significantly improves collaborative success by reducing grounding errors.",
      "mindmap": ""
    },
    {
      "title": "Large Video Planner Enables Generalizable Robot Control",
      "authors": "Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du",
      "institution": "MIT, UC Berkeley, Harvard",
      "link": "https://arxiv.org/pdf/2512.15840",
      "code": null,
      "tags": [
        "robotics",
        "foundation models",
        "large-scale video pretraining",
        "generative video planning",
        "vision-language-action (VLA)",
        "zero-shot planning",
        "internet-scale video dataset"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes an alternative paradigm for building robot foundation models by using large-scale video pretraining, rather than extending multimodal LLMs, to create a generative video planner. The model produces zero-shot video plans from novel instructions, which are then post-processed into executable robot actions. The approach demonstrates robust generalization and real-world feasibility in robot control tasks.",
      "mindmap": ""
    },
    {
      "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
      "authors": "Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax",
      "institution": "Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc.",
      "link": "https://arxiv.org/pdf/2512.15940",
      "code": null,
      "tags": [
        "multi-modal inference",
        "retrieval-augmented generation",
        "4D spatio-temporal reasoning",
        "structured memory",
        "vision-language models",
        "embodied AI"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces R4, a training-free framework that equips vision-language models with a structured, lifelong memory by anchoring object-level semantic descriptions in a 4D spatio-temporal database. This enables retrieval-augmented reasoning over space and time for embodied tasks like question answering and navigation. Experiments show that R4 substantially improves reasoning over spatio-temporal information compared to baselines.",
      "mindmap": ""
    },
    {
      "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
      "authors": "Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Hao-Tien Lewis Chiang, Marynel Vázquez",
      "institution": "Yale University, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.16019",
      "code": null,
      "tags": [
        "llm inference",
        "few-shot learning",
        "in-context learning",
        "ablation study",
        "personalized examples"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes using Large Language Models (LLMs) with few-shot in-context learning to predict human perceptions of robot performance in social navigation tasks. The method requires significantly less labeled data than traditional supervised learning models and achieves comparable or better performance. The study also finds that using personalized examples from the same user further improves prediction accuracy.",
      "mindmap": ""
    },
    {
      "title": "Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving",
      "authors": "Jiaheng Geng, Jiatong Du, Xinyu Zhang, Ye Li, Panqu Wang, Yanjun Huang",
      "institution": "Tongji University, ZERON",
      "link": "https://arxiv.org/pdf/2512.16055",
      "code": null,
      "tags": [
        "others",
        "flow matching",
        "adversarial policy",
        "closed-loop evaluation",
        "end-to-end autonomous driving",
        "real-world image generation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a closed-loop evaluation platform for end-to-end autonomous driving that uses a flow matching-based real-world image generator and an adversarial traffic policy to create safety-critical corner cases. The platform efficiently generates realistic driving scenes to test models like UniAD and VAD, demonstrating their performance degradation in adversarial scenarios. This method effectively identifies model weaknesses to improve the safety and robustness of autonomous driving systems.",
      "mindmap": ""
    },
    {
      "title": "Towards Closing the Domain Gap with Event Cameras",
      "authors": "M. Oltan Sevinc, Liao Wu, Francisco Cruz",
      "institution": "University of New South Wales, Universidad Central de Chile",
      "link": "https://arxiv.org/pdf/2512.16178",
      "code": null,
      "tags": [
        "multi-modal inference",
        "event cameras",
        "domain gap",
        "end-to-end driving",
        "neural networks",
        "illumination invariance"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes using event cameras instead of traditional frame-based cameras for end-to-end autonomous driving to address the domain gap caused by day-night lighting differences. The method involves training neural networks on event camera data and evaluating their performance across different lighting conditions. The main conclusion is that event cameras maintain more consistent performance across lighting domains, showing smaller performance degradation and superior baseline performance in cross-domain scenarios compared to grayscale cameras.",
      "mindmap": ""
    },
    {
      "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
      "authors": "Enis Yalcin, Joshua O'Hara, Maria Stamatopoulou, Chengxu Zhou, Dimitrios Kanoulas",
      "institution": "University College London",
      "link": "https://arxiv.org/pdf/2512.16446",
      "code": null,
      "tags": [
        "multi-modal training",
        "reinforcement learning",
        "reward engineering",
        "vision-language models",
        "environment perception",
        "terrain sensor analysis"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces E-SDS, a framework that integrates vision-language models with real-time terrain sensor analysis to automatically generate reward functions for training humanoid locomotion policies. The method uniquely enables robust navigation on complex terrains like stairs, significantly reducing velocity tracking error and manual engineering effort. The main conclusion is that this environment-aware approach produces more capable policies than manual or non-perceptive automated methods.",
      "mindmap": ""
    },
    {
      "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
      "authors": "Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax",
      "institution": "Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc.",
      "link": "https://arxiv.org/pdf/2512.16461",
      "code": null,
      "tags": [
        "multi-modal inference",
        "4D scene graph",
        "spatio-temporal tokenized patch encoding",
        "HDBSCAN clustering",
        "SAM2 segmentation",
        "SLAM"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes SNOW, a training-free framework that integrates Vision-Language Model semantics with 3D point cloud geometry and temporal consistency to build a unified 4D Scene Graph for open-world embodied reasoning. It achieves this through object-level proposals, Spatio-Temporal Tokenized Patch Encoding, and a SLAM backend for spatial grounding. Experiments show that SNOW sets new state-of-the-art performance, demonstrating the importance of structured 4D priors for autonomous robotics.",
      "mindmap": ""
    },
    {
      "title": "Olaf: Bringing an Animated Character to Life in the Physical World",
      "authors": "David Müller, Espen Knoop, Dario Mylonopoulos, Agon Serifi, Michael A. Hopkins, Ruben Grandia, Moritz Bächer",
      "institution": "Disney Research Imagineering",
      "link": "https://arxiv.org/pdf/2512.16705",
      "code": null,
      "tags": [
        "others",
        "reinforcement learning",
        "imitation rewards",
        "thermal actuator modeling",
        "spherical linkages",
        "planar linkages",
        "asymmetric leg design"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper brings the animated character Olaf to life as a physical robot using reinforcement learning guided by animation references for control. Key innovations include a compact mechanical design with hidden asymmetric legs and linkages, and a policy that incorporates actuator temperature inputs to prevent overheating. The approach successfully achieves a high level of believability for the costumed robotic character.",
      "mindmap": ""
    },
    {
      "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
      "authors": "Yixiang Chen, Yan Huang, Keji He, Peiyan Li, Liang Wang",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; FiveAges; Shandong University",
      "link": "https://arxiv.org/pdf/2512.16724",
      "code": null,
      "tags": [
        "multi-modal inference",
        "virtual task-adaptive view",
        "depth-aware module",
        "dynamic coarse-to-fine procedure",
        "3D point cloud",
        "foundation models"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes VERM, a method that uses foundation models to generate a virtual, task-adaptive camera view from a 3D point cloud to reduce visual redundancy in robotic manipulation. This approach, combined with a depth-aware module and a coarse-to-fine procedure, improves efficiency and accuracy. Experiments show it outperforms previous methods while significantly speeding up both training and inference.",
      "mindmap": ""
    },
    {
      "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
      "authors": "Jingjing Qian, Boyao Han, Chen Shi, Lei Xiao, Long Yang, Shaoshuai Shi, Li Jiang",
      "institution": "The Chinese University of Hong Kong, Shenzhen, Hunan University, LiAuto Inc., Voyager Research, Didi Chuxing",
      "link": "https://arxiv.org/pdf/2512.16811",
      "code": null,
      "tags": [
        "multi-modal inference",
        "predictive kinematics",
        "3D Gaussian geometry",
        "trajectory-level prediction",
        "depth-based rendering",
        "continuous-action policy"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes GeoPredict, a geometry-aware Vision-Language-Action framework that enhances robotic manipulation by incorporating predictive kinematic priors (for 3D keypoint trajectories) and predictive 3D Gaussian geometry modules. These modules provide training-time supervision via depth-based rendering, while inference remains lightweight. Experiments show GeoPredict outperforms existing VLA baselines in tasks requiring precise 3D reasoning and spatial awareness.",
      "mindmap": ""
    },
    {
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "authors": "Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang",
      "institution": "MIT, Duke University, Brown University, University of Washington, Harvard University",
      "link": "https://arxiv.org/pdf/2512.16842",
      "code": null,
      "tags": [
        "multi-modal training",
        "tactile sensing",
        "egocentric video",
        "cross-modal alignment",
        "grasp understanding",
        "retrieval benchmarks",
        "classification benchmarks"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces OpenTouch, a multimodal dataset combining synchronized in-the-wild egocentric video, full-hand tactile signals, and hand-pose data. It uses this dataset to create benchmarks for retrieval and classification, demonstrating that tactile information is a powerful cue for understanding grasps and improving cross-modal alignment. The main conclusion is that touch data significantly grounds perception and action, advancing research in multimodal egocentric perception and contact-rich robotics.",
      "mindmap": ""
    },
    {
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": "Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett",
      "institution": "University of Toronto, Vector Institute, Georgia Institute of Technology, NVIDIA Research",
      "link": "https://arxiv.org/pdf/2512.16861",
      "code": null,
      "tags": [
        "others",
        "imitation learning",
        "reinforcement learning",
        "task decomposition",
        "motion planning",
        "data generation",
        "skill policies"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes ReinforceGen, a hybrid system for long-horizon robot manipulation that first decomposes tasks into skills trained via imitation learning on generated data, and then refines them using reinforcement learning. It achieves an 80% success rate on benchmark tasks, with fine-tuning contributing to an 89% average performance increase.",
      "mindmap": ""
    },
    {
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "authors": "Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, Karl Pertsch",
      "institution": "University of Washington, Princeton University, University of California, Berkeley, Stanford University, Toyota Research Institute, University of Southern California, Cornell University, Physical Intelligence",
      "link": "https://arxiv.org/pdf/2512.16881",
      "code": null,
      "tags": [
        "others",
        "neural reconstruction",
        "real-to-sim",
        "co-training",
        "simulation environments",
        "policy evaluation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces PolaRiS, a framework that uses neural reconstruction to convert short video scans of real-world scenes into interactive simulation environments for robot policy evaluation. It also employs a simulation data co-training method to bridge the real-to-sim gap. The authors demonstrate that evaluations in PolaRiS correlate more strongly with real-world performance than existing simulated benchmarks, enabling scalable and democratized testing for generalist robot policies.",
      "mindmap": ""
    },
    {
      "title": "Sceniris: A Fast Procedural Scene Generation Framework",
      "authors": "Jinghuan Shang, Harsh Patel, Ran Gong, Karl Schmeckpeper",
      "institution": "Robotics and AI Institute, University of Waterloo",
      "link": "https://arxiv.org/pdf/2512.16896",
      "code": null,
      "tags": [
        "others",
        "procedural scene generation",
        "batch sampling",
        "GPU acceleration",
        "collision checking",
        "robot reachability",
        "cuRobo"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "Sceniris is a fast procedural scene generation framework that uses batch sampling and GPU-accelerated collision checking via cuRobo to create large-scale, collision-free 3D scenes. It achieves at least 234x speed-up over prior methods and includes optional robot reachability checks for manipulation tasks.",
      "mindmap": ""
    },
    {
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "authors": "Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu",
      "institution": "Meta, University of Washington",
      "link": "https://arxiv.org/pdf/2512.16907",
      "code": null,
      "tags": [
        "multi-modal training",
        "reasoning-to-motion framework",
        "trajectory-token interface",
        "stage-aware trajectory prediction",
        "6DoF trajectory generation",
        "vision-language reasoning",
        "progressive training"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces the EgoMAN dataset and model, a reasoning-to-motion framework that links vision-language reasoning with motion generation via a trajectory-token interface to predict 3D hand trajectories. It demonstrates that progressive training aligning reasoning with motion dynamics yields accurate, stage-aware trajectories with generalization across real-world scenes.",
      "mindmap": ""
    },
    {
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "authors": "Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine",
      "institution": "UC Berkeley, Stanford",
      "link": "https://arxiv.org/pdf/2512.16911",
      "code": null,
      "tags": [
        "reinforcement learning",
        "behavioral cloning",
        "posterior behavioral cloning",
        "policy pretraining",
        "RL finetuning",
        "generative models"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces Posterior Behavioral Cloning (PostBC), a method that pretrains a policy by modeling the posterior distribution of demonstrator behavior from a dataset, rather than exactly matching it. This ensures better coverage of demonstrator actions, which serves as a more effective initialization for subsequent reinforcement learning finetuning. The method is shown to improve RL finetuning performance on robotic control benchmarks and real-world manipulation tasks compared to standard behavioral cloning.",
      "mindmap": ""
    },
    {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "authors": "Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath",
      "institution": "University of California, Berkeley, University of Maryland, College Park, University of Toronto",
      "link": "https://arxiv.org/pdf/2512.16909",
      "code": null,
      "tags": [
        "multi-modal training",
        "scene graph",
        "vision-language model",
        "reinforcement learning",
        "task planning",
        "graph-then-plan",
        "state-aware representation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces MomaGraph, a unified scene graph representation for embodied agents that integrates spatial, functional, and part-level information, and develops a 7B vision-language model (MomaGraph-R1) trained with reinforcement learning to predict task-oriented graphs for planning. The model, evaluated on a new dataset and benchmark, achieves state-of-the-art performance among open-source models for task planning and generalizes to real-robot experiments.",
      "mindmap": ""
    },
    {
      "title": "DVGT: Driving Visual Geometry Transformer",
      "authors": "Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu",
      "institution": "Tsinghua University, Xiaomi EV, University of Macau, Peking University",
      "link": "https://arxiv.org/pdf/2512.16919",
      "code": null,
      "tags": [
        "autonomous driving",
        "3D reconstruction",
        "transformer",
        "DINO backbone",
        "cross-view spatial attention",
        "cross-frame temporal attention",
        "multi-view geometry",
        "point map"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes DVGT, a transformer-based model that reconstructs a global 3D point map from unposed multi-view image sequences using alternating attention mechanisms. It eliminates reliance on precise camera parameters and external sensor alignment, achieving superior performance across diverse driving scenarios.",
      "mindmap": ""
    },
    {
      "title": "SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning",
      "authors": "Tomohito Kawabata, Xinyu Zhang, Ling Xiao",
      "institution": "Hokkaido University",
      "link": "https://arxiv.org/pdf/2512.14757",
      "code": null,
      "tags": [
        "multi-modal inference",
        "mixture-of-experts",
        "reinforcement fine-tuning",
        "semantic similarity reward",
        "vision language model",
        "socially compliant navigation"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant robot navigation, which is fine-tuned using reinforcement learning with a novel semantic similarity reward. The method balances navigation accuracy and computational efficiency by exploring different small language models and vision encoders. Experiments show it outperforms baseline rewards and achieves a good trade-off for real-time deployment.",
      "mindmap": ""
    },
    {
      "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
      "authors": "Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.15047",
      "code": null,
      "tags": [
        "embodied navigation",
        "3D scene graphs",
        "hierarchical traversable graphs",
        "movable obstacles",
        "path planning",
        "scene understanding"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines.",
      "mindmap": ""
    },
    {
      "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization",
      "authors": "Dongmyeong Lee, Jesse Quattrociocchi, Christian Ellis, Rwik Rana, Amanda Adkins, Adam Uccello, Garrett Warnell, Joydeep Biswas",
      "institution": "The University of Texas at Austin, DEVCOM Army Research Laboratory",
      "link": "https://arxiv.org/pdf/2512.15111",
      "code": null,
      "tags": [
        "multi-modal inference",
        "particle filter",
        "bird's-eye-view (BEV)",
        "feature matching",
        "cross-view geo-localization",
        "absolute trajectory error (ATE)"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces BEV-Patch-PF, a sequential geo-localization system that uses a particle filter to match learned bird's-eye-view features from onboard RGB-D images with patches from aerial feature maps. The method significantly outperforms retrieval-based baselines in off-road environments, achieving much lower trajectory error on both seen and unseen routes while running in real-time.",
      "mindmap": ""
    },
    {
      "title": "I am here for you\": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable",
      "authors": "Pilyoung Kim, Yun Xie, Sujin Yang",
      "institution": "University of Denver, Ewha Womans University",
      "link": "https://arxiv.org/pdf/2512.15117",
      "code": null,
      "tags": [
        "human-computer interaction",
        "conversational AI",
        "relational style",
        "transparent style",
        "anthropomorphism",
        "emotional reliance",
        "online experiment",
        "adolescent psychology"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper uses a preregistered online experiment with adolescent-parent dyads to compare how relational versus transparent conversational styles in AI chatbots affect adolescents' perceptions. It finds that a relational style increases anthropomorphism, trust, and emotional closeness, and is especially preferred by socially and emotionally vulnerable adolescents, highlighting a design consideration for youth AI safety.",
      "mindmap": ""
    },
    {
      "title": "Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving",
      "authors": "Jörg Gamerdinger, Sven Teufel, Stephan Amann, Oliver Bringmann",
      "institution": "University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.15181",
      "code": null,
      "tags": [
        "others",
        "criticality metrics",
        "relevance classification",
        "object detection",
        "safety evaluation",
        "bidirectional criticality rating",
        "multi-metric aggregation",
        "DeepAccident dataset"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper analyzes criticality metrics for evaluating the safety of object detection systems in automated driving. It proposes two novel strategies, bidirectional criticality rating and multi-metric aggregation, to improve classification accuracy. The approach demonstrates up to a 100% improvement in criticality classification accuracy, advancing the safety evaluation of automated vehicle perception systems.",
      "mindmap": ""
    },
    {
      "title": "EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving",
      "authors": "Jörg Gamerdinger, Sven Teufel, Stephan Amann, Lukas Marc Listl, Oliver Bringmann",
      "institution": "University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.15195",
      "code": null,
      "tags": [
        "others",
        "environmental perception safety metric",
        "object detection safety",
        "lane detection safety",
        "joint safety assessment",
        "criticality classification",
        "DeepAccident dataset"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a novel Environmental Perception Safety Metric (EPSM) that jointly evaluates the safety of object and lane detection systems for autonomous driving, integrating a lightweight object safety metric and considering task interdependence. It demonstrates that this approach identifies safety-critical errors missed by conventional metrics like precision and recall. The findings emphasize the need for safety-centric evaluation methods in autonomous vehicle perception.",
      "mindmap": ""
    },
    {
      "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
      "authors": "Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Xin Zhou, Fei Gao",
      "institution": "Zhejiang University, Differential Robotics",
      "link": "https://arxiv.org/pdf/2512.15258",
      "code": null,
      "tags": [
        "multi-modal inference",
        "3D Gaussian Splatting",
        "progressive three-stage training",
        "geometric safety correction",
        "onboard deployment optimization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes VLA-AN, an efficient onboard Vision-Language-Action framework for drone navigation. Its core method uses a high-fidelity dataset built with 3D Gaussian Splatting and a three-stage training pipeline, coupled with a lightweight safety-corrected action module. The conclusion is that the framework achieves robust real-time performance and high navigation success rates, providing a practical solution for autonomous aerial robots.",
      "mindmap": ""
    },
    {
      "title": "Remotely Detectable Robot Policy Watermarking",
      "authors": "Michael Amir, Manon Flageat, Amanda Prorok",
      "institution": "University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.15379",
      "code": null,
      "tags": [
        "others",
        "policy watermarking",
        "colored noise coherency",
        "glimpse sequence",
        "remote detection",
        "spectral signal",
        "stochasticity"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot's motions using the policy's inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies.",
      "mindmap": ""
    },
    {
      "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
      "authors": "Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, ingkuan Song, Lianli Gao, Heng Tao Shen",
      "institution": "Tongji University, University of Electronic Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.15411",
      "code": null,
      "tags": [
        "multi-modal training",
        "human-robot mutual imitation",
        "kinematic rules",
        "left/right hand coordinate systems",
        "vision-language-action model",
        "behavioral priors",
        "cross-embodiment generalization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes MiVLA, a vision-language-action model pre-trained using human-robot mutual imitation, which aligns human and robot action spaces via kinematic rules and coordinate systems to integrate behavioral knowledge from human videos and simulated robot data. This approach enhances generalization across different camera views, appearances, and robot embodiments. Experiments on multiple robots show MiVLA outperforms state-of-the-art models in both simulation and real-world control tasks.",
      "mindmap": ""
    },
    {
      "title": "Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry",
      "authors": "Hoang Nguyen, Xiaohao Xu, Xiaonan Huang",
      "institution": "University of Michigan, Ann Arbor",
      "link": "https://arxiv.org/pdf/2512.15423",
      "code": null,
      "tags": [
        "computer vision",
        "monocular depth estimation",
        "3D hallucination",
        "Laplacian-based evaluation",
        "grounded self-distillation",
        "depth foundation models"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a framework to address the \"3D Mirage\" problem in monocular depth estimation, where models hallucinate 3D structures from flat but ambiguous inputs like street art. The method includes a new benchmark (3D-Mirage), a Laplacian-based evaluation with metrics (DCS, CCS), and a parameter-efficient correction technique called Grounded Self-Distillation. The work concludes that depth model evaluation must shift from pixel accuracy to structural robustness to mitigate this safety-critical vulnerability.",
      "mindmap": ""
    },
    {
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "authors": "Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava",
      "institution": "mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.15692",
      "code": null,
      "tags": [
        "multi-modal training",
        "video-action model",
        "flow matching",
        "inverse dynamics model",
        "imitation learning",
        "vision-language-action model"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.",
      "mindmap": ""
    }
  ]
}