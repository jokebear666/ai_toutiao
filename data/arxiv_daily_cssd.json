{
  "label": "cs.SD",
  "slug": "cssd",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
      "authors": "Xiaomi LLM-Core Team, Dong Zhang, Gang Wang, Jinlong Xue, Kai Fang, Liang Zhao, Rui Ma, Shuhuai Ren, Shuo Liu, Tao Guo, Weiji Zhuang, Xin Zhang, Xingchen Song, Yihan Yan, Yongzhe He, Cici, Bowen Shen, Chengxuan Zhu, Chong Ma, Chun Chen, Heyu Chen, Jiawei Li, Lei Li, Menghang Zhu, Peidian Li, Qiying Wang, Sirui Deng, Weimin Xiong, Wenshan Huang, Wenyu Yang, Yilin Jiang, Yixin Yang, Yuanyuan Tian, Yue Ma, Yue Yu, Zihan Zhang, Zihao Yue, Bangjun Xiao, Bingquan Xia, Bofei Gao, Bowen Ye, Can Cai, Chang Liu, Chenhong He, Chunan Li, Dawei Zhu, Duo Zhang, Fengyuan Shi, Guoan Wang, Hailin Zhang, Hanglong Lv, Hanyu Li, Hao Tian, Heng Qu, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianguang Zuo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Linghao Zhang, Meng Chen, Nuo Chen, Peng Zhang, Qianli Chen, Qiantong Wang, Rang Li, Shaohui Liu, Shengfan Wang, Shicheng Li, Shihua Yu, Shijie Cao, Shimao Chen, Shuhao Gu, Weikun Wang, Wenhan Ma, Xiangwei Deng, Xing Yong, Xing Zhang, Xu Wang, Yifan Song, Yihao Zhao, Yingbo Zhao, Yizhao Gao, Yu Cheng, Yu Tu, Yudong Wang, Zhaojun Huang, Zhengju Tang, Zhenru Lin, Zhichao Song, Zhipeng Xu, Zhixian Zheng, Zihan Jiang",
      "institution": "Xiaomi",
      "link": "https://arxiv.org/pdf/2512.23808",
      "code": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "tags": [
        "multi-modal training",
        "audio language model",
        "few-shot learning",
        "instruction tuning",
        "scaling pretraining",
        "speech continuation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb9eb77f0c6bfe175c709a98be8909fd963e33e44e830db2cd023ecf616b62_w640_q70.webp",
      "contributions": "1. Scaled audio language model pretraining to over 100 million hours, demonstrating emergent few-shot learning capabilities across diverse audio tasks. 2. Introduced a systematic evaluation framework and showed that the base model achieves SOTA performance among open-source models on speech intelligence and audio understanding benchmarks, with generalization to unseen tasks. 3. Developed an instruction-tuned variant with a curated corpus and thinking mechanisms, achieving open-source SOTA on multiple audio understanding, spoken dialogue, and TTS benchmarks, rivaling closed-source models.",
      "summary": "The paper proposes MiMo-Audio, a large-scale audio language model pretrained on over 100 million hours of data, which demonstrates emergent few-shot learning capabilities for various audio tasks without task-specific fine-tuning. The instruction-tuned variant further achieves state-of-the-art performance on multiple benchmarks, showing strong generalization in audio understanding and generation.",
      "mindmap": "graph TB\n        Root[”MiMo-Audio: Audio Language Models are Few-Shot Learners”] --> Problem[”核心问题/Problem: Existing audio models require task-specific fine-tuning, lacking human-like generalization.”]\n        Root --> Method[”主要方法/Method: Scale next-token prediction pretraining on 100M+ hours of audio; Use instruction-tuning with thinking mechanisms.”]\n        Root --> Results[”关键结果/Results: Emergent few-shot learning; SOTA open-source performance; Generalizes to unseen tasks like voice conversion.”]"
    },
    {
      "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
      "authors": "Roee Ziv, Raz Lapid, Moshe Sipper",
      "institution": "Ben Gurion University of the Negev, Deepkeep",
      "link": "https://arxiv.org/pdf/2512.23881",
      "code": null,
      "tags": [
        "adversarial attacks",
        "universal adversarial perturbation",
        "latent-space attack",
        "audio-language models",
        "encoder-level vulnerability",
        "targeted attack"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aec73155c062c3c66b8e33c0e7892e17d374292349cfa71e3389850584cf8195_w640_q70.webp",
      "contributions": "1. Proposes a universal targeted latent-space attack against audio-language models, focusing solely on the audio encoder. 2. Introduces an attack method that learns a single perturbation effective across different inputs and speakers, without needing access to the downstream language model. 3. Demonstrates high attack success rates with minimal perceptual distortion on a state-of-the-art model, revealing a critical new attack surface in multimodal systems.",
      "summary": "This paper identifies a security vulnerability in audio-language models where adversarial attacks can be launched by manipulating only the audio encoder's latent representations. The proposed method learns a universal perturbation that forces the model to generate attacker-specified text outputs, and experiments show it is highly effective and stealthy. This reveals a significant and previously underexplored attack surface at the encoder level of multimodal AI systems.",
      "mindmap": "graph TB\n        Root[”Breaking Audio Large Language Models by Attacking Only the Encoder<br>仅攻击编码器来攻破音频大语言模型”] --> Problem[”核心问题/Problem<br>Audio-language models have new security vulnerabilities.<br>音频-语言模型存在新的安全漏洞”]\n        Root --> Method[”主要方法/Method<br>Universal targeted latent-space attack on the encoder.<br>针对编码器的通用目标潜空间攻击”]\n        Root --> Results[”关键结果/Results<br>High attack success with minimal distortion.<br>高攻击成功率，最小感知失真”]"
    },
    {
      "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
      "authors": "Tianxin Xie, Wentao Lei, Guanjie Huang, Pengfei Zhang, Kai Jiang, Chunhui Zhang, Fengji Ma, Haoyu He, Han Zhang, Jiangshan He, Jinting Wang, Linghan Fang, Lufei Gao, Orkesh Ablet, Peihua Zhang, Ruolin Hu, Shengyu Li, Weilin Lin, Xiaoyang Feng, Xinyue Yang, Yan Rong, Yanyun Wang, Zihang Shao, Zelin Zhao, Chenxing Li, Shan Yang, Wenfu Wang, Meng Yu, Dong Yu, Li Liu",
      "institution": "HKUST(GZ), Tencent, Shanghai Jiao Tong University, Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.23994",
      "code": "https://imxtx.github.io/PhyAVBench/",
      "tags": [
        "audio-visual generation",
        "text-to-audio-video",
        "physics-sensitivity",
        "benchmark",
        "audio-physics grounding",
        "contrastive physical response score"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eec12bf437e946a08a88614c7454f1315e7f14f21f2ebe631265de8a77d352a_w640_q70.webp",
      "contributions": "1. Introduces PhyAVBench, a novel benchmark for evaluating the audio physics-sensitivity of T2AV models., 2. Proposes the Audio-Physics Sensitivity Test (APST) paradigm using paired prompts with controlled physical variables., 3. Defines the Contrastive Physical Response Score (CPRS) to quantitatively measure a model's understanding of physical principles.",
      "summary": "The paper identifies that current text-to-audio-video (T2AV) models lack physical plausibility in generated sounds. To address this, it introduces PhyAVBench, a challenging benchmark designed to systematically evaluate models' audio physics grounding through a novel Audio-Physics Sensitivity Test (APST). The authors argue that this benchmark will stimulate progress in generating physically consistent audio-visual content.",
      "mindmap": "graph TB\n        A[PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有T2AV模型无法生成物理合理的声音 / Existing T2AV models generate physically implausible sounds]\n        C --> C1[提出PhyAVBench基准与APST评估范式 / Propose PhyAVBench benchmark & APST evaluation paradigm]\n        C --> C2[使用成对提示控制物理变量 / Use paired prompts with controlled physical variables]\n        C --> C3[引入对比物理响应分数(CPRS) / Introduce Contrastive Physical Response Score (CPRS)]\n        D --> D1[系统性评估模型对声学物理的理解 / Systematically evaluate models' understanding of acoustic physics]\n        D --> D2[推动物理基础T2AV生成的研究 / Stimulate research in physically-grounded T2AV generation]"
    },
    {
      "title": "AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives",
      "authors": "Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang",
      "institution": "Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley",
      "link": "https://arxiv.org/pdf/2512.24052",
      "code": "https://github.com/LLM-VLM-GSL/AHA",
      "tags": [
        "multi-modal reasoning",
        "audio-language models",
        "hallucination mitigation",
        "counterfactual hard negatives",
        "preference alignment",
        "temporal reasoning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp",
      "contributions": "1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models.",
      "summary": "The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors.",
      "mindmap": "graph TB\n        Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Large Audio-Language Models (LALMs) suffer from hallucinations / 大型音频语言模型存在幻觉问题]\n        Method[主要方法/Method] --> M1[Propose AHA framework with counterfactual hard negative mining / 提出AHA框架，使用反事实硬负例挖掘]\n        Method --> M2[Construct preference dataset for alignment / 构建用于对齐的偏好数据集]\n        Results[关键结果/Results] --> R1[13.7% improvement on AHA-Eval benchmark / 在AHA-Eval基准上提升13.7%]\n        Results --> R2[Gains on public benchmarks (MMAU-Test, MMAR) / 在公开基准(MMAU-Test, MMAR)上取得提升]"
    },
    {
      "title": "Environmental Sound Deepfake Detection Challenge: An Overview",
      "authors": "Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Ting Dang",
      "institution": "KAIST, University of Melbourne, Fortemedia Singapore, Xi’an University of Posts & Telecommunications",
      "link": "https://arxiv.org/pdf/2512.24140",
      "code": "https://github.com/apple-yinhan/EnvSDD",
      "tags": [
        "audio deepfake detection",
        "environmental sound",
        "deepfake detection",
        "anti-spoofing",
        "acoustic scene understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7537d7ea96f7125c1cdeae053e793ee0aabbb9a9ca6ce7fceaf6aa3e5967f959_w640_q70.webp",
      "contributions": "1. Introduced EnvSDD, the first large-scale curated dataset for environmental sound deepfake detection. 2. Organized the ICASSP 2026 ESDD Challenge with two novel tracks focusing on robustness to unseen generators and black-box, low-resource detection. 3. Provided a detailed analysis of the challenge results and summarized effective design choices from top-performing systems.",
      "summary": "This paper addresses the problem of detecting AI-generated fake environmental sounds by introducing the EnvSDD dataset and launching the ESDD Challenge. The challenge features two tracks to evaluate model robustness against unseen audio generators and in black-box, low-resource scenarios. The work aims to advance the field by providing a benchmark and analyzing the performance of submitted detection methods.",
      "mindmap": "graph TB\n        Root(”ENVIRONMENTAL SOUND DEEPFAKE DETECTION CHALLENGE: AN OVERVIEW”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”AI生成环境音/AI-generated environmental sounds”)\n        Problem --> P2(”存在滥用风险/Risk of misuse”)\n        Method --> M1(”创建EnvSDD数据集/Create EnvSDD dataset”)\n        Method --> M2(”组织ESDD挑战赛/Organize ESDD Challenge”)\n        Results --> R1(”分析挑战赛结果/Analyze challenge results”)\n        Results --> R2(”总结有效设计/Summarize effective designs”)"
    },
    {
      "title": "AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels",
      "authors": "Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang",
      "institution": "Weill Cornell Medicine",
      "link": "https://arxiv.org/pdf/2512.24628",
      "code": null,
      "tags": [
        "medical audio classification",
        "hierarchical classification",
        "acoustic biomarkers",
        "mel-spectrograms",
        "voice disorders",
        "sustained vowels"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp",
      "contributions": "1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.",
      "summary": "This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework's potential as a scalable tool for early voice disorder screening and diagnostic triage.",
      "mindmap": "graph TB\n        A[AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[良性喉部嗓音疾病分类/Benign Laryngeal Voice Disorder Classification]\n        C --> C1[三级分层机器学习框架/Three-Stage Hierarchical ML Framework]\n        C1 --> C1_1[阶段1: 病理筛查/Stage 1: Pathological Screening]\n        C1 --> C1_2[阶段2: 粗粒度分层/Stage 2: Coarse Stratification]\n        C1 --> C1_3[阶段3: 细粒度分类/Stage 3: Fine-Grained Classification]\n        C1_1 --> C1_1a[融合CNN梅尔谱特征与21种声学生物标志物/Integrates CNN Mel-Spectrogram & 21 Acoustic Biomarkers]\n        D --> D1[性能优于平面多类分类器与预训练模型/Outperforms Flat Classifiers & Pre-trained Models (HuBERT, HeAR)]\n        D --> D2[结合深度表征与可解释特征，增强临床可操作性/Enhances Transparency & Clinical Alignment via Deep & Interpretable Features]"
    },
    {
      "title": "AudioFab: Building A General and Intelligent Audio Factory through Tool Learning",
      "authors": "Cheng Zhu, Jing Han, Qianshuai Xue, Kehan Wang, Huan Zhao, Zixing Zhang",
      "institution": "Hunan University, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.24645",
      "code": "https://github.com/SmileHnu/AudioFab",
      "tags": [
        "agent system",
        "audio agent",
        "tool learning",
        "modular design",
        "few-shot learning",
        "natural language interface"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37da2d211f7f1dcdae67f8b3dfe41860d2730828abf8f568b168e7809aca6b38_w640_q70.webp",
      "contributions": "1. Introduces AudioFab, an open-source, modular agent framework designed to resolve dependency conflicts and simplify tool integration for audio processing. 2. Optimizes tool learning through intelligent tool selection and few-shot learning to improve efficiency and accuracy in complex tasks. 3. Provides a user-friendly natural language interface tailored for non-expert users, aiming to establish an open and intelligent audio-processing ecosystem.",
      "summary": "The paper introduces AudioFab, a general and intelligent agent framework for audio processing. It addresses the fragmentation of audio tools and inefficient collaboration in existing frameworks through a modular design and optimized tool learning. The main conclusion is that AudioFab provides a stable, extensible platform to facilitate future research and development in audio and multimodal AI.",
      "mindmap": "graph TB\n        A[AudioFab: Building A General and Intelligent Audio Factory through Tool Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[音频工具碎片化/Fragmented audio tools]\n        B --> B2[现有框架配置复杂且协作低效/Existing frameworks have complex config & inefficient collaboration]\n        C --> C1[模块化设计解决依赖冲突/Modular design resolves dependency conflicts]\n        C --> C2[智能工具选择与少样本学习优化工具学习/Intelligent tool selection & few-shot learning optimize tool learning]\n        C --> C3[为非专家用户提供自然语言接口/Natural language interface for non-experts]\n        D --> D1[构建开放智能的音频处理生态系统/Builds an open & intelligent audio-processing ecosystem]\n        D --> D2[为未来音频与多模态AI研究提供稳定可扩展平台/Provides a stable & extensible platform for future audio & multimodal AI R&D]"
    },
    {
      "title": "SLM-TTA: A Framework for Test-Time Adaptation of Generative Spoken Language Models",
      "authors": "Yuan-Kuei Wu, Yang Liu, Yiteng Huang, Zhaojun Yang, Haibin Wu, Ruizhe Huang, Yi-Te, Shuyu Kong, Ming Sun, Florian Metze, Li Wan",
      "institution": "National Taiwan University, Meta",
      "link": "https://arxiv.org/pdf/2512.24739",
      "code": null,
      "tags": [
        "llm inference",
        "test-time adaptation",
        "spoken language models",
        "acoustic shift",
        "parameter-efficient fine-tuning",
        "robustness"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8a7defcaf1d981313d94ffcbed71bedbbb3c351330247513f7ac8c831945074_w640_q70.webp",
      "contributions": "1. Introduces the first test-time adaptation (TTA) framework specifically designed for generative Spoken Language Models (SLMs) that process interleaved audio-text prompts. 2. Proposes a method that updates only a small, targeted subset of model parameters during inference using a single incoming utterance, requiring no source data or labels, making it compute- and memory-efficient. 3. Demonstrates consistent performance gains across diverse tasks (ASR, speech translation, audio understanding) under various acoustic corruptions without degrading core task accuracy.",
      "summary": "This paper addresses the performance degradation of generative Spoken Language Models (SLMs) under real-world acoustic shifts like noise and reverberation. It proposes SLM-TTA, a test-time adaptation framework that efficiently updates a small subset of model parameters using only the incoming test utterance, improving robustness without extra data or labels. The method shows consistent gains across multiple speech tasks and is efficient enough for resource-constrained deployment.",
      "mindmap": "graph TB\n        Root[SLM-TTA: A Framework for Test-Time Adaptation of Generative Spoken Language Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: SLM性能在声学偏移下下降/SLM performance degrades under acoustic shift]\n        Method[主要方法/Method: 仅用测试语音更新少量参数的TTA框架/TTA framework updating few parameters with test utterance]\n        Results[关键结果/Results: 鲁棒性提升且高效/Robustness gains and efficiency]"
    },
    {
      "title": "Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification",
      "authors": "Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.22148",
      "code": "https://github.com/sadPororo/LAP",
      "tags": [
        "speaker verification",
        "Layer Attentive Pooling",
        "Attentive Statistical Temporal Pooling",
        "pre-trained speech models",
        "multi-level features",
        "speaker embeddings"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp",
      "contributions": "1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.",
      "summary": "The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.",
      "mindmap": "graph TB\n        Root[重新思考利用预训练多层表示进行说话人验证<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>静态加权平均聚合多层特征的局限性<br/>Limitations of static weighted average for multi-layer feature aggregation] --> Problem_Detail[细节/Detail<br/>未充分利用高层表示<br/>Underutilization of high-level representations]\n        Method[主要方法/Method<br/>提出层注意力池化<br/>Propose Layer Attentive Pooling (LAP)] --> Method_Detail1[细节/Detail<br/>动态多视角评估层重要性<br/>Time-dynamically assess layer significance from multiple perspectives]\n        Method --> Method_Detail2[细节/Detail<br/>使用最大池化而非平均<br/>Employ max pooling instead of averaging]\n        Method --> Method_Detail3[细节/Detail<br/>轻量级后端模型 (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]\n        Results[关键结果/Results<br/>在VoxCeleb上达到SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --> Results_Detail[细节/Detail<br/>性能优越且大幅减少训练时间<br/>Superior performance and greatly reduced training time]"
    },
    {
      "title": "A Robust framework for sound event localization and detection on real recordings",
      "authors": "Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.22156",
      "code": null,
      "tags": [
        "audio event detection",
        "SELD",
        "data augmentation",
        "test time augmentation",
        "ensemble",
        "ACCDOA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12addf93fd64ffb5001f18188632e3d13c2b6e99ad52d2339022072d1c08696c_w640_q70.webp",
      "contributions": "1. A robust training framework for SELD that mixes real recordings and emulated data with augmentation to improve generalization on real-world scenes. 2. A test time augmentation and clustering-based ensemble method to aggregate confident predictions and reject abnormal ones. 3. Application of the framework to a ResNet-based model, achieving competitive performance on the DCASE2022 challenge task.",
      "summary": "This paper proposes a robust framework for sound event localization and detection (SELD) to improve performance on real-world recordings. The method combines data augmentation, a pipeline mixing real and emulated datasets, and a test-time clustering ensemble. Experimental results show it outperforms baselines and achieves competitive performance in the DCASE2022 challenge.",
      "mindmap": "graph TB\n    A[A Robust framework for sound event localization and detection on real recordings] --> B(核心问题/Problem: SELD generalization on real-world recordings)\n    A --> C(主要方法/Method: ResNet-based model with augmentation, real+emulated data mixing, test time ensemble)\n    A --> D(关键结果/Results: Outperforms baseline, achieves competitive performance)"
    },
    {
      "title": "Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation",
      "authors": "Xuanfan Ni, Fei Yang, Fengping Tian, Qingjuan Li, Chenyang Lyu, Yichao Du, Longyue Wang, Weihua Luo, Kaifu Zhang",
      "institution": "Alibaba International Digital Commerce",
      "link": "https://arxiv.org/pdf/2512.22165",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "domain adaptation",
        "fine-tuning",
        "learning rate optimization",
        "data augmentation",
        "word error rate"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edef09334bdd6f2c438c4ea7c62b2c8a134ebb61ef00ffed51f52a270a2fa141_w640_q70.webp",
      "contributions": "1. A principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. 2. A learning rate optimization strategy based on performance metrics (e.g., WER) rather than just loss, to prevent overfitting and instability. 3. A domain-specific data analysis and augmentation pipeline to address data mismatch and linguistic variability.",
      "summary": "This paper proposes a principled fine-tuning framework to adapt large-scale ASR models like Whisper and Qwen2-Audio to specialized domains. The framework uses metric-driven learning rate optimization and domain-specific data augmentation. Empirical results validate the framework and establish practical protocols for improving domain-specific performance while preventing overfitting.",
      "mindmap": "graph TB\n        Root[”MARCO-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation”] --> Problem[”核心问题/Problem: ASR模型在领域特定应用中性能下降/Degraded ASR performance in domain-specific applications”]\n        Root --> Method[”主要方法/Method: 基于指标的微调框架/Metric-driven fine-tuning framework with learning rate optimization & data augmentation”]\n        Root --> Results[”关键结果/Results: 框架验证与性能提升/Validated framework and improved performance while preventing overfitting”]"
    },
    {
      "title": "AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation",
      "authors": "HaeChun Chung",
      "institution": "KT Corporation",
      "link": "https://arxiv.org/pdf/2512.22166",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Generative Adversarial Networks (GANs)",
        "Single-Double-Triple (SDT) Attention",
        "Time-Frequency Cross-Attention (TF-CA)",
        "contrastive losses",
        "real-time generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb3eeaa0b35c7e10c5a5530b703d70eeb9b9309a57df9a97ae1234ec0c25981_w640_q70.webp",
      "contributions": "1. Proposes AudioGAN, the first successful GAN-based framework for text-to-audio generation, enabling single-pass inference. 2. Introduces novel architectural components, Single-Double-Triple (SDT) Attention and Time-Frequency Cross-Attention (TF-CA), to enhance model capability. 3. Integrates multiple contrastive losses to overcome the inherent training difficulties of GANs, improving stability and performance.",
      "summary": "This paper introduces AudioGAN, a GAN-based framework for efficient text-to-audio generation. It overcomes GAN training challenges with novel attention mechanisms and contrastive losses, achieving state-of-the-art results with 90% fewer parameters and 20x faster inference than diffusion models.",
      "mindmap": "graph TB\n        Root[”AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation”] --> Problem[”核心问题/Problem: Diffusion-based TTA models are slow and computationally expensive”]\n        Root --> Method[”主要方法/Method: First successful GAN-based TTA framework with SDT Attention, TF-CA, and contrastive losses”]\n        Root --> Results[”关键结果/Results: SOTA performance, 90% fewer parameters, 20x faster inference (<1 second)”]"
    },
    {
      "title": "Chord Recognition with Deep Learning",
      "authors": "Pierre Mackenzie",
      "institution": "University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.22621",
      "code": null,
      "tags": [
        "music information retrieval",
        "chord recognition",
        "deep learning",
        "generative models",
        "pitch augmentation",
        "beat detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field.",
      "summary": "This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.",
      "mindmap": "graph TB\n        Root(Chord Recognition with Deep Learning) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(进展缓慢/Slow Progress)\n        Method --> M1(实验现有方法/Experiment with Existing Methods)\n        Method --> M2(测试生成模型假设/Test Generative Model Hypotheses)\n        Results --> R1(罕见和弦表现差/Poor Performance on Rare Chords)\n        Results --> R2(音高增强提升准确率/Pitch Augmentation Boosts Accuracy)\n        Results --> R3(节拍检测提升可解释性/Beat Detection Improves Interpretability)"
    },
    {
      "title": "Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study",
      "authors": "Saifelden M. Ismail",
      "institution": "University of Science and Technology, Zewail City",
      "link": "https://arxiv.org/pdf/2512.23435",
      "code": null,
      "tags": [
        "on-device ai",
        "DistilHuBERT",
        "8-bit quantization",
        "cross-corpus validation",
        "Leave-One-Session-Out (LOSO)",
        "model compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp",
      "contributions": "1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a \"theatricality effect\" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.",
      "summary": "This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.",
      "mindmap": "graph TB\n        A[”Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study”] --> B[”核心问题/Problem: SER部署受限于大模型的计算需求/SER deployment constrained by computational demands of large models”]\n        A --> C[”主要方法/Method: 使用蒸馏与8位量化的DistilHuBERT，并进行跨语料库训练/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training”]\n        A --> D[”关键结果/Results: 模型仅23MB，精度达基准91%，跨语料库训练提升泛化性/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization”]"
    },
    {
      "title": "Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models",
      "authors": "Yu-Xiang Lin, Cheng-Han Chiang, Hung-yi Lee",
      "institution": "National Taiwan University",
      "link": "https://arxiv.org/pdf/2512.23578",
      "code": null,
      "tags": [
        "spoken language understanding",
        "spoken language models",
        "style amnesia",
        "multi-turn conversation",
        "paralinguistic features",
        "instruction following"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp",
      "contributions": "1. Identifies and defines the \"style amnesia\" problem where spoken language models fail to maintain a user-specified speaking style across multiple conversation turns. 2. Provides a comprehensive evaluation across multiple proprietary and open-source SLMs, demonstrating the pervasiveness of the issue across different emotion, accent, volume, and speed styles. 3. Investigates mitigation strategies, finding that explicit recall prompts can partially alleviate the problem and revealing a counter-intuitive weakness when style instructions are placed in system messages.",
      "summary": "This paper investigates the problem of \"style amnesia\" in spoken language models (SLMs), where models instructed to adopt a specific speaking style fail to maintain it over a multi-turn conversation. The authors evaluate several SLMs and find that explicitly prompting the model to recall the style instruction can partially mitigate the issue. The study concludes that current SLMs struggle with long-term style consistency, a critical challenge for natural spoken interactions.",
      "mindmap": "graph TB\n        Root(”Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”SLMs在多轮对话中无法维持指定的副语言风格/SLMs cannot maintain specified paralinguistic style in multi-turn conversation”)\n        Method --> M1(”在多轮对话开始时指定风格并评估/Instruct style at conversation start and evaluate”)\n        Method --> M2(”使用自动评估器测量指令遵循率/Use automatic judges to measure instruction-following rate”)\n        Method --> M3(”测试不同的提示策略/Test different prompting strategies”)\n        Results --> R1(”发现风格遗忘现象，指令遵循率随轮次下降/Style amnesia found, IF rate degrades over turns”)\n        Results --> R2(”显式回忆指令可部分缓解问题/Explicit recall can partially mitigate”)\n        Results --> R3(”系统提示中的指令效果不佳/Instructions in system prompts perform poorly”)"
    },
    {
      "title": "PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech",
      "authors": "Deepak Babu Piskala",
      "institution": "Independent Researcher (affiliation inferred from email domain: gmail.com)",
      "link": "https://arxiv.org/pdf/2512.23686",
      "code": "https://github.com/prdeepakbabu/ProfASR-Bench",
      "tags": [
        "speech recognition",
        "context-conditioned ASR",
        "entity-aware evaluation",
        "professional speech"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp",
      "contributions": "1. Introduces ProfASR-Bench, a benchmark for evaluating context-conditioned ASR in high-stakes professional domains (finance, medicine, legal, technology). 2. Identifies and defines the \"context-utilization gap\" (CUG), showing current promptable models underuse textual context for improving recognition. 3. Provides a standardized evaluation framework with a context ladder, entity/slice-aware reporting, and a reproducible testbed for comparing fusion strategies.",
      "summary": "This paper introduces ProfASR-Bench, a benchmark for evaluating Automatic Speech Recognition (ASR) in high-stakes professional settings. It tests models like Whisper and Qwen-Omni with various contextual prompts and finds a \"context-utilization gap,\" where current systems fail to effectively use available side information to improve accuracy, despite being promptable. The benchmark provides tools for entity-aware and slice-wise evaluation to advance context-conditioned ASR.",
      "mindmap": "graph TB\n        A[PROFASR-BENCH: A Benchmark for Context-Conditioned ASR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有基准低估专业场景挑战 / Existing benchmarks underplay professional challenges]\n        B1 --> B2[密集术语, 正式语体, 关键实体零容忍 / Dense terminology, formal register, zero tolerance for entity errors]\n        C --> C1[构建专业语音评估套件 / Build professional-talk evaluation suite]\n        C1 --> C2[配对自然语言提示与目标话语 / Pair natural-language prompts with target utterances]\n        C2 --> C3[支持实体感知和分片报告 / Support entity-aware and slice-wise reporting]\n        D --> D1[发现上下文利用差距(CUG) / Uncover context-utilization gap (CUG)]\n        D1 --> D2[轻量级上下文提示对WER改善甚微 / Lightweight textual context yields little WER change]\n        D2 --> D3[对抗性提示不会可靠降低性能 / Adversarial prompts do not reliably degrade performance]"
    },
    {
      "title": "EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG",
      "authors": "Hanbeot Park, Yunjeong Cho, Hunhee Kim",
      "institution": "Pukyong National University",
      "link": "https://arxiv.org/pdf/2512.22146",
      "code": null,
      "tags": [
        "brain-computer interface",
        "EEG-to-Voice",
        "mel-spectrogram",
        "domain adaptation",
        "automatic speech recognition",
        "language model correction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp",
      "contributions": "1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy.",
      "summary": "This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance.",
      "mindmap": "graph TB\n        A[EEG-to-Voice Decoding of Spoken and Imagined speech] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: EEG-based speech reconstruction is challenging due to noise, low resolution, and lack of aligned targets for imagined speech.]\n        C[主要方法/Method: Direct, open-loop EEG-to-mel-spectrogram generation with subject-specific generators, domain adaptation from spoken to imagined speech, and optional LM-based ASR correction.]\n        D[关键结果/Results: Feasibility demonstrated for both speech types; stable acoustic/linguistic performance; LM correction reduces CER/WER without semantic distortion.]"
    },
    {
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "authors": "Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan",
      "institution": "Başkent University, Gaziantep University",
      "link": "https://arxiv.org/pdf/2512.22564",
      "code": null,
      "tags": [
        "medical audio classification",
        "Audio Spectrogram Transformer",
        "Sharpness-Aware Minimization",
        "ICBHI 2017",
        "class imbalance",
        "loss landscape"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp",
      "contributions": "1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.",
      "summary": "This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.",
      "mindmap": "graph TB\n        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]\n        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]\n    \n        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]\n        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]\n        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]\n    \n        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]\n        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]"
    },
    {
      "title": "Semantic Codebooks as Effective Priors for Neural Speech Compression",
      "authors": "Liuyang Bai, Weiyi Lu, Li Guo",
      "institution": "NYU Shanghai",
      "link": "https://arxiv.org/pdf/2512.21653",
      "code": null,
      "tags": [
        "speech compression",
        "semantic codebooks",
        "residual vector quantization (RVQ)",
        "HuBERT",
        "FiLM-conditioned decoder",
        "neural audio codec"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",
      "contributions": "1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.",
      "summary": "The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.",
      "mindmap": "graph TB\n        Root[”Semantic Codebooks as Effective Priors for Neural Speech Compression”] --> Problem[”核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure.”]\n        Root --> Method[”主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder.”]\n        Root --> Results[”关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps).”]"
    },
    {
      "title": "Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning",
      "authors": "Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman",
      "institution": "Not explicitly stated in the provided content.",
      "link": "https://arxiv.org/pdf/2512.21702",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "transfer learning",
        "zero-shot inference",
        "fine-tuning",
        "Bengali audio",
        "BanglaFake dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp",
      "contributions": "1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.",
      "summary": "This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.",
      "mindmap": "graph TB\n        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)\n        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)\n        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))"
    },
    {
      "title": "Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models",
      "authors": "Ruihao Jing, Cheng Gong, Yu Jiang, Boyu Zhu, Shansong Liu, Chi Zhang, Xiao-Lei Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.21894",
      "code": null,
      "tags": [
        "speech recognition & translation",
        "task vector",
        "rare word recognition",
        "catastrophic forgetting",
        "speech-to-text",
        "parameter arithmetic"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd10590bdc1608f8eae90820d97325d5b60e598c9d06e1275430238b0313b56_w640_q70.webp",
      "contributions": "1. Proposes a training-free paradigm for rare word handling using task vectors, eliminating the need for fine-tuning. 2. Introduces word-level task vector arithmetic for flexible composition and reuse of rare-word capabilities. 3. Demonstrates that the method matches or surpasses fine-tuning on target words, improves general performance (~5 BLEU), and mitigates catastrophic forgetting.",
      "summary": "The paper addresses the bottleneck of rare word recognition in speech-to-text systems. It proposes a training-free method based on task vector arithmetic to compose rare-word capabilities, which avoids the costs and forgetting issues of fine-tuning. Experiments show the method performs comparably to fine-tuning on target words while improving overall translation quality and reducing catastrophic forgetting.",
      "mindmap": "graph TB\n        A[论文标题: Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Rare words are a bottleneck for speech-to-text systems. Fine-tuning is costly and causes forgetting.]\n        C[主要方法/Method: Training-free paradigm using task vector arithmetic for flexible composition of rare-word capabilities.]\n        D[关键结果/Results: Matches/surpasses fine-tuning on target words, improves general performance (~5 BLEU), mitigates forgetting.]"
    },
    {
      "title": "SACodec: Asymmetric Quantization with Semantic Anchoring for Low-Bitrate High-Fidelity Neural Speech Codecs",
      "authors": "Zhongren Dong, Bin Wang, Jing Han, Haotian Guo, Xiaojun Mo, Yimin Cao, Zixing Zhang",
      "institution": "Hunan University, Beijing Xiaomi Mobile Software Co., Ltd",
      "link": "https://arxiv.org/pdf/2512.20944",
      "code": "https://github.com/SmileHnu/SACodec",
      "tags": [
        "model compression (quantization/pruning)",
        "neural speech codec",
        "asymmetric quantization",
        "semantic anchoring",
        "residual vector quantization",
        "low-bitrate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384253f52eb69a47b5425625dd427efd5329874714378c79c8064748111f7059_w640_q70.webp",
      "contributions": "1. Proposes SACodec, a novel neural speech codec based on an asymmetric dual-quantizer design that decouples semantic and acoustic detail quantization. 2. Introduces a Semantic Anchoring mechanism using a lightweight projector to align features with a frozen mHuBERT codebook, injecting linguistic priors and ensuring full codebook utilization. 3. Employs a residual activation module with SimVQ in the acoustic path, enabling a single-layer quantizer to recover fine-grained information at a low bitrate of 1.5 kbps.",
      "summary": "The paper addresses the trade-off between acoustic fidelity and semantic richness in low-bitrate neural speech codecs. It proposes SACodec, which uses an asymmetric dual-quantizer with semantic anchoring to decouple and efficiently quantize semantic and acoustic information. At 1.5 kbps, SACodec achieves state-of-the-art performance, delivering high-fidelity audio and semantically rich tokens for downstream tasks.",
      "mindmap": "graph LR\n        A[SACodec] --> B[核心问题/Problem: 低比特率下保真度与语义丰富度的权衡<br/>Trade-off between fidelity & semantics at low bitrate]\n        A --> C[主要方法/Method: 非对称双量化器与语义锚定<br/>Asymmetric dual-quantizer & Semantic Anchoring]\n        A --> D[关键结果/Results: 1.5 kbps SOTA，高保真与语义丰富<br/>1.5 kbps SOTA, high-fidelity & semantic richness]"
    },
    {
      "title": "Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study",
      "authors": "Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang",
      "institution": "Hunan University",
      "link": "https://arxiv.org/pdf/2512.20948",
      "code": null,
      "tags": [
        "multi-modal learning",
        "foundation models",
        "multi-modal fusion",
        "cross-corpus evaluation",
        "neuropsychiatric disorders",
        "multi-lingual datasets"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18e74bbfe865915192b7a6c5c53058f33d0688ed82ef83023913d356622a3899_w640_q70.webp",
      "contributions": "1. Proposed FEND, a comprehensive multi-modal framework using foundation models for evaluating neuropsychiatric disorders across the lifespan. 2. Conducted a systematic evaluation using 13 multi-lingual datasets, identifying strengths and limitations of multi-modal fusion for different disorders. 3. Provided extensive benchmarks and analysis of performance-influencing factors (e.g., modality imbalance, dataset heterogeneity) to advance reproducible research in the field.",
      "summary": "The paper proposes FEND, a foundation model-based multi-modal framework for detecting neuropsychiatric disorders like Alzheimer's, depression, and autism from speech and text. It evaluates the framework on 13 multi-lingual datasets, finding that multi-modal fusion works well for Alzheimer's and depression but underperforms for autism due to dataset heterogeneity, and identifies modality imbalance as a key challenge.",
      "mindmap": "graph LR\n    A[Foundation Model-based Evaluation of Neuropsychiatric Disorders] --> B(核心问题/Problem: Multi-lingual generalization & lack of unified framework)\n    A --> C(主要方法/Method: FEND multi-modal framework using speech & text)\n    A --> D(关键结果/Results: Multi-modal fusion excels for AD/depression, underperforms for ASD)"
    },
    {
      "title": "Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning",
      "authors": "Wan Ki Wong, Ka Ho To, Chuck-jee Chau, Lucas Wong, Kevin Y. Yip, Irwin King",
      "institution": "The Chinese University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.21324",
      "code": null,
      "tags": [
        "music information retrieval",
        "semi-supervised learning",
        "BERT",
        "MidiBERT",
        "piano reduction",
        "music simplification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fa6dffcdfac263a4e2d329afe0244496c97160392fba638eb7885e17b6f2cd0_w640_q70.webp",
      "contributions": "1. Proposes a novel semi-supervised learning approach for automatic piano reduction to overcome the scarcity of labeled data. 2. Formulates the task as a two-step process of music simplification followed by harmonization. 3. Adapts and implements the MidiBERT framework to demonstrate practical and realistic piano reduction outputs.",
      "summary": "This paper addresses the challenge of automatic piano reduction, which is difficult due to a lack of labeled training data. The authors propose a semi-supervised learning method using a two-step simplification and harmonization approach based on the MidiBERT framework. They demonstrate that their method can produce practical piano reductions requiring only minor post-processing adjustments.",
      "mindmap": "graph LR\n    A[Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning] --> B(核心问题/Problem: Lack of labeled data for piano reduction);\n    A --> C(主要方法/Method: Semi-supervised learning with two-step simplification & harmonization using MidiBERT);\n    A --> D(关键结果/Results: Outputs practical, realistic samples needing small post-processing);"
    },
    {
      "title": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "authors": "Soumen Garai, Suman Samui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19739",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp",
      "contributions": "",
      "summary": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "mindmap": ""
    },
    {
      "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "authors": "Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20117",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp",
      "contributions": "",
      "summary": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "mindmap": ""
    },
    {
      "title": "Fun-Audio-Chat Technical Report",
      "authors": "Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20156",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp",
      "contributions": "",
      "summary": "Fun-Audio-Chat Technical Report",
      "mindmap": ""
    },
    {
      "title": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions",
      "authors": "Aviad Eisenberg, Sharon Gannot, Shlomo E. Chazan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20165",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2e6c293f3a92cf0f0415d5fec939a5c5f428a3e0c209c365ab28c3e809efdf2_w640_q70.webp",
      "contributions": "",
      "summary": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions",
      "mindmap": ""
    },
    {
      "title": "Aliasing-Free Neural Audio Synthesis",
      "authors": "Yicheng Gu, Junan Zhang, Chaoren Wang, Jerry Li, Zhizheng Wu, Lauri Juvela",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20211",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a549aa0c2de9d6cdc7bb5318340c3e483cb7107e9edceb93cca1996336055d9b_w640_q70.webp",
      "contributions": "",
      "summary": "Aliasing-Free Neural Audio Synthesis",
      "mindmap": ""
    },
    {
      "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
      "authors": "Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20308",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6351d1c5218be825df7f7572c20ff77011eb6baf9648cff6ea5fd370a23fda1_w640_q70.webp",
      "contributions": "",
      "summary": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
      "mindmap": ""
    },
    {
      "title": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
      "authors": "Ye Tao, Xuenan Xu, Wen Wu, Shuai Wang, Mengyue Wu, Chao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20339",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f6587dd18333bec8a3c1ef09e4b0d88ae24a552834119a986773e4cab8d287_w640_q70.webp",
      "contributions": "",
      "summary": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
      "mindmap": ""
    },
    {
      "title": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
      "authors": "Xiaoxuan Guo, Hengyan Huang, Jiayi Zhou, Renhe Sun, Jian Liu, Haonan Cheng, Long Ye, Qin Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20369",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c97fcd2fb08810e50b437e65adda92aaad995c7c6a2fe1b0d5e18fe64a60d23_w640_q70.webp",
      "contributions": "",
      "summary": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
      "mindmap": ""
    },
    {
      "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "authors": "Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20407",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp",
      "contributions": "",
      "summary": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "mindmap": ""
    },
    {
      "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "authors": "Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19703",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",
      "contributions": "",
      "summary": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "QuarkAudio Technical Report",
      "authors": "Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Xiaofu Chen, Bin Gong, Zheng Xue, Gang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20151",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e02363a4a98352b6c8c9414798dd67bf2f94507522a80052452e339cb11e583_w640_q70.webp",
      "contributions": "",
      "summary": "QuarkAudio Technical Report",
      "mindmap": ""
    },
    {
      "title": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "authors": "Mason Youngblood",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17935",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp",
      "contributions": "",
      "summary": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "mindmap": ""
    },
    {
      "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "authors": "Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17946",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",
      "contributions": "",
      "summary": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Influence of string register locations on vibratos among violoncellists",
      "authors": "Steven Hu, Sophia H. Kim, Helena H. Kim, Hugo Mackay, Eric J. Heller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18162",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1976f9763c20928a682e746b3da0df875eff504fde311658b1b374a93a86593e_w640_q70.webp",
      "contributions": "",
      "summary": "Influence of string register locations on vibratos among violoncellists",
      "mindmap": ""
    },
    {
      "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
      "authors": "Wen Huang, Yuchen Mao, Yanmin Qian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18210",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11c5c62108382427b9c65030ed5660aed9d27504a01d24a882c770942db88020_w640_q70.webp",
      "contributions": "",
      "summary": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
      "mindmap": ""
    },
    {
      "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "authors": "Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18232",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp",
      "contributions": "",
      "summary": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "mindmap": ""
    },
    {
      "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
      "authors": "Sudip Chakrabarty, Pappu Bishwas, Rajdeep Chatterjee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18298",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2d1fa5f1ea12d3598f0bd43290aa418e4ebda7629f53494201d317e1a493a95_w640_q70.webp",
      "contributions": "",
      "summary": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "authors": "Pengchao Feng, Yao Xiao, Ziyang Ma, Zhikang Niu, Shuai Fan, Yao Li, Sheng Wang, Xie Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18699",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp",
      "contributions": "",
      "summary": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "mindmap": ""
    },
    {
      "title": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
      "authors": "Zhanxun Liu, Yifan Duan, Mengmeng Wang, Pengchao Feng, Haotian Zhang, Xiaoyu Xing, Yijia Shan, Haina Zhu, Yuhang Dai, Chaochao Lu, Xipeng Qiu, Lei Xie, Lan Wang, Nan Yan, Zilong Zheng, Ziyang Ma, Kai Yu, Xie Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18706",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91aa95b8cdf1da17e389c0ae53a6896f970612bb3bc4d4b3762866d213442d93_w640_q70.webp",
      "contributions": "",
      "summary": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
      "mindmap": ""
    },
    {
      "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "authors": "Lisan Al Amin, Vandana P. Janeja",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18797",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e389f385848f8816c83b5a1ff23be8e5adce564472d3ca92ed4e1c1107846a61_w640_q70.webp",
      "contributions": "",
      "summary": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "mindmap": ""
    },
    {
      "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "authors": "Yichuan Zhang, Chengxin Li, Yujie Gu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18791",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/828d54530ed9add4098a79bb9dd1f4047ed230dfaa399d57cade241c18713658_w640_q70.webp",
      "contributions": "",
      "summary": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "mindmap": ""
    },
    {
      "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "authors": "Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18804",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",
      "contributions": "",
      "summary": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "mindmap": ""
    },
    {
      "title": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
      "authors": "Saurabh Bhardwaj, Smriti Srivastava, Abhishek Bhandari, Krit Gupta, Hitesh Bahl, J.R.P. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18902",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6caea0e80d84c68574b410a0a5b25554cd842375056a2d48aeaecfbde8e7de29_w640_q70.webp",
      "contributions": "",
      "summary": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
      "mindmap": ""
    },
    {
      "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
      "authors": "Fan Yu, Tao Wang, You Wu, Lin Zhu, Wei Deng, Weisheng Han, Wenchao Wang, Lin Hu, Xiangyu Liang, Xiaodong He, Yankun Huang, Yu Gu, Yuan Liu, Yuxuan Wang, Zhangyu Xiao, Ziteng Wang, Boya Dong, Feng Dang, Jinming Chen, Jingdong Li, Jun Wang, Yechen Jin, Yuan Zhang, Zhengyan Sheng, Xin Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19090",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f755dde99b4324e798ea9a6068c6ff699cdf819d3ac6581fe6856fcbfb048957_w640_q70.webp",
      "contributions": "",
      "summary": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
      "mindmap": ""
    },
    {
      "title": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
      "authors": "Wenyu Luo, Jinhui Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19374",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c0a852bcb45255507a3386c23b0bf527a541804415204b268f6689a7f82bed8_w640_q70.webp",
      "contributions": "",
      "summary": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
      "mindmap": ""
    },
    {
      "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "authors": "Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19687",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp",
      "contributions": "",
      "summary": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "mindmap": ""
    },
    {
      "title": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge",
      "authors": "Ram C. M. C. Shekar, Iván López-Espejo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17937",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/233c21744d1a90963372126b2d4d2d816073bc137a142aacddff67d2c28bb772_w640_q70.webp",
      "contributions": "",
      "summary": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge",
      "mindmap": ""
    },
    {
      "title": "MEGState: Phoneme Decoding from Magnetoencephalography Signals",
      "authors": "Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17978",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82b8623a222c1e415238f07bfd00f186b0fd97345fe3a78288d0e4e33530c69c_w640_q70.webp",
      "contributions": "",
      "summary": "MEGState: Phoneme Decoding from Magnetoencephalography Signals",
      "mindmap": ""
    },
    {
      "title": "Continual Learning for Acoustic Event Classification",
      "authors": "Yang Xiao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17932",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ab087a2804d86d0265d9e36f2c4b37428d7f82d8a8dbb1c750ee9c6811ecc4a_w640_q70.webp",
      "contributions": "",
      "summary": "Continual Learning for Acoustic Event Classification",
      "mindmap": ""
    },
    {
      "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "authors": "Te Ma, Nanjie Li, Hao Huang, Zhijian Ou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18371",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5833ba374008a7d74b1de29306f41fcd84ac79c51363d046bb5612486395c04_w640_q70.webp",
      "contributions": "",
      "summary": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "mindmap": ""
    },
    {
      "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "authors": "Simon Welker, Bunlong Lay, Maris Hillemann, Tal Peer, Timo Gerkmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19442",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50f384e81e75842e398563da395c9a42cafd7dfeb79c90bc7c4489b17d033102_w640_q70.webp",
      "contributions": "",
      "summary": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "mindmap": ""
    },
    {
      "title": "Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling",
      "authors": "Maria Mannone, Paulo Vitor Itaborai, Omar Costa Hamido, Miriam Goldack, Norbert Marwan, Peppino Fazio, Patrizia Ribino",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19272",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9688c851d6f89a1c52df7b967d23351ceaa8d6d34b065ab9a2ccaf7e65fd9a17_w640_q70.webp",
      "contributions": "",
      "summary": "Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling",
      "mindmap": ""
    },
    {
      "title": "Do Foundational Audio Encoders Understand Music Structure?",
      "authors": "Keisuke Toyama, Zhi Zhong, Akira Takahashi, Shusuke Takahashi, Yuki Mitsufuji",
      "institution": "Sony Group Corporation, Sony AI",
      "link": "https://arxiv.org/pdf/2512.17209",
      "code": null,
      "tags": [
        "music information retrieval",
        "music structure analysis",
        "foundational audio encoders",
        "self-supervised learning",
        "masked language modeling",
        "boundary detection",
        "function prediction"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper investigates the use of pretrained foundational audio encoders (FAEs) for music structure analysis (MSA). Through comprehensive experiments on 11 FAE types, it finds that models using self-supervised learning with masked language modeling on music data are particularly effective for MSA tasks like boundary detection and function prediction.",
      "mindmap": ""
    },
    {
      "title": "LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection",
      "authors": "Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan",
      "institution": "Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.17281",
      "code": null,
      "tags": [
        "speech processing",
        "voice activity detection",
        "vision transformer",
        "MFCC",
        "Gammatone filter bank cepstral coefficients",
        "dataset augmentation",
        "out-of-distribution evaluation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.",
      "mindmap": ""
    },
    {
      "title": "Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track",
      "authors": "June Young Yi, Hyeongju Kim, Juheon Lee",
      "institution": "Supertone Inc.",
      "link": "https://arxiv.org/pdf/2512.17293",
      "code": null,
      "tags": [
        "diffusion training",
        "Self-Purifying Flow Matching (SPFM)",
        "flow matching",
        "text-to-speech (TTS)",
        "Supertonic",
        "fine-tuning",
        "in-the-wild speech",
        "label noise mitigation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents a lightweight TTS system that fine-tunes the Supertonic model using Self-Purifying Flow Matching (SPFM) to robustly adapt to noisy, in-the-wild speech data. SPFM handles label noise by comparing conditional and unconditional flow matching losses, routing suspicious samples for unconditional training while still using their acoustic information. The resulting model achieved the best word error rate in the WildSpoof 2026 challenge, demonstrating that open-weight architectures can be effectively adapted to real-world conditions with explicit noise-handling mechanisms.",
      "mindmap": ""
    },
    {
      "title": "When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems",
      "authors": "Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati",
      "institution": "EkaCare (Orbi Health Private Limited)",
      "link": "https://arxiv.org/pdf/2512.17562",
      "code": null,
      "tags": [
        "others",
        "MetricGAN-plus-voicebank",
        "semantic WER",
        "noise robustness",
        "speech enhancement"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.",
      "mindmap": ""
    },
    {
      "title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification",
      "authors": "Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo",
      "institution": "Strathmore University, Google Research, Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.16271",
      "code": null,
      "tags": [
        "multi-modal training",
        "transformer",
        "causal attention",
        "hierarchical representation",
        "multi-task learning",
        "domain-adversarial training",
        "controlled perturbation training",
        "counterfactual simulation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes DACH-TIC, a domain-agnostic causal-aware hierarchical audio transformer that integrates causal attention, multi-task learning, and adversarial domain generalization for robust infant cry classification. It outperforms state-of-the-art baselines in accuracy and macro-F1 score and demonstrates strong generalization to unseen acoustic environments with minimal performance degradation.",
      "mindmap": ""
    },
    {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "authors": "Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vilém Zouhar, Carlos Escolano, Gerard I. Gállego, Jorge Iranzo-Sánchez, Ahrii Kim, Dominik Macháček, Patricia Schmidtova, Maike Züfle",
      "institution": "Fondazione Bruno Kessler, Barcelona Supercomputing Center, University of Zurich, ETH Zurich, Universitat Politècnica de Catalunya, Universitat Politècnica de València, AI-Bio Convergence Research Institute, Charles University, KIT",
      "link": "https://arxiv.org/pdf/2512.16378",
      "code": null,
      "tags": [
        "multi-modal inference",
        "SpeechLLMs",
        "cascaded systems",
        "speech foundation models",
        "speech-to-text translation",
        "benchmarking",
        "Whisper",
        "SeamlessM4T",
        "Gemma",
        "Tower+"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces the \"Hearing to Translate\" test suite to benchmark SpeechLLMs against cascaded and direct speech-to-text translation systems. It finds that cascaded systems, which combine speech recognition with LLM-based translation, remain the most reliable overall, while current SpeechLLMs only match them in specific scenarios. The study concludes that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
      "mindmap": ""
    },
    {
      "title": "Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders",
      "authors": "Nikolaos Ellinas, Alexandra Vioni, Panos Kakoulidis, Georgios Vamvoukakis, Myrsini Christidou, Konstantinos Markopoulos, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis",
      "institution": "Innoetics, Samsung Electronics, Samsung Electronics Mobile eXperience Business",
      "link": "https://arxiv.org/pdf/2512.16519",
      "code": null,
      "tags": [
        "speech synthesis",
        "cepstrum",
        "pitch shifting",
        "mel-spectrogram",
        "DCT",
        "pseudo-inverse mel transform",
        "neural vocoder"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a cepstrum-based pitch modification method that directly manipulates the cepstral feature space to shift the harmonic structure in a mel-spectrogram, making it compatible with any mel-based neural vocoder without retraining. The method is validated through objective and subjective evaluations, showing its effectiveness compared to traditional pitch modification techniques.",
      "mindmap": ""
    },
    {
      "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms",
      "authors": "Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut",
      "institution": "Flinders University, PrioriAnalytica",
      "link": "https://arxiv.org/pdf/2512.14714",
      "code": null,
      "tags": [
        "others",
        "learnable Gabor filters",
        "ResNeXt",
        "squeeze-and-excitation attention",
        "spectrograms",
        "deep learning"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios.",
      "mindmap": ""
    },
    {
      "title": "Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction",
      "authors": "Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda Uçar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He",
      "institution": "Scale AI",
      "link": "https://arxiv.org/pdf/2512.14865",
      "code": null,
      "tags": [
        "multi-modal inference",
        "end-to-end spoken dialogue systems",
        "audio language models",
        "multi-turn evaluation",
        "speech-to-speech",
        "audio-native benchmark",
        "inference memory",
        "instruction retention",
        "self coherence",
        "voice editing"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new \"Voice Editing\" axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.",
      "mindmap": ""
    },
    {
      "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
      "authors": "Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou",
      "institution": "The Chinese University of Hong Kong, Snap Inc.",
      "link": "https://arxiv.org/pdf/2512.14938",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion transformer",
        "video VAE",
        "sliding window mechanism",
        "motion-frame context",
        "latent noise injection",
        "MLLM director"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities",
      "authors": "Aref Farhadipour, Teodora Vukovic, Volker Dellwo, Petr Motlicek, Srikanth Madikeri",
      "institution": "University of Zurich, Idiap Research Institute",
      "link": "https://arxiv.org/pdf/2512.14961",
      "code": null,
      "tags": [
        "multi-modal inference",
        "multi-task learning",
        "cross-attention",
        "gated fusion",
        "confidence-weighted fusion",
        "adaptive fusion"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a robust trimodal person recognition framework that integrates voice, face, and gesture data using multi-task learning, cross-attention, and a gated, confidence-weighted fusion strategy to handle missing or degraded modalities. It achieves high accuracy on the CANDOR and VoxCeleb1 datasets and maintains performance even when modalities are unavailable, demonstrating robustness for real-world applications.",
      "mindmap": ""
    },
    {
      "title": "BEAT2AASIST model with layer fusion for ESDD 2026 Challenge",
      "authors": "Sanghyeok Chung, Eujin Kim, Donggun Kim, Gaeun Heo, Jeongbin You, Nahyun Lee, Sunmook Choi, Soyul Han, Seungsang Oh, Il-Youp Kwak",
      "institution": "Korea University, Chung-Ang University, Cornell University, Hannam University",
      "link": "https://arxiv.org/pdf/2512.15180",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "BEATs",
        "AASIST",
        "transformer layer fusion",
        "vocoder-based data augmentation",
        "dual-branch architecture"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes BEAT2AASIST, a model for environmental sound deepfake detection that extends BEATs-AASIST by splitting features into dual AASIST branches and incorporating transformer layer fusion strategies. It also uses vocoder-based data augmentation for robustness. The approach demonstrates competitive performance on the ESDD 2026 Challenge test sets.",
      "mindmap": ""
    },
    {
      "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization",
      "authors": "Elio Gruttadauria, Mathieu Fontaine, Jonathan Le Roux, Slim Essid",
      "institution": "Télécom Paris, Institut polytechnique de Paris, Mitsubishi Electric Research Laboratories (MERL)",
      "link": "https://arxiv.org/pdf/2512.15229",
      "code": null,
      "tags": [
        "others",
        "EEND-EDA",
        "RNN-based stitching",
        "centroid refinement decoder",
        "permutation-invariant training (PIT)",
        "online speaker diarization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces O-EENC-SD, an efficient online end-to-end neural clustering system for speaker diarization. It is based on EEND-EDA and features a novel RNN-based stitching mechanism and a centroid refinement decoder for online prediction. The system is shown to be competitive with state-of-the-art methods on the CallHome dataset while offering a better trade-off between diarization error rate and computational complexity.",
      "mindmap": ""
    },
    {
      "title": "Time-Varying Audio Effect Modeling by End-to-End Adversarial Training",
      "authors": "Yann Bourdin, Pierrick Legrand, Fanny Roche",
      "institution": "Arturia, Inria, IMS, University of Bordeaux, Bordeaux INP",
      "link": "https://arxiv.org/pdf/2512.15313",
      "code": null,
      "tags": [
        "others",
        "Generative Adversarial Network (GAN)",
        "convolutional-recurrent architecture",
        "adversarial training",
        "State Prediction Network (SPN)",
        "chirp-train signals"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a two-stage GAN framework for black-box modeling of time-varying audio effects, using only input-output recordings without needing control signals. The method involves an adversarial training phase followed by supervised fine-tuning with a State Prediction Network for synchronization. Experiments on a vintage phaser demonstrate the approach's effectiveness in capturing time-varying dynamics.",
      "mindmap": ""
    },
    {
      "title": "A Conditioned UNet for Music Source Separation",
      "authors": "Ken O'Hanlon, Basil Woods, Lin Wang, Mark Sandler",
      "institution": "Queen Mary University of London, AudioStrip Ltd.",
      "link": "https://arxiv.org/pdf/2512.15532",
      "code": null,
      "tags": [
        "others",
        "conditioned UNet",
        "music source separation",
        "QSCNet",
        "Sparse Compressed Network",
        "Bandsplit RNN",
        "Banquet",
        "MoisesDb"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters.",
      "mindmap": ""
    }
  ]
}