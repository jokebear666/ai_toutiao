# 20260105-20260111

## 2026-01-05

**cs.DC total: 9**

- **[arXiv260105] Impact of Clustering on the Observability and Controllability of Complex Networks**
  - **tags:** [other], [network controllability], [clustering, scale-free networks, observability, controllability, structured systems theory]
  - **authors:** Mohammadreza Doostmohammadian, Hamid R. Rabiee
  - **institution:** Semnan University, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2601.00221
  - **contributions:** 1. Investigates and quantifies the relationship between network clustering and the requirements for observability and controllability in scale-free networks., 2. Demonstrates through simulations that densely clustered networks require fewer driver and observer nodes, offering a structural optimization principle., 3. Provides practical insights for reducing sensor/actuator placement in resource-constrained applications like social networks and intelligent transportation systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee38f16190dd3c9de93bdef0b47cb72f5c579d59a7937f107666cb7810142401_w640_q70.webp
  - **Simple LLM Summary:** This paper studies how clustering affects the observability and controllability of complex scale-free networks. Using structured systems theory and Monte-Carlo simulations, it shows that higher clustering reduces the number of required driver and observer nodes. The findings suggest that network design can be optimized for control and monitoring by increasing clustering, especially in resource-limited scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Impact of Clustering on Observability and Controllability) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(网络可观测性与可控性需求/Network Observability & Controllability Requirements)
        Method --> M1(结构化系统理论/Structured Systems Theory)
        Method --> M2(蒙特卡洛模拟与案例研究/Monte-Carlo Simulations & Case Studies)
        Results --> R1(密集聚类减少驱动与观测节点/Dense Clustering Reduces Driver & Observer Nodes)
        Results --> R2(优化资源受限网络设计/Optimizes Resource-Constrained Network Design)
    ```

- **[arXiv260105] From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm**
  - **tags:** [sys], [distributed consensus], [RAFT, replay attack, message forgery, authenticated verification, freshness check]
  - **authors:** Tamer Afifi, Abdelfatah Hegazy, Ehab Abousaif
  - **institution:** Arab Academy for Science, Technology and Maritime Transport (AASTMT)
  - **link:** https://arxiv.org/pdf/2601.00273
  - **contributions:** 1. A systematic security analysis of the RAFT protocol, identifying its susceptibility to message replay and forgery attacks. 2. Examination of the practical feasibility of these attacks through simulated scenarios. 3. Proposal of a novel cryptographic approach for enhancing RAFT's security, incorporating authenticated message verification and freshness checks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f61353e1cee5179dd015df5e513ba3552de676525e8ac5e6c2fb0a48786e62f3_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies security vulnerabilities in the RAFT distributed consensus algorithm, specifically to replay and forgery attacks, which can disrupt consensus and cause data inconsistency. To address this, the authors propose a novel security framework using cryptography, authenticated message verification, and freshness checks. The proposed solution aims to enhance the security of RAFT implementations and guide the development of more resilient distributed systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm"] --> Problem["核心问题/Problem: RAFT协议的安全漏洞未被充分认知/Security vulnerabilities in RAFT are not fully recognized"]
        Root --> Method["主要方法/Method: 系统安全分析与基于密码学的解决方案/Systematic security analysis & cryptographic solution"]
        Root --> Results["关键结果/Results: 提出增强安全性的框架/Proposed a framework for enhancing security"]
        Problem --> P1["攻击类型/Attack Types: 消息重放与伪造/Message replay & forgery"]
        Problem --> P2["后果/Consequence: 共识破坏与数据不一致/Consensus disruption & data inconsistency"]
        Method --> M1["分析/Analysis: 模拟攻击场景/Simulated attack scenarios"]
        Method --> M2["方案/Solution: 认证、验证与新鲜性检查/Authentication, verification & freshness check"]
        Results --> R1["成果/Outcome: 识别设计弱点/Identified design weaknesses"]
        Results --> R2["成果/Outcome: 提供安全增强框架/Provided security enhancement framework"]
    ```

- **[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems**
  - **tags:** [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]
  - **authors:** Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lovén
  - **institution:** University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Luleå University of Technology, Peking University, TU Wien
  - **link:** https://arxiv.org/pdf/2601.00339
  - **contributions:** 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bio-inspired Agentic Self-healing Framework<br>生物启发的智能体自愈框架"] --> Problem["核心问题/Problem<br>DCCS中的复杂性与故障频发<br>Complexity & Frequent Faults in DCCS"]
        Root --> Method["主要方法/Method<br>ReCiSt框架: 仿生四层与LM智能体<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]
        Root --> Results["关键结果/Results<br>数十秒内自愈，低CPU开销<br>Self-healing in tens of seconds, low CPU overhead"]
    ```

- **[arXiv260105] Word Frequency Counting Based on Serverless MapReduce**
  - **tags:** [sys], [serverless computing], [Serverless Computing, MapReduce, Word Frequency Counting, Function as a Service (FaaS), Cloud Computing]
  - **authors:** Hanzhe Li, Bingchen Lin, Mengyuan Xu
  - **institution:** Xi'an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00380
  - **contributions:** 1. Proposes a novel combination of the serverless computing paradigm (FaaS) with the MapReduce programming model for data processing tasks. 2. Investigates and determines the optimal number of Map and Reduce functions for a given workload within a serverless MapReduce framework. 3. Demonstrates through experiments that increasing the number of functions reduces execution time and improves overall efficiency for the word frequency counting task.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of optimizing big data processing efficiency by integrating the serverless computing model (FaaS) with the MapReduce framework. It proposes a serverless MapReduce approach for word frequency counting and experimentally finds the optimal number of Map and Reduce functions to minimize execution time. The results show that this method improves processing efficiency, offering a cost-effective solution for cloud-based data analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Word Frequency Counting Based on Serverless MapReduce] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[需求: 高性能与高效率计算 / Demand: High-performance & High-efficiency Computing]
        C --> C1[结合: 无服务器计算(FaaS)与MapReduce / Combine: Serverless Computing (FaaS) & MapReduce]
        C --> C2[优化: Map与Reduce函数数量 / Optimize: Number of Map & Reduce Functions]
        D --> D1[结果: 执行时间减少 / Result: Execution Time Reduces]
        D --> D2[结果: 程序效率提升 / Result: Program Efficiency Improves]
    ```

- **[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving**
  - **tags:** [mlsys], [llm inference], [time-warp emulation, CUDA interception, virtual time coordination, performance modeling, discrete-event simulation]
  - **authors:** Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov
  - **institution:** Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00397
  - **contributions:** 1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp
  - **Simple LLM Summary:** The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GPU集群评估成本高且慢/Evaluating serving configs on GPU clusters is slow and expensive]
        B --> B2[模拟器需要重写控制逻辑/Simulators require re-implementing complex control logic]
        C --> C1[拦截CUDA API调用/Intercept CUDA API calls]
        C --> C2[虚拟时间跳跃/Virtual time jumps based on kernel predictions]
        C --> C3[分布式协调协议/Distributed coordination protocol]
        D --> D1[<5%预测误差/<5% prediction error]
        D --> D2[5-17倍加速/5-17x faster than real execution]
    ```

- **[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution**
  - **tags:** [sec], [Privacy-preserving data aggregation], [unanimous-release confidentiality, consensus locking, malicious deviation detection]
  - **authors:** Prajwal Panth, Sahaj Raj Malla
  - **institution:** KIIT University, Kathmandu University
  - **link:** https://arxiv.org/pdf/2601.00418
  - **contributions:** 1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (σ_S, σ_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework's properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Secure, Verifiable, and Scalable Multi-Client Data Sharing<br>安全、可验证、可扩展的多客户端数据共享] --> B(核心问题/Problem: Secure multi-client data aggregation with privacy and verifiability<br>安全、可验证的多客户端隐私数据聚合)
        A --> C(主要方法/Method: Consensus-Based Privacy-Preserving Data Distribution (CPPDD)<br>基于共识的隐私保护数据分发)
        C --> C1(Affine Masking & Consensus Locking<br>仿射掩码与共识锁定)
        C --> C2(Step/Data Checksums (σ_S, σ_D)<br>步骤/数据校验和)
        A --> D(关键结果/Results: Linear scalability, 100% deviation detection, lower FLOPs vs MPC/HE<br>线性可扩展性，100%异常检测，相比MPC/HE更低的计算量)
    ```

- **[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights**
  - **tags:** [mlsys], [federated learning], [federated learning, prefix-tuning, large model customization, efficient fine-tuning, retrieval-augmented generation]
  - **authors:** Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato
  - **institution:** Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2601.00526
  - **contributions:** 1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp
  - **Simple LLM Summary:** This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Federated Customization of Large Models: Approaches, Experiments, and Insights] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[在联邦学习框架下定制大模型的挑战/Challenges of customizing large models within FL]
        C --> C1[回顾大模型定制技术/Review LM customization techniques]
        C --> C2[讨论联邦学习实现/Discuss FL implementations]
        C --> C3[实验联邦前缀调优/Experiment with federated prefix-tuning]
        D --> D1[验证联邦前缀调优可行性/Validate feasibility of federated prefix-tuning]
        D --> D2[性能接近集中式方法/Performance close to centralized]
        D --> D3[展示竞争力与鲁棒性/Show competitive performance & robustness]
    ```

- **[arXiv260105] Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure**
  - **tags:** [sys], [cloud computing], [cloud benchmarking, point-of-sale systems, performance analysis, cost optimization, retail technology]
  - **authors:** Ravi Teja Pagidoju
  - **institution:** Campbellsville University
  - **link:** https://arxiv.org/pdf/2601.00530
  - **contributions:** 1. Presents a systematic, repeatable, and transparent methodology for evaluating POS workloads on cloud platforms using free-tier resources and open-source benchmarking code. 2. Provides the first comprehensive, code-driven comparison of POS-specific workloads across Google Cloud Platform and Microsoft Azure, analyzing performance metrics and cost efficiency. 3. Establishes an open benchmarking framework and offers practical insights for merchants considering cloud POS implementation, linking architectural components to observed performance-cost trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/084205dec68906e341b24595e452d764be0491b5d3d16bfccff9cf4f8d4f1eca_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a systematic methodology to compare the performance and cost of cloud-based Point-of-Sale systems on Google Cloud Platform and Microsoft Azure using free-tier resources and open-source code. The analysis finds that GCP offers faster response times, while Azure demonstrates higher cost efficiency for steady-state operations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏零售工作负载的实证研究 / Lack of empirical research on retail workloads]
        C --> C1[使用免费资源和开源代码进行系统化基准测试 / Systematic benchmarking using free-tier resources and open-source code]
        D --> D1[GCP响应时间快23.0% / GCP 23.0% faster response]
        D --> D2[Azure成本效率高71.9% / Azure 71.9% higher cost efficiency]
    ```

- **[arXiv260105] FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding**
  - **tags:** [mlsys], [llm inference], [speculative decoding, edge-cloud collaboration, communication-efficient inference, adaptive speculation, shared-backbone architecture]
  - **authors:** Yuchen Li, Rui Kong, Zhonghao Lyu, Qiyang Li, Xinran Chen, Hengyi Cai, Lingyong Yan, Shuaiqiang Wang, Jiashu Zhao, Guangxu Zhu, Linghe Kong, Guihai Chen, Haoyi Xiong, Dawei Yin
  - **institution:** Baidu Inc., Shanghai Jiao Tong University, KTH Royal Institute of Technology, Wilfrid Laurier University, Shenzhen Research Institute of Big Data
  - **link:** https://arxiv.org/pdf/2601.00644
  - **contributions:** 1. Proposed a shared-backbone architecture enabling a single static edge-side draft model to be compatible with a family of evolving cloud-side target models, decoupling edge deployment from cloud updates. 2. Developed a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel conditions and device energy budgets. 3. Designed the FlexSpec framework, which reduces communication and maintenance costs for edge-cloud collaborative LLM inference, improving scalability and efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7429a9e45e5a64278f26ba8b33031aac6b2f433fdae9180afd7344ec56b1fffd_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FlexSpec, a communication-efficient framework for edge-cloud collaborative LLM inference using speculative decoding. Its core innovation is a shared-backbone architecture that allows a frozen edge-side draft model to work with evolving cloud target models, paired with an adaptive speculation mechanism. Experiments show it achieves superior inference efficiency compared to conventional speculative decoding approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[FlexSpec: Frozen Drafts Meet Evolving Targets] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[LLM部署在边缘受限/LLM Deployment on Edge is Constrained]
        Problem --> P2[现有协同推理通信开销大/Existing Collaborative Inference has High Comm. Overhead]
        Problem --> P3[模型频繁更新限制可扩展性/Frequent Model Updates Limit Scalability]
        Method[主要方法/Method] --> M1[共享主干架构/Shared-Backbone Architecture]
        Method --> M2[解耦边缘与云端部署/Decouple Edge and Cloud Deployment]
        Method --> M3[信道感知自适应推测/Channel-Aware Adaptive Speculation]
        Results[关键结果/Results] --> R1[减少通信与维护成本/Reduced Communication & Maintenance Cost]
        Results --> R2[提升推理效率/Improved Inference Efficiency]
        Results --> R3[优于传统推测解码方法/Superior to Conventional SD Approaches]
    ```


**cs.AI/cs.LG contains "reinforcement learning" total: 22**
- **[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning**
  - **tags:** [ai], [reinforcement learning], [Metric Interval Temporal Logic (MITL), Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA), Q-learning, POMDP]
  - **authors:** Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao
  - **institution:** University of Iowa, Talus Renewables, Inc., Roma Tre University
  - **link:** https://arxiv.org/pdf/2601.00087
  - **contributions:** 1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement learning with timed constraints for robotics motion planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[复杂任务序列与严格时间约束/Complex task sequences & strict temporal constraints]
        B --> B2[随机动态与部分可观测性/Stochastic dynamics & partial observability]
        C --> C1[MITL公式转换为Timed-LDGBA/MITL to Timed-LDGBA translation]
        C --> C2[构建产品定时模型与Q学习/Construct product timed models for Q-learning]
        C --> C3[简单而富有表现力的奖励结构/Simple yet expressive reward structure]
        D --> D1[满足时间约束的策略/Policies satisfy time-bounded requirements]
        D --> D2[扩展到更大状态空间/Scales to larger state spaces]
        D --> D3[在部分可观测环境中有效/Effective in partially observable environments]
    ```

- **[arXiv260105] Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning**
  - **tags:** [mlsys], [llm inference], [meta-reinforcement learning, constraint propagation, graph attention network, structured inference, green ai]
  - **authors:** Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma
  - **institution:** Iowa State University
  - **link:** https://arxiv.org/pdf/2601.00095
  - **contributions:** 1. Introduces MetaJuLS, a meta-reinforcement learning framework for learning universal constraint propagation policies applicable across languages and tasks without task-specific retraining. 2. Formulates structured inference as adaptive constraint propagation and trains a Graph Attention Network policy via meta-learning, achieving significant speedups (1.5-2.0x) over GPU-optimized baselines with minimal accuracy loss. 3. Demonstrates rapid cross-domain adaptation (5-15 seconds) and contributes to Green AI by reducing inference carbon footprint through fewer propagation steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5284c89e9a9eec9c1882da4c36e7d1e9bbce97ea559fe29f19c435e5f8b8854_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of structured inference (e.g., JSON parsing) in large language models by proposing MetaJuLS, a meta-reinforcement learning method that learns adaptive constraint propagation policies. This approach achieves up to 2x speedup over baselines while maintaining high accuracy and enables fast adaptation to new languages and tasks. The work contributes to more efficient and environmentally friendly LLM inference.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Universal Adaptive Constraint Propagation<br>通用自适应约束传播"] --> Problem
        Root --> Method
        Root --> Results
        Problem["LLMs need efficient structured inference<br>LLM需要高效的结构化推理"] --> P1["Wasted computation from static checking<br>静态检查导致计算浪费"]
        Problem --> P2["Need for cross-domain generalization<br>需要跨领域泛化"]
        Method["MetaJuLS: Meta-RL for constraint propagation<br>MetaJuLS: 用于约束传播的元强化学习"] --> M1["Learns universal policies via meta-learning<br>通过元学习学习通用策略"]
        Method --> M2["Uses Graph Attention Network<br>使用图注意力网络"]
        Results["Key Results<br>关键结果"] --> R1["1.5-2.0x speedup<br>1.5-2.0倍加速"]
        Results --> R2["Fast adaptation (5-15s)<br>快速适应(5-15秒)"]
        Results --> R3["Contributes to Green AI<br>助力绿色AI"]
    ```

- **[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments**
  - **tags:** [ai], [reinforcement learning], [geometric reinforcement learning, Hamiltonian optimization, simultaneous navigation and mapping, local sensory observations, path differential]
  - **authors:** Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj
  - **institution:** University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2601.00116
  - **code:** https://github.com/ (as per the abstract "The code is publicly available on Github." The specific URL is not provided in the given text, only a placeholder link.)
  - **contributions:** 1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GRL-SNAM: Geometric RL for SNAM] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Simultaneous Navigation and Mapping in mapless environments]
        C[主要方法/Method: Geometric RL with Path Differential Hamiltonians, local energy landscapes]
        D[关键结果/Results: High-quality navigation with minimal exploration, generalizes to unseen layouts]
    ```

- **[arXiv260105] Reinforcement Learning with Function Approximation for Non-Markov Processes**
  - **tags:** [ai], [reinforcement learning], [non-Markov processes, linear function approximation, policy evaluation, Q-learning, partially observed MDPs]
  - **authors:** Ali Devran Kara
  - **institution:** Florida State University
  - **link:** https://arxiv.org/pdf/2601.00151
  - **contributions:** 1. Proved convergence of policy evaluation with linear function approximation under ergodic non-Markov processes, linking the limit to a fixed point of a joint projection-Bellman operator. 2. Established convergence for a special case of Q-learning with linear approximation where basis functions are based on quantization maps under similar ergodicity conditions. 3. Applied the theoretical results to Partially Observed MDPs (POMDPs) using finite-memory state representations and derived explicit error bounds for the learning algorithm limits.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp
  - **Simple LLM Summary:** This paper studies reinforcement learning with linear function approximation for non-Markov processes. It proves convergence for policy evaluation and a special case of Q-learning under ergodicity conditions, and applies the theory to POMDPs to derive error bounds.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Reinforcement Learning with Function Approximation for Non-Markov Processes] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>RL with linear function approximation for non-Markov processes] --> P1[非马尔可夫过程/Non-Markov Processes]
        Method[主要方法/Method<br>Theoretical analysis under ergodicity conditions] --> M1[策略评估/Policy Evaluation]
        Method --> M2[Q学习/Q-learning]
        M2 --> M2_1[特殊情况:基于量化的基函数/Special Case: Quantization-based basis]
        Results[关键结果/Results] --> R1[收敛性证明/Convergence Proofs]
        Results --> R2[应用于POMDPs/Application to POMDPs]
        R2 --> R2_1[显式误差界/Explicit Error Bounds]
    ```

- **[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients**
  - **tags:** [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]
  - **authors:** Junkai Luo, Yinglun Zhu
  - **institution:** University of California, Riverside
  - **link:** https://arxiv.org/pdf/2601.00167
  - **contributions:** 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Online Finetuning Decision Transformers with Pure RL Gradients] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[在线微调DT时，纯RL梯度方法未被探索/Pure RL gradients for online DT finetuning unexplored]
    B --> B2[后见之益回报重标注与RL算法不兼容/Hindsight return relabeling incompatible with RL]
    C --> C1[适配GRPO至DT/Adapt GRPO to DTs]
    C --> C2[引入关键修改: 子轨迹优化等/Introduce key modifications]
    D --> D1[超越现有在线DT基线/Outperform online DT baselines]
    D --> D2[实现SOTA性能/Achieve SOTA performance]
    ```

- **[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings**
  - **tags:** [ai], [semantic communication], [reinforcement learning, unequal error protection, adaptive repetition coding, semantic distortion metric, per-dimension protection]
  - **authors:** Moirangthem Tiken Singh, Adnan Arif
  - **institution:** Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University
  - **link:** https://arxiv.org/pdf/2601.00186
  - **contributions:** 1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[带宽受限下保持语义/Bandwidth-constrained Semantic Preservation]
        C --> C1[基于RL的自适应重复编码/RL-based Adaptive Repetition Coding]
        C --> C2[复合语义失真度量/Composite Semantic Distortion Metric]
        D --> D1[性能显著超越均匀保护/Significant Gains Over Uniform Protection]
        D --> D2[挑战传统信道编码范式/Challenges Traditional Channel Coding]
    ```

- **[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]
  - **authors:** Omar Sharif, Eftekhar Hossain, Patrick Ng
  - **institution:** Dartmouth College, University of Central Florida
  - **link:** https://arxiv.org/pdf/2601.00215
  - **contributions:** 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"] --> Problem["核心问题/Problem: MLLMs lack visual grounding in reasoning"]
        Root --> Method["主要方法/Method: RL with reward functions & GRPO"]
        Root --> Results["关键结果/Results: 5.56% improvement on Qwen-2.5-VL-7B"]
    ```

- **[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [neuromorphic computing, state-space models, sparse attention, surrogate gradients, local learning rules]
  - **authors:** Osvaldo Simeone
  - **institution:** Northeastern University London (Intelligent Networked Systems Institute - INSI)
  - **link:** https://arxiv.org/pdf/2601.00245
  - **contributions:** 1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI能耗增长 / Escalating AI Energy Requirements]
        C --> C1[神经形态计算原则 / Neuromorphic Computing Principles]
        C1 --> C2[离散稀疏激活 / Discrete & Sparse Activations]
        C1 --> C3[循环动态 / Recurrent Dynamics]
        C --> C4[处理框架: 令牌内与令牌间 / Processing Framework: Intra-Token vs. Inter-Token]
        D --> D1[现代AI体现神经形态原则 / Modern AI Embodies Neuromorphic Principles]
        D --> D2[连接SNN、状态空间模型、Transformer / Connects SNNs, State-Space Models, Transformers]
    ```

- **[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective**
  - **tags:** [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]
  - **authors:** Aly Sabri Abdalla, Vuk Marojevic
  - **institution:** Mississippi State University
  - **link:** https://arxiv.org/pdf/2601.00257
  - **contributions:** 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --> P1["子问题/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]
        Method["主要方法/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --> M1["组件/Component: Semantic-aware rApp (terrain interpreter)"]
        Method --> M2["组件/Component: RL-enabled xApp (trajectory planner)"]
        Results["关键结果/Results: Framework enables closed-loop, AI-optimized LAE operations"] --> R1["评估/Evaluation: Feasibility and performance analysis presented"]
        Results --> R2["展望/Outlook: Research challenges and standardization needs surveyed"]
    ```

- **[arXiv260105] Can Optimal Transport Improve Federated Inverse Reinforcement Learning?**
  - **tags:** [mlsys], [federated learning], [Inverse Reinforcement Learning, Federated Learning, Optimal Transport, Wasserstein Barycenter, Maximum Entropy IRL]
  - **authors:** David Millard, Ali Baheri
  - **institution:** Rochester Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00309
  - **contributions:** 1. Introduces an optimal transport-based approach for federating learned reward functions in Inverse Reinforcement Learning (IRL). 2. Proposes using a Wasserstein barycenter for reward fusion, which accounts for the geometric structure of the reward landscape, as opposed to simple parameter averaging. 3. Provides a theoretical proof that the barycentric fusion yields a more faithful global reward estimate than conventional federated averaging methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of learning a shared reward function across heterogeneous agents in privacy-sensitive, communication-limited settings. It proposes a federated IRL framework where agents perform local Maximum Entropy IRL and then fuse their reward functions via a Wasserstein barycenter. The authors prove this method provides a more accurate global reward estimate than standard parameter averaging, offering a principled and efficient solution for multi-agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Can Optimal Transport Improve Federated Inverse Reinforcement Learning?"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Heterogeneous agents need a shared reward, but data pooling is impractical due to privacy, dynamics differences, and limited bandwidth."]
        Method["主要方法/Method<br>Local lightweight MaxEnt IRL followed by reward fusion via Wasserstein barycenter."]
        Results["关键结果/Results<br>Barycentric fusion yields a more faithful global reward estimate than parameter averaging."]
    ```

- **[arXiv260105] Offline Multi-Agent Reinforcement Learning for 6G Communications: Fundamentals, Applications and Future Directions**
  - **tags:** [ai], [multi-agent reinforcement learning], [offline reinforcement learning, conservative Q-learning, meta-learning, radio resource management, UAV networks]
  - **authors:** Eslam Eldeeb, Hirley Alves
  - **institution:** University of Oulu
  - **link:** https://arxiv.org/pdf/2601.00321
  - **contributions:** 1. Proposes a novel offline multi-agent reinforcement learning algorithm based on conservative Q-learning (CQL) for safe and efficient training in wireless networks., 2. Extends the offline MARL approach with meta-learning to enhance adaptability in dynamic environments., 3. Validates the proposed framework through practical use cases in radio resource management and UAV network applications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cc964c00876ee81486a12f18505ed613255db0f942d692eb3a3d428f15d72c6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the cost, safety, and scalability limitations of online multi-agent reinforcement learning (MARL) in complex 6G networks by proposing an offline MARL algorithm based on conservative Q-learning (CQL), enhanced with meta-learning for dynamic environments. The method is validated in wireless use cases like radio resource management and UAV networks. The work concludes that offline MARL is a promising direction for future wireless applications, highlighting its advantages and limitations.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Offline Multi-Agent RL for 6G] --> B[核心问题/Problem: Online MARL faces cost, safety, scalability limits in 6G networks]
    A --> C[主要方法/Method: Novel offline MARL algorithm based on CQL + meta-learning]
    A --> D[关键结果/Results: Validated in RRM & UAV use cases; highlights advantages & future directions]
    ```

- **[arXiv260105] Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach**
  - **tags:** [ai], [reinforcement learning], [geolocalization, vision-language models, chain of region, haversine distance, retrieval-free]
  - **authors:** Biao Wu, Meng Fang, Ling Chen, Ke Xu, Tao Cheng, Jun Wang
  - **institution:** University of Technology Sydney, University of Liverpool, University College London
  - **link:** https://arxiv.org/pdf/2601.00388
  - **contributions:** 1. Proposes Geo-R, a retrieval-free framework for image geolocalization that uses reinforcement learning. 2. Introduces Chain of Region, a rule-based hierarchical reasoning paradigm to generate interpretable supervision from GPS coordinates. 3. Develops a lightweight RL strategy with coordinate-aligned rewards based on Haversine distance for spatially meaningful feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/986e7d2e1e80caef1d7794a999a5e00e8f2a503a4cae673b68dc3cfafa02122c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing image geolocalization methods by proposing Geo-R, a retrieval-free framework that uses a rule-based Chain of Region for hierarchical reasoning and a reinforcement learning strategy with Haversine distance rewards. The approach improves localization accuracy, generalization, and interpretability without relying on synthetic labels or external retrieval, as validated across multiple benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Vision-Language Reasoning for Geolocalization] --> B[核心问题/Problem: Existing methods rely on synthetic annotations or retrieval, limiting interpretability and generalization.]
        A --> C[主要方法/Method: Proposes Geo-R, a retrieval-free framework using Chain of Region for hierarchical reasoning and RL with Haversine distance rewards.]
        A --> D[关键结果/Results: Improved accuracy, stronger generalization, and more transparent inference, establishing a new retrieval-free paradigm.]
    ```

- **[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models**
  - **tags:** [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]
  - **authors:** Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00423
  - **code:** https://github.com/shengjun-zhang/VisualGRPO
  - **contributions:** 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法在多个去噪步上优化，奖励信号稀疏模糊/Existing methods suffer from sparse & ambiguous rewards over multiple steps]
        C --> C1[提出E-GRPO: 熵感知分组相对策略优化/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]
        C1 --> C2[合并低熵步为高熵SDE采样步，其他步用ODE采样/Merge low-entropy steps for SDE, use ODE elsewhere]
        C1 --> C3[引入多步分组归一化优势计算/Introduce multi-step group normalized advantage]
        D --> D1[在不同奖励设置下验证了方法的有效性/Method effectiveness demonstrated across different reward settings]
    ```

- **[arXiv260105] CPPO: Contrastive Perception for Vision Language Policy Optimization**
  - **tags:** [ai], [reinforcement learning], [contrastive perception loss, entropy shift, vision-language models, policy optimization, multimodal reasoning]
  - **authors:** Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari
  - **institution:** Huawei Technologies Canada Co. Ltd., Huawei Cloud
  - **link:** https://arxiv.org/pdf/2601.00501
  - **contributions:** 1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp
  - **Simple LLM Summary:** CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CPPO: Contrastive Perception for Vision Language Policy Optimization] --> B[核心问题/Problem: Disentangling perception and reasoning tokens in VLMs is difficult with prior methods requiring extra LLMs or indiscriminate rewards]
        A --> C[主要方法/Method: Detect perception tokens via entropy shifts under perturbed images; apply Contrastive Perception Loss (CPL) for consistency/sensitivity]
        A --> D[关键结果/Results: CPPO surpasses previous perception-rewarding methods, avoids extra models, and improves training efficiency/scalability]
    ```

- **[arXiv260105] Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [Graph Neural Network, Q-learning, traffic-aware optimization]
  - **authors:** Sonia Khetarpaul, P Y Sharan
  - **institution:** Shiv Nadar Institution of Eminence
  - **link:** https://arxiv.org/pdf/2601.00607
  - **contributions:** 1. Proposes a novel traffic-aware, graph-based reinforcement learning framework for optimal taxi placement that integrates real-time traffic data (e.g., congestion scores) with historical demand patterns. 2. Employs Graph Neural Network (GNN) embeddings to encode spatial-temporal dependencies within the urban road network, enhancing the agent's understanding of network topology and dynamics. 3. Designs a multi-objective reward mechanism that jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance, leading to significant performance improvements over a baseline.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of inefficient taxi supply-demand matching in smart cities by proposing a framework that models the urban road network as a graph and uses Graph Neural Networks combined with Q-learning to recommend optimal taxi placement hotspots. The method integrates real-time traffic conditions and historical data to optimize for passenger waiting time and driver travel distance. Experiments on a simulated Delhi dataset show the model reduces passenger waiting time by 56% and travel distance by 38% compared to a stochastic baseline.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Traffic-Aware Optimal Taxi Placement<br>Using Graph Neural Network-Based Reinforcement Learning] --> B(核心问题/Problem: Conventional taxi hotspot models overlook dynamic traffic influences.)
        A --> C(主要方法/Method: Graph-based RL with GNN embeddings for spatial-temporal dependencies.)
        A --> D(关键结果/Results: Reduced passenger waiting time by 56% and travel distance by 38%.)
    ```

- **[arXiv260105] Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, robust adaptive control, visual pose estimation, hierarchical learning, safety supervisor]
  - **authors:** Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila
  - **institution:** Tampere University
  - **link:** https://arxiv.org/pdf/2601.00610
  - **contributions:** 1. A hierarchical learning framework that decomposes the goal-reaching control problem into tightly coupled modules, including RL for planning and supervised learning for dynamics modeling. 2. Integration of a model-based robust adaptive controller with the learned dynamics model to guarantee wheel command tracking on slip-prone terrain, ensuring uniform exponential stability. 3. Design of a mathematical safety supervisor to autonomously monitor the robot, stop it on unsafe faults, and guide it back to a safe area, reducing human intervention.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/588ba572bdf33b4091ce883350b57f99b3a43b7383f0188394f0a36af791cfc3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hierarchical learning framework for safe, vision-based goal-reaching control of large mobile robots. The method combines reinforcement learning for motion planning, supervised learning for robot dynamics modeling, and a robust adaptive controller for stable actuation, all overseen by a safety supervisor. Experiments on a 6,000 kg robot confirm the framework's effectiveness and safety guarantees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[RL探索不安全/Unsafe RL Exploration]
        B --> B2[大型机器人应用受限/Limited Application for Large Robots]
        C --> C1[视觉位姿估计/Visual Pose Estimation]
        C --> C2[RL运动规划器/RL Motion Planner]
        C --> C3[监督学习动力学模型/Supervised Learning Dynamics Model]
        C --> C4[鲁棒自适应控制器/Robust Adaptive Controller]
        C --> C5[数学安全监督器/Mathematical Safety Supervisor]
        D --> D1[保证稳定性/Guarantees Stability]
        D --> D2[实验验证有效性/Experimental Validation]
    ```

- **[arXiv260105] RoboReward: General-Purpose Vision-Language Reward Models for Robotics**
  - **tags:** [ai], [reinforcement learning], [vision-language models, reward modeling, reinforcement learning, data augmentation, robotics]
  - **authors:** Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn
  - **institution:** Stanford University, UC Berkeley
  - **link:** https://arxiv.org/pdf/2601.00675
  - **contributions:** 1. Introduces RoboReward, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment and RoboArena. 2. Proposes a negative examples data augmentation pipeline to generate calibrated negatives and near-misses for training. 3. Trains and deploys general-purpose 4B/8B vision-language reward models that outperform larger VLMs and improve real-robot RL policy learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3d93ea0589a39158950d54cbe397e38c6b0ab250252ddc7e117e74f8d59010_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of designing rewards for robotic reinforcement learning by introducing RoboReward, a dataset and benchmark for training vision-language reward models. The method includes a data augmentation pipeline to create negative examples and trains compact 4B/8B parameter models. The results show these models outperform larger VLMs on short-horizon tasks and significantly improve real-robot policy learning compared to a frontier VLM.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RoboReward: General-Purpose Vision-Language Reward Models for Robotics] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Reward design for RL is labor-intensive or brittle/RL奖励设计费时或脆弱]
        C --> C1[Build dataset & benchmark from OXE & RoboArena/基于OXE和RoboArena构建数据集与基准]
        C --> C2[Propose negative examples augmentation pipeline/提出负样本数据增强流程]
        C --> C3[Train RoboReward 4B/8B VLMs/训练RoboReward 4B/8B VLM]
        D --> D1[No existing VLM excels across all tasks/现有VLM无全能模型]
        D --> D2[RoboReward models outperform larger VLMs/RoboReward模型优于更大VLM]
        D --> D3[Improves real-robot RL over Gemini Robotics-ER/在真实机器人RL中大幅超越Gemini]
    ```

- **[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]
  - **authors:** Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang
  - **institution:** HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00677
  - **contributions:** 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[IRPO: Scaling the Bradley-Terry Model via RL] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Pairwise GRMs create O(n²) bottleneck in RL/成对GRM在RL中造成O(n²)瓶颈]
        C --> C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: 将Bradley-Terry融入GRPO实现逐点评分]
        D --> D1[SOTA among pointwise GRMs/在逐点GRM中达到SOTA]
        D --> D2[Outperforms pairwise GRMs in post-training/在训练后评估中优于成对GRM]
    ```

- **[arXiv260105] ARISE: Adaptive Reinforcement Integrated with Swarm Exploration**
  - **tags:** [ai], [reinforcement learning], [swarm intelligence, policy gradient, adaptive exploration, non-stationary rewards, particle swarm]
  - **authors:** Rajiv Chaitanya M, D R Ramesh Babu
  - **institution:** Dayananda Sagar College of Engineering
  - **link:** https://arxiv.org/pdf/2601.00693
  - **contributions:** 1. Introduces ARISE, a lightweight framework that augments standard policy-gradient RL methods with a swarm-based exploration layer., 2. Proposes an adaptive mechanism that modulates exploration intensity based on reward-variance cues., 3. Demonstrates significant performance improvements and robustness, particularly in challenging and non-stationary environments, without altering core algorithmic structures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces ARISE, a framework that enhances reinforcement learning by integrating a swarm-based exploration layer with standard policy-gradient methods to improve exploration. It adaptively blends policy actions with particle-driven proposals and modulates exploration using reward variance. The method shows substantial performance gains on complex tasks and improved robustness in non-stationary environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[ARISE: Adaptive Reinforcement Integrated with Swarm Exploration] --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1[RL探索挑战/RL Exploration Challenge]
        Problem --> P2[非平稳奖励/Non-stationary Rewards]
        Method --> M1[群体智能探索层/Swarm-based Exploration Layer]
        Method --> M2[自适应调节/Adaptive Modulation]
        Method --> M3[策略-粒子混合/Policy-Particle Blending]
        Results --> R1[性能显著提升/Substantial Performance Gains]
        Results --> R2[鲁棒性增强/Enhanced Robustness]
        Results --> R3[架构无关/Architecture-agnostic]
    ```

- **[arXiv260105] Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [reinforcement learning, precision autotuning, contextual bandit, mixed-precision, linear solvers]
  - **authors:** Erin Carson, Xinye Chen
  - **institution:** Charles University, Sorbonne Université, CNRS, LIP6
  - **link:** https://arxiv.org/pdf/2601.00728
  - **contributions:** 1. Proposes a novel reinforcement learning framework formulated as a contextual bandit problem for adaptive precision tuning of numerical algorithms. 2. Applies the framework to iterative refinement for linear solvers, using a Q-table and epsilon-greedy strategy to dynamically select precision configurations based on system features. 3. Demonstrates the framework's effectiveness and generalization, reducing computational cost while maintaining accuracy comparable to double-precision baselines on unseen data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework for adaptive precision tuning, formulated as a contextual bandit problem, to optimize the trade-off between computational cost and accuracy in linear solvers. The method dynamically selects precision configurations based on system features using a Q-learning approach. Empirical results show it reduces cost while maintaining accuracy, and it generalizes well to unseen data.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(高精度计算能耗高/High-precision computing is energy-intensive)
        Problem --> P2(低精度计算可能不稳定/Reduced-precision can be unstable)
        Method --> M1(将问题建模为上下文赌博机/Formulate as Contextual Bandit)
        Method --> M2(使用Q表和epsilon-greedy策略/Use Q-table & epsilon-greedy)
        Method --> M3(应用于线性求解器的迭代精化/Apply to iterative refinement for linear solvers)
        Results --> R1(有效降低计算成本/Effectively reduces computational cost)
        Results --> R2(保持与双精度相当的精度/Maintains accuracy comparable to double-precision)
        Results --> R3(泛化到未见数据/Generalizes to unseen data)
    ```

- **[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty**
  - **tags:** [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]
  - **authors:** Uğurcan Özalp
  - **institution:** Turkish Aerospace
  - **link:** https://arxiv.org/pdf/2601.00737
  - **contributions:** 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Critic网络系统性高估价值/Critic Networks Systematically Overestimate Value Estimates]
        C --> C1[使用单分布评论家建模时序偶然不确定性/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]
        C --> C2[在TD更新中应用基于不确定性的悲观偏差/Apply Uncertainty-Based Pessimistic Bias in TD Updates]
        C --> C3[对评论家和行动者网络使用Dropout正则化/Use Dropout Regularization on Critic and Actor Networks]
        D --> D1[缓解高估偏差/Mitigates Overestimation Bias]
        D --> D2[在随机环境中产生风险规避行为/Leads to Risk-Averse Behavior in Stochastic Environments]
        D --> D3[提高计算效率和训练稳定性/Improves Computational Efficiency and Training Stability]
    ```

- **[arXiv260105] Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks**
  - **tags:** [ai], [reinforcement learning], [multi-functional RIS, NOMA, energy efficiency, hybrid deep reinforcement learning, parametrized sharing]
  - **authors:** Chi-Te Kuo, Li-Hsiang Shen, Jyun-Jhe Huang
  - **institution:** National Central University
  - **link:** https://arxiv.org/pdf/2601.00538
  - **contributions:** 1. Formulates an energy efficiency maximization problem for a multi-MF-RIS-aided NOMA downlink network, jointly optimizing power, beamforming, RIS configurations, and RIS positions. 2. Proposes a Parametrized Sharing scheme for Multi-Agent Hybrid Deep Reinforcement Learning (PMHRL) that combines multi-agent PPO for continuous variables and DQN for discrete variables. 3. Demonstrates through simulations that the proposed PMHRL and multi-MF-RIS architecture achieve superior energy efficiency compared to various benchmarks and alternative system scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of maximizing energy efficiency in downlink NOMA networks assisted by multiple multi-functional RISs. The authors propose a novel parametrized sharing scheme for a multi-agent hybrid deep reinforcement learning algorithm (PMHRL) to jointly optimize power, beamforming, and RIS parameters. Simulation results show that the proposed method achieves the highest energy efficiency compared to other benchmarks and system configurations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("最大化多MF-RIS辅助NOMA网络的能效/Maximize EE of multi-MF-RIS-aided NOMA network")
        Method --> M1("参数共享的多智能体混合DRL (PMHRL)/Parametrized Sharing Multi-Agent Hybrid DRL (PMHRL)")
        M1 --> M1_1("PPO处理连续变量/PPO for continuous variables")
        M1 --> M1_2("DQN处理离散变量/DQN for discrete variables")
        Results --> R1("PMHRL能效最高/PMHRL achieves highest EE")
        Results --> R2("优于无参数共享、纯PPO/DQN基准/Superior to benchmarks without sharing, pure PPO/DQN")
        Results --> R3("多MF-RIS NOMA优于传统RIS等场景/Multi-MF-RIS NOMA outperforms traditional RIS, etc.")
    ```


**cs.AI/cs.LG contains "accelerate" total: 4**
- **[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]
  - **authors:** Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian Möller, Vera Schmitt
  - **institution:** Technische Universität Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen
  - **link:** https://arxiv.org/pdf/2601.00282
  - **contributions:** 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Can Large Language Models Still Explain Themselves?<br/>大语言模型还能解释自己吗？"] --> Problem["Quantization's effect on Self-Explanations is unknown.<br/>量化对自我解释的影响未知"]
        Root --> Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>评估量化后LLM的自然语言解释和反事实示例"]
        Root --> Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>质量/忠实度适度下降；影响因上下文而异"]
    ```

- **[arXiv260105] Robust Assembly Progress Estimation via Deep Metric Learning**
  - **tags:** [cv], [metric learning], [assembly progress estimation, deep metric learning, quadruplet loss, anomaly detection, small-scale dataset]
  - **authors:** Kazuma Miura, Sarthak Pathak, Kazunori Umeda
  - **institution:** Chuo University
  - **link:** https://arxiv.org/pdf/2601.00422
  - **contributions:** 1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robust Assembly Progress Estimation via Deep Metric Learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 装配任务中视觉变化细微导致误分类/Misclassification due to subtle visual changes in assembly tasks]
        C[主要方法/Method: 基于四元组损失和自定义数据加载器的异常检测网络/Anomaly Quadruplet-Net with Quadruplet Loss & custom data loader]
        D[关键结果/Results: 在PC装配数据集上准确率提升1.3%，相邻任务误分类减少1.9%/1.3% accuracy improvement & 1.9% reduction in adjacent task misclassification on PC dataset]
    ```

- **[arXiv260105] SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array**
  - **tags:** [cv], [medical imaging reconstruction], [photoacoustic imaging, point cloud, iterative reconstruction, irregular array, hierarchical optimization]
  - **authors:** Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li
  - **institution:** Peking University, Nanjing University, Pohang University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00551
  - **code:** https://github.com/ShuangLiPKU/SlingBAG-Pro
  - **contributions:** 1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SingBAG Pro: 3D光声成像/SlingBAG Pro: 3D Photoacoustic Imaging] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[不规则阵列重建慢/Irregular Array Reconstruction is Slow]
        C --> C1[点云迭代与分层优化/Point Cloud Iteration & Hierarchical Optimization]
        D --> D1[速度提升2.2倍/Speedup of 2.2x]
        D --> D2[仿真与活体验证/Simulation & In Vivo Validation]
    ```

- **[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization**
  - **tags:** [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]
  - **authors:** Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang
  - **institution:** National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2601.00705
  - **contributions:** 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GS-SLAM的残差驱动致密化效率低/Inefficient residual-driven densification in GS-SLAM]
        C --> C1[一次性密集初始化/One-shot dense initialization]
        C1 --> C2[使用DINOv3特征与置信内点分类器/Using DINOv3 features & confidence-aware inlier classifier]
        C2 --> C3[多视角三角化生成高斯先验/Multi-view triangulation for Gaussian prior]
        D --> D1[收敛加速~20%/~20% faster convergence]
        D --> D2[更高渲染保真度/Higher rendering fidelity]
        D --> D3[实时性能达925 FPS/Real-time performance up to 925 FPS]
    ```
