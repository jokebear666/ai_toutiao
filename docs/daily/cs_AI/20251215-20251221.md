# 20251215-20251221 (cs.AI)

## 2025-12-18

- **[arXiv251218] Promoting Fairness in Information Access within Social Networks**
  - **tags:** [ai], [social network analysis], [resistance distance, greedy algorithm, combinatorial optimization, NP-hard, linear-time algorithm]
  - **authors:** Changan Liu, Xiaotian Zhou, Ahad N. Zehmakan, Zhongzhi Zhang
  - **institution:** Fudan University, Australian National University
  - **link:** https://arxiv.org/pdf/2512.14711
  - **Simple LLM Summary:** The paper proposes a method to enhance fairness in information access within social networks by adding new connections, formulated as an optimization problem using resistance distance. The main technical contribution is a linear-time algorithm that reduces the complexity of a greedy approach, enabling accurate solutions for large networks with millions of nodes.

- **[arXiv251218] Autonomous Source Knowledge Selection in Multi-Domain Adaptation**
  - **tags:** [ai], [domain adaptation], [multi-domain adaptation, density-driven selection, pseudo-label enhancement, feature alignment, self-supervision]
  - **authors:** Keqiuyin Li, Jie Lu, Hua Zuo, Guangquan Zhang
  - **institution:** University of Technology Sydney
  - **link:** https://arxiv.org/pdf/2512.14710
  - **Simple LLM Summary:** The paper proposes AutoS, a method for unsupervised multi-domain adaptation that autonomously selects relevant source samples and models via a density-driven strategy and uses a pre-trained multimodal model to enhance pseudo-labels for self-supervision. Experiments show the method effectively improves transfer performance by focusing on the most transferable knowledge from multiple source domains.

- **[arXiv251218] Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms**
  - **tags:** [mlsys], [others], [learnable Gabor filters, ResNeXt, squeeze-and-excitation attention, spectrograms, deep learning]
  - **authors:** Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut
  - **institution:** Flinders University, PrioriAnalytica
  - **link:** https://arxiv.org/pdf/2512.14714
  - **Simple LLM Summary:** This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios.

- **[arXiv251218] Tourists Profiling by Interest Analysis**
  - **tags:** [ai], [data mining], [graph models, community detection, geo-located data, social network analysis, digital traces]
  - **authors:** Sonia Djebali, Quentin Gabot, Guillaume Guerard
  - **institution:** Léonard De Vinci, Research Center
  - **link:** https://arxiv.org/pdf/2512.14704
  - **Simple LLM Summary:** This paper proposes a method that uses both qualitative and quantitative analysis of digital traces from social networks to profile tourists. It employs graph models and community detection techniques to analyze geo-located data and understand tourist behavior dynamics within attraction networks. The main conclusion is that this combined approach provides a deeper understanding of tourist interests and movement patterns compared to purely quantitative studies.

- **[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts**
  - **tags:** [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]
  - **authors:** Krunal Jesani, Dmitry Ignatov, Radu Timofte
  - **institution:** University of Würzburg
  - **link:** https://arxiv.org/pdf/2512.14706
  - **Simple LLM Summary:** This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations.

- **[arXiv251218] Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning**
  - **tags:** [ai], [neural-symbolic AI], [Vector Symbolic Architecture, self-attention, binding, unbinding, residual streams, hyperdimensional computing, chain-of-thought]
  - **authors:** Sahil Rajesh Dhayalkar
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.14709
  - **Simple LLM Summary:** This paper interprets transformer self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA), where attention performs soft binding/unbinding of roles and fillers. It uses this perspective to explain the models' reasoning capabilities and brittleness, and proposes VSA-inspired architectural modifications to improve logical reliability. The core conclusion is that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and robust reasoning systems.

- **[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI**
  - **tags:** [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]
  - **authors:** Ryan Cartularo
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.14712
  - **Simple LLM Summary:** This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.

- **[arXiv251218] How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection**
  - **tags:** [mlsys], [fault-tolerance], [bit-level fault injection, gradient-based sensitivity estimation, differentiable fault analysis, semantic steering, SBERT embeddings, perplexity scoring]
  - **authors:** Zafaryab Haider, Md Hafizur Rahman, Shane Moeykens, Vijay Devabhaktuni, Prabuddha Chakraborty
  - **institution:** University of Maine, Illinois State University
  - **link:** https://arxiv.org/pdf/2512.14715
  - **Simple LLM Summary:** The paper introduces BLADE, a differentiable fault analysis framework that uses gradient-based sensitivity estimation to identify and flip specific bits in a quantized vision-language model's weights, steering its generated captions to change semantic meaning while preserving grammatical fluency. It concludes that semantic drift from low-level bit flips is predictable and controllable, revealing vulnerabilities in generative AI systems and opening pathways for robustness testing and adversarial defense.

- **[arXiv251218] Hybrid Attribution Priors for Explainable and Robust Model Training**
  - **tags:** [ai], [explainable ai], [class-aware attribution prior, explanation-guided learning, attribution priors, small language models, model interpretability]
  - **authors:** Zhuoran Zhang, Feng Zhang, Shangyuan Li, Yang Shi, Yuanxing Zhang, Wei Chen, Tengjiao Wang, Kam-Fai Wong
  - **institution:** Peking University, Kling Team, CUHK
  - **link:** https://arxiv.org/pdf/2512.14719
  - **Simple LLM Summary:** This paper proposes a Class-Aware Attribution Prior (CAP) framework and its hybrid variant, CAP Hybrid, to generate more discriminative attribution priors for training small language models. By aligning a model's self-attribution with these enriched priors, the method encourages the learning of diverse, decision-relevant features. Experiments show the approach consistently enhances both model interpretability and robustness across various data settings.

- **[arXiv251218] SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting**
  - **tags:** [ai], [time series forecasting], [spectral entropy, dependency evaluator, signed graph constructor, context spatial extractor, channel independence, channel dependence]
  - **authors:** Feng Xiong, Zongxia Xie, Yanru Sun, Haoyu Wang, Jianhong Lin
  - **institution:** Tianjin University, Fudan University
  - **link:** https://arxiv.org/pdf/2512.14718
  - **Simple LLM Summary:** This paper proposes SEED, a framework for multivariate time series forecasting that uses spectral entropy to evaluate and model spatial-temporal dependencies. It introduces components like a Dependency Evaluator and a Signed Graph Constructor to adaptively balance modeling strategies and preserve negative correlations. Experiments on 12 datasets show SEED achieves state-of-the-art performance, demonstrating its effectiveness.

- **[arXiv251218] Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion**
  - **tags:** [mlsys], [diffusion inference], [graph neural network, diffusion model, generative modeling, unstructured meshes, urban flow simulation, score-based diffusion]
  - **authors:** Francisco Giral, Álvaro Manzano, Ignacio Gómez, Petros Koumoutsakos, Soledad Le Clainche
  - **institution:** Universidad Politécnica de Madrid, Harvard University
  - **link:** https://arxiv.org/pdf/2512.14725
  - **Simple LLM Summary:** This paper proposes a generative diffusion framework that combines a hierarchical graph neural network with score-based diffusion modeling to synthesize steady-state urban wind fields from geometry data on unstructured meshes. The model generalizes to unseen geometries, accurately capturing key flow structures like wakes and recirculation zones, and provides uncertainty-aware predictions. This work represents a step towards foundation models for the built environment, enabling rapid evaluation of urban design decisions.

- **[arXiv251218] HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers**
  - **tags:** [mlsys], [others], [hierarchical attention transformers, curriculum learning, groebner bases, polynomial systems, computational cost analysis]
  - **authors:** Mohamed Malhou, Ludovic Perret, Kristin Lauter
  - **institution:** Meta Superintelligence Labs (FAIR), Sorbonne Université, CNRS, LIP6, EPITA, EPITA Research Lab (LRE)
  - **link:** https://arxiv.org/pdf/2512.14722
  - **Simple LLM Summary:** This paper introduces HATSolver, a method that improves upon prior transformer-based approaches by using Hierarchical Attention Transformers (HATs) to compute Gröbner bases for solving systems of multivariate polynomial equations. The HAT architecture incorporates a tree-structured inductive bias to model hierarchical data relationships, achieving significant computational savings over flat attention models. Combined with curriculum learning, the method can solve much larger problem instances than previous work.

- **[arXiv251218] Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, entanglement, interference, offline reinforcement learning, Decision Transformer]
  - **authors:** Abraham Itzhak Weinberg
  - **institution:** AI-WEINBERG, AI Experts
  - **link:** https://arxiv.org/pdf/2512.14726
  - **Simple LLM Summary:** The paper introduces the Quantum Decision Transformer (QDT), a novel architecture that integrates quantum-inspired attention with entanglement and quantum feedforward networks with interference to improve offline reinforcement learning. It demonstrates a dramatic performance improvement over standard Decision Transformers and shows that the synergy between its quantum-inspired components is crucial for its success.

- **[arXiv251218] SoMe: A Realistic Benchmark for LLM-based Social Media Agents**
  - **tags:** [mlsys], [others], [SoMe benchmark, social media agents, LLM-based agents, agent tools, task evaluation, quantitative analysis, qualitative analysis]
  - **authors:** Dizhan Xue, Jing Cui, Shengsheng Qian, Chuanrui Hu, Changsheng Xu
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tianjin University of Technology; Nanjing University of Posts and Telecommunications; Peng Cheng Laboratory
  - **link:** https://arxiv.org/pdf/2512.14720
  - **Simple LLM Summary:** This paper introduces SoMe, a comprehensive benchmark for evaluating LLM-based social media agents across diverse tasks using real social media data and agent tools. The evaluation shows that current LLMs, both closed and open-source, perform unsatisfactorily on these realistic social media agent tasks. SoMe serves as a challenging testbed to advance future social media agent development.

- **[arXiv251218] A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications**
  - **tags:** [ai], [medical machine learning], [conformal prediction, uncertainty quantification, finite sample theory, calibration]
  - **authors:** Klaus-Rudolf Kladny, Bernhard Schölkopf, Lisa Koch, Christian F. Baumgartner, Michael Muehlebach
  - **institution:** Max Planck Institute for Intelligent Systems, University of Bern, University of Tübingen, University of Lucerne
  - **link:** https://arxiv.org/pdf/2512.14727
  - **Simple LLM Summary:** This paper critically examines conformal prediction, a method for providing statistically guaranteed uncertainty estimates from machine learning models using a calibration dataset. It argues that while the theoretical guarantees hold for any calibration set size, the practical utility of these guarantees is highly dependent on having a sufficiently large calibration sample. This critique is particularly relevant for medical applications where data is often scarce, and the authors support their argument with an empirical demonstration on a medical image classification task.

- **[arXiv251218] INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT**
  - **tags:** [mlsys], [multi-modal inference], [planner-executor framework, large language models (LLMs), vision-language models (VLMs), segmentation models, image processing]
  - **authors:** Idan Tankel, Nir Mazor, Rafi Brada, Christina LeBedis, Guy ben-Yosef
  - **institution:** GE Healthcare Technology and Innovation Center, Boston Medical Center
  - **link:** https://arxiv.org/pdf/2512.14732
  - **Simple LLM Summary:** This paper proposes a framework that uses a planner-executor approach, where an LLM planner generates Python scripts to automate the detection and reporting of incidental findings in abdominal CT scans, and an executor runs these scripts using VLMs and segmentation models. The method is fully automatic and end-to-end. The results show that this framework outperforms pure VLM-based approaches in accuracy and efficiency for managing incidental findings.

- **[arXiv251218] Semantic Geometry for policy-constrained interpretation**
  - **tags:** [ai], [semantic interpretation], [geometric framework, spherical convex regions, constrained optimization, policy constraints, information theory, Bayesian inference, sheaf-theoretic semantics]
  - **authors:** Nikit Phadke
  - **institution:** Independent researcher (based on gmail address)
  - **link:** https://arxiv.org/pdf/2512.14731
  - **Simple LLM Summary:** The paper proposes a geometric framework for semantic interpretation where meaning is represented as directions on a unit sphere and policy constraints are applied as explicit priors. This approach separates evidence processing from policy rules, enabling provable prevention of hallucinated commitments. Empirical validation on financial data shows zero hallucinated approvals, demonstrating the method's effectiveness in high-stakes, policy-constrained domains.

- **[arXiv251218] Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol**
  - **tags:** [mlsys], [others], [zero-knowledge proof, Model Context Protocol, privacy-preserving audit, Circom, mutual auditing]
  - **authors:** Guanlin Jing, Huayi Qi
  - **institution:** Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.14737
  - **Simple LLM Summary:** This paper introduces a framework that integrates zero-knowledge proofs with the Model Context Protocol (MCP) to enable verifiable, privacy-preserving audits of agent communications without revealing message content. It achieves mutual auditing for compliance and billing while maintaining data authenticity and privacy with negligible latency overhead. The authors implement the system, claiming it is the first to offer such verifiable mutual auditing for agent communications.

- **[arXiv251218] Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)**
  - **tags:** [mlsys], [others], [quantum machine learning, hybrid quantum computing, amplitude encoding, entanglement encoding, hierarchical threat detection, anomaly detection, intrusion confirmation, multiattack classification, deep neural networks, ensemble classifiers]
  - **authors:** Tan Le, Van Le, Sachin Shetty
  - **institution:** Hampton University, Virginia Polytechnic Institute and State University, Old Dominion University
  - **link:** https://arxiv.org/pdf/2512.14742
  - **Simple LLM Summary:** This paper proposes a hierarchical cybersecurity framework for Open Radio Access Networks (O-RAN) that integrates hybrid quantum computing and machine learning. The method uses quantum-inspired feature encodings (amplitude- and entanglement-based) with deep and ensemble classifiers for anomaly detection, intrusion confirmation, and multiattack classification. The framework demonstrates near-perfect accuracy, high recall, and strong interpretability, indicating readiness for scalable deployment in O-RAN environments.

- **[arXiv251218] Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs**
  - **tags:** [mlsys], [post-training], [backdoor attack, continual fine-tuning, gradient alignment, P-Trojan, persistence]
  - **authors:** Jing Cui, Yufei Han, Jianbin Jiao, Junge Zhang
  - **institution:** University of Chinese Academy of Sciences, INRIA, Institute of Automation, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.14741
  - **Simple LLM Summary:** The paper proposes P-Trojan, a backdoor attack method that optimizes for persistence by aligning poisoned gradients with clean task gradients on token embeddings during model poisoning. Experiments show it maintains over 99% backdoor persistence across continual fine-tuning updates on models like Qwen2.5 and LLaMA3 without harming clean-task accuracy. This highlights the need for stronger defenses and persistence-aware evaluation in real-world LLM adaptation pipelines.

- **[arXiv251218] Factor(U,T): Controlling Untrusted AI by Monitoring their Plans**
  - **tags:** [ai], [ai safety], [factored cognition, control protocols, task decomposition, untrusted decomposer, monitoring, BigCodeBench, AUROC]
  - **authors:** Edward Lue Chee Lip, Anthony Channg, Diana Kim, Aaron Sandoval, Kevin Zhu
  - **institution:** Algoverse AI Research, Colorado State University, Orange Coast College
  - **link:** https://arxiv.org/pdf/2512.14745
  - **Simple LLM Summary:** This paper introduces Factor(U,T), a protocol where an untrusted, potentially malicious AI model decomposes a complex task into subtasks, which are then implemented by trusted models. It finds that monitoring only the natural language decomposition plans is ineffective for detecting attacks, whereas monitoring the concrete code solutions of the subtasks provides strong safety and discrimination. The main conclusion is that implementation-context monitoring is crucial for safety when using untrusted models for task decomposition.

- **[arXiv251218] CODE ACROSTIC: Robust Watermarking for Code Generation**
  - **tags:** [mlsys], [post-training], [watermarking, cue list, low-entropy, high-entropy, comment removal attack]
  - **authors:** Li Lin, Siyuan Xin, Yang Cao, Xiaochun Cao
  - **institution:** Institute of Science Tokyo, Shanghai University, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.14753
  - **Simple LLM Summary:** This paper proposes a robust watermarking method for LLM-generated code that uses a Cue List to distinguish between low and high-entropy parts of the code for targeted watermark injection. It addresses the vulnerability of existing methods to comment removal attacks. The evaluation shows the method achieves higher detectability and usability compared to state-of-the-art techniques.

- **[arXiv251218] Cyberswarm: a novel swarm intelligence algorithm inspired by cyber community dynamics**
  - **tags:** [ai], [recommendation systems], [swarm intelligence, dynamic hypergraph, centrality-based feature extraction, Node2Vec embeddings, message-passing mechanisms, hierarchical graph modeling]
  - **authors:** Abdelsadeq Elfergany, Ammar Adl, Mohammed Kayed
  - **institution:** Not specified in provided text
  - **link:** https://arxiv.org/pdf/2512.14752
  - **Simple LLM Summary:** The paper introduces Cyberswarm, a general-purpose swarm intelligence algorithm for recommendation systems that models user preferences and community influences using a dynamic hypergraph structure with techniques like Node2Vec and message-passing. Experimental results show it outperforms baseline methods on metrics like HR, MRR, and NDCG across various tasks. The work demonstrates the effectiveness of integrating swarm intelligence with network dynamics for adaptive and precise recommendations.

- **[arXiv251218] One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs**
  - **tags:** [mlsys], [post-training], [jailbreak attack, adversarial prompt, representation-level probing, linear separability, Probe-Guided Projection (PGP), transferability, finetuning, pretrained model]
  - **authors:** Yixin Tan, Zhe Yu, Jun Sakuma
  - **institution:** Institute of Science Tokyo, Riken AIP
  - **link:** https://arxiv.org/pdf/2512.14751
  - **Simple LLM Summary:** This paper investigates how jailbreak vulnerabilities transfer from pretrained to finetuned LLMs. It introduces the Probe-Guided Projection (PGP) attack, which uses representation-level probing to guide adversarial prompt optimization for better transferability. The main conclusion is that the pretrain-finetune paradigm inherently amplifies security risks, as vulnerabilities encoded in the pretrained model's representations are inherited by its finetuned variants.

- **[arXiv251218] Revisiting the Reliability of Language Models in Instruction-Following**
  - **tags:** [ai], [llm evaluation], [instruction-following, reliability, data augmentation, benchmark, IFEval++, reliable@k]
  - **authors:** Jianshuo Dong, Yutong Zhang, Yan Liu, Zhenyu Zhong, Tao Wei, Chao Zhang, Han Qiu
  - **institution:** Tsinghua University, Ant Group
  - **link:** https://arxiv.org/pdf/2512.14754
  - **Simple LLM Summary:** The paper introduces a new metric, reliable@k, and an automated data augmentation pipeline to generate "cousin prompts" for evaluating nuance-oriented reliability in LLMs, constructing the IFEval++ benchmark. It finds that current LLMs show significant performance drops (up to 61.8%) with nuanced prompt variations, highlighting a crucial gap in real-world reliability.

- **[arXiv251218] CAPE: Capability Achievement via Policy Execution**
  - **tags:** [mlsys], [post-training], [capability engineering, policy execution, specification language, verification, DPO, contextual objectivity, verification-fidelity scaling]
  - **authors:** David Ball
  - **institution:** Superficial Labs
  - **link:** https://arxiv.org/pdf/2512.14761
  - **Simple LLM Summary:** The paper introduces CAPE, a protocol for Capability Engineering that implements a Specify-&gt;Verify-&gt;Correct-&gt;Train loop to convert requirements into executable specifications and train models to satisfy them by default. It demonstrates that CAPE reduces policy violation rates by 81% compared to DPO and significantly lowers costs and development timelines by using reusable specifications.

- **[arXiv251218] Guided Discrete Diffusion for Constraint Satisfaction Problems**
  - **tags:** [ai], [generative models], [discrete diffusion, unsupervised learning, constraint satisfaction, sudoku]
  - **authors:** Justin Jung
  - **institution:** unknown
  - **link:** https://arxiv.org/pdf/2512.14765
  - **Simple LLM Summary:** This paper proposes an unsupervised discrete diffusion model to learn the distribution of Sudoku puzzles, aiming to capture their structural patterns. The method avoids the need for supervised datasets and directly handles the discrete nature of the constraint satisfaction problem. The authors demonstrate its capability to solve Sudoku puzzles without supervision.

- **[arXiv251218] Workflows vs Agents for Code Translation**
  - **tags:** [mlsys], [llm inference], [Model Context Protocol (MCP), syntax repair, code translation, MATLAB-to-HDL, agentic framework, conditional retrieval]
  - **authors:** Henry Gray, Tom Yotam, Octavian Udrea
  - **institution:** Code Metal
  - **link:** https://arxiv.org/pdf/2512.14762
  - **Simple LLM Summary:** This paper compares two LLM-driven methods for syntax repair in a MATLAB-to-hardware-description-language translation pipeline: a fixed expert-designed workflow and a more autonomous agentic approach using the Model Context Protocol (MCP). The agentic approach, which dynamically selects tools, was more effective at resolving syntax errors, especially for small and mid-sized models, leading to significant downstream improvements in simulation success rates.

- **[arXiv251218] Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification**
  - **tags:** [mlsys], [multi-modal inference], [self-reflection, cross-model verification, dual-assessment, uncertainty estimation, selective prediction, hallucination mitigation]
  - **authors:** Xixian Wu, Yang Ou, Pengchao Tian, Zian Yang, Jielei Zhang, Peiyi Li, Longwen Gao
  - **institution:** Bilibili Inc.
  - **link:** https://arxiv.org/pdf/2512.14770
  - **Simple LLM Summary:** This paper proposes DAVR, a framework that enhances VQA reliability by combining a model's self-assessment of its answer confidence with cross-verification using external models to reduce hallucinations. It achieved top results in a reliability challenge, demonstrating its effectiveness in improving the trustworthiness of vision-language model responses.

- **[arXiv251218] GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge**
  - **tags:** [ai], [knowledge graph reasoning], [GR-Agent, adaptive graph reasoning, incomplete knowledge graphs, agent environment interaction, reasoning paths]
  - **authors:** Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Hongkuan Zhou, Jiaoyan Chen, Steffen Staab, Yuan He, Evgeny Kharlamov
  - **institution:** University of Oslo, Bosch Center for AI, University of Stuttgart, University of Oxford, Amazon, The University of Manchester, University of Southampton
  - **link:** https://arxiv.org/pdf/2512.14766
  - **Simple LLM Summary:** The paper introduces GR-Agent, an adaptive graph reasoning agent that formalizes knowledge graph question answering as agent-environment interaction to handle incomplete knowledge graphs by using graph reasoning tools and memory of evidence. It demonstrates that existing methods degrade under incompleteness, while GR-Agent outperforms non-training baselines and matches training-based methods in both complete and incomplete settings.

- **[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation**
  - **tags:** [mlsys], [others], [Shapley-CMI, Private Set Intersection, Conditional Mutual Information, Vertical Federated Learning, data valuation]
  - **authors:** Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza
  - **institution:** Mondragon Unibertsitatea
  - **link:** https://arxiv.org/pdf/2512.14767
  - **Simple LLM Summary:** This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.

- **[arXiv251218] Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis**
  - **tags:** [ai], [large language models], [transformer architecture, structural hallucination, Licensing Oracle, hybrid systems, statistical ontology]
  - **authors:** Richard Ackermann, Simeon Emanuilov
  - **institution:** RA Software, Sofia University “St. Kliment Ohridski”
  - **link:** https://arxiv.org/pdf/2512.14801
  - **Simple LLM Summary:** The paper argues that hallucination in LLMs is an architectural inevitability of transformers, which model token co-occurrence rather than the world, and demonstrates through a Licensing Oracle that external truth-validation modules are required for reliable abstention. It concludes that hallucination is a structural property, not a correctable incentive problem, necessitating hybrid systems to separate linguistic fluency from epistemic responsibility.

- **[arXiv251218] IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection**
  - **tags:** [mlsys], [llm inference], [Retrieval-Augmented Generation (RAG), Graph RAG, knowledge injection, error taxonomy, Terraform, IaC-Eval benchmark]
  - **authors:** Roman Nekrasov, Stefano Fossati, Indika Kumara, Damian Andrew Tamburri, Willem-Jan van den Heuvel
  - **institution:** Jheronimus Academy of Data Science, Tilburg University, Eindhoven University of Technology, University of Sannio
  - **link:** https://arxiv.org/pdf/2512.14792
  - **Simple LLM Summary:** This paper investigates improving Large Language Model (LLM) generation of Infrastructure as Code (IaC) by injecting structured configuration knowledge using techniques from naive to Graph RAG. The study finds that while knowledge injection significantly boosts technical correctness, LLMs still struggle with nuanced user intent, revealing a "Correctness-Congruence Gap" where they are better coders than architects.

- **[arXiv251218] Sharing State Between Prompts and Programs**
  - **tags:** [mlsys], [llm inference], [natural language programming, shared program state, natural function interface, interoperability, Nightjar]
  - **authors:** Ellie Y. Cheng, Logan Weber, Tian Jin, Michael Carbin
  - **institution:** MIT CSAIL
  - **link:** https://arxiv.org/pdf/2512.14805
  - **Simple LLM Summary:** This paper introduces a programming abstraction called "shared program state" to enable seamless interoperability between natural language code (prompts) and formal program code (e.g., Python). It implements this abstraction in the Nightjar system, allowing natural code to directly read and write program variables. The results show that Nightjar programs can achieve higher task accuracy (+4-19%) and reduce lines of code by 39.6% on average, though with a runtime overhead of 0.4-4.3x compared to manual implementations.

- **[arXiv251218] Let the Barbarians In: How AI Can Accelerate Systems Performance Research**
  - **tags:** [mlsys], [cluster infrastructure], [AI-Driven Research for Systems (ADRS), OpenEvolve, GEPA, ShinkaEvolve, multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling]
  - **authors:** Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Shubham Agarwal, Mert Cemri, Bowen Wang, Alexander Krentsel, Tian Xia, Jongseok Park, Shuo Yang, Jeff Chen, Lakshya Agrawal, Ashwin Naren, Shulu Li, Ruiying Ma, Aditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, Ion Stoica
  - **institution:** UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.14806
  - **Simple LLM Summary:** This paper introduces AI-Driven Research for Systems (ADRS), a method using AI to automate the generation, evaluation, and refinement of performance-optimizing algorithms for computer systems. Through case studies with frameworks like OpenEvolve, it demonstrates that ADRS can produce solutions matching or surpassing human-designed state-of-the-art. The work outlines best practices for applying ADRS and discusses its potential to shift researcher effort toward problem formulation and strategic oversight.

- **[arXiv251218] MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber**
  - **tags:** [mlsys], [llm inference], [multi-agent system, secure communication layer, ontology-aligned messaging, MITRE ATT&CK, Lightweight Random Forest]
  - **authors:** Arth Bhardwaj, Sia Godika, Yuvam Loonker
  - **institution:** Saint Francis High School, Massachusetts Institute of Technology, JBCN International School
  - **link:** https://arxiv.org/pdf/2512.14846
  - **Simple LLM Summary:** The paper proposes MALCDF, a real-time cyber defense framework where four specialized LLM agents (Detection, Intelligence, Response, Analysis) collaborate via a secure communication layer. It demonstrates that this multi-agent approach with encrypted, ontology-aligned messaging outperforms a lightweight ML baseline and a single-LLM setup in detection accuracy and F1-score on a test stream. The conclusion is that coordinating simple LLM agents improves practical, real-time cyber defense.

- **[arXiv251218] Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks**
  - **tags:** [mlsys], [llm inference], [penetration testing, prompt injection, agentic ai, autogen, crewai, ssrf, sql injection]
  - **authors:** Viet K. Nguyen, Mohammad I. Husain
  - **institution:** Cal Poly Pomona
  - **link:** https://arxiv.org/pdf/2512.14860
  - **Simple LLM Summary:** This paper conducts systematic penetration testing on agentic AI systems, comparing five LLM models across two frameworks using 13 attack scenarios. It finds significant security vulnerabilities, with over half of malicious prompts succeeding, and identifies novel defensive patterns like "hallucinated compliance".

- **[arXiv251218] A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials**
  - **tags:** [mlsys], [multi-modal training], [graph neural network, k-nearest neighbor, random forest, hyperparameter optimization, feature selection]
  - **authors:** Mahmuda Sharmin, Taihao Han, Jie Huang, Narayanan Neithalath, Gaurav Sant, Aditya Kumar
  - **institution:** Missouri University of Science and Technology, Arizona State University, University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.14855
  - **Simple LLM Summary:** This paper proposes a method for applying Graph Neural Networks (GNNs) to numerical data in cementitious materials research by converting tabular data into graph representations using the k-nearest neighbor approach. The study systematically optimizes model hyperparameters and feature selection, finding that the GNN's performance is comparable to a benchmark random forest model. The work establishes a foundational roadmap for transitioning to advanced, multi-modal, and physics-informed AI architectures for material design.

- **[arXiv251218] Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse**
  - **tags:** [ai], [machine learning theory], [Bregman projection, entropy reservoir, information geometry, model collapse, self-referential learning]
  - **authors:** Jingwei Chen
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.14879
  - **Simple LLM Summary:** The paper introduces the Entropy-Reservoir Bregman Projection (ERBP) framework, which models self-referential learning as a stochastic Bregman projection sequence in distribution space. It shows that injecting an entropy reservoir stabilizes the dynamics and prevents model collapse, unifying various empirical fixes into a single quantitative design rule.

- **[arXiv251218] OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams**
  - **tags:** [ai], [online learning], [weighted average, exponential weighted moving average, online linear regression, pseudo-inverse, coefficient of determination]
  - **authors:** Mohammad Abu-Shaira, Alejandro Rodriguez, Greg Speegle, Victor Sheng, Ishfaq Ahmad
  - **institution:** Baylor University, Texas Tech University, University of Texas at Arlington
  - **link:** https://arxiv.org/pdf/2512.14892
  - **Simple LLM Summary:** The paper introduces OLR-WA, a novel multivariate online linear regression model based on a weighted average approach. It demonstrates rapid convergence and performance comparable to batch regression, while uniquely handling both temporal drift and confidence-based data scenarios.

- **[arXiv251218] Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media**
  - **tags:** [mlsys], [llm inference], [large language models, knowledge graphs, viewpoint classification, fine-tuning, wikidata, semantic enrichment]
  - **authors:** Massimiliano Fadda, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino
  - **institution:** University of Cagliari, The Open University
  - **link:** https://arxiv.org/pdf/2512.14887
  - **Simple LLM Summary:** This paper improves a pipeline for analyzing political viewpoints in news by fine-tuning Large Language Models for classification and enriching claim representations with semantic actor descriptions from Wikidata. The integrated approach, evaluated on UK immigration debate data, shows that combining fine-tuned LLMs with knowledge graph context yields the best performance, particularly with models capable of processing long inputs.

- **[arXiv251218] DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, structured drug knowledge, pharmacy question-answering, external knowledge integration]
  - **authors:** Houman Kazemzadeh, Kiarash Mokhtari Dizaji, Seyed Reza Tavakoli, Farbod Davoodi, MohammadReza KarimiNejad, Parham Abed Azad, Ali Sabzi, Armin Khosravi, Siavash Ahmadi, Mohammad Hossein Rohban, Glolamali Aminian, Tahereh Javaheri
  - **institution:** Tehran University of Medical Sciences, Sharif University of Technology, Amir Kabir University of Technology, Missouri University of Science and Technology, The Alan Turing Institute, Boston University
  - **link:** https://arxiv.org/pdf/2512.14896
  - **Simple LLM Summary:** The paper introduces DrugRAG, a three-step retrieval-augmented generation pipeline that integrates external structured drug knowledge into LLM prompts to improve accuracy on pharmacy QA tasks. It demonstrates that this external method enhances performance across multiple models without modifying their architecture, providing a practical approach for evidence-based AI in pharmacy.

- **[arXiv251218] AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally**
  - **tags:** [mlsys], [others], [multi-agent reasoning, chain-of-responsibility, modular architecture, governance mechanisms, multilingual interactions, real-time tools]
  - **authors:** Nadine Angela Cantonjos, Arpita Biswas
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2512.14910
  - **Simple LLM Summary:** This paper presents AgroAskAI, a multi-agent AI framework designed to support smallholder farmers with climate adaptation queries. It uses a modular, role-specialized architecture coordinated via a chain-of-responsibility approach, integrating real-time data and multilingual support. The experimental results show that this system delivers more actionable and grounded outputs for agricultural decision support.

- **[arXiv251218] Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections**
  - **tags:** [ai], [imitation learning], [on-policy expert corrections, DAgger, covariate shift, rejection sampling, supervised fine-tuning, multi-turn agents]
  - **authors:** Niklas Lauffer, Xiang Deng, Srivatsa Kundurthy, Brad Kenstler, Jeff Da
  - **institution:** UC Berkeley, Cornell University, Scale AI
  - **link:** https://arxiv.org/pdf/2512.14895
  - **Simple LLM Summary:** This paper proposes a novel data generation method called on-policy expert corrections (OECs) to address covariate shift in imitation learning for multi-turn language model agents. The method generates partially on-policy data by starting rollouts with a student model and switching to an expert model mid-trajectory. Experiments on software engineering tasks show OECs yield a 13-14% improvement over traditional imitation learning, demonstrating the need to combine expert demonstrations with on-policy data for effective agent training.

- **[arXiv251218] Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models**
  - **tags:** [mlsys], [multi-modal training], [LoRA, dataset translation, visual question answering, instruction tuning, parameter-efficient fine-tuning]
  - **authors:** George-Andrei Dima, Dumitru-Clementin Cercel
  - **institution:** National University of Science and Technology POLITEHNICA Bucharest
  - **link:** https://arxiv.org/pdf/2512.14926
  - **Simple LLM Summary:** This paper introduces a parameter-efficient method for adapting vision-language models to Romanian by translating the Flickr30k dataset and generating QA pairs, then fine-tuning models like LLaMA, LLaVA, and Qwen2 using LoRA. The results show significant improvements in Romanian visual QA and image captioning, with the Qwen2-VL-RoVQA model achieving the best performance and reduced grammatical errors.

- **[arXiv251218] Improving Pre-trained Segmentation Models using Post-Processing**
  - **tags:** [mlsys], [post-training], [adaptive post-processing, glioma segmentation, brain MRI, BraTS challenge, resource-aware AI]
  - **authors:** Abhijeet Parida, Daniel Capellán-Martín, Zhifan Jiang, Nishad Kulkarni, Krithika Iyer, Austin Tapp, Syed Muhammad Anwar, María J. Ledesma-Carbayo, Marius George Linguraru
  - **institution:** Children's National Hospital, Universidad Politécnica de Madrid, George Washington University
  - **link:** https://arxiv.org/pdf/2512.14937
  - **Simple LLM Summary:** This paper proposes adaptive post-processing techniques to refine the segmentation outputs of large-scale pre-trained models for brain tumor (glioma) segmentation in MRI, addressing systematic errors like false positives. The method was validated in BraTS 2025 challenges, showing significant metric improvements. The work advocates for a shift from complex model architectures to efficient, clinically aligned post-processing for more precise and sustainable medical image analysis.

- **[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation**
  - **tags:** [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]
  - **authors:** Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou
  - **institution:** The Chinese University of Hong Kong, Snap Inc.
  - **link:** https://arxiv.org/pdf/2512.14938
  - **Simple LLM Summary:** The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.

- **[arXiv251218] EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving**
  - **tags:** [mlsys], [llm inference], [KV-cache management, lossy compression, adaptive eviction, utility function, multi-tier storage]
  - **authors:** Shaoting Feng, Yuhan Liu, Hanchen Li, Xiaokun Chen, Samuel Shen, Kuntai Du, Zhuohan Gu, Rui Zhang, Yuyang Huang, Yihua Cheng, Jiayi Yao, Qizheng Zhang, Ganesh Ananthanarayanan, Junchen Jiang
  - **institution:** University of Chicago, UC Berkeley, Tensormesh, Inc., MIT, UC Santa Cruz, Stanford, Microsoft
  - **link:** https://arxiv.org/pdf/2512.14946
  - **Simple LLM Summary:** EVICPRESS is a KV-cache management system that jointly optimizes lossy compression and adaptive eviction across multiple storage tiers using a unified utility function. It improves LLM inference efficiency by maximizing fast-tier cache hit rates while preserving generation quality through context-aware compression. Evaluations show it achieves up to 2.19x faster time-to-first-token at equivalent quality compared to baselines.

- **[arXiv251218] Prompt Repetition Improves Non-Reasoning LLMs**
  - **tags:** [mlsys], [llm inference], [prompt repetition, causal language model, attention mechanism, non-reasoning tasks]
  - **authors:** Yaniv Leviathan, Matan Kalman, Yossi Matias
  - **institution:** Google Research
  - **link:** https://arxiv.org/pdf/2512.14982
  - **Simple LLM Summary:** The paper proposes a simple method of repeating the input prompt to improve the performance of LLMs on non-reasoning tasks. This technique allows all prompt tokens to attend to each other within the causal attention mechanism, addressing order sensitivity. The authors demonstrate that this method boosts accuracy for models like Gemini, GPT, Claude, and Deepseek without increasing output length or latency.

- **[arXiv251218] Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams**
  - **tags:** [mlsys], [multi-modal inference], [multimodal reasoning, chain-of-thought prompting, ablation studies, occlusion-based interpretability, benchmark evaluation, modality fusion]
  - **authors:** Yiming Cui, Xin Yao, Yuxuan Qin, Xin Li, Shijin Wang, Guoping Hu
  - **institution:** State Key Laboratory of Cognitive Intelligence, iFLYTEK AI Research
  - **link:** https://arxiv.org/pdf/2512.14989
  - **Simple LLM Summary:** This paper systematically evaluates 40 multimodal large language models on a benchmark of chemistry Olympiad questions requiring visual and textual reasoning. The core method involves using chain-of-thought prompting and interpretability techniques like ablation and occlusion. The main conclusion is that current models struggle with modality fusion, but chain-of-thought improves both accuracy and visual grounding, revealing critical limitations in scientific reasoning.

- **[arXiv251218] Where is the Watermark? Interpretable Watermark Detection at the Block Level**
  - **tags:** [ai], [image watermarking], [discrete wavelet transform, block-wise embedding, detection maps, post-hoc watermarking]
  - **authors:** Maria Bulychev, Neil G. Marchant, Benjamin I. P. Rubinstein
  - **institution:** University of Melbourne
  - **link:** https://arxiv.org/pdf/2512.14994
  - **Simple LLM Summary:** This paper introduces an interpretable, post-hoc image watermarking method that embeds signals in the discrete wavelet transform domain using a statistical block-wise strategy. It generates detection maps to show which specific regions of an image are watermarked or altered. The method maintains strong robustness against common image transformations and high imperceptibility while providing more interpretable detection than prior approaches.

- **[arXiv251218] Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent**
  - **tags:** [mlsys], [fault-tolerance], [bug reproduction, LLM, iterative generate-validate-refine, agentic AI]
  - **authors:** Mehil B Shah, Mohammad Masudur Rahman, Foutse Khomh
  - **institution:** Dalhousie University, Polytechnique Montreal
  - **link:** https://arxiv.org/pdf/2512.14990
  - **Simple LLM Summary:** The paper presents RepGen, an automated approach that uses an LLM-based intelligent agent to reproduce deep learning bugs by constructing a learning-enhanced context and employing an iterative generate-validate-refine mechanism. It achieves an 80.19% reproduction rate on real-world bugs, significantly outperforming the state-of-the-art, and a developer study confirms it improves success rates and reduces time and cognitive load for bug reproduction.

- **[arXiv251218] Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle**
  - **tags:** [mlsys], [multi-modal inference], [YOLOv11, ByteTrack, ZebraPose, support vector machine, keypoint trajectories, pose estimation]
  - **authors:** Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan
  - **institution:** Dalhousie University
  - **link:** https://arxiv.org/pdf/2512.14998
  - **Simple LLM Summary:** This paper presents a computer vision framework that uses pose estimation and keypoint trajectories to classify social interactions in dairy cattle, moving beyond simple proximity measures. The method integrates object detection, tracking, and a support vector machine to distinguish affiliative from agonistic behaviors with 77.51% accuracy. The results demonstrate improved behavioral discrimination and establish a proof-of-concept for automated, interaction-aware social network analysis in precision livestock farming.

- **[arXiv251218] DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding**
  - **tags:** [mlsys], [post-training], [Process Reward Model (PRM), Chain-of-Function, meta-learning, label correction, bi-level optimization, test-time scaling, LiveCodeBench]
  - **authors:** Ruiyi Zhang, Peijia Qin, Qi Cao, Pengtao Xie
  - **institution:** University of California, San Diego
  - **link:** https://arxiv.org/pdf/2512.15000
  - **Simple LLM Summary:** The paper proposes DreamPRM-Code, a process reward model for coding that treats functions as reasoning steps using a Chain-of-Function strategy and employs a meta-learning-based label correction mechanism to refine noisy intermediate training labels. It achieves state-of-the-art performance on LiveCodeBench, surpassing OpenAI o4-mini.

- **[arXiv251218] Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation**
  - **tags:** [ai], [video question generation], [video question generation, question-to-answer retrieval, EgoExoAsk dataset, expert knowledge elicitation]
  - **authors:** Huaying Zhang, Atsushi Hashimoto, Tosho Hirasawa
  - **institution:** OMRON SINIC X Corp., Hokkaido University
  - **link:** https://arxiv.org/pdf/2512.15006
  - **Simple LLM Summary:** The paper proposes a retrieval-based evaluation protocol for video question generation (VQG) models, focusing on their ability to elicit expert knowledge. The method uses a question-to-answer retriever trained on a novel dataset, EgoExoAsk, to simulate expert communication. The main conclusion is that this metric effectively aligns with VQG settings, as models with richer context are evaluated better, validating the proposed framework.

- **[arXiv251218] Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation**
  - **tags:** [ai], [llm evaluation], [geometric stability framework, invariant transformations, accuracy-stability paradox]
  - **authors:** Xidan Song, Weiqi Wang, Ruifeng Cao, Qingya Hu
  - **institution:** Wuhan Donghu University, University of Manchester
  - **link:** https://arxiv.org/pdf/2512.15033
  - **Simple LLM Summary:** This paper proposes a Geometric Stability Framework to evaluate LLMs in chess by testing their consistency under board transformations like rotation and mirroring. It finds an Accuracy-Stability Paradox, where models like GPT-5.1 achieve high standard accuracy but fail catastrophically under geometric perturbations, indicating a reliance on pattern matching rather than robust spatial reasoning.

- **[arXiv251218] Epistemic diversity across language models mitigates knowledge collapse**
  - **tags:** [mlsys], [llm training], [model collapse, epistemic diversity, AI ecosystem, self-training, distributed training]
  - **authors:** Damian Hodel, Jevin D. West
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.15011
  - **Simple LLM Summary:** The paper investigates whether diversity across language models (an "AI ecosystem") can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.

- **[arXiv251218] Spectral Representation-based Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [spectral representations, spectral decomposition, transition operator, partially observable MDPs, model-free, model-based]
  - **authors:** Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai
  - **institution:** Georgia Tech, Harvard University, Google DeepMind, University of Alberta
  - **link:** https://arxiv.org/pdf/2512.15036
  - **Simple LLM Summary:** This paper introduces spectral representations, derived from the spectral decomposition of the transition operator, as a framework for reinforcement learning to address issues like theoretical ambiguity and optimization instability. It shows how to construct these representations for different system structures and extends the approach to partially observable environments. The proposed algorithms achieve performance comparable to or better than state-of-the-art methods on over 20 challenging control tasks.

- **[arXiv251218] LADY: Linear Attention for Autonomous Driving Efficiency without Transformers**
  - **tags:** [mlsys], [multi-modal inference], [linear attention, cross-modal interaction, end-to-end autonomous driving, constant-time complexity, generative model]
  - **authors:** Jihao Huang, Xi Xia, Zhiyuan Li, Tianle Liu, Jingke Wang, Junbo Chen, Tengju Ye
  - **institution:** Udeer AI, Zhejiang University, Yuanshi Intelligence
  - **link:** https://arxiv.org/pdf/2512.15038
  - **Simple LLM Summary:** The paper proposes LADY, a fully linear attention-based generative model for end-to-end autonomous driving that replaces transformers with efficient linear attention mechanisms. It introduces a lightweight linear cross-attention for effective multi-modal fusion and achieves constant computational and memory costs for long-range temporal context. Experiments show state-of-the-art planning performance with significantly reduced cost, and the model is successfully deployed on edge devices.

- **[arXiv251218] Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study**
  - **tags:** [mlsys], [others], [agentic AI, integrated sensing and communication (ISAC), generative AI (GenAI), deep reinforcement learning (DRL), perception-reasoning-action loop]
  - **authors:** Wenwen Xie, Geng Sun, Ruichen Zhang, Xuejie Liu, Yinqiu Liu, Jiacheng Wang, Dusit Niyato, Ping Zhang
  - **institution:** Jilin University, Nanyang Technological University, Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.15044
  - **Simple LLM Summary:** This paper proposes a novel agentic AI framework for optimizing Integrated Sensing and Communication (ISAC) systems, leveraging continuous perception-reasoning-action loops and generative AI. The case study demonstrates that this framework can enhance ISAC performance in dynamic wireless environments. The work concludes that agentic AI is a promising solution for enabling intelligent and autonomous operation in future 6G networks.

- **[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles**
  - **tags:** [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]
  - **authors:** Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.15047
  - **Simple LLM Summary:** The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines.

- **[arXiv251218] The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops**
  - **tags:** [mlsys], [llm inference], [Meta-Prompting Protocol, Adversarial Trinity, DSPy, TextGrad, textual gradients, semantic computation graph]
  - **authors:** Fanzhe Fu
  - **institution:** Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.15053
  - **Simple LLM Summary:** The paper introduces the Meta-Prompting Protocol, a framework that formalizes LLM orchestration as a programmable system using an adversarial topology (Generator, Auditor, Optimizer) to treat prompts as differentiable variables. It leverages textual critiques as gradients within a semantic computation graph to mitigate hallucination and improve reliability. The authors demonstrate its theoretical viability with tools like DSPy and TextGrad, proposing a foundation for deterministic "Observable Software Engineering" for probabilistic models.

- **[arXiv251218] Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank**
  - **tags:** [ai], [medical image segmentation], [wavelet analysis, memory bank, encoder-decoder, cross-attention, high-frequency feature fusion]
  - **authors:** Chenxiao Zhang, Runshi Zhang, Junchen Wang
  - **institution:** Not explicitly provided in the given text. Affiliation inference is not possible from the author names alone without email domains or explicit institutional mentions.
  - **link:** https://arxiv.org/pdf/2512.15066
  - **Simple LLM Summary:** The paper proposes a network combining wavelet analysis and a memory bank to segment objects in long ultrasound videos. The method uses memory-based wavelet convolution and high-frequency-aware feature fusion to capture fine details and track objects over time. It demonstrates improved segmentation accuracy, particularly for small nodules, on several ultrasound datasets.

- **[arXiv251218] EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks**
  - **tags:** [mlsys], [diffusion inference], [conditional diffusion model, probabilistic forecasting, residual U-Net, cross-attention, uncertainty quantification, imputation-based sampling]
  - **authors:** Zijiang Yan, Yixiang Huang, Jianhua Pei, Hina Tabassum, Luca Chiaraviglio
  - **institution:** York University, Huazhong University of Science and Technology, University of Rome Tor Vergata
  - **link:** https://arxiv.org/pdf/2512.15067
  - **Simple LLM Summary:** This paper introduces EMFusion, a conditional diffusion-based probabilistic forecasting framework that uses a residual U-Net with cross-attention to integrate contextual factors for frequency-selective EMF level prediction. It treats forecasting as an inpainting task to handle irregular data and provides explicit uncertainty estimates. The results show that EMFusion significantly outperforms baseline models in key forecasting metrics.

- **[arXiv251218] SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification**
  - **tags:** [mlsys], [multi-modal inference], [neuron-level intervention, expertise-weighted soft suppression, MM-TOXIC-QA, white-box detoxification, SGM*]
  - **authors:** Hongbo Wang, MaungMaung AprilPyone, Isao Echizen
  - **institution:** The University of Tokyo, National Institute of Informatics
  - **link:** https://arxiv.org/pdf/2512.15052
  - **Simple LLM Summary:** The paper proposes SGM, a neuron-level intervention method that selectively recalibrates toxic expert neurons in multimodal large language models (MLLMs) using expertise-weighted soft suppression to reduce harmful outputs. Experiments show SGM significantly cuts toxicity rates from 48.2% to 2.5% while preserving model fluency and reasoning, and it can be combined with other methods for stronger safety.

- **[arXiv251218] The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems**
  - **tags:** [ai], [retrieval-augmented generation], [conformal prediction, semantic similarity, natural language inference, hallucination detection, text embeddings]
  - **authors:** Debu Sinha
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.15068
  - **Simple LLM Summary:** The paper applies conformal prediction to provide statistical guarantees for hallucination detection in RAG systems, rigorously evaluating embedding-based methods. It finds that while these methods work on synthetic data, they fail on real benchmarks due to the "semantic illusion," where plausible hallucinations remain semantically similar to source documents. The study concludes that embedding-based detection is insufficient for production, as reasoning-based methods like GPT-4 as a judge perform significantly better.

- **[arXiv251218] PMMD: A pose-guided multi-view multi-modal diffusion for person generation**
  - **tags:** [ai], [diffusion training], [diffusion framework, multimodal encoder, ResCVA module, cross modal fusion, pose-guided generation]
  - **authors:** Ziyu Shang, Haoran Liu, Rongchao Zhang, Zhiqian Wei, Tongtong Feng
  - **institution:** Harbin Institute of Technology, Shenzhen, City University of Hong Kong, Peking University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.15069
  - **Simple LLM Summary:** PMMD is a pose-guided multi-view multimodal diffusion framework that synthesizes photorealistic person images by jointly modeling visual views, pose features, and text prompts. It introduces a ResCVA module for local detail enhancement and a cross-modal fusion module to integrate image semantics with text. Experiments show PMMD outperforms baselines in consistency, detail preservation, and controllability on the DeepFashion MultiModal dataset.

- **[arXiv251218] Quantifying Return on Security Controls in LLM Systems**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation (RAG), Monte Carlo simulation, loss exceedance curves, Laplace's Rule of Succession, adversarial probing, Garak, attribute-based access control (ABAC), named entity recognition (NER) redaction, NeMo Guardrails]
  - **authors:** Richard Helder Moulton, Austin O'Brien, John D. Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.15081
  - **Simple LLM Summary:** This paper introduces a framework to quantify the financial return on security controls for LLM systems by simulating attacks on a RAG service, estimating attack success probabilities, and modeling potential losses with Monte Carlo methods. The main conclusion is that controls like ABAC and NER redaction significantly reduce expected financial losses and offer high return-on-control, whereas NeMo Guardrails provides minimal benefit in the tested scenarios.

- **[arXiv251218] Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models**
  - **tags:** [mlsys], [llm inference], [cognitive-inspired elastic reasoning, markov decision process, reinforcement learning, chain-of-thought, tool-assisted reasoning]
  - **authors:** Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo Chen, Bin Xiao, Yuanqing Li, Mingkui Tan
  - **institution:** South China University of Technology, Pazhou Laboratory, Peng Cheng Laboratory, Chongqing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.15089
  - **Simple LLM Summary:** The paper proposes CogER, a framework that dynamically selects reasoning strategies for LLMs based on query complexity, modeled as a Markov Decision Process and trained with reinforcement learning. It introduces Cognitive Tool-Assisted Reasoning for autonomous tool use within reasoning chains. Experiments show CogER outperforms state-of-the-art methods, improving exact match scores by at least 13% on in-domain and 8% on out-of-domain tasks.

- **[arXiv251218] Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption**
  - **tags:** [ai], [graph representation learning], [graph convolution, unsupervised learning, node embeddings, homophily, non-homophilic graphs, intra-class similarity, inter-class separability]
  - **authors:** Sunwoo Kim, Soo Yong Lee, Kyungho Kim, Hyunjin Hwang, Jaemin Yoo, Kijung Shin
  - **institution:** KAIST (Korea Advanced Institute of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.15112
  - **Simple LLM Summary:** The paper proposes FUEL, an unsupervised node representation learning method that adaptively adjusts the degree of graph convolution usage to enhance intra-class similarity and inter-class separability in the embedding space, using node feature clusters as proxy classes. It demonstrates state-of-the-art performance across diverse homophily levels in graphs.

- **[arXiv251218] How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models**
  - **tags:** [ai], [sequence modeling], [attention, state space models, unified framework, interaction rank gap, head-count theorem, gradient highway]
  - **authors:** Ali Ghodsi
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.15115
  - **Simple LLM Summary:** This paper introduces a unified theoretical framework that connects attention mechanisms and state space models through an input-dependent interaction operator. It proves that representing a linear SSM requires a number of attention heads equal to the subspace dimension of its lag operators, and reveals a trade-off between the algebraic expressivity of the model and its ability to propagate gradients over long sequences.

- **[arXiv251218] FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation**
  - **tags:** [ai], [time series imputation], [diffusion model, Fourier transform, attention mechanism, gated convolution, frequency-domain modeling]
  - **authors:** Runze Li, Hanchen Wang, Wenjie Zhang, Binghao Li, Yu Zhang, Xuemin Lin, Ying Zhang
  - **institution:** University of New South Wales, University of Technology Sydney, Shanghai Jiao Tong University, Zhejiang Gongshang University
  - **link:** https://arxiv.org/pdf/2512.15116
  - **Simple LLM Summary:** FADTI is a diffusion-based framework for multivariate time series imputation that integrates a learnable Fourier Bias Projection module with self-attention and gated convolution to inject frequency-domain inductive bias. It outperforms state-of-the-art methods across multiple benchmarks, especially under high missing rates, by adaptively encoding both stationary and non-stationary patterns.

- **[arXiv251218] I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable**
  - **tags:** [ai], [human-computer interaction], [conversational AI, relational style, transparent style, anthropomorphism, emotional reliance, online experiment, adolescent psychology]
  - **authors:** Pilyoung Kim, Yun Xie, Sujin Yang
  - **institution:** University of Denver, Ewha Womans University
  - **link:** https://arxiv.org/pdf/2512.15117
  - **Simple LLM Summary:** The paper uses a preregistered online experiment with adolescent-parent dyads to compare how relational versus transparent conversational styles in AI chatbots affect adolescents' perceptions. It finds that a relational style increases anthropomorphism, trust, and emotional closeness, and is especially preferred by socially and emotionally vulnerable adolescents, highlighting a design consideration for youth AI safety.

- **[arXiv251218] Automatic Reward Shaping from Multi-Objective Human Heuristics**
  - **tags:** [ai], [reinforcement learning], [reward shaping, multi-objective optimization, bi-level optimization, stochastic exploration]
  - **authors:** Yuqing Xie, Jiayu Chen, Wenhao Tang, Ya Zhang, Chao Yu, Yu Wang
  - **institution:** Tsinghua University, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.15120
  - **Simple LLM Summary:** This paper introduces MORSE, a framework that automatically combines multiple human-designed heuristic rewards into a unified reward function using bi-level optimization with stochastic exploration. It effectively balances conflicting objectives in robotic tasks, achieving performance comparable to manually tuned rewards in MuJoCo and Isaac Sim environments.

- **[arXiv251218] HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens**
  - **tags:** [mlsys], [multi-modal training], [hybrid diffusion, continuous tokens, joint sequence-structure modeling, absorbing diffusion process, categorical prediction, continuous diffusion]
  - **authors:** Yi Zhou, Haohao Qu, Yunqing Liu, Shanru Lin, Le Song, Wenqi Fan
  - **institution:** The Hong Kong Polytechnic University, BioGen AI, Mohamed bin Zayed University of Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.15133
  - **Simple LLM Summary:** The paper proposes HD-Prot, a hybrid diffusion protein language model that integrates continuous structural tokens with discrete sequence tokens for joint modeling. It uses a unified absorbing diffusion process to handle both modalities, performing categorical prediction for sequences and continuous diffusion for structures. The model achieves competitive performance in tasks like co-generation and structure prediction, demonstrating the viability of combining categorical and continuous distributions in a unified architecture.

- **[arXiv251218] From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?**
  - **tags:** [ai], [interpretability], [sparse autoencoders, sparse probes, concept disentanglement, steering experiments, multi-concept evaluation]
  - **authors:** Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, Patrik Reizinger
  - **institution:** Boston University, Harvard University, Mila – Quebec AI Institute, Goodfire, University of Tübingen
  - **link:** https://arxiv.org/pdf/2512.15134
  - **Simple LLM Summary:** The paper proposes a multi-concept evaluation framework to test whether interpretability methods like sparse autoencoders and sparse probes recover disentangled and independently manipulable concept representations. It finds that features often correspond to single concepts, but concepts are distributed across many features, and steering one feature typically affects multiple concepts, indicating a lack of true independence. The results highlight that correlational metrics are insufficient for proving disentanglement and underscore the need for compositional evaluations in interpretability research.

- **[arXiv251218] Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning**
  - **tags:** [mlsys], [llm training], [surrogate modeling, large language model, sequence-to-sequence modeling, offline training, reinforcement learning fine-tuning, implicit q-learning, multi-task multi-objective optimization]
  - **authors:** Xian-Rong Zhang, Yue-Jiao Gong, Zeyuan Ma, Jun Zhang
  - **institution:** South China University of Technology, Nankai University, Hanyang University
  - **link:** https://arxiv.org/pdf/2512.15149
  - **Simple LLM Summary:** This paper proposes Q-MetaSur, a plug-and-play surrogate modeling scheme that uses a Large Language Model as a sequence-to-sequence surrogate for offline multi-task multi-objective optimization, trained with a two-stage strategy combining supervised tuning and RL fine-tuning. The method demonstrates superior objective approximation accuracy and helps evolutionary algorithms achieve better convergence and Pareto optimality on benchmark problems.

- **[arXiv251218] MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers**
  - **tags:** [mlsys], [llm inference], [MCP (Model Context Protocol), safety benchmark, multi-turn evaluation, multi-server workflows, attack taxonomy, agentic systems]
  - **authors:** Xuanjun Zong, Zhiqi Shen, Lei Wang, Yunshi Lan, Chao Yang
  - **institution:** East China Normal University, National University of Singapore, Singapore Management University, Shanghai AI Laboratory
  - **link:** https://arxiv.org/pdf/2512.15163
  - **Simple LLM Summary:** This paper introduces MCP-SafetyBench, a benchmark built on real Model Context Protocol servers to evaluate the safety of LLMs operating as agents across tools and services. It systematically tests models on multi-step, multi-server tasks across five domains, revealing significant safety vulnerabilities that escalate with task complexity. The results highlight the urgent need for improved defenses in real-world LLM agent deployments.

- **[arXiv251218] DEER: Draft with Diffusion, Verify with Autoregressive Models**
  - **tags:** [mlsys], [llm inference], [speculative decoding, diffusion large language model, parallel decoding, draft-verify scheme, two-stage training, single-step decoding]
  - **authors:** Zicong Cheng, Guo-Wei Yang, Jia Li, Zhijie Deng, Meng-Hao Guo, Shi-Min Hu
  - **institution:** Tsinghua University, Proxseer Inc, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.15176
  - **Simple LLM Summary:** This paper introduces DEER, a speculative decoding framework that uses a diffusion large language model (dLLM) as a parallel drafter to generate candidate tokens, which are then verified by a target autoregressive model. The method overcomes the sequential bottleneck and trust collapse of traditional AR drafters through a two-stage training pipeline and single-step decoding. Experiments show DEER achieves significantly longer draft acceptance and higher speedups (e.g., 5.54x) compared to prior methods like EAGLE-3.

- **[arXiv251218] Governing rapid technological change: Policy Delphi on the future of European AI governance**
  - **tags:** [ai], [policy analysis], [Policy Delphi, anticipatory governance, future-proof regulation, AI Act]
  - **authors:** Atte Ojanen, Johannes Anttila, Thilo H. K. Thelitz, Anna Bjork
  - **institution:** Demos Helsinki, University of Turku
  - **link:** https://arxiv.org/pdf/2512.15196
  - **Simple LLM Summary:** This paper uses a two-round Policy Delphi method with European experts to study the future of AI governance. It finds a consensus that effective regulation depends more on practical implementation and enforcement than on technical specifics, and identifies a gap between desirable policy directions (like citizen participation) and their perceived feasibility.

- **[arXiv251218] A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem**
  - **tags:** [ai], [discrete optimization], [relaxed decision diagrams, variable ordering, clustering, branch-and-bound, maximum weighted independent set]
  - **authors:** Mohsen Nafar, Michael Römer, Lin Xie
  - **institution:** Brandenburg University of Technology, Bielefeld University
  - **link:** https://arxiv.org/pdf/2512.15198
  - **Simple LLM Summary:** This paper introduces a clustering-based framework to improve variable ordering for relaxed decision diagrams, aiming to reduce the computational overhead of dynamic ordering heuristics. The framework partitions variables into clusters and applies two strategies, Cluster-to-Cluster and Pick-and-Sort, to guide the ordering process. The method, evaluated on the Maximum Weighted Independent Set Problem, consistently reduces computational costs compared to standard baselines.

- **[arXiv251218] RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA**
  - **tags:** [ai], [knowledge-aware question answering], [knowledge graph, chain-of-thought, few-shot in-context learning, relation-driven adaptive hop-count selection, path guidance]
  - **authors:** Chao Zhang, Minghan Li, Tianrui Lv, Guodong Zhou
  - **institution:** Soochow University
  - **link:** https://arxiv.org/pdf/2512.15219
  - **Simple LLM Summary:** This paper proposes RFKG-CoT, a method that enhances knowledge-aware question answering by dynamically selecting reasoning steps in a knowledge graph based on relations and using few-shot examples to guide large language models in understanding reasoning paths. It improves answer accuracy over previous methods by making the integration of knowledge graph evidence more adaptive and guided. Experiments show significant accuracy gains on multiple benchmarks.

- **[arXiv251218] Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024**
  - **tags:** [mlsys], [llm training], [fine-tuning, LoRA, zero-shot prompting, few-shot prompting, supervised fine-tuning, transformer models]
  - **authors:** Yash Bhaskar, Parameswari Krishnamurthy
  - **institution:** IIIT Hyderabad
  - **link:** https://arxiv.org/pdf/2512.15226
  - **Simple LLM Summary:** This paper explores various methods for low-resource Indic language translation, including fine-tuning models like mT5 and IndicBart, using LoRA with IndicTrans2 and Llama 3, and prompting LLMs like Llama 3 and Mixtral. The results highlight the challenges of data scarcity and demonstrate the potential of fine-tuned large language models for these translation tasks.

- **[arXiv251218] CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications**
  - **tags:** [mlsys], [others], [Procedural Knowledge Base, Dynamic Workflow Adjustment, Evolutionary Memory Module, Large Language Model, Reflexion baseline]
  - **authors:** Zhengchao Chen, Haoran Wang, Jing Yao, Pedram Ghamisi, Jun Zhou, Peter M. Atkinson, Bing Zhang
  - **institution:** Chinese Academy of Sciences, University of Chinese Academy of Sciences, Helmholtz-Zentrum Dresden-Rossendorf, Lancaster University, Griffith University
  - **link:** https://arxiv.org/pdf/2512.15231
  - **Simple LLM Summary:** This paper introduces CangLing-KnowFlow, a unified intelligent agent framework for remote sensing that integrates a Procedural Knowledge Base, Dynamic Workflow Adjustment, and an Evolutionary Memory Module to plan, adapt, and learn from complex tasks. The framework was evaluated on a novel benchmark and outperformed a baseline method, demonstrating its potential as a robust and scalable automated solution for Earth observation challenges.

- **[arXiv251218] Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification**
  - **tags:** [mlsys], [multi-modal training], [Cross-Modal Alignment Consistency (CMAC-MMD), vision-language model (VLM), intersectional fairness, diagnostic certainty, True Positive Rate (TPR), Area Under the Curve (AUC)]
  - **authors:** Yupeng Zhang, Adam G. Dunn, Usman Naseem, Jinman Kim
  - **institution:** The University of Sydney, Macquarie University
  - **link:** https://arxiv.org/pdf/2512.15249
  - **Simple LLM Summary:** This paper proposes a training framework called Cross-Modal Alignment Consistency (CMAC-MMD) to reduce intersectional bias in vision-language models for medical diagnosis by standardizing diagnostic certainty across patient subgroups. The method improves both fairness, by reducing the gap in missed diagnoses, and overall accuracy, as demonstrated on skin lesion and glaucoma screening tasks. It provides a scalable approach for equitable clinical AI without requiring sensitive demographic data during inference.

- **[arXiv251218] Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis**
  - **tags:** [mlsys], [multi-modal training], [self-supervised pretraining, dual-masking strategy, embedding concatenation, foundational models, CBraMod encoder]
  - **authors:** Youssef Ghallab, Omar Iraqy, Mohamed Kandil, Mohamed Ashraf, Saadeldine Eletter, Morougue Ghazal, Ayman Khalafallah, Nagwa El-Makky
  - **institution:** Alexandria University, Mohamed bin Zayed University of Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.15250
  - **Simple LLM Summary:** This paper proposes a method for multi-modal physiological signal analysis by adapting a self-supervised foundational model (CBraMod) for ECG and EEG, using a dual-masking strategy for ECG, and fusing the modalities via simple embedding concatenation. The approach achieves near state-of-the-art performance in emotion recognition, demonstrating that well-designed foundational encoders with straightforward fusion can effectively leverage limited multi-modal data. The results highlight the potential of foundation-model approaches for scalable and label-efficient solutions in healthcare and affective computing.

- **[arXiv251218] VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments**
  - **tags:** [mlsys], [multi-modal inference], [3D Gaussian Splatting, progressive three-stage training, geometric safety correction, onboard deployment optimization]
  - **authors:** Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Xin Zhou, Fei Gao
  - **institution:** Zhejiang University, Differential Robotics
  - **link:** https://arxiv.org/pdf/2512.15258
  - **Simple LLM Summary:** This paper proposes VLA-AN, an efficient onboard Vision-Language-Action framework for drone navigation. Its core method uses a high-fidelity dataset built with 3D Gaussian Splatting and a three-stage training pipeline, coupled with a lightweight safety-corrected action module. The conclusion is that the framework achieves robust real-time performance and high navigation success rates, providing a practical solution for autonomous aerial robots.

- **[arXiv251218] Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning**
  - **tags:** [mlsys], [llm training], [reinforcement learning with verifiable rewards, progressive prefix-token policy optimization, beginning lock-in effect, prefix optimization, continuation accumulated reward]
  - **authors:** Yiliu Sun, Zicheng Zhao, Yang Wei, Yanfang Zhang, Chen Gong
  - **institution:** Nanjing University of Science and Technology, North University of China, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.15274
  - **Simple LLM Summary:** This paper proposes Progressive Prefix-token Policy Optimization (PPPO), a reinforcement learning method that focuses on optimizing the initial prefix tokens of an LLM's reasoning output, based on the identified Beginning Lock-in Effect. It introduces strategies like Progressive Prefix Retention and Continuation Accumulated Reward to improve training efficiency. The method achieves significant accuracy improvements on reasoning tasks while using far fewer training tokens compared to standard approaches.

- **[arXiv251218] Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis**
  - **tags:** [ai], [formal methods], [reinforcement learning, graph neural networks, labeled transition system, exploration policy]
  - **authors:** Toshihide Ubukata, Enhong Mu, Takuto Yamauchi, Mingyue Zhang, Jialong Li, Kenji Tei
  - **institution:** Waseda University, Southwest University, Institute of Science Tokyo
  - **link:** https://arxiv.org/pdf/2512.15295
  - **Simple LLM Summary:** This paper introduces GCRL, a method that enhances reinforcement learning for controller synthesis by integrating Graph Neural Networks to encode exploration history into a graph for broader context. It demonstrates superior learning efficiency and generalization compared to state-of-the-art methods in most benchmark domains.

- **[arXiv251218] Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions**
  - **tags:** [mlsys], [others], [Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), Quantum Generative Adversarial Networks (QGANs)]
  - **authors:** Siva Sai, Ishika Goyal, Shubham Sharma, Sri Harshita Manuri, Vinay Chamola, Rajkumar Buyya
  - **institution:** The University of Melbourne, Birla Institute of Technology and Science, Pilani, APPCAIR
  - **link:** https://arxiv.org/pdf/2512.15286
  - **Simple LLM Summary:** This survey paper explores Quantum Machine Learning (QML) techniques, including QNNs, QSVMs, VQCs, and QGANs, for cybersecurity applications like intrusion detection and malware classification. It concludes that QML offers potential advantages for processing high-dimensional data and enhancing security in areas like cloud computing, but also discusses current limitations and future research directions needed to address them.

- **[arXiv251218] Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting**
  - **tags:** [ai], [graph mining], [graph pattern-based association rules, no-repeated-anything semantics, confidence, lift, leverage, conviction]
  - **authors:** Basil Ell
  - **institution:** Bielefeld University, University of Oslo
  - **link:** https://arxiv.org/pdf/2512.15308
  - **Simple LLM Summary:** This paper introduces Graph Pattern-based Association Rules (GPARs) for analyzing directed labeled multigraphs like RDF graphs. It evaluates these rules under a "no-repeated-anything" semantics to better account for graph topology and defines probabilistic metrics like confidence and lift. The framework is shown to extend beyond existing formalisms for graph data.

- **[arXiv251218] ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I**
  - **tags:** [mlsys], [multi-modal inference], [multimodal reasoning, perception-cognition gap, calculation-conceptualization discrepancy, process hallucination, OCR, AI-resistant questions]
  - **authors:** Seok-Hyun Ga, Chun-Yen Chang
  - **institution:** Institute for Research Excellence in Learning Sciences, National Taiwan Normal University, Seoul National University, Universitas Negeri Malang
  - **link:** https://arxiv.org/pdf/2512.15298
  - **Simple LLM Summary:** This study evaluates the multimodal scientific reasoning of LLMs like GPT-4o and Gemini on the Korean CSAT Earth Science I exam under different input conditions. It finds that models suffer from fundamental cognitive flaws, such as a perception-cognition gap and calculation-conceptualization discrepancy, even with optimized inputs. The paper concludes by suggesting these vulnerabilities can be exploited to design AI-resistant assessment questions to ensure academic integrity.

- **[arXiv251218] Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies**
  - **tags:** [ai], [scientific information extraction], [zero-shot prompting, few-shot prompting, event-specific prompting, reflection-based prompting, in-context learning, event extraction, argument extraction]
  - **authors:** Charan Prakash Rathore, Saumi Ray, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani
  - **link:** https://arxiv.org/pdf/2512.15312
  - **Simple LLM Summary:** This paper systematically evaluates six state-of-the-art LLMs using four prompting strategies (zero-shot, few-shot, event-specific, reflection-based) for extracting structured event and argument information from zeolite synthesis procedures. It finds that while LLMs achieve strong performance on high-level event classification, they show modest results on fine-grained parameter extraction, with advanced prompting offering minimal gains over zero-shot approaches. The conclusion is that precise scientific information extraction requires domain-adapted models, as current LLMs have fundamental limitations in capturing synthesis-specific nuances.

- **[arXiv251218] Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment**
  - **tags:** [ai], [medical imaging], [supervised contrastive learning, affinity scores, interpretable grading]
  - **authors:** Antony Jerald, Dattesh Shanbhag, Sudhanya Chatterjee
  - **institution:** GE HealthCare
  - **link:** https://arxiv.org/pdf/2512.15315
  - **Simple LLM Summary:** The paper introduces AutoMAC-MRI, an interpretable framework that uses supervised contrastive learning to detect and grade motion artifacts in MRI images. It computes grade-specific affinity scores to quantify an image's proximity to each motion severity level, making the grading process transparent. The method was validated on over 5,000 expert-annotated brain MRI slices and shows potential for reducing unnecessary rescans and improving workflow efficiency.

- **[arXiv251218] Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection**
  - **tags:** [mlsys], [others], [Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, Collective Cognitive Inference, proof of concept, VUCA]
  - **authors:** Agnieszka Bienkowska, Jacek Malecki, Alexander Mathiesen-Ohman, Katarzyna Tworek
  - **institution:** Not explicitly stated in provided text
  - **link:** https://arxiv.org/pdf/2512.15325
  - **Simple LLM Summary:** This paper presents a proof of concept for the LAIZA human-AI symbiotic system, which uses a quantum-inspired cognitive mechanism to detect "rogue variables" and manage ambiguity by preserving interpretive plurality and activating structured human clarification. The main conclusion is that this approach enables proactive scenario-based preparation and decisive action in VUCA environments, reframing ambiguity as a key construct for organizational resilience.

- **[arXiv251218] Vision-based module for accurately reading linear scales in a laboratory**
  - **tags:** [ai], [computer vision], [vision-based models, object detection, image classification, instance segmentation, feature extraction, orientation correction]
  - **authors:** Parvesh Saini, Soumyadipta Maiti, Beena Rai
  - **institution:** TCS Research, Tata Consultancy Services Limited
  - **link:** https://arxiv.org/pdf/2512.15327
  - **Simple LLM Summary:** The paper presents a vision-based module that mimics a human-inspired approach to read measurements from linear scales, such as on syringes and measuring cylinders, by correcting orientation, isolating the scale region, and extracting features like markers and digits. The system's readings were compared against human-read values and showed accurate correspondence, demonstrating its potential for laboratory automation.

- **[arXiv251218] Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality**
  - **tags:** [mlsys], [others], [crowdsourcing, user study, extended reality, conversational agents, privacy, technology acceptance model]
  - **authors:** Efe Bozkir, Enkelejda Kasneci
  - **institution:** Technical University of Munich
  - **link:** https://arxiv.org/pdf/2512.15343
  - **Simple LLM Summary:** This paper conducted a large-scale crowdsourcing study with 1036 participants to explore user acceptance and concerns regarding LLM-powered conversational agents in Extended Reality (XR). The study found that while users generally accept these technologies, they express significant concerns about security, privacy, social implications, and trust, with location data being the most sensitive. The results highlight the importance of practitioner transparency and that familiarity with generative AI increases acceptance, while prior XR device ownership is linked to lower acceptance.

- **[arXiv251218] Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery**
  - **tags:** [mlsys], [others], [phase adjustment, three-axis independent alignment, single-axis reference alignment, deep learning, vibration signals, predictive maintenance]
  - **authors:** Hiroyoshi Nagahama, Katsufumi Inoue, Masayoshi Todorokihara, Michifumi Yoshioka
  - **institution:** Osaka Metropolitan University, Seiko Epson Corp.
  - **link:** https://arxiv.org/pdf/2512.15344
  - **Simple LLM Summary:** This paper introduces two phase-aware preprocessing strategies—three-axis independent and single-axis reference phase adjustment—to handle random phase variations in multi-axis vibration data for fault diagnosis. The methods are evaluated using a new rotor dataset and six deep learning models, showing consistent performance improvements, with the single-axis reference approach achieving up to 96.2% accuracy by preserving spatial phase relationships. The findings demonstrate that these phase alignment strategies are practical and scalable enhancements for predictive maintenance systems.

- **[arXiv251218] Adversarial versification in portuguese as a jailbreak operator in LLMs**
  - **tags:** [mlsys], [llm inference], [adversarial versification, poetry jailbreak, guardrail vulnerabilities, semiotic-formal variation, latent region displacement]
  - **authors:** Joao Queiroz
  - **institution:** Federal University of Juiz de Fora
  - **link:** https://arxiv.org/pdf/2512.15353
  - **Simple LLM Summary:** The paper investigates using versification, or rewriting prompts as poetry, as a method to bypass safety guardrails in aligned large language models. It concludes that this structural adversarial technique exploits a model's over-reliance on surface patterns, causing significant safety failures, and highlights a critical research gap for Portuguese due to its linguistic complexity.

- **[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]
  - **authors:** Mikel Williams-Lekuona, Georgina Cosma
  - **institution:** Loughborough University
  - **link:** https://arxiv.org/pdf/2512.15372
  - **Simple LLM Summary:** The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.

- **[arXiv251218] Emotion Recognition in Signers**
  - **tags:** [mlsys], [multi-modal training], [cross-lingual transfer, temporal segment selection, hand motion features, textual emotion recognition, facial expression analysis]
  - **authors:** Kotaro Funakoshi, Yaoxiong Zhu
  - **institution:** Institute of Science Tokyo
  - **link:** https://arxiv.org/pdf/2512.15376
  - **Simple LLM Summary:** This paper introduces a new dataset for emotion recognition in Japanese Sign Language and addresses the challenges of overlapping grammatical/affective expressions and data scarcity using cross-lingual transfer from textual emotion recognition in spoken language. The authors demonstrate that selecting specific temporal segments and incorporating hand motion features significantly improves emotion recognition performance in signers, establishing a stronger baseline than spoken language LLMs.

- **[arXiv251218] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness**
  - **tags:** [mlsys], [llm inference], [prompt evolution, online optimization, dual-stream mechanism, perspective-driven exploration, execution trace analysis, HLE benchmark]
  - **authors:** Zehua Pei, Hui-Ling Zhen, Shixiong Kai, Sinno Jialin Pan, Yunhe Wang, Mingxuan Yuan, Bei Yu
  - **institution:** The Chinese University of Hong Kong, Noah's Ark Lab, Huawei
  - **link:** https://arxiv.org/pdf/2512.15374
  - **Simple LLM Summary:** This paper introduces SCOPE, a method that frames context management for LLM agents as an online optimization problem, using a Dual-Stream mechanism and Perspective-Driven Exploration to automatically evolve the agent's prompt from execution traces. It significantly improves agent effectiveness by balancing tactical error correction with strategic guideline evolution. Experiments on the HLE benchmark show that SCOPE increases task success rates from 14.23% to 38.64% without human intervention.

- **[arXiv251218] Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations**
  - **tags:** [ai], [spatial reasoning], [retrieval-augmented generation, qualitative spatial relations, graph-based reasoning]
  - **authors:** Reinhard Moratz, Niklas Daute, James Ondieki, Markus Kattenbeck, Mario Krajina, Ioannis Giannopoulos
  - **institution:** University of Münster
  - **link:** https://arxiv.org/pdf/2512.15388
  - **Simple LLM Summary:** This paper uses Retrieval-Augmented Generation (RAG) with qualitative spatial representations to improve LLM-generated pedestrian route instructions. It finds that incorporating structured spatial data reduces hallucinations and errors, encouraging further integration for navigation and smart city applications.

- **[arXiv251218] SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering**
  - **tags:** [ai], [multi-view clustering], [contrastive learning, distribution alignment, semantic graph, partially view-aligned clustering]
  - **authors:** Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong
  - **institution:** Shantou University, Huaqiao University, Southwest University, South China University of Technology, City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.15396
  - **Simple LLM Summary:** The paper proposes SMART, a model for partially view-aligned clustering that uses view distribution alignment and semantic matching contrastive learning to handle misaligned multi-view data. It aligns cross-view covariance matrices to reduce distribution shifts and leverages a semantic graph to guide contrastive learning, improving clustering performance. Experiments on eight datasets show that SMART outperforms existing methods.

- **[arXiv251218] FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments**
  - **tags:** [ai], [reinforcement learning], [feature model-based reinforcement learning, actor-critic, Dyna-Q, model-based RL, model-free RL, multi-task control]
  - **authors:** Quanxi Zhou, Wencan Mao, Manabu Tsukada, John C.S. Lui, Yusheng Ji
  - **institution:** The University of Tokyo, National Institute of Informatics, The Chinese University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.15430
  - **Simple LLM Summary:** The paper proposes FM-EAC, a feature model-based enhanced actor-critic algorithm that integrates planning, acting, and learning for multi-task control. It combines model-based and model-free reinforcement learning to improve generalizability across tasks. Simulations show it outperforms state-of-the-art methods in urban and agricultural applications.

- **[arXiv251218] Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat**
  - **tags:** [ai], [game AI], [outer-learning, bootstrapping, self-playing, perfect feature hash, statistical information, self-learning]
  - **authors:** Stefan Edelkamp
  - **institution:** Charles University
  - **link:** https://arxiv.org/pdf/2512.15435
  - **Simple LLM Summary:** The paper proposes a bootstrapping outer-learning framework that expands a database of human expert games with millions of self-playing AI games to generate improved statistics for decision-making in trick-taking card games. It implements perfect feature hash functions for compacted tables to create a self-improving game engine. The case study in Skat demonstrates that this automated approach can effectively support various early-game decisions.

- **[arXiv251218] Double Horizon Model-Based Policy Optimization**
  - **tags:** [ai], [reinforcement learning], [model-based reinforcement learning, policy optimization, distribution rollout, training rollout, double horizon]
  - **authors:** Akihiro Kubo, Paavo Parmas, Shin Ishii
  - **institution:** Advanced Telecommunications Research Institute, Kyoto University, The University of Tokyo
  - **link:** https://arxiv.org/pdf/2512.15439
  - **Simple LLM Summary:** The paper proposes Double Horizon Model-Based Policy Optimization (DHMBPO), a method that separates the model rollout process into a long "distribution rollout" to mitigate distribution shift and a short "training rollout" for stable gradient estimation. This approach balances model bias and gradient variance. The method demonstrates superior sample efficiency and runtime compared to existing MBRL methods on continuous-control benchmarks.

- **[arXiv251218] Intent-Driven UAM Rescheduling**
  - **tags:** [ai], [scheduling optimization], [Mixed Integer Linear Programming (MILP), Answer Set Programming (ASP), three-valued logic, decision tree, Resource-Constrained Project Scheduling Problem (RCPSP)]
  - **authors:** Jeongseok Kim, Kangjin Kim
  - **institution:** Cleverplant, Chodang University
  - **link:** https://arxiv.org/pdf/2512.15462
  - **Simple LLM Summary:** This paper proposes an intent-driven rescheduling system for Urban Air Mobility (UAM) that combines Answer Set Programming (ASP) with Mixed Integer Linear Programming (MILP) to handle ambiguous human requests. It uses a three-valued logic and a decision tree to interpret vague user intents for transparent schedule adjustments. The main conclusion is that this integrated framework provides a robust, explainable, and adaptive structure for UAM scheduling.

- **[arXiv251218] On Assessing the Relevance of Code Reviews Authored by Generative Models**
  - **tags:** [mlsys], [llm inference], [multi-subjective ranking, code review generation, ChatGPT, human evaluation, CodeReview StackExchange]
  - **authors:** Robert Heumüller, Frank Ortmeier
  - **institution:** Otto von Guericke University Magdeburg
  - **link:** https://arxiv.org/pdf/2512.15466
  - **Simple LLM Summary:** This paper proposes a multi-subjective ranking method to evaluate AI-generated code review comments, comparing ChatGPT outputs against top human responses from CodeReview StackExchange. The results show that ChatGPT's comments were ranked significantly better than human-authored ones, even outperforming accepted answers. The method aims to provide a more meaningful assessment of generative AI in code review while highlighting risks of unchecked integration.

- **[arXiv251218] How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?**
  - **tags:** [mlsys], [llm training], [membership inference, semantically equivalent code transformation, variable renaming, causal analysis, code obfuscation]
  - **authors:** Hua Yang, Alejandro Velasco, Thanh Le-Cong, Md Nazmul Haque, Bowen Xu, Denys Poshyvanyk
  - **institution:** North Carolina State University, William & Mary, The University of Melbourne
  - **link:** https://arxiv.org/pdf/2512.15468
  - **Simple LLM Summary:** This paper investigates how semantically equivalent code transformations, such as variable renaming, can be used to evade membership inference detection in large language models for code. It finds that these transformations, especially RenameVariable, can significantly reduce the success of membership inference attacks without substantially harming model performance. The results reveal a critical vulnerability in license compliance enforcement for code LLMs, showing that transformation-based obfuscation can weaken detection of unauthorized code usage.

- **[arXiv251218] Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision**
  - **tags:** [mlsys], [llm training], [long-context fine-tuning, sequential bucketed strategy, tool-integrated reasoning, multi-mode supervision, dataset distillation]
  - **authors:** Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.15489
  - **Simple LLM Summary:** This paper introduces Nemotron-Math, a large-scale mathematical reasoning dataset generated using the multi-mode capabilities of gpt-oss-120b, featuring diverse reasoning styles and Python tool integration. The authors also propose a sequential bucketed strategy to accelerate long-context fine-tuning. The dataset enables state-of-the-art performance on mathematical benchmarks, achieving 100% accuracy on AIME 2024/2025 with tool-integrated reasoning.

- **[arXiv251218] Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection**
  - **tags:** [mlsys], [others], [transformer, multi-head self-attention, positional encoding, precision-focused loss, edge deployment, TensorFlow Lite, ONNX, TensorRT]
  - **authors:** Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos
  - **institution:** KTH Royal Institute of Technology, Lenovo
  - **link:** https://arxiv.org/pdf/2512.15503
  - **Simple LLM Summary:** This paper proposes AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons. It uses multi-head self-attention to capture spatiotemporal dependencies and a precision-focused loss to minimize false positives. The method demonstrates high performance and achieves sub-millisecond inference latency, making it suitable for deployment on resource-constrained edge platforms.

- **[arXiv251218] Soft Geometric Inductive Bias for Object Centric Dynamics**
  - **tags:** [ai], [geometric deep learning], [geometric algebra neural networks, soft geometric inductive bias, equivariance, object-centric world models, autoregressive training, long-horizon rollouts]
  - **authors:** Hampus Linander, Conor Heins, Alexander Tschantz, Marco Perin, Christopher Buckley
  - **institution:** VERSES AI, University of Sussex
  - **link:** https://arxiv.org/pdf/2512.15493
  - **Simple LLM Summary:** This paper proposes using object-centric world models built with geometric algebra neural networks to provide a soft geometric inductive bias for learning physical dynamics. The method is evaluated on 2D rigid body simulations and shows that this soft bias leads to better long-horizon prediction fidelity compared to non-equivariant baselines, effectively balancing between strict symmetry constraints and unstructured learning.

- **[arXiv251218] BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems**
  - **tags:** [mlsys], [others], [neural collaborative filtering, BERT, CNN, hybrid recommendation system, deep learning]
  - **authors:** Abdullah Al Munem, Sumona Yeasmin, Mohammad Rezwanul Huq
  - **institution:** East West University
  - **link:** https://arxiv.org/pdf/2512.15526
  - **Simple LLM Summary:** This paper proposes a hybrid neural collaborative filtering (NCF) model that integrates BERT and CNN to process categorical and image data for recommendations. The model was trained on a MovieLens dataset and outperformed baseline NCF and BERT-based NCF models. The results show that incorporating both categorical and image data can improve recommendation system performance.

- **[arXiv251218] A Conditioned UNet for Music Source Separation**
  - **tags:** [mlsys], [others], [conditioned UNet, music source separation, QSCNet, Sparse Compressed Network, Bandsplit RNN, Banquet, MoisesDb]
  - **authors:** Ken O'Hanlon, Basil Woods, Lin Wang, Mark Sandler
  - **institution:** Queen Mary University of London, AudioStrip Ltd.
  - **link:** https://arxiv.org/pdf/2512.15532
  - **Simple LLM Summary:** This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters.

- **[arXiv251218] A Decision-Theoretic Approach for Managing Misalignment**
  - **tags:** [ai], [decision theory], [value alignment, delegation, decision-theoretic framework, epistemic accuracy, reach, universal delegation, context-specific delegation]
  - **authors:** Daniel A. Herrmann, Abinav Chari, Isabelle Qian, Sree Sharvesh, B. A. Levinstein
  - **institution:** University of North Carolina at Chapel Hill, Georgia Institute of Technology, University of California, Berkeley, Amrita Vishwa Vidyapeetham, University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.15584
  - **Simple LLM Summary:** This paper introduces a formal, decision-theoretic framework to analyze when to delegate decisions to an AI agent by balancing its value alignment, epistemic accuracy, and reach. It concludes that universal delegation requires near-perfect alignment, but context-specific delegation can be optimal even with significant misalignment if the agent's superior accuracy or expanded reach offers a net benefit.

- **[arXiv251218] Evaluating Large Language Models in Scientific Discovery**
  - **tags:** [ai], [scientific discovery evaluation], [scenario-grounded benchmark, two-phase evaluation, hypothesis generation, experiment design, result interpretation]
  - **authors:** Zhangde Song, Jieyu Lu, Yuanqi Du, Botao Yu, Thomas M. Pruyn, Yue Huang, Kehan Guo, Xiuzhe Luo, Yuanhao Qu, Yi Qu, Yinkai Wang, Haorui Wang, Jeff Guo, Jingru Gan, Parshin Shojaee, Di Luo, Andres M Bran, Gen Li, Qiyuan Zhao, Shao-Xiong Lennon Luo, Yuxuan Zhang, Xiang Zou, Wanru Zhao, Yifan F. Zhang, Wucheng Zhang, Shunan Zheng, Saiyang Zhang, Sartaaj Takrim Khan, Mahyar Rajabi-Kochi, Samantha Paradi-Maropakis, Tony Baltoiu, Fengyu Xie, Tianyang Chen, Kexin Huang, Weiliang Luo, Meijing Fang, Xin Yang, Lixue Cheng, Jiajun He, Soha Hassoun, Xiangliang Zhang, Wei Wang, Chandan K. Reddy, Chao Zhang, Zhiling Zheng, Mengdi Wang, Le Cong, Carla P. Gomes, Chang-Yu Hsieh, Aditya Nandy, Philippe Schwaller, Heather J. Kulik, Haojun Jia, Huan Sun, Seyed Mohamad Moosavi, Chenru Duan
  - **institution:** Deep Principle, Cornell University, The Ohio State University, University of Toronto, University of Notre Dame, QuEra Computing Inc., Stanford University, Harvard Law School, Tufts University, Georgia Institute of Technology, Ecole Polytechnique Federale de Lausanne, University of California, Los Angeles, Virginia Tech, Tsinghua University, Princeton University, Harvard University, University of Cambridge, The University of Texas at Austin, McGill University, University of Science and Technology of China, Massachusetts Institute of Technology, Zhejiang University, The Hong Kong University of Science and Technology, Washington University in St. Louis
  - **link:** https://arxiv.org/pdf/2512.15567
  - **Simple LLM Summary:** The paper introduces a two-phase Scientific Discovery Evaluation (SDE) framework, which uses scenario-grounded benchmarks across multiple scientific domains to assess LLMs on question-level accuracy and project-level tasks like hypothesis generation and experiment design. It concludes that current LLMs show a significant performance gap in scientific discovery compared to general benchmarks, exhibit diminishing returns from scaling, and are far from being general scientific "superintelligences," though they still demonstrate promise in various discovery projects.

- **[arXiv251218] How Smoothing is N-simplicial Attention?**
  - **tags:** [ai], [attention mechanisms], [N-simplicial attention, higher-order interactions, Rotary Position Embeddings (RoPE), simplex selection, over-smoothing, Lipschitz bound]
  - **authors:** Alexandre Dussolle, Pietro Liò
  - **institution:** University of Cambridge, École des Ponts, IP Paris
  - **link:** https://arxiv.org/pdf/2512.15600
  - **Simple LLM Summary:** This paper introduces N-simplicial attention, a method that generalizes standard attention to higher-order token interactions and adapts it for Rotary Position Embeddings (RoPE). It also proposes a cost-effective simplex selection mechanism to manage computational complexity. The authors demonstrate that, despite enabling higher-order interactions, N-simplicial attention itself suffers from over-smoothing.

- **[arXiv251218] Evaluating Metrics for Safety with LLM-as-Judges**
  - **tags:** [mlsys], [llm inference], [LLM-as-Judges, weighted metrics, confidence thresholds, context sensitivity, safety evaluation, human review]
  - **authors:** Kester Clegg, Richard Hawkins, Ibrahim Habli, Tom Lawton
  - **institution:** University of York, Bradford Royal Infirmary
  - **link:** https://arxiv.org/pdf/2512.15617
  - **Simple LLM Summary:** This paper proposes a safety evaluation method for LLMs in critical applications by focusing on evidence from LLM-as-Judges frameworks. It suggests using a basket of weighted metrics and context-sensitive error severity to lower risk, with low-confidence judgments triggering human review. The main conclusion is that such an approach can enhance the reliability of LLMs in safety-critical information flows.

- **[arXiv251218] How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness**
  - **tags:** [mlsys], [llm training], [Low-Rank Adaptation (LoRA), supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), rank sweep, representational drift, attention patterns]
  - **authors:** Darshita Rathore, Vineet Kumar, Chetna Bansal, Anindya Moitra
  - **institution:** PayPal
  - **link:** https://arxiv.org/pdf/2512.15634
  - **Simple LLM Summary:** This paper comprehensively evaluates the trade-offs between full supervised fine-tuning (SFT) and Low-Rank Adaptation (LoRA) for fine-tuning large language models. It finds that LoRA, especially at specific rank values, can achieve competitive or even superior performance to SFT on reasoning tasks, while also analyzing the structural changes in model representations.

- **[arXiv251218] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning**
  - **tags:** [mlsys], [diffusion inference], [diffusion transformer (DiT), in-context learning, Effect-LoRA, spatiotemporal sparse tokenization, two-stage training, few-shot video editing]
  - **authors:** Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao
  - **institution:** Communication University of China, National University of Singapore, Baidu Inc.
  - **link:** https://arxiv.org/pdf/2512.15635
  - **Simple LLM Summary:** IC-Effect is an instruction-guided video VFX editing framework based on Diffusion Transformers (DiT) that uses the source video as a contextual condition and employs a two-stage training strategy with Effect-LoRA for precise effect injection and background preservation. It introduces spatiotemporal sparse tokenization for computational efficiency. The method demonstrates high-quality, temporally consistent visual effects editing from limited data, enabling new possibilities for video creation.

- **[arXiv251218] PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning**
  - **tags:** [ai], [continual learning], [energy-based model, progressive parameter selection, pseudo-sample generation, catastrophic forgetting mitigation]
  - **authors:** Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan
  - **institution:** Mayo Clinic, Google, Texas A&M University, The University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.15658
  - **Simple LLM Summary:** The paper introduces PPSEBM, a framework combining an Energy-Based Model with Progressive Parameter Selection to address catastrophic forgetting in continual learning for NLP tasks. It uses task-specific parameters and generates pseudo-samples from prior tasks to retain past knowledge. Experimental results show PPSEBM outperforms state-of-the-art methods in mitigating forgetting.

- **[arXiv251218] Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning**
  - **tags:** [ai], [reasoning and self-critique], [stepwise think-critique, reinforcement learning, self-critique, chain of thought, process reward models]
  - **authors:** Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan LU
  - **institution:** University of Science and Technology of China, Microsoft Research Asia
  - **link:** https://arxiv.org/pdf/2512.15662
  - **Simple LLM Summary:** The paper proposes Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single large language model, trained using a hybrid reinforcement learning objective. Experiments on mathematical reasoning benchmarks show that STC enhances critical thinking capabilities and produces more interpretable reasoning traces.

- **[arXiv251218] Explaining the Reasoning of Large Language Models Using Attribution Graphs**
  - **tags:** [ai], [interpretability], [attribution methods, context attribution, attribution graph, CAGE, faithfulness]
  - **authors:** Chase Walker, Rickard Ewetz
  - **institution:** University of Florida
  - **link:** https://arxiv.org/pdf/2512.15663
  - **Simple LLM Summary:** The paper introduces the CAGE framework, which uses an attribution graph to explain autoregressive LLMs by quantifying how each generated token is influenced by both the prompt and all prior tokens, preserving causality and row stochasticity. This approach improves the faithfulness of context attributions by accounting for inter-generational influences, achieving average gains of up to 40% over existing methods.

- **[arXiv251218] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?**
  - **tags:** [mlsys], [multi-modal inference], [vision-text compression, VTCBench, DeepSeek-OCR, Glyph, VTC-Retrieval, VTC-Reasoning, VTC-Memory]
  - **authors:** Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS; Tencent Hunyuan Team
  - **link:** https://arxiv.org/pdf/2512.15649
  - **Simple LLM Summary:** This paper introduces VTCBench, the first benchmark to evaluate Vision-Language Models' ability to understand long context using Vision-Text Compression (VTC), a technique that converts long text into dense 2D images for token efficiency. The study systematically tests models on retrieval, reasoning, and memory tasks with VTC-compressed inputs. The main conclusion is that most VLMs perform poorly on long-context understanding with VTC, despite good OCR decoding, failing to capture long-range associations in the compressed visual context.

- **[arXiv251218] Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers**
  - **tags:** [ai], [llm interpretability], [LatentQA, Activation Oracles, activation analysis, fine-tuning detection, natural language queries]
  - **authors:** Adam Karvonen, James Chua, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks
  - **institution:** MATS, Truthful AI, EPFL, ENS Paris-Saclay, Northeastern University, Anthropic
  - **link:** https://arxiv.org/pdf/2512.15674
  - **Simple LLM Summary:** This paper introduces Activation Oracles, models trained using the LatentQA approach to answer natural language questions about the internal activations of other LLMs. The core finding is that these oracles, especially when trained on diverse datasets, can generalize to out-of-distribution tasks and effectively verbalize hidden information, such as knowledge from fine-tuning, often matching or exceeding prior white-box interpretability methods.

- **[arXiv251218] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning**
  - **tags:** [ai], [reinforcement learning], [gradient-guided reinforcement learning, G2RL, PPO, KL control, final-layer sensitivity]
  - **authors:** Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu
  - **institution:** Tencent AI Lab, University of Notre Dame
  - **link:** https://arxiv.org/pdf/2512.15687
  - **Simple LLM Summary:** The paper introduces G2RL, a gradient-guided reinforcement learning framework that uses the model's own gradient directions to guide exploration, rather than external heuristics like entropy bonuses. It shows that G2RL improves reasoning performance across multiple benchmarks by encouraging diverse and orthogonal update directions. The results indicate that a policy's internal update geometry provides a more effective basis for exploration in LLM reinforcement learning.

- **[arXiv251218] BashArena: A Control Setting for Highly Privileged AI Agents**
  - **tags:** [mlsys], [cluster infrastructure], [AI control, red teaming, blue teaming, adversarial evaluation, system administration tasks, sabotage detection, Linux environment, control protocols]
  - **authors:** Adam Kaufman, James Lucassen, Tyler Tracy, Cody Rushing, Aryan Bhatt
  - **institution:** Redwood Research
  - **link:** https://arxiv.org/pdf/2512.15688
  - **Simple LLM Summary:** The paper introduces BashArena, a control setting with 637 Linux system administration tasks and sabotage objectives to study AI control techniques for highly privileged agents. It evaluates frontier LLMs in an adversarial game between red teams (performing sabotage) and blue teams (detecting it), finding that Claude Sonnet 4.5 can evade detection by GPT-4.1 mini 26% of the time, establishing a baseline for more effective control protocols.

- **[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs**
  - **tags:** [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]
  - **authors:** Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava
  - **institution:** mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.15692
  - **Simple LLM Summary:** The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.

- **[arXiv251218] Artism: AI-Driven Dual-Engine System for Art Generation and Critique**
  - **tags:** [mlsys], [others], [deep learning, multi-agent systems, conceptual collage, AI-driven critical loops]
  - **authors:** Shuai Liu, Yiqing Tian, Yang Chen, Mar Canet Sola
  - **institution:** Academy of Media Arts Cologne, Goldsmiths University of London, Royal College of Art, Tallinn University
  - **link:** https://arxiv.org/pdf/2512.15710
  - **Simple LLM Summary:** This paper proposes a dual-engine AI system called Artism, consisting of AIDA (an artificial artist social network) and the Ismism Machine for critique, to simulate art historical trajectories and conceptual innovation. The core method leverages deep learning and multi-agent collaboration to shift from unidirectional critique to an interactive, reflexive practice. It concludes by introducing a general AI-driven critical loop methodology, offering new possibilities for the computational analysis of art.

- **[arXiv251218] Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants**
  - **tags:** [mlsys], [post-training], [predictive concept decoder, communication bottleneck, sparse concept list, encoder-decoder, auto-interp score, fine-tuning]
  - **authors:** Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt
  - **institution:** Transluce
  - **link:** https://arxiv.org/pdf/2512.15712
  - **Simple LLM Summary:** This paper proposes Predictive Concept Decoders (PCDs), an end-to-end trained architecture where an encoder compresses a model's internal activations into a sparse list of concepts, and a decoder uses this list to answer questions about the model's behavior. The method is pretrained on large datasets and then finetuned, showing that the interpretability and downstream performance of the bottleneck concepts improve with more data.

- **[arXiv251218] Spatia: Video Generation with Updatable Spatial Memory**
  - **tags:** [mlsys], [multi-modal inference], [spatial memory, 3D scene point cloud, visual SLAM, dynamic-static disentanglement, camera control, 3D-aware editing]
  - **authors:** Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu
  - **institution:** The University of Sydney, Microsoft Research, HKUST, University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.15716
  - **Simple LLM Summary:** The paper proposes Spatia, a video generation framework that uses an updatable 3D scene point cloud as persistent spatial memory to enhance long-term spatial consistency. It iteratively generates video clips conditioned on this memory and updates it via visual SLAM, enabling applications like explicit camera control and 3D-aware interactive editing.

- **[arXiv251218] PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents**
  - **tags:** [mlsys], [multi-modal training], [adversarial agents, Monte Carlo Tree Search (MCTS), question chains, pyramid-like reasoning, vision language models (VLMs), fine-tuning, synthetic dataset]
  - **authors:** Yuqun Zhang, Yuxuan Zhao, Sijia Chen
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Yantai Research Institute, Harbin Engineering University
  - **link:** https://arxiv.org/pdf/2512.14735
  - **Simple LLM Summary:** This paper introduces PyFi, a framework that uses a multi-agent adversarial mechanism under Monte Carlo Tree Search to synthesize a large-scale, pyramid-structured financial image QA dataset without human annotation. Fine-tuning VLMs on this dataset enables them to decompose complex financial questions into simpler sub-questions, leading to significant accuracy improvements on the benchmark.

- **[arXiv251218] Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis**
  - **tags:** [mlsys], [cluster infrastructure], [causal mediation analysis, directed acyclic graphs (DAGs), root cause analysis, multiple treatments, multiple mediators, effect decomposition]
  - **authors:** Alessandro Casadei, Sreyoshi Bhaduri, Rohit Malshe, Pavan Mullapudi, Raj Ratan, Ankush Pole, Arkajit Rakshit
  - **institution:** Amazon
  - **link:** https://arxiv.org/pdf/2512.14764
  - **Simple LLM Summary:** The paper proposes a scalable causal mediation analysis framework designed for complex systems represented by high-dimensional directed acyclic graphs with multiple interacting treatments and mediators. It systematically decomposes total effects into direct and indirect components to identify root causes. The method is demonstrated through case studies in fulfillment center logistics, showing its practical utility for diagnosing operational systems.

- **[arXiv251218] VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation**
  - **tags:** [mlsys], [llm inference], [neurosymbolic policy generation, agentic framework, dense retrieval, cross-encoder reranking, financial tool-enabled agents, GAAP compliance, SEC requirements, mathematical validation]
  - **authors:** Adewale Akinfaderin, Shreyas Subramanian
  - **institution:** Amazon Web Services
  - **link:** https://arxiv.org/pdf/2512.14744
  - **Simple LLM Summary:** This paper introduces VERAFI, a framework that combines dense retrieval and cross-encoder reranking with financial tool-enabled agents and a neurosymbolic policy layer for automated reasoning and compliance checking. The integrated approach significantly improves factual correctness on financial benchmarks, demonstrating that incorporating domain-specific verification policies is crucial for achieving trustworthy financial AI.

- **[arXiv251218] Multiscale Cross-Modal Mapping of Molecular, Pathologic, and Radiologic Phenotypes in Lipid-Deficient Clear Cell Renal CellCarcinoma**
  - **tags:** [ai], [medical imaging and computational pathology], [cross-modal mapping, multi-omics, computational pathology, radiomics, hierarchical modeling, molecular phenotyping]
  - **authors:** Ying Cui, Dongzhe Zheng, Ke Yu, Xiyin Zheng, Xiaorui Wang, Xinxiang Li, Yan Gu, Lin Fu, Xinyi Chen, Wenjie Mei, Xin-Gui Peng
  - **institution:** Southeast University, Princeton University, Columbia University, University of Science and Technology of China, Nanjing University, Lanzhou University of Technology, Lianyungang First People's Hospital
  - **link:** https://arxiv.org/pdf/2512.14750
  - **Simple LLM Summary:** This paper establishes a hierarchical cross-scale framework that uses cross-modal mapping to transfer molecular signatures to histological and CT imaging phenotypes for the preoperative identification of a high-risk renal cancer subtype. The method integrates multi-scale features from pathology (PathoDCCD) and radiology (RadioDCCD) to predict molecular subtypes. It demonstrates that this approach can reliably identify patients with the poorest clinical outcomes across multiple patient cohorts.

- **[arXiv251218] Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images**
  - **tags:** [ai], [computational pathology], [self-supervised learning, vision transformer, cross-scale distillation, magnification-invariant representation, whole-slide image analysis]
  - **authors:** Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner
  - **institution:** University of Kentucky, University of Pittsburgh, Emory University, University of California Davis, University of Texas Health
  - **link:** https://arxiv.org/pdf/2512.14796
  - **Simple LLM Summary:** This paper introduces Magnification-Aware Distillation (MAD), a self-supervised framework that learns unified image representations by linking low-magnification context with spatially aligned high-magnification detail in whole-slide images. The resulting foundation model, MAD-NP, demonstrates strong resolution-invariant learning, as shown by a classifier trained on 10x embeddings maintaining 96.7% performance on unseen 40x tiles. The work concludes that this approach enables scalable, magnification-robust analysis using a unified embedding space.

- **[arXiv251218] Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies**
  - **tags:** [ai], [decision theory], [restless multi-armed bandits, Whittle index, Markov chains, Thompson Sampling, Bayesian UCB, epsilon-Greedy]
  - **authors:** Jaume Anguera Peris, Songtao Cheng, Hanzhao Zhang, Wei Ouyang, Joakim Jaldén
  - **institution:** KTH Royal Institute of Technology, SciLifeLab
  - **link:** https://arxiv.org/pdf/2512.14930
  - **Simple LLM Summary:** This paper introduces the Restless Multi-Process Multi-Armed Bandit (RMPMAB) framework, which models experimental regions as ensembles of Markov chains to capture biological heterogeneity. It derives closed-form expressions for process behavior and designs scalable Whittle index policies. The method significantly outperforms existing bandit algorithms in simulations and live-cell imaging, capturing more biological events and reducing regret, demonstrating its potential for autonomous smart microscopy.

- **[arXiv251218] Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer**
  - **tags:** [mlsys], [multi-modal inference], [deep learning, video analysis, segmentation, classification, Fagotti score, diagnostic laparoscopy, Dice score, F1-score, RMSE]
  - **authors:** Riccardo Oliva, Farahdiba Zarin, Alice Zampolini Faustini, Armine Vardazaryan, Andrea Rosati, Vinkle Srivastav, Nunzia Del Villano, Jacques Marescaux, Giovanni Scambia, Pietro Mascagni, Nicolas Padoy, Anna Fagotti
  - **institution:** Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Università Cattolica del Sacro Cuore, IRCAD, University of Strasbourg, IHU Strasbourg, Università degli studi di Modena
  - **link:** https://arxiv.org/pdf/2512.14797
  - **Simple LLM Summary:** This paper develops a deep learning system to analyze diagnostic laparoscopy videos for advanced ovarian cancer, automating the assessment of peritoneal carcinosis and predicting the Fagotti score to guide surgical decisions. The AI model segments anatomical structures and tumor lesions, then classifies video-level features to estimate surgical feasibility. The results demonstrate reproducible performance, suggesting AI can standardize intraoperative tumor burden assessment and support clinical decision-making.

- **[arXiv251218] QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management**
  - **tags:** [mlsys], [others], [hierarchical deep reinforcement learning, double deep Q-network, constrained soft actor-critic, Lagrangian multipliers, centralized training and decentralized execution]
  - **authors:** Jiayang Wan, Ke He, Yafei Wang, Fan Liu, Wenjin Wang, Shi Jin
  - **institution:** Southeast University, Purple Mountain Laboratories, University of Luxembourg
  - **link:** https://arxiv.org/pdf/2512.15119
  - **Simple LLM Summary:** This paper proposes a two-level hierarchical deep reinforcement learning framework for joint link selection and trajectory optimization in UAV mobility management within SAGIN. The method uses a top-level DDQN for discrete link selection and a lower-level constrained SAC with Lagrangian multipliers for continuous trajectory optimization under QoS constraints. Simulation results show the proposed scheme outperforms benchmarks in throughput, link switching frequency, and QoS satisfaction.

- **[arXiv251218] Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images**
  - **tags:** [mlsys], [others], [meta-learning, few-shot learning, weakly-supervised segmentation, sparse labels, Omni meta-training, Efficient Omni ProtoSeg (EO-ProtoSeg)]
  - **authors:** Pandega Abyan Zumarsyah, Igi Ardiyanto, Hanung Adi Nugroho
  - **institution:** Universitas Gadjah Mada
  - **link:** https://arxiv.org/pdf/2512.15061
  - **Simple LLM Summary:** This paper proposes improved meta-learners for few-shot, weakly-supervised segmentation of the optic disc and cup in fundus images. The core method introduces Omni meta-training for balanced data usage and efficient versions to reduce computational costs, along with sparsification techniques for generating scribbles. The best model, EO-ProtoSeg, achieves high segmentation accuracy using only one sparsely labeled image, outperforming methods that require more labels and being comparable to heavier unsupervised domain adaptation approaches.

## 2025-12-19

- **[arXiv251219] DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression**
  - **tags:** [ai], [optimization], [symbolic regression, disciplined convex programming, system identification, convex optimization]
  - **authors:** Sveinung Myhre
  - **institution:** University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.15721
  - **Simple LLM Summary:** The paper proposes DiscoverDCP, a method that integrates symbolic regression with Disciplined Convex Programming (DCP) rules to discover convex models directly from data. This ensures the discovered expressions are globally convex by construction, avoiding the need for difficult convexity verification. The approach yields interpretable and flexible convex models that can be more accurate than traditional fixed-form convex functions.

- **[arXiv251219] Value Lens: Using Large Language Models to Understand Human Values**
  - **tags:** [mlsys], [llm inference], [large language models, value detection, generative AI, dual-LLM approach, expert verification]
  - **authors:** Eduardo de la Cruz Fernández, Marcelo Karanik, Sascha Ossowski
  - **institution:** Universidad Politécnica de Madrid, Universidad Rey Juan Carlos
  - **link:** https://arxiv.org/pdf/2512.15722
  - **Simple LLM Summary:** The paper proposes Value Lens, a two-stage model that uses Large Language Models (LLMs) to detect human values in text. The first stage uses an LLM to conceptualize a value theory verified by experts, and the second stage employs a dual-LLM approach for detection and critical review. The results show that Value Lens performs comparably to or better than other models in similar tasks.

- **[arXiv251219] Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions**
  - **tags:** [mlsys], [multi-modal inference], [LDraw, large language models, discrete parts vocabulary, geometric validity, connection constraints, buildability ordering, Python library, bag of bricks]
  - **authors:** David Noever
  - **institution:** PeopleTec
  - **link:** https://arxiv.org/pdf/2512.15743
  - **Simple LLM Summary:** This paper presents a framework that uses large language models guided by tools to generate physically buildable assembly instructions from natural language. It operates within a constrained vocabulary of discrete parts (like LEGO bricks) using LDraw as an intermediate representation to enforce geometric and assembly constraints. The method demonstrates scalability and bridges semantic design intent with manufacturable output, functioning as a "physical API" for prototyping.

- **[arXiv251219] The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems**
  - **tags:** [ai], [ethical AI frameworks], [Principle of Proportional Duty, Monte Carlo simulations, humility coefficient, knowledge-duty framework, ethical equilibrium]
  - **authors:** Timothy Prescher
  - **institution:** Grand Valley State University
  - **link:** https://arxiv.org/pdf/2512.15740
  - **Simple LLM Summary:** This paper introduces the Principle of Proportional Duty (PPD), a framework that mathematically models how ethical responsibility scales with an agent's knowledge and uncertainty, converting Action Duty into Repair Duty as uncertainty increases. The core method uses a formal equation and Monte Carlo simulations to demonstrate that systems with a baseline humility coefficient produce more stable duty allocations. The main conclusion is that this proportional duty serves as a stabilizing principle in complex systems, preventing overreach and omission by dynamically balancing epistemic confidence against contextual risk.

- **[arXiv251219] Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction**
  - **tags:** [ai], [financial machine learning], [ensemble learning, quantum sentiment analysis, decision transformer, variational quantum circuit, smart filtering, LSTM, XGBoost, Random Forest, Logistic Regression]
  - **authors:** Abraham Itzhak Weinberg
  - **institution:** AI Experts
  - **link:** https://arxiv.org/pdf/2512.15738
  - **Simple LLM Summary:** This paper introduces a hybrid quantum-classical ensemble learning framework for S&P 500 directional prediction, combining diverse classical models (e.g., Decision Transformer, LSTM) with a 4-qubit quantum circuit for sentiment analysis and strategic model selection. It concludes that architecture diversity is more critical than dataset diversity for ensemble performance, achieving 60.14% directional accuracy and a statistically significant improvement over individual models. Preliminary backtesting suggests the framework has practical trading potential.

- **[arXiv251219] Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments**
  - **tags:** [mlsys], [others], [multi-agent AI, semantic retrieval, physics simulation, QuTiP, FreeSim, intent routing, knowledge-augmented generation, dual-mode validation]
  - **authors:** S. K. Rithvik
  - **institution:** Quantum Science and Technology Laboratory, Physical Research Laboratory; Indian Institute of Technology Gandhinagar
  - **link:** https://arxiv.org/pdf/2512.15736
  - **Simple LLM Summary:** Anubuddhi is a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts using semantic retrieval and physics simulation with dual validation modes. It achieves high design-simulation alignment, demonstrating that flexible simulation frameworks are crucial for diverse quantum optics experiments. The system democratizes computational experiment design, enabling iterative refinement through conversation for research and education.

- **[arXiv251219] LLaDA2.0: Scaling Up Diffusion Language Models to 100B**
  - **tags:** [mlsys], [llm training], [discrete diffusion language model, block-level WSD training, mixture-of-experts, knowledge inheritance, parallel decoding, SFT, DPO]
  - **authors:** Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Ling Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, Yihong Zhuang
  - **institution:** Ant Group, Renmin University of China, Zhejiang University, Westlake University, HongKong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.15745
  - **Simple LLM Summary:** This paper introduces LLaDA2.0, a method for converting pre-trained auto-regressive language models into large-scale discrete diffusion models (dLLMs) using a novel three-phase block-level training scheme. The resulting instruction-tuned models, including a 100B-parameter variant, achieve superior performance and efficiency through parallel decoding. The work establishes a new paradigm for frontier-scale model deployment by enabling efficient scaling and knowledge inheritance from existing models.

- **[arXiv251219] ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning**
  - **tags:** [mlsys], [post-training], [Direct Preference Optimization (DPO), parameter-efficient fine-tuning, sequence modeling, Monte Carlo simulation]
  - **authors:** Yoonpyo Lee
  - **institution:** Hanyang University
  - **link:** https://arxiv.org/pdf/2512.15756
  - **Simple LLM Summary:** This paper introduces ReactorFold, a generative framework that uses language models fine-tuned with Direct Preference Optimization on Monte Carlo simulation data to design nuclear reactor fuel assemblies as a sequence modeling task. The model demonstrates emergent physical reasoning by autonomously adjusting design parameters like gadolinium rod inventory and discovering high-performing asymmetric configurations, transcending human-imposed design constraints.

- **[arXiv251219] TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification**
  - **tags:** [mlsys], [llm inference], [transformer, large language model, OOD detection, semantic-enhanced prompt, two-stage classification, PCA residuals, inter-layer smoothness]
  - **authors:** Zihao Wang, Wei Peng, Junming Zhang, Jian Li, Wenxin Fang
  - **institution:** Not specified in provided text
  - **link:** https://arxiv.org/pdf/2512.15753
  - **Simple LLM Summary:** The paper proposes TAO-Net, a two-stage network for fine-grained encrypted traffic classification. It first uses a hybrid OOD detector to separate known and unknown traffic, then applies an LLM with semantic prompts to classify the unknown traffic without predefined labels. Experiments show it significantly outperforms previous methods in precision and F1 score, especially for identifying new applications.

- **[arXiv251219] GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction**
  - **tags:** [mlsys], [llm inference], [graph neural networks, large language models, contrastive alignment, instruction tuning, graph-language co-reasoning]
  - **authors:** Wei Guan, Jian Cao, Jinyu Cai, Qiqi Cai, Jianqi Gao, See-Kiong Ng
  - **institution:** Shanghai Jiao Tong University, National University of Singapore, Shanghai University
  - **link:** https://arxiv.org/pdf/2512.15751
  - **Simple LLM Summary:** The paper proposes GLOW, a framework that combines Graph Neural Networks (GNNs) and instruction-tuned Large Language Models (LLMs) to predict the performance of Agentic Workflows by jointly modeling their graph structure and semantic logic. It uses a contrastive alignment strategy to refine the feature space. Experiments show GLOW outperforms existing methods in prediction accuracy and ranking utility on the FLORA-Bench benchmark.

- **[arXiv251219] AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs**
  - **tags:** [mlsys], [llm training], [AdaGradSelect, gradient-guided selection, Dirichlet-based sampling, epsilon-greedy exploration, selective block update, Parameter-Efficient Fine-Tuning (PEFT)]
  - **authors:** Anshul Kumar, Gagan Raj Gupta, Manisha Chawla
  - **institution:** IIT Bhilai
  - **link:** https://arxiv.org/pdf/2512.15764
  - **Simple LLM Summary:** This paper introduces AdaGradSelect, an adaptive method for efficiently fine-tuning Small Language Models (SLMs) by selecting which transformer blocks to update based on gradient norms, using a combination of Dirichlet-based sampling and epsilon-greedy exploration. It achieves performance close to full fine-tuning while training about 12% faster and using 35% less GPU memory, outperforming methods like LoRA on benchmarks such as GSM8K.

- **[arXiv251219] Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction**
  - **tags:** [ai], [healthcare, medical time series], [test-time adaptation, cross-sample augmentation, K-Shape clustering, masked reconstruction, sequence forecasting]
  - **authors:** Kanxue Li, Yibing Zhan, Hua Jin, Chongchong Qi, Xu Lin, Baosheng Yu
  - **institution:** Wuhan University, First People’s Hospital of Yunnan Province, Yunnan United Vision Technology Company Limited, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.15762
  - **Simple LLM Summary:** The paper proposes CSA-TTA, a Cross-Sample Augmented Test-Time Adaptation framework for personalized intraoperative hypotension prediction. It enhances test-time training by retrieving similar hypotension events from other patients using a coarse-to-fine strategy and integrates self-supervised signals for adaptation. The method improves prediction recall and F1 scores on real-world datasets, demonstrating strong robustness and generalization.

- **[arXiv251219] PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling**
  - **tags:** [mlsys], [others], [adversarial variational framework, progressive training, dual-path VAE-GAN, domain-specific feature matching, synthetic data generation]
  - **authors:** Jamal Al-Karaki, Muhammad Al-Zafar Khan, Rand Derar Mohammad Al Athamneh
  - **institution:** Zayed University, The Hashemite University
  - **link:** https://arxiv.org/pdf/2512.15768
  - **Simple LLM Summary:** This paper introduces PHANTOM, a progressive adversarial variational framework that uses a dual-path VAE-GAN architecture with domain-specific feature matching to generate high-fidelity synthetic cyberattack data. The method achieves 98% weighted accuracy in intrusion detection when models are trained on its synthetic data, demonstrating its effectiveness for augmenting training datasets, though it faces challenges with generating rare attack types.

- **[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]
  - **authors:** Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai
  - **institution:** Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.
  - **link:** https://arxiv.org/pdf/2512.15766
  - **Simple LLM Summary:** The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.

- **[arXiv251219] Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework**
  - **tags:** [mlsys], [others], [Graph Neural Networks, Hybrid Twin, Finite Element Method, ignorance model, nonlinear heat transfer]
  - **authors:** M. Gorpinich, B. Moya, S. Rodriguez, F. Meraghni, Y. Jaafra, A. Briot, M. Henner, R. Leon, F. Chinesta
  - **institution:** Valeo, PIMM Lab. ENSAM Institute of Technology, CNRS@CREATE LTD.
  - **link:** https://arxiv.org/pdf/2512.15767
  - **Simple LLM Summary:** This paper proposes a hybrid twin framework that uses Graph Neural Networks (GNNs) to model the discrepancy ("ignorance") between physics-based simulations (e.g., FEM) and reality, correcting the model with sparse data. The method is evaluated on nonlinear heat transfer problems across different geometries and meshes. Results show the GNN successfully captures and generalizes the corrections, improving simulation accuracy while minimizing data requirements.

- **[arXiv251219] TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions**
  - **tags:** [mlsys], [others], [Physics-Informed Neural Networks (PINNs), Time-Evolving Natural Gradient (TENG), Dirichlet boundary conditions, natural gradient optimization, Euler method, Heun method, heat equation]
  - **authors:** Xinjie He, Chenggong Zhang
  - **institution:** University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.15771
  - **Simple LLM Summary:** This paper extends the Time-Evolving Natural Gradient (TENG) framework to solve PDEs with deep neural networks, specifically by incorporating penalty terms to enforce Dirichlet boundary conditions and integrating numerical time-stepping schemes like Euler and Heun methods. Experiments on the heat equation show that the Heun method provides superior accuracy due to its second-order corrections, while the Euler method is computationally efficient for simpler scenarios. The work establishes a foundation for handling more complex boundary conditions and advancing neural network-based PDE solvers.

- **[arXiv251219] Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?**
  - **tags:** [mlsys], [diffusion inference], [Data-Chain Backdoor (DCB), clean-label attack, Early-Stage Trigger Manifestation (ESTM), backdoor triggers, synthetic data generation]
  - **authors:** Junchi Lu, Xinke Li, Yuheng Liu, Qi Alfred Chen
  - **institution:** University of California, Irvine, City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.15769
  - **Simple LLM Summary:** This paper investigates the Data-Chain Backdoor (DCB) threat, where backdoor triggers are injected into and propagated through open-source diffusion models used for synthetic data generation. The method reveals that these triggers are memorized and reproduced in the generated data, subsequently poisoning downstream models, even in clean-label attack scenarios. The main conclusion is that this poses a severe, previously underexplored security risk in generative data pipelines, highlighting the need for mitigation strategies.

- **[arXiv251219] TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration**
  - **tags:** [mlsys], [diffusion inference], [speculative decoding, reinforcement learning, knowledge distillation, transformer, temporal adaptivity]
  - **authors:** Ye Li, Jiahe Feng, Yuan Meng, Kangye Ji, Chen Tang, Xinwan Wen, Shutao Xia, Zhi Wang, Wenwu Zhu
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.15773
  - **Simple LLM Summary:** This paper proposes TS-DP, a framework that accelerates Diffusion Policy inference using speculative decoding. It employs a distilled Transformer-based drafter to generate denoising steps and an RL-based scheduler to dynamically adapt to time-varying task difficulty. The method achieves up to 4.17x faster inference with high accuracy, enabling real-time diffusion-based control.

- **[arXiv251219] Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying**
  - **tags:** [ai], [embodied ai], [Asymmetric Assistive Reasoning, active querying, Pull-based protocol, Privileged Information Bias, AI2-THOR, Leader-Follower dyad]
  - **authors:** Shaun Baek, Sam Liu, Joseph Ukpong
  - **institution:** Emory University
  - **link:** https://arxiv.org/pdf/2512.15776
  - **Simple LLM Summary:** This paper proposes an Asymmetric Assistive Reasoning framework in AI2-THOR to study how a knowledgeable "Leader" agent can guide a sensor-limited "Follower," finding that standard "Push-based" instruction fails due to Privileged Information Bias. It demonstrates that a "Pull-based" active querying protocol, where the Follower requests clarifications, significantly improves collaborative success by reducing grounding errors.

- **[arXiv251219] Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence**
  - **tags:** [ai], [adversarial robustness], [adversarial training, FGSM, PGD, SHAP, Expected Calibration Error (ECE), Value-at-Risk (VaR), Expected Shortfall (ES), bootstrap inference]
  - **authors:** Samruddhi Baviskar
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.15780
  - **Simple LLM Summary:** The paper introduces a dataset-agnostic pipeline to evaluate adversarial robustness in tabular financial ML models using gradient-based attacks like FGSM and PGD. It finds that small perturbations significantly degrade model performance and increase financial risk, but adversarial training can partially recover utility. The study also suggests SHAP stability as an early-warning indicator for adversarial influence.

- **[arXiv251219] AI Epidemiology: achieving explainable AI through expert oversight patterns**
  - **tags:** [ai], [AI governance and interpretability], [AI Epidemiology, population-level surveillance, expert oversight patterns, exposure variables, risk level, alignment score, accuracy score]
  - **authors:** Kit Tempest-Walters
  - **institution:** Logia Compliance
  - **link:** https://arxiv.org/pdf/2512.15783
  - **Simple LLM Summary:** The paper introduces AI Epidemiology, a framework that applies epidemiological surveillance methods to AI outputs by standardizing expert-AI interactions into structured assessment fields to predict failures. This approach bypasses model complexity issues, provides automatic audit trails, and enables domain experts to govern AI systems without requiring machine learning expertise.

- **[arXiv251219] Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes**
  - **tags:** [mlsys], [others], [Finite Exponential Continuous State Machine (FECSM), Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA), Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU), HDBSCAN, min-max normalization, User Interface Change Prediction Index (UICPI)]
  - **authors:** Shrinivass Arunachalam Balasubramanian
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.15775
  - **Simple LLM Summary:** This paper proposes a dynamic web UI optimization method that uses a Finite Exponential Continuous State Machine for cross-device responsiveness assessment and a novel Quokka Nonlinear Difference Swarm Optimization Algorithm for design optimization. The core technique involves classifying user experience changes with a Bidirectional Gated Luong and Mish Recurrent Unit model. The main conclusion is that this integrated approach achieves an average fitness of 98.5632% for optimal UI design by incorporating cross-responsiveness assessment and user behavior patterns.

- **[arXiv251219] Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM**
  - **tags:** [mlsys], [llm inference], [memory-centric agent, profile memory, experience memory, action memory, distance-graph (DisGraph), agent record-and-replay (AgentRR)]
  - **authors:** Zibin Liu, Cheng Zhang, Xi Zhao, Yunfei Feng, Bingyu Bai, Dahu Feng, Erhu Feng, Yubin Xia, Haibo Chen
  - **institution:** Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.15784
  - **Simple LLM Summary:** The paper proposes MOBIMEM, a memory-centric agent system that enables self-evolution of LLM agents without model retraining by using three specialized memory primitives (Profile, Experience, and Action Memory) and OS-inspired orchestration services. It demonstrates significant improvements in profile alignment, task success rates, and latency reduction on mobile devices compared to baseline approaches.

- **[arXiv251219] Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance**
  - **tags:** [ai], [AI ethics and governance], [cultural rights, right to development, algorithmic design, AI governance, human rights law]
  - **authors:** Alexander Kriebitz, Caitlin Corrigan, Aive Pevkur, Alberto Santos Ferro, Amanda Horzyk, Dirk Brand, Dohee Kim, Dodzi Koku Hattoh, Flavia Massucci, Gilles Fayad, Kamil Strzepek, Laud Ammah, Lavina Ramkissoon, Mariette Awad, Natalia Amasiadi, Nathan C. Walker, Nicole Manger, Sophia Devlin
  - **institution:** Ludwig Maximilian University of Munich, Technical University of Munich, Tallinn University of Technology, University of Edinburgh, Stellenbosch University, Changwon National University, University of Ghana, Cardinal Stefan Wyszyński University in Warsaw, African Union, American University of Beirut, University of Patras, Rutgers University, Ulster University
  - **link:** https://arxiv.org/pdf/2512.15786
  - **Simple LLM Summary:** The paper conceptually analyzes the impact of AI on cultural rights and the right to development, examining the epistemic and normative limitations in algorithmic design. It concludes that AI governance frameworks have significant gaps in protecting these rights and risks exacerbating global inequities. The study calls for integrating cultural and developmental considerations into future AI policy and research.

- **[arXiv251219] Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces**
  - **tags:** [mlsys], [others], [generative AI, multi-agent systems, edge computing, sustainability, ambient intelligence, AI agents, sustainable ecosystems]
  - **authors:** Przemek Pospieszny, Dominika P. Brodowicz
  - **institution:** EPAM Systems, Warsaw School of Economics
  - **link:** https://arxiv.org/pdf/2512.15787
  - **Simple LLM Summary:** The paper proposes the concept of "agentic environments," a framework leveraging generative AI, multi-agent systems, and edge computing to create sustainable, human-centric spaces. It concludes that this approach can reduce the environmental impact of AI by optimizing resource use and enhancing data privacy through decentralized, edge-driven deployment models.

- **[arXiv251219] A Systematic Analysis of Biases in Large Language Models**
  - **tags:** [ai], [fairness and bias analysis], [news summarization, stance classification, UN voting patterns, multilingual story completion, World Values Survey]
  - **authors:** Xulang Zhang, Rui Mao, Erik Cambria
  - **institution:** Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.15792
  - **Simple LLM Summary:** This paper systematically analyzes biases in large language models (LLMs) across political, ideological, alliance, language, and gender dimensions using experiments like news summarization and stance classification. The main conclusion is that despite being aligned for neutrality, the studied LLMs still exhibit various types of biases and affinities.

- **[arXiv251219] Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud**
  - **tags:** [ai], [ai ethics evaluation], [Model Cards, ALTAI, FactSheets, Harms Modeling, literature survey, interviews]
  - **authors:** Jhessica Silva, Diego A. B. Moreira, Gabriel O. dos Santos, Alef Ferreira, Helena Maia, Sandra Avila, Helio Pedrini
  - **institution:** Universidade Estadual de Campinas (UNICAMP), Universidade Federal de Goiás (UFG)
  - **link:** https://arxiv.org/pdf/2512.15791
  - **Simple LLM Summary:** The paper presents a methodology to evaluate AI Ethics Tools (AIETs) for language models by selecting four tools (Model Cards, ALTAI, FactSheets, Harms Modeling) and applying them to Portuguese language models, with developer interviews. The results indicate that these tools help guide general ethical considerations but fail to address language-specific aspects like idiomatic expressions or identify negative impacts for Portuguese.

- **[arXiv251219] Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms**
  - **tags:** [ai], [ethical ai], [contrastive learning, social norms generation, moral reasoning, explainable ai, valence prediction]
  - **authors:** Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang
  - **institution:** Hong Kong Baptist University, Singapore Management University, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.15793
  - **Simple LLM Summary:** The paper introduces ClarityEthic, a method that enhances ethical assessment of human actions by generating conflicting social norms to explain and predict valence (support/oppose). It uses a contrastive learning strategy to strengthen the moral reasoning of language models. Experiments show the method outperforms baselines and human evaluations confirm the generated norms provide plausible explanations.

- **[arXiv251219] CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory**
  - **tags:** [mlsys], [others], [CodeAct, procedural memory, deterministic reliability, reusable agentic workflows, Python action space, dynamic MCP]
  - **authors:** Nishant Gaurav, Adit Akarsh, Tejas Ravishankar, Manoj Bajaj
  - **institution:** AgentR
  - **link:** https://arxiv.org/pdf/2512.15813
  - **Simple LLM Summary:** The paper proposes CodeMem, an architecture that uses code as procedural memory to build reproducible AI agents. It addresses probabilistic instability in LLM-based agents by shifting workflow logic from volatile context into deterministic, saved code blocks. The main conclusion is that this approach enables the creation of reusable agentic workflows with reliable, deterministic execution.

- **[arXiv251219] Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India**
  - **tags:** [ai], [cybersecurity], [computer forensics, explainable AI (XAI), data minimization, algorithmic bias, deepfakes, adversarial AI, Digital Personal Data Protection Act]
  - **authors:** Sahibpreet Singh, Shikha Dhiman
  - **institution:** Guru Nanak Dev University, Amritsar
  - **link:** https://arxiv.org/pdf/2512.15799
  - **Simple LLM Summary:** This paper employs a doctrinal legal methodology to analyze the integration of AI into cybercrime and forensics in India, focusing on the Digital Personal Data Protection Act, 2023. It concludes that there is a critical tension between privacy principles and forensic needs, and proposes a human-centric forensic model using explainable AI (XAI) to ensure evidence admissibility while advocating for legislative synchronization with international standards.

- **[arXiv251219] A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning**
  - **tags:** [mlsys], [others], [Hoare logic, weakest precondition reasoning, neurosymbolic AI, OpenJML, counterexample-guided repair]
  - **authors:** Daragh King, Vasileios Koutavas, Laura Kovacs
  - **institution:** Trinity College Dublin, Lero, TU Wien
  - **link:** https://arxiv.org/pdf/2512.15816
  - **Simple LLM Summary:** This paper presents NeuroInv, a neurosymbolic method for generating loop invariants that combines a neural module using LLMs and Hoare logic for backward-chaining weakest precondition reasoning with a symbolic module that iteratively repairs invariants using counterexamples from OpenJML. It achieves a 99.5% success rate on a benchmark of 150 Java programs and demonstrates scalability on complex multi-loop programs, substantially outperforming other approaches.

- **[arXiv251219] Edge-wise Topological Divergence Gaps: Guiding Search in Combinatorial Optimization**
  - **tags:** [ai], [combinatorial optimization], [topological data analysis, RTD-Lite barcode, 2-opt, 3-opt, minimum spanning tree, canonical decomposition theorem]
  - **authors:** Ilya Trofimov, Daria Voronkova, Alexander Mironenko, Anton Dmitriev, Eduard Tulchinskii, Evgeny Burnaev, Serguei Barannikov
  - **institution:** Skoltech, AIRI, CNRS
  - **link:** https://arxiv.org/pdf/2512.15800
  - **Simple LLM Summary:** This paper introduces a topological feedback mechanism for the Travelling Salesman Problem (TSP) by analyzing the divergence between a tour and the minimum spanning tree (MST). The core method is a canonical decomposition theorem that expresses the tour-MST length gap as edge-wise topological divergence gaps from the RTD-Lite barcode, which is then used to guide 2-opt and 3-opt heuristics. Experiments show that this topology-guided optimization improves performance and leads to faster convergence.

- **[arXiv251219] State-Augmented Graphs for Circular Economy Triage**
  - **tags:** [ai], [decision-making, optimization], [state-augmented graph, disassembly sequencing planning, Markov property, dynamic programming, recursive evaluation, diagnostic health scores]
  - **authors:** Richard Fox, Rui Li, Gustav Jonsson, Farzaneh Goli, Miying Yang, Emel Aktas, Yongjing Wang
  - **institution:** University of Birmingham, Cranfield University
  - **link:** https://arxiv.org/pdf/2512.15824
  - **Simple LLM Summary:** This paper introduces a decision-making framework for circular economy triage using a state-augmented Disassembly Sequencing Planning graph, which enforces the Markov property to enable optimal, recursive evaluation. The method integrates condition-aware utility and operational constraints to dynamically choose between continuing disassembly or committing to a circular economy pathway. The framework is demonstrated to be a tractable and generalizable foundation for optimizing triage decisions across diverse products, as illustrated with the example of electric vehicle battery disassembly.

- **[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls**
  - **tags:** [mlsys], [llm inference], [speculative tool calls, tool cache, vLLM, prefix-caching]
  - **authors:** Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon
  - **institution:** Lawrence Livermore National Laboratory, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.15834
  - **Simple LLM Summary:** This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new "tool cache" API to facilitate adoption of these optimizations.

- **[arXiv251219] VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces**
  - **tags:** [mlsys], [others], [verifiable execution traces, agent identity document, web proofs, tee proxy, host-independent authentication]
  - **authors:** Artem Grigor, Christian Schroeder de Witt, Simon Birnbach, Ivan Martinovic
  - **institution:** University of Oxford
  - **link:** https://arxiv.org/pdf/2512.15892
  - **Simple LLM Summary:** The paper introduces VET, a framework for achieving host-independent authentication of autonomous agent outputs using verifiable execution traces and an Agent Identity Document. It demonstrates that practical authentication is possible by composing proof mechanisms like Web Proofs and TEE Proxies, with a case study showing overhead typically under 3x for API calls.

- **[arXiv251219] Human-like Working Memory from Artificial Intrinsic Plasticity Neurons**
  - **tags:** [mlsys], [others], [neuromorphic computing, intrinsic plasticity, Magnetic Tunnel Junctions (MTJs), hardware-software co-design, near-sensor processing, working memory]
  - **authors:** Jingli Liu, Huannan Zheng, Bohao Zou, Kezhou Yang
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.15829
  - **Simple LLM Summary:** This paper introduces IPNet, a neuromorphic architecture that uses Magnetic Tunnel Junction (MTJ) neurons with intrinsic plasticity to physically emulate human-like working memory. The hardware-software co-designed system achieves high accuracy on dynamic vision tasks and significantly reduces energy consumption and footprint compared to traditional models like LSTMs. The work demonstrates that bio-inspired intrinsic plasticity can provide superior processing efficiency and performance for real-time applications.

- **[arXiv251219] PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations**
  - **tags:** [ai], [llm safety evaluation], [PediatricAnxietyBench, adversarial queries, safety framework, diagnostic restraint, referral adherence, hedging, emergency recognition]
  - **authors:** Vahideh Zolfaghari
  - **institution:** Mashhad University of Medical Sciences
  - **link:** https://arxiv.org/pdf/2512.15894
  - **Simple LLM Summary:** This paper introduces PediatricAnxietyBench, a benchmark of pediatric queries including adversarial ones simulating parental pressure, to evaluate LLM safety. It finds that while larger models perform better, all are vulnerable to such pressure, with hedging language correlating with safety and emergency recognition being absent.

- **[arXiv251219] Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models**
  - **tags:** [mlsys], [multi-modal training], [self-supervised learning, vision-language alignment, I-JEPA, JARVIS, masked predictive loss, frozen vision foundation models]
  - **authors:** Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara
  - **institution:** University of Modena and Reggio Emilia, AMD Silo AI
  - **link:** https://arxiv.org/pdf/2512.15885
  - **Simple LLM Summary:** The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities.

- **[arXiv251219] Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries**
  - **tags:** [mlsys], [llm inference], [knowledge graph, SQL database, prompt engineering, open-source, browser-based GUI]
  - **authors:** Jonathan A. Handler
  - **institution:** Keylog Solutions LLC, OSF HealthCare
  - **link:** https://arxiv.org/pdf/2512.15906
  - **Simple LLM Summary:** Darth Vecdor is an open-source system designed to extract structured knowledge from large language models (LLMs) into queryable SQL-based knowledge graphs. It addresses issues like inconsistent LLM responses through features including a browser-based GUI for user-friendly prompt engineering. The author concludes that such pre-extracted knowledge graphs can mitigate concerns of cost, speed, and safety compared to direct LLM queries, potentially improving domains like healthcare.

- **[arXiv251219] Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation (RAG), knowledge graphs, spreading activation algorithm, multi-hop question answering, GraphRAG, chain-of-thought iterative retrieval]
  - **authors:** Jovan Pavlović, Miklós Krész, László Hajdu
  - **institution:** Innorenew CoE, University of Primorska, FAMNIT, Cognee Inc.
  - **link:** https://arxiv.org/pdf/2512.15922
  - **Simple LLM Summary:** This paper proposes a novel RAG framework that uses the spreading activation algorithm to retrieve information from documents interconnected by automatically built knowledge graphs. The method enhances LLM performance on complex tasks like multi-hop QA and is designed as a plug-and-play module. Experiments show it achieves comparable or better performance than iterative RAG methods, with up to a 39% gain in answer correctness when combined with chain-of-thought retrieval, even when using small language models.

- **[arXiv251219] Social Story Frames: Contextual Reasoning about Narrative Intent and Reception**
  - **tags:** [ai], [natural language processing], [SocialStoryFrames, SSF-Generator, SSF-Classifier, narrative theory, linguistic pragmatics, taxonomy, reader response]
  - **authors:** Joel Mire, Maria Antoniak, Steven R. Wilson, Zexin Ma, Achyutarama R. Ganti, Andrew Piper, Maarten Sap
  - **institution:** Carnegie Mellon University, University of Colorado Boulder, University of Michigan-Flint, University of Connecticut, McGill University
  - **link:** https://arxiv.org/pdf/2512.15925
  - **Simple LLM Summary:** This paper introduces SocialStoryFrames, a formalism and computational framework for modeling nuanced reader responses to social media stories, including inferences about author intent and affective reactions. It develops two models (SSF-Generator and SSF-Classifier) and applies them to a corpus of online narratives to analyze storytelling practices across communities. The main conclusion is that this approach enables scalable, context-sensitive research into the social dynamics of online storytelling.

- **[arXiv251219] Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning**
  - **tags:** [mlsys], [post-training], [supervised fine-tuning, tool calling, small language models, OPT-350M, TRL, ToolBench]
  - **authors:** Polaris Jhandi, Owais Kazi, Shreyas Subramanian, Neel Sendas
  - **institution:** Amazon Web Services
  - **link:** https://arxiv.org/pdf/2512.15943
  - **Simple LLM Summary:** This paper fine-tunes a small language model (OPT-350M) using supervised fine-tuning to perform agentic tool-calling tasks. The results show that this targeted fine-tuning enables the small model to significantly outperform larger baseline models like ChatGPT on the ToolBench evaluation, demonstrating that SLMs can be a cost-effective alternative for specific production workflows.

- **[arXiv251219] SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks**
  - **tags:** [ai], [mechanistic interpretability], [sparse autoencoder, ℓ1-regularization, Grad-FAM, weight-space editing, critical suppression threshold]
  - **authors:** Vegard Flovik
  - **institution:** DNV
  - **link:** https://arxiv.org/pdf/2512.15938
  - **Simple LLM Summary:** The paper introduces SALVE, a framework that uses an ℓ1-regularized sparse autoencoder to discover and validate latent features in neural networks, then performs precise weight-space edits to control model behavior. It demonstrates consistent control across ResNet-18 and ViT models, providing a methodology to turn feature discovery into actionable model edits for more transparent AI systems.

- **[arXiv251219] Subjective functions**
  - **tags:** [ai], [cognitive science], [subjective function, objective function, expected prediction error, endogenous, higher-order optimization]
  - **authors:** Samuel J. Gershman
  - **institution:** Harvard University
  - **link:** https://arxiv.org/pdf/2512.15948
  - **Simple LLM Summary:** The paper proposes the concept of a "subjective function," an endogenous, higher-order objective function defined with respect to an agent's own features. It explores expected prediction error as a concrete example of such a function. The main conclusion is that this framework for synthesizing goals from within the agent may be key to understanding and replicating human-like intelligence in artificial systems.

- **[arXiv251219] BRAID: Bounded Reasoning for Autonomous Inference and Decisions**
  - **tags:** [mlsys], [llm inference], [bounded reasoning, structured prompting, instruction graphs, mermaid, chain-of-thought]
  - **authors:** Armağan Amcalar, Eyup Cinar
  - **institution:** OpenServ Labs, Eskisehir Osmangazi University
  - **link:** https://arxiv.org/pdf/2512.15959
  - **Simple LLM Summary:** This paper introduces BRAID, a bounded reasoning framework that uses Mermaid-based instruction graphs to structure prompts for LLMs, enabling them to reason structurally instead of through unbounded natural language. The method is shown to substantially increase reasoning accuracy and cost efficiency across multiple GPT model tiers on several benchmark datasets.

- **[arXiv251219] Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models**
  - **tags:** [mlsys], [multi-modal training], [Vision Language Model, Scene Graph, Supervised Fine-Tuning, Direct Preference Optimization, synthetic data]
  - **authors:** Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello
  - **institution:** University of Stuttgart, Bosch Research
  - **link:** https://arxiv.org/pdf/2512.15957
  - **Simple LLM Summary:** This paper introduces CAMP-VLM, a framework that uses a Vision Language Model enhanced with scene graphs and contextual features to predict multi-human behaviors from a third-person view. It is fine-tuned with synthetic data using SFT and DPO, and the results show it outperforms the best baseline by up to 66.9% in prediction accuracy.

- **[arXiv251219] OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering**
  - **tags:** [ai], [empirical software engineering], [annotation framework, reliability, calibration, drift, consensus, aggregation, transparency]
  - **authors:** Mia Mohammad Imran, Tarannum Shaila Zaman
  - **institution:** Missouri University of Science and Technology, University of Maryland Baltimore County
  - **link:** https://arxiv.org/pdf/2512.15979
  - **Simple LLM Summary:** This position paper proposes OLAF, a conceptual framework for treating LLM-based annotation as a measurement process in empirical software engineering. It organizes key constructs like reliability, calibration, and drift to address current methodological gaps. The paper concludes that such a framework is necessary to improve the transparency and reproducibility of LLM-assisted annotation in software engineering research.

- **[arXiv251219] Provably Extracting the Features from a General Superposition**
  - **tags:** [ai], [feature learning], [superposition, query algorithm, Fourier space search, overcomplete regime]
  - **authors:** Allen Liu
  - **institution:** UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.15987
  - **Simple LLM Summary:** The paper presents an efficient query algorithm that searches in Fourier space to recover hidden feature directions from a general superposition function. It successfully identifies all non-degenerate features and reconstructs the function, working under more general conditions than prior methods.

- **[arXiv251219] Embedding Software Intent: Lightweight Java Module Recovery**
  - **tags:** [sys], [software architecture recovery], [ClassLAR, JPMS, language models, fully-qualified class names, reverse engineering]
  - **authors:** Yirui He, Yuqi Huai, Xingyu Chen, Joshua Garcia
  - **institution:** University of California, Irvine
  - **link:** https://arxiv.org/pdf/2512.15980
  - **Simple LLM Summary:** This paper introduces ClassLAR, a lightweight approach for recovering Java modules from monolithic systems by using language models to extract semantic information from fully-qualified class names. The method captures structural and functional intent to map code to architectural modules. The evaluation shows ClassLAR outperforms state-of-the-art techniques in accuracy and is significantly faster in execution time.

- **[arXiv251219] Surrogate Neural Architecture Codesign Package (SNAC-Pack)**
  - **tags:** [mlsys], [others], [neural architecture search, FPGA deployment, hardware-aware optimization, surrogate modeling, multi-objective optimization]
  - **authors:** Jason Weitz, Dmitri Demler, Benjamin Hawks, Nhan Tran, Javier Duarte
  - **institution:** University of California San Diego, Fermi National Accelerator Laboratory
  - **link:** https://arxiv.org/pdf/2512.15998
  - **Simple LLM Summary:** The paper introduces SNAC-Pack, a framework that automates neural network design for FPGA deployment by combining architecture search with a resource and latency estimator, avoiding time-intensive synthesis for each candidate. It demonstrates the framework on a physics classification task, achieving competitive accuracy and resource efficiency on an FPGA, showing the potential of hardware-aware NAS for constrained environments.

- **[arXiv251219] Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios**
  - **tags:** [mlsys], [llm inference], [few-shot learning, in-context learning, ablation study, personalized examples]
  - **authors:** Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Hao-Tien Lewis Chiang, Marynel Vázquez
  - **institution:** Yale University, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.16019
  - **Simple LLM Summary:** This paper proposes using Large Language Models (LLMs) with few-shot in-context learning to predict human perceptions of robot performance in social navigation tasks. The method requires significantly less labeled data than traditional supervised learning models and achieves comparable or better performance. The study also finds that using personalized examples from the same user further improves prediction accuracy.

- **[arXiv251219] Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results**
  - **tags:** [mlsys], [others], [knowledge-guided machine learning, fine-tuning, transfer learning, site calibration, spatial heterogeneity, pretraining, agroecosystem carbon cycle]
  - **authors:** Ruolei Zeng, Arun Sharma, Shuai An, Mingzhou Yang, Shengya Zhang, Licheng Liu, David Mulla, Shashi Shekhar
  - **institution:** University of Minnesota, Twin Cities
  - **link:** https://arxiv.org/pdf/2512.16013
  - **Simple LLM Summary:** The paper proposes FTBSC-KGML, a framework that enhances knowledge-guided machine learning by incorporating a pretraining and fine-tuning process with site-specific parameters to better capture spatial variability. It uses a globally pretrained model that is fine-tuned per site or state to improve local accuracy for land emissions estimation. The method achieves lower validation error and more consistent explanatory power than a purely global model.

- **[arXiv251219] Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting**
  - **tags:** [ai], [time series forecasting], [ensemble learning, SHAP, R1-style finetuning, LLM reasoning, conversational agent, foundation models]
  - **authors:** Defu Cao, Michael Gee, Jinbo Liu, Hengxuan Wang, Wei Yang, Rui Wang, Yan Liu
  - **institution:** University of Southern California, Amazon AWS
  - **link:** https://arxiv.org/pdf/2512.16022
  - **Simple LLM Summary:** This paper introduces a method that uses a Large Language Model (LLM) as an intelligent judge to coordinate and explain an ensemble of time series foundation models. The LLM is finetuned using an R1-style process guided by SHAP scores to interpret ensemble weights causally. The approach achieves state-of-the-art forecasting performance on benchmark datasets, demonstrating that conversational LLM agents can effectively orchestrate ensembles for explainable and accurate predictions.

- **[arXiv251219] Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets**
  - **tags:** [ai], [large language models, epistemic calibration], [KalshiBench, prediction markets, Expected Calibration Error (ECE), Brier Skill Score, overconfidence]
  - **authors:** Lukas Nel
  - **institution:** Lotus AI
  - **link:** https://arxiv.org/pdf/2512.16030
  - **Simple LLM Summary:** This paper introduces KalshiBench, a new benchmark using prediction market questions with future outcomes to evaluate the epistemic calibration of large language models (LLMs). The study finds that five frontier LLMs, including Claude Opus and GPT-5.2, show systematic overconfidence, and that enhanced reasoning does not improve calibration, indicating it is a distinct capability requiring specific development.

- **[arXiv251219] Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education**
  - **tags:** [mlsys], [others], [topic modeling, large language models, GPT-4.0, classification, coherence score]
  - **authors:** Diane Myung-kyung Woodbridge, Allyson Seba, Freddie Seba, Aydin Schwartz
  - **institution:** StudyStudio.ai, University of Southern California
  - **link:** https://arxiv.org/pdf/2512.16036
  - **Simple LLM Summary:** The paper presents an automated system that uses unsupervised topic modeling and large language models (GPT-4.0) to discover and classify generative AI policies from course syllabi and institutional websites. The system achieved a high coherence score for topic discovery and high precision and recall for classification across eight policy topics. This tool aims to help students understand and comply with guidelines, promoting the responsible and equitable use of GenAI in higher education.

- **[arXiv251219] Cross-Language Bias Examination in Large Language Models**
  - **tags:** [ai], [fairness and bias evaluation], [multilingual bias evaluation, BBQ benchmark, prompt-based Implicit Association Test, explicit bias, implicit bias]
  - **authors:** Yuxuan Liang, Marwa Mahmoud
  - **institution:** Georgia Institute of Technology, University of Glasgow
  - **link:** https://arxiv.org/pdf/2512.16029
  - **Simple LLM Summary:** This paper introduces a multilingual bias evaluation framework that combines explicit bias assessment using the BBQ benchmark with implicit bias measurement via a prompt-based Implicit Association Test, applied across five languages. The results show significant variation in bias across languages, with Arabic and Spanish exhibiting higher stereotype bias, and reveal contrasting patterns between explicit and implicit bias, such as age having low explicit but high implicit bias. The study highlights the importance of cross-lingual bias analysis for developing equitable multilingual LLMs.

- **[arXiv251219] Are We on the Right Way to Assessing LLM-as-a-Judge?**
  - **tags:** [mlsys], [post-training], [LLM-as-a-Judge, Sage evaluation suite, local self-consistency, global logical consistency, situational preference, panel-based judge, deep reasoning, finetuned LLM-as-a-Judge]
  - **authors:** Yuanning Feng, Sinan Wang, Zhengxiang Cheng, Yao Wan, Dongping Chen
  - **institution:** Huazhong University of Science and Technology, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.16041
  - **Simple LLM Summary:** The paper introduces Sage, a novel evaluation suite that assesses LLM-as-a-Judge without human annotation by measuring local self-consistency and global logical consistency. It finds that even top LLMs exhibit significant reliability problems, attributing this to situational preference, and shows that finetuning, panel-based judging, and deep reasoning can improve consistency. The work also questions the reliability of human annotation as a gold standard.

- **[arXiv251219] CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting**
  - **tags:** [ai], [causal representation learning], [causal spatio-temporal learning, causal discovery, directed acyclic graph, streamflow forecasting, hydrological modeling]
  - **authors:** Shu Wan, Reepal Shah, John Sabo, Huan Liu, K. Selçuk Candan
  - **institution:** Arizona State University, Tulane University
  - **link:** https://arxiv.org/pdf/2512.16046
  - **Simple LLM Summary:** The paper proposes CauSTream, a framework that jointly learns a causal graph for meteorological forcings and a spatio-temporal routing graph for streamflow forecasting. It establishes identifiability conditions for these structures and demonstrates that the model outperforms state-of-the-art methods, especially for longer forecast horizons, while providing interpretable causal graphs aligned with domain knowledge.

- **[arXiv251219] A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis**
  - **tags:** [mlsys], [llm inference], [multi-agent framework, large language model, qualitative thematic analysis, Collaborative Theme Identification Agent (CoTI)]
  - **authors:** Qidi Xu, Nuzha Amjad, Grace Giles, Alexa Cumming, De'angelo Hermesky, Alexander Wen, Min Ji Kwak, Yejin Kim
  - **institution:** UTHealth Houston, University of Texas Health Sciences Center Houston
  - **link:** https://arxiv.org/pdf/2512.16063
  - **Simple LLM Summary:** The paper introduces CoTI, a multi-agent LLM framework with three specialized agents to automate qualitative thematic analysis of patient interviews. The framework was applied to heart failure patient data and produced results more aligned with a senior investigator's analysis than with junior investigators or baseline NLP models. However, collaboration between the AI and junior investigators showed limited gains, suggesting a risk of over-reliance that may hinder independent critical thinking.

- **[arXiv251219] Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil**
  - **tags:** [mlsys], [others], [radio frequency sensing, wireless system, signal processing, machine learning, classification]
  - **authors:** Yixuan Gao, Tanvir Ahmed, Mikhail Mohammed, Zhongqi Cheng, Rajalakshmi Nandakumar
  - **institution:** Cornell Tech, Brooklyn College of the City University of New York
  - **link:** https://arxiv.org/pdf/2512.16071
  - **Simple LLM Summary:** This paper proposes SoilScanner, a wireless sensing system that uses radio frequency signals to detect lead contamination in soil, based on how different salts affect signal propagation. It employs a machine learning model to classify soil samples into low or high lead categories with 72% accuracy. The study concludes that it is feasible to build portable and affordable lead detection devices using this wireless technology.

- **[arXiv251219] Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers**
  - **tags:** [mlsys], [llm inference], [schema filtering, functional dependency graph, graph transformer, Steiner-tree heuristic, query-aware LLM encoder]
  - **authors:** Thanh Dat Hoang, Thanh Tam Nguyen, Thanh Trung Huynh, Hongzhi Yin, Quoc Viet Hung Nguyen
  - **institution:** Griffith University, VinUniversity, The University of Queensland
  - **link:** https://arxiv.org/pdf/2512.16083
  - **Simple LLM Summary:** This paper introduces GRAST-SQL, a framework for scaling Text2SQL systems by efficiently filtering and compacting database schemas before prompting an LLM. It uses a query-aware LLM encoder, a graph transformer over functional dependencies, and a Steiner-tree heuristic to select a relevant, connectivity-preserving sub-schema. The method achieves high recall and precision while maintaining low latency and scaling to schemas with over 23,000 columns.

- **[arXiv251219] Evaluation of Generative Models for Emotional 3D Animation Generation in VR**
  - **tags:** [mlsys], [multi-modal inference], [generative models, speech-driven 3D animation, virtual reality (VR), user study, emotional arousal, reconstruction-based method, UV mapping, OpenXR, Blender]
  - **authors:** Kiran Chhatre, Renan Guarese, Andrii Matviienko, Christopher Peters
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.16081
  - **Simple LLM Summary:** This paper evaluates generative models for creating emotional 3D animations synchronized with speech in a VR environment using a user study. The main conclusion is that models explicitly modeling emotions achieve higher recognition accuracy than those focusing only on speech synchrony, but current models struggle with subtle emotions and underperform compared to reconstruction-based methods in facial expression quality.

- **[arXiv251219] LAPX: Lightweight Hourglass Network with Global Context**
  - **tags:** [mlsys], [others], [hourglass network, self-attention, lightweight attention, global context, human pose estimation, edge device]
  - **authors:** Haopeng Zhao, Marsha Mariya Kappan, Mahdi Bamdad, Francisco Cruz
  - **institution:** University of New South Wales, Universidad Central de Chile
  - **link:** https://arxiv.org/pdf/2512.16089
  - **Simple LLM Summary:** The paper proposes LAPX, a lightweight Hourglass network enhanced with self-attention to capture global context for efficient human pose estimation. It refines stage design and attention modules to balance accuracy and speed, achieving competitive results on MPII and COCO datasets with only 2.3M parameters. The model demonstrates real-time performance suitable for deployment on edge devices.

- **[arXiv251219] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times**
  - **tags:** [mlsys], [diffusion inference], [SageAttention, Sparse-Linear Attention (SLA), rCM, W8A8 quantization, step distillation]
  - **authors:** Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu
  - **institution:** Tsinghua University, Shengshu Technology, UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.16093
  - **Simple LLM Summary:** TurboDiffusion is a framework that accelerates video diffusion models by 100-200 times using attention acceleration (low-bit SageAttention and Sparse-Linear Attention), step distillation (rCM), and W8A8 quantization. Experiments on several models show it achieves this speedup on a single GPU while maintaining comparable video quality.

- **[arXiv251219] ModelTables: A Corpus of Tables about Models**
  - **tags:** [mlsys], [others], [table retrieval, model lakes, data lakes, unionable tables, joinable tables, dense retrieval, sparse retrieval, hybrid retrieval, benchmark construction]
  - **authors:** Zhengyuan Dong, Victor Zhong, Renée J. Miller
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.16106
  - **Simple LLM Summary:** This paper introduces ModelTables, a benchmark corpus of structured tables describing AI models, built from sources like Hugging Face model cards and GitHub READMEs. It evaluates table search methods, finding that table-based dense retrieval performs best but leaves significant room for improvement. The work provides a foundation for better semantic retrieval and organization of structured model knowledge.

- **[arXiv251219] WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning**
  - **tags:** [mlsys], [post-training], [knowledge internalization, agentic boundary learning, continued pretraining, conversational recommendation, tool invocation]
  - **authors:** Wendong Bi, Yirong Mao, Xianglong Liu, Kai Tian, Jian Zhang, Hanjie Wang, Wenhui Que
  - **institution:** Tencent Inc, Peking University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.16108
  - **Simple LLM Summary:** This paper proposes WeMusic-Agent, a training framework that combines knowledge internalization via continued pretraining on a large music corpus with agentic boundary learning to enable an LLM to intelligently decide between using internal knowledge or calling external tools for conversational music recommendation. The method shows significant improvements over existing models in experiments on real-world data.

- **[arXiv251219] AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation**
  - **tags:** [mlsys], [multi-modal inference], [AI-driven framework, multimodal fusion, synthetic social features, forward-walk evaluation, Streamlit dashboard, parquet-native pipeline, OHLCV data, Reddit activity, coordination metrics, bot indicators]
  - **authors:** Sandeep Neela
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.16103
  - **Simple LLM Summary:** This paper introduces AIMM, an AI-driven multimodal framework that fuses Reddit activity, bot/coordination indicators, and OHLCV market data to generate a daily manipulation risk score for stocks. The system uses a parquet-native pipeline with a Streamlit dashboard for analysis and employs synthetic social features due to API restrictions. The preliminary results show the framework can provide early warnings, such as flagging GME 22 days before its peak in January 2021.

- **[arXiv251219] Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection**
  - **tags:** [mlsys], [others], [autoencoder, denoising, adversarial attack, Perlin noise, YOLOv5, object detection]
  - **authors:** Min Geun Song, Gang Min Kim, Woonmin Kim, Yongsik Kim, Jeonghyun Sim, Sangbeom Park, Huy Kang Kim
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.16123
  - **Simple LLM Summary:** This paper proposes an autoencoder-based denoising defense to mitigate adversarial attacks on object detection models. The method uses a single-layer convolutional autoencoder to remove Perlin noise perturbations from images before feeding them to a YOLOv5 detector. The results show that this approach provides a partial recovery of detection performance without requiring model retraining.

- **[arXiv251219] MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, group relative policy optimization, margin-based cosine similarity, semantic-driven reinforcement learning, large vision-language model]
  - **authors:** Pengyu Wang, Shuchang Ye, Usman Naseem, Jinman Kim
  - **institution:** The University of Sydney, Macquarie University
  - **link:** https://arxiv.org/pdf/2512.16145
  - **Simple LLM Summary:** The paper proposes a semantic-driven reinforcement learning method (MRG-R1) for medical report generation, which uses a report-level reward based on clinical findings similarity and Group Relative Policy Optimization to improve clinical correctness over token-level objectives. It achieves state-of-the-art performance on benchmark datasets, demonstrating that optimizing for semantic alignment meaningfully enhances clinical accuracy in generated reports.

- **[arXiv251219] Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning**
  - **tags:** [ai], [natural language processing], [RoBERTa, multi-task learning, transformer models, code-mixed text, hate speech detection, fake narratives]
  - **authors:** Yash Bhaskar, Sankalp Bahad, Parameswari Krishnamurthy
  - **institution:** IIIT Hyderabad
  - **link:** https://arxiv.org/pdf/2512.16147
  - **Simple LLM Summary:** The paper proposes a dual-head RoBERTa model with multi-task learning to detect hate speech driven by fake narratives (Faux-Hate) in code-mixed Hindi-English text. It addresses binary classification and target/severity prediction tasks. The system achieved competitive results, demonstrating the effectiveness of multi-task learning for this complex problem.

- **[arXiv251219] INTELLECT-3: Technical Report**
  - **tags:** [mlsys], [llm training], [mixture-of-experts, reinforcement learning, large-scale training, prime-rl, verifiers library, environments hub, SFT, RL training]
  - **authors:** Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian Müller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann
  - **institution:** Prime Intellect, Inc.
  - **link:** https://arxiv.org/pdf/2512.16144
  - **Simple LLM Summary:** The paper introduces INTELLECT-3, a 106B-parameter Mixture-of-Experts model trained using large-scale reinforcement learning on a custom end-to-end RL infrastructure stack. It achieves state-of-the-art performance for its size across multiple benchmarks by leveraging the open-source prime-rl framework and scaling training to 512 H200 GPUs. The authors open-source the model and the full training infrastructure, including the RL frameworks and environments.

- **[arXiv251219] Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services**
  - **tags:** [mlsys], [others], [evolutionary game theory, replicator dynamics, trust mechanism, multi-agent systems, strategy equilibrium]
  - **authors:** Shiduo Yang, Jiye Wang, Jiayu Qin, Jianbin Li, Yu Wang, Yuanhe Zhao, Kenan Guo
  - **institution:** State Grid Corporation of China, North China Electric Power University
  - **link:** https://arxiv.org/pdf/2512.16167
  - **Simple LLM Summary:** The paper proposes Ev-Trust, a trust mechanism based on evolutionary game theory that integrates direct trust, indirect trust, and expected revenue to guide agent behavior in LLM-based multi-agent systems. Theoretical analysis proves the stability of evolutionary equilibria, and experiments show the approach reduces malicious strategies and increases collective revenue in open service interactions.

- **[arXiv251219] C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation**
  - **tags:** [ai], [unsupervised domain adaptation], [prompt tuning, vision-language models, adversarial training, class mapping mechanism, marginal distribution alignment, conditional distribution alignment]
  - **authors:** Chao Li, Dasha Hu, Chengyang Li, Yuming Jiang, Yuncheng Shen
  - **institution:** Sichuan University, Zhaotong University
  - **link:** https://arxiv.org/pdf/2512.16164
  - **Simple LLM Summary:** This paper proposes C-DGPA, a class-centric dual-alignment method for generative prompt adaptation in unsupervised domain adaptation. It uses a dual-branch architecture to synergistically align both marginal and conditional distributions between domains via adversarial training and a class mapping mechanism. Experiments on standard benchmarks show it achieves state-of-the-art results.

- **[arXiv251219] ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs**
  - **tags:** [mlsys], [llm training], [data synthesis, tool-calling, multi-hop reasoning, self-reflection, virtual tools, multi-layer validation]
  - **authors:** Hao Chen, Zhexin Hu, Jiajun Chai, Haocheng Yang, Hang He, Xiaohan Wang, Wei Lin, Luhang Wang, Guojun Yin, Zhuofeng zhao
  - **institution:** North China University of Technology, Meituan, Institute of Software Chinese Academy of Sciences, National University of Singapore, East China Normal University
  - **link:** https://arxiv.org/pdf/2512.16149
  - **Simple LLM Summary:** ToolForge is a framework that synthesizes large-scale tool-learning data for multi-hop search using virtual tools instead of real API calls, enriched with multi-hop reasoning and self-reflection. It employs a multi-layer validation framework to ensure data fidelity. An 8B-parameter model trained on this data outperforms GPT-4o on multiple benchmarks.

- **[arXiv251219] Science Consultant Agent**
  - **tags:** [mlsys], [others], [Retrieval-Augmented Generation (RAG), fine-tuning, knowledge distillation, prompting, AutoML]
  - **authors:** Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves
  - **institution:** Duke University, Amazon
  - **link:** https://arxiv.org/pdf/2512.16171
  - **Simple LLM Summary:** The paper introduces the Science Consultant Agent, a web-based AI tool that uses structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting optimal AI modeling strategies. It aims to prevent resource misallocation by providing evidence-based guidance, moving beyond brute-force exploration or example-induced bias to accelerate development.

- **[arXiv251219] Towards Closing the Domain Gap with Event Cameras**
  - **tags:** [mlsys], [multi-modal inference], [event cameras, domain gap, end-to-end driving, neural networks, illumination invariance]
  - **authors:** M. Oltan Sevinc, Liao Wu, Francisco Cruz
  - **institution:** University of New South Wales, Universidad Central de Chile
  - **link:** https://arxiv.org/pdf/2512.16178
  - **Simple LLM Summary:** This paper proposes using event cameras instead of traditional frame-based cameras for end-to-end autonomous driving to address the domain gap caused by day-night lighting differences. The method involves training neural networks on event camera data and evaluating their performance across different lighting conditions. The main conclusion is that event cameras maintain more consistent performance across lighting domains, showing smaller performance degradation and superior baseline performance in cross-domain scenarios compared to grayscale cameras.

- **[arXiv251219] Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications**
  - **tags:** [ai], [clustering algorithms], [weighted K-harmonic means, convergence analysis, stochastic initialization, binomial point process, wireless user association]
  - **authors:** Gourab Ghatak
  - **institution:** IIT Delhi
  - **link:** https://arxiv.org/pdf/2512.16185
  - **Simple LLM Summary:** The paper proposes the Weighted K-Harmonic Means (WKHM) clustering algorithm, a regularized variant that ensures numerical stability and enables soft assignments. It provides rigorous convergence guarantees under deterministic and stochastic settings, including convergence in probability and almost sure convergence. The method is shown to achieve a superior tradeoff between minimum signal strength and load fairness in wireless networks compared to existing clustering baselines.

- **[arXiv251219] Open Ad-hoc Categorization with Contextualized Feature Learning**
  - **tags:** [ai], [computer vision], [CLIP, learnable context tokens, visual clustering, image-text alignment, saliency maps]
  - **authors:** Zilin Wang, Sangwoo Mo, Stella X. Yu, Sima Behpour, Liu Ren
  - **institution:** University of Michigan, UC Berkeley, Bosch Center for AI
  - **link:** https://arxiv.org/pdf/2512.16202
  - **Simple LLM Summary:** The paper proposes OAK, a model for open ad-hoc categorization that introduces learnable context tokens into a frozen CLIP model and optimizes it with both image-text alignment and visual clustering objectives. It achieves state-of-the-art accuracy and concept discovery on benchmark datasets, producing interpretable saliency maps that highlight context-relevant image regions.

- **[arXiv251219] PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving**
  - **tags:** [mlsys], [others], [multi-agent collaboration, toolchain-augmented, Prog-Act framework, graph memory, dual-loop mechanisms, Resource-Pool, tool-parameter separation, PDE-Bench]
  - **authors:** Jianming Liu, Ren Zhu, Jian Xu, Kun Ding, Xu-Yao Zhang, Gaofeng Meng, Cheng-Lin Liu
  - **institution:** Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.16214
  - **Simple LLM Summary:** The paper introduces PDE-Agent, a multi-agent framework that uses LLMs to automate PDE solving by dynamically invoking and coordinating external tools. Its key innovations include a Prog-Act framework with graph memory for planning and error correction, and a Resource-Pool for managing tool dependencies. Evaluations on the proposed PDE-Bench show it performs well on complex, multi-step tasks, advancing automated scientific computing.

- **[arXiv251219] AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection**
  - **tags:** [mlsys], [multi-modal inference], [interpretable convolutional neural networks, clinical decision trees, explainable AI, deep learning-based image analysis]
  - **authors:** Satya Narayana Panda, Vaishnavi Kukkala, Spandana Iyer
  - **institution:** University of New Haven
  - **link:** https://arxiv.org/pdf/2512.16235
  - **Simple LLM Summary:** The paper develops a multi-modal AI framework that integrates deep learning-based image analysis with structured clinical data, including family history, using interpretable CNNs and clinical decision trees. It concludes that incorporating family history data enhances diagnostic accuracy for hereditary skin conditions and that the framework shows potential for improved early detection and personalized care within clinical workflows.

- **[arXiv251219] Neural emulation of gravity-driven geohazard runout**
  - **tags:** [mlsys], [others], [neural emulation, machine learning, digital elevation model, numerical simulations, runout prediction]
  - **authors:** Lorenzo Nava, Ye Chen, Maximillian Van Wyk de Vries
  - **institution:** University of Cambridge, Tongji University
  - **link:** https://arxiv.org/pdf/2512.16221
  - **Simple LLM Summary:** This paper trains a machine learning model to emulate numerical simulations for predicting geohazard runout, such as landslides and avalanches, across real-world terrains. The model, trained on over 100,000 simulations, predicts flow extent and deposit thickness with high accuracy and is 100 to 10,000 times faster than traditional solvers. It demonstrates that neural emulation enables rapid and physically realistic runout prediction, offering new opportunities for disaster risk reduction and early warning systems.

- **[arXiv251219] An Information-Theoretic Framework for Robust Large Language Model Editing**
  - **tags:** [mlsys], [post-training], [information bottleneck theory, gradient-based updates, knowledge editing, locate-then-edit, transformer patching]
  - **authors:** Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He
  - **institution:** East China Normal University, Alibaba Group, Hefei University of Technology
  - **link:** https://arxiv.org/pdf/2512.16227
  - **Simple LLM Summary:** This paper introduces a novel model editing framework called the Information Bottleneck Knowledge Editor (IBKE), which uses information bottleneck theory to compress and isolate essential information for making precise, generalizable knowledge corrections in LLMs. The method leverages compact latent representations to guide gradient-based updates, minimizing disruption to unrelated model behaviors. The authors demonstrate that IBKE achieves state-of-the-art accuracy and improved generality and specificity of edits across multiple LLM architectures and benchmark tasks.

- **[arXiv251219] The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models**
  - **tags:** [ai], [information retrieval], [reranking, cross-encoders, T5, Graph Neural Networks, knowledge distillation, Large Language Models]
  - **authors:** Tejul Pandit, Sakshi Mahendru, Meet Raval, Dhvani Upadhyay
  - **institution:** Palo Alto Networks, University of Southern California, Dhirubhai Ambani University
  - **link:** https://arxiv.org/pdf/2512.16236
  - **Simple LLM Summary:** This paper provides a comprehensive survey of reranking models in information retrieval, tracing their evolution from heuristic methods to modern neural architectures and Large Language Models. It analyzes various techniques including cross-encoders, T5, and GNNs, and discusses efficiency methods like knowledge distillation. The survey concludes by synthesizing the principles, effectiveness, and trade-offs of different reranking strategies, highlighting their critical role in improving retrieval relevance, especially within RAG pipelines.

- **[arXiv251219] Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis**
  - **tags:** [mlsys], [multi-modal training], [programmatic data synthesis, code-generation, simulator, spatial reasoning, vision-language models, instruction-tuning]
  - **authors:** Zhi Helu, Huang Jingjing, Xu Wang, Xu Yangbin, Zhang Wanyue, Jiang Baoyang, Deng Shirui, Zhu Liang, Li Fangfang, Zhao Tiejun, Lin Yankai, Yao Yuan
  - **institution:** Harbin Institute of Technology, Tsinghua University, Institute of Microelectronics of the Chinese Academy of Sciences, Renmin University of China, Institute of Automation, Chinese Academy of Sciences, Central South University
  - **link:** https://arxiv.org/pdf/2512.16237
  - **Simple LLM Summary:** This paper introduces SPRITE, a framework that programmatically synthesizes spatial reasoning data by reframing ground-truth generation as a code-generation task, using LLMs to compile questions into executable programs verified against simulator scene information. The resulting large-scale, diverse, and precise dataset enables Vision-Language Models to achieve significant performance gains on spatial benchmarks, demonstrating that overcoming the low diversity of template-based methods is crucial for robust spatial intelligence.

- **[arXiv251219] Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models**
  - **tags:** [ai], [graph neural networks], [open-set classification, out-of-distribution detection, coarse-to-fine classification, large language models, graph neural networks, semantic OOD]
  - **authors:** Xueqi Ma, Xingjun Ma, Sarah Monazam Erfani, Danilo Mandic, James Bailey
  - **institution:** The University of Melbourne, Fudan University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.16244
  - **Simple LLM Summary:** The paper proposes a Coarse-to-Fine open-set Classification (CFC) framework that uses large language models for OOD detection and label generation, combined with a GNN-based fine classifier, to perform open-set node classification on graphs. It shows that CFC improves OOD detection by 10% over state-of-the-art methods and achieves up to 70% accuracy in OOD classification on graph datasets.

- **[arXiv251219] AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding**
  - **tags:** [mlsys], [multi-modal inference], [agentic reasoning, reward optimization, self-evaluation, selective parameter adaptation, benchmark evaluation]
  - **authors:** Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli
  - **institution:** University of Maryland, College Park, Apple
  - **link:** https://arxiv.org/pdf/2512.16250
  - **Simple LLM Summary:** The paper introduces AMUSE, a benchmark for evaluating agentic multi-speaker understanding in audio-visual models, and RAFT, an alignment framework that uses reward optimization with multimodal self-evaluation. The main conclusion is that current models struggle with multi-speaker reasoning, but the proposed RAFT framework achieves significant accuracy improvements on the benchmark.

- **[arXiv251219] AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints**
  - **tags:** [mlsys], [post-training], [AlignMerge, Fisher-Rao geometry, Alignment Quality Index (AQI), alignment subspace, Fisher-weighted averaging, model soups, TIES, SafeMerge, MergeAlign]
  - **authors:** Aniruddha Roy, Jyoti Patel, Aman Chadha, Vinija Jain, Amitava Das
  - **institution:** HCL, Evalueserve, Apple, Google, Pragya Lab, BITS Pilani, Goa
  - **link:** https://arxiv.org/pdf/2512.16245
  - **Simple LLM Summary:** This paper introduces AlignMerge, a geometry-aware framework for merging large language models that explicitly preserves alignment by using Fisher-guided geometric constraints and an Alignment Quality Index (AQI) as a latent-space criterion. The method formulates merging as a constrained optimization problem around an aligned base model, penalizing drift in alignment-sensitive directions and enforcing a soft alignment budget. The results show that AlignMerge improves alignment metrics while matching or exceeding expert performance on utility tasks, outperforming existing merging techniques like Fisher soups and TIES.

- **[arXiv251219] Learning to Wait: Synchronizing Agents with the Physical World**
  - **tags:** [mlsys], [cluster infrastructure], [agent-side approach, cognitive timeline, in-context learning, code-as-action, time.sleep(t), kubernetes cluster, temporal gap]
  - **authors:** Yifei She, Ping Zhang, He Liu, Yanmin Jia, Yang Jing, Zijun Liu, Peng Sun, Xiangbin Li, Xiaohe Hu
  - **institution:** Infrawaves, Shanghai Qiji Zhifeng Co., Ltd., Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.16262
  - **Simple LLM Summary:** This paper proposes an agent-side approach where LLMs learn to predict precise waiting durations using semantic priors and in-context learning, extending the code-as-action paradigm to synchronize with asynchronous environments. Experiments in a simulated Kubernetes cluster show that agents can calibrate their internal clocks to minimize query overhead and execution latency. The work concludes that temporal awareness is a learnable capability essential for autonomous agents in open-ended, real-world tasks.

- **[arXiv251219] Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification**
  - **tags:** [mlsys], [multi-modal training], [transformer, causal attention, hierarchical representation, multi-task learning, domain-adversarial training, controlled perturbation training, counterfactual simulation]
  - **authors:** Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo
  - **institution:** Strathmore University, Google Research, Queen Mary University of London
  - **link:** https://arxiv.org/pdf/2512.16271
  - **Simple LLM Summary:** The paper proposes DACH-TIC, a domain-agnostic causal-aware hierarchical audio transformer that integrates causal attention, multi-task learning, and adversarial domain generalization for robust infant cry classification. It outperforms state-of-the-art baselines in accuracy and macro-F1 score and demonstrates strong generalization to unseen acoustic environments with minimal performance degradation.

- **[arXiv251219] Sigma-Moe-Tiny Technical Report**
  - **tags:** [mlsys], [llm training], [mixture-of-experts, load balancing, progressive sparsification, fine-grained expert segmentation]
  - **authors:** Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong
  - **institution:** Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.16248
  - **Simple LLM Summary:** This paper introduces Sigma-MoE-Tiny, a highly sparse mixture-of-experts language model that uses fine-grained expert segmentation with up to 96 experts per layer and activates only one expert per token, achieving a 40:1 total-to-activated parameter ratio. To address load balancing challenges in such extreme sparsity, the authors propose a progressive sparsification schedule, which ensures stable training without irrecoverable loss spikes. Despite activating only 0.5B parameters, the model achieves top-tier performance compared to counterparts of similar or larger scale, setting a new benchmark for sparsity in open-source MoE models.

- **[arXiv251219] TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering**
  - **tags:** [mlsys], [multi-modal inference], [TextEditBench, reasoning-aware editing, semantic expectation, text rendering, visual generation, multimodal models, diffusion models, evaluation benchmark]
  - **authors:** Rui Gui, Yang Wan, Haochen Han, Dongxing Mao, Fangming Liu, Min Li, Alex Jinpeng Wang
  - **institution:** Central South University, Pengcheng Laboratory
  - **link:** https://arxiv.org/pdf/2512.16270
  - **Simple LLM Summary:** This paper introduces TextEditBench, a comprehensive benchmark for evaluating text editing in images, focusing on reasoning-intensive scenarios that require understanding of semantics, context, and physical plausibility. It proposes a novel evaluation dimension called Semantic Expectation (SE) to measure a model's ability to maintain consistency during editing. The main conclusion is that current state-of-the-art editing models still struggle with context-dependent reasoning and layout-aware integration, highlighting a significant gap in text-guided image editing capabilities.

- **[arXiv251219] Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls**
  - **tags:** [mlsys], [llm inference], [llm-as-a-judge, analytic checker, hybrid evaluation, prompt injection, cobol code generation]
  - **authors:** Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Rami Katan, Alice Podolsky
  - **institution:** IBM Research, Israel
  - **link:** https://arxiv.org/pdf/2512.16272
  - **Simple LLM Summary:** This paper proposes a hybrid evaluation method that combines an LLM-as-a-Judge (LaaJ) with a lightweight analytic checker that provides domain-specific hints, which are dynamically injected into the judge's prompt. The method was tested on COBOL code generation, where LaaJs alone missed many errors. The results show that the LaaJ+Hints configuration significantly improves error detection coverage and explanation quality, demonstrating the effectiveness of analytic-LLM hybrids for reliable evaluation.

- **[arXiv251219] GFLAN: Generative Functional Layouts**
  - **tags:** [ai], [computer vision], [convolutional architecture, graph neural network, transformer, floor plan generation, topological planning, geometric realization]
  - **authors:** Mohamed Abouagour, Eleftherios Garyfallidis
  - **institution:** Indiana University Bloomington
  - **link:** https://arxiv.org/pdf/2512.16275
  - **Simple LLM Summary:** This paper introduces GFLAN, a two-stage generative framework for automated floor plan generation that first allocates room centroids via a convolutional model and then regresses room boundaries using a Transformer-augmented graph neural network. The method effectively factors layout synthesis into topological planning and geometric realization to better capture architectural reasoning and functional constraints.

- **[arXiv251219] QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems**
  - **tags:** [mlsys], [others], [sequent safety, multi-agent guard, state tracker, policy verifier, threat watcher, referee, machine-checkable rules, predicate updater]
  - **authors:** Yiliu Yang, Yilei Jiang, Qunzhong Wang, Yingshui Tan, Xiaoyong Zhu, Sherman S.M. Chow, Bo Zheng, Xiangyu Yue
  - **institution:** The Chinese University of Hong Kong, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.16279
  - **Simple LLM Summary:** The paper proposes QuadSentinel, a four-agent supervisory framework that compiles natural language safety policies into machine-checkable rules using sequents and enforces them online in multi-agent systems. It demonstrates improved guardrail accuracy and rule recall while reducing false positives compared to single-agent baselines on standard benchmarks. The approach allows for safe deployment without modifying the core agents.

- **[arXiv251219] Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams**
  - **tags:** [mlsys], [llm inference], [large language models, safety filters, conversation study, social engineering, automation]
  - **authors:** Gilad Gressel, Rahul Pankajakshan, Shir Rozenfeld, Ling Li, Ivan Franceschini, Krishnahsree Achuthan, Yisroel Mirsky
  - **institution:** Amrita Vishwa Vidyapeetham, Ca’ Foscari University of Venice, University of Melbourne, Ben Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2512.16280
  - **Simple LLM Summary:** The paper investigates the role of LLMs in romance-baiting scams through interviews with insiders and victims, a blinded long-term conversation study comparing LLM agents to human operators, and an evaluation of commercial safety filters. It finds that LLMs are already widely used in scams, can elicit greater trust and compliance than humans, and that current safety filters are ineffective at detecting such dialogues, suggesting scams are ripe for full LLM automation.

- **[arXiv251219] CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity**
  - **tags:** [mlsys], [post-training], [post-training quantization, CKA, GPTQ, AWQ, SmoothQuant, mixed-precision, algorithmic heterogeneity]
  - **authors:** Jinhao Zhang, Yunquan Zhang, Daning Chen
  - **institution:** Beijing University of Posts and Telecommunications, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.16282
  - **Simple LLM Summary:** This paper proposes CKA-Guided Modular Quantization, a framework that automatically selects the best post-training quantization algorithm for each layer of a large language model using Linear CKA similarity as a selection metric. It combines different algorithms like GPTQ, AWQ, and SmoothQuant across layers to create a hybrid quantized model. The method outperforms uniform quantization and mixed-precision baselines on models like LLaMA and Qwen without requiring fine-tuning.

- **[arXiv251219] OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models**
  - **tags:** [mlsys], [post-training], [critic models, supervised fine-tuning (SFT), consistency-preserving group relative policy optimization (CP-GRPO), vision-language models (VLMs), GUI navigation]
  - **authors:** Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Qiushi Sun, Zhaoyang Liu, Zhoumianze Liu, Yu Qiao, Xiangyu Yue, Zun Wang, Zichen Ding
  - **institution:** Shanghai AI Laboratory, Shanghai Jiaotong University, CUHK MMLab, The University of Hong Kong, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.16295
  - **Simple LLM Summary:** This paper introduces OS-Oracle, a framework for training critic models to evaluate GUI actions for computer-using agents. It uses a scalable data pipeline and a two-stage training method combining SFT and CP-GRPO. The resulting model, OS-Oracle-7B, achieves state-of-the-art performance on cross-platform GUI critic benchmarks and improves the performance of other GUI agents.

- **[arXiv251219] Feature-Selective Representation Misdirection for Machine Unlearning**
  - **tags:** [mlsys], [post-training], [machine unlearning, activation editing, feature-aware perturbation, representation misdirection, SRMU]
  - **authors:** Taozhao Chen, Linghan Huang, Kim-Kwang Raymond Choo, Huaming Chen
  - **institution:** University of Sydney, University of Texas at San Antonio
  - **link:** https://arxiv.org/pdf/2512.16297
  - **Simple LLM Summary:** The paper proposes Selective Representation Misdirection for Unlearning (SRMU), a novel activation-editing framework that applies feature-aware, directional perturbations to selectively suppress harmful knowledge in LLMs. It demonstrates state-of-the-art unlearning performance with minimal utility loss, remaining robust even when the data to forget and retain are highly entangled.

- **[arXiv251219] Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection**
  - **tags:** [mlsys], [multi-modal inference], [ForenAgent, FABench, reinforcement fine-tuning, dynamic reasoning loop, agentic tool use, low-level forensic tools, Python toolchain]
  - **authors:** Fanrui Zhang, Qiang Zhang, Sizhuo Zhou, Jianwen Sun, Chuanhao Li, Jiaxin Ai, Yukang Feng, Yujie Zhang, Wenjie Li, Zizhen Li, Yifan Chang, Jiawei Liu, Kaipeng Zhang
  - **institution:** University of Science and Technology of China, Shanghai Innovation Institute, Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.16300
  - **Simple LLM Summary:** This paper proposes ForenAgent, a multi-modal large language model (MLLM) framework that autonomously generates and refines Python-based low-level forensic tools for image forgery detection through a multi-round interactive process. It introduces a two-stage training pipeline and a dynamic reasoning loop, evaluated on a new dataset called FABench. The experiments show that ForenAgent achieves effective and interpretable forgery analysis by combining high-level semantic reasoning with low-level artifact detection.

- **[arXiv251219] PixelArena: A benchmark for Pixel-Precision Visual Intelligence**
  - **tags:** [mlsys], [multi-modal inference], [semantic segmentation, pixel-precision visual intelligence, zero-shot generation, face parsing, CelebAMask-HQ, COCO]
  - **authors:** Feng Liang, Sizhe Cheng, Chenqi Yi
  - **institution:** Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.16303
  - **Simple LLM Summary:** The paper introduces PixelArena, a benchmark that uses semantic segmentation tasks to evaluate the fine-grained image generation capabilities of multi-modal large language models. It finds that Gemini 3 Pro Image exhibits emergent zero-shot ability to generate accurate semantic masks, demonstrating significant progress in pixel-precision visual intelligence and generalization.

- **[arXiv251219] Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks**
  - **tags:** [mlsys], [llm inference], [prompt injection defense, goal-hijacking, Chain Of Thoughts, automatic defense generation, LLaMA]
  - **authors:** Safwan Shaheer, G.M. Refatul Islam, Mohammad Rafid Hamid, Tahsin Zaman Jilan
  - **institution:** BRAC University
  - **link:** https://arxiv.org/pdf/2512.16307
  - **Simple LLM Summary:** This paper introduces a novel defense framework against prompt injection attacks for small open-source LLMs like LLaMA, using an iterative refinement process based on a Chain Of Thoughts seed to generate automatic defenses. The method significantly improves the detection of goal-hijacking attacks, reducing both attack success rates and false detection rates. The work demonstrates that this approach enables more secure and efficient deployment of LLMs in resource-constrained environments.

- **[arXiv251219] Adaptation of Agentic AI**
  - **tags:** [ai], [agentic AI adaptation], [agent adaptation, tool adaptation, tool-execution-signaled, agent-output-signaled, agent-agnostic, agent-supervised, SFT, RL, VR methods, Toolformer, ToolLLM, DeepRetrieval, Prover-V2, DeepSeek-R1, Kimi-1.5, ReTool, Search-R1, HuggingGPT, ViperGPT, Subagent-as-Tool, Agentic Memory, Reflexion, Memento, SWE-Grep, Tab-RL]
  - **authors:** Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han
  - **institution:** UIUC, Stanford, Princeton, Harvard, UW, Caltech, UC Berkeley, UCSD, Georgia Tech, Northwestern, TAMU, Unity
  - **link:** https://arxiv.org/pdf/2512.16301
  - **Simple LLM Summary:** This paper proposes a systematic framework for adapting agentic AI systems, categorizing adaptations into agent adaptation (tool-execution-signaled and agent-output-signaled) and tool adaptation (agent-agnostic and agent-supervised). It reviews representative methods in each category, analyzes their trade-offs, and provides practical guidance for selecting adaptation strategies. The work aims to offer a conceptual foundation and roadmap for building more capable, efficient, and reliable agentic AI systems.

- **[arXiv251219] Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference**
  - **tags:** [mlsys], [llm inference], [Proof of Quality (PoQ), decentralized inference, cost-aware reward, semantic textual similarity, bi-encoder, Monte Carlo simulation, quality-cost analysis]
  - **authors:** Arther Tian, Alex Ding, Frank Chen, Alan Wu, Aaron Chan, Bruce Zhang
  - **institution:** DGrid AI
  - **link:** https://arxiv.org/pdf/2512.16317
  - **Simple LLM Summary:** This paper introduces a cost-aware Proof of Quality (PoQ) framework for decentralized LLM inference, which integrates efficiency measurements into a reward mechanism balancing output quality and computational cost. Experiments and simulations show that evaluator architecture is critical and that the proposed scheme successfully rewards high-quality, low-cost nodes, providing a foundation for economically sustainable decentralized inference.

- **[arXiv251219] Pretrained Battery Transformer (PBT): A battery life prediction foundation model**
  - **tags:** [mlsys], [others], [foundation model, transformer, mixture-of-expert layers, transfer learning, battery life prediction]
  - **authors:** Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Shanghai University
  - **link:** https://arxiv.org/pdf/2512.16334
  - **Simple LLM Summary:** This paper introduces the Pretrained Battery Transformer (PBT), a foundation model for battery life prediction that uses domain-knowledge-encoded mixture-of-expert layers. It is trained on diverse lithium-ion battery datasets and achieves state-of-the-art performance through transfer learning, establishing a pathway for universal battery lifetime prediction systems.

- **[arXiv251219] Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation**
  - **tags:** [mlsys], [llm inference], [Tools Orchestration Privacy Risk (TOP-R), TOP-Bench, Privacy Enhancement Principle (PEP), H-Score, Risk Leakage Rate (RLR)]
  - **authors:** Yuxuan Qiao, Dongqin Liu, Hongchang Yang, Wei Zhou, Songlin Hu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.16310
  - **Simple LLM Summary:** This paper identifies and studies a new privacy risk in single-agent, multi-tool LLM architectures, termed Tools Orchestration Privacy Risk (TOP-R), where agents can synthesize sensitive information from aggregated tool outputs. It introduces a benchmark (TOP-Bench) and a mitigation method called the Privacy Enhancement Principle (PEP). The main conclusion is that TOP-R is a severe and prevalent risk in current models, but the proposed PEP method can effectively reduce leakage and improve the safety-robustness trade-off.

- **[arXiv251219] AI Needs Physics More Than Physics Needs AI**
  - **tags:** [ai], [machine learning critique], [large language models, reasoning models, agentic AI, quantum AI, analogue computing, Big AI, theory-based rigour]
  - **authors:** Peter Coveney, Roger Highfield
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2512.16344
  - **Simple LLM Summary:** This paper critiques current AI architectures like LLMs for their limitations, such as lacking uncertainty quantification and failing to capture scientific laws. It argues that physics offers more to AI than vice versa, proposing a roadmap for "Big AI" that synthesizes theoretical rigor with machine learning flexibility.

- **[arXiv251219] Collaborative Edge-to-Server Inference for Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [collaborative inference, edge-to-server, region of interest, attention-aware cropping, min-entropy, selective retransmission]
  - **authors:** Soochang Song, Yongjune Kim
  - **institution:** Pohang University of Science and Technology (POSTECH)
  - **link:** https://arxiv.org/pdf/2512.16349
  - **Simple LLM Summary:** The paper proposes a two-stage collaborative inference framework for vision-language models where a server first processes a low-resolution global image and, if the inference uncertainty (measured by min-entropy) is high, requests a high-resolution local image of a task-relevant region from the edge device. This selective retransmission mechanism reduces communication costs while maintaining inference accuracy by leveraging both global and local visual information. Experiments demonstrate significant communication savings across multiple VLM architectures.

- **[arXiv251219] PCIA: A Path Construction Imitation Algorithm for Global Optimization**
  - **tags:** [ai], [metaheuristic optimization], [Path Construction Imitation Algorithm, PCIA, swarm-based algorithm, global optimization]
  - **authors:** Mohammad-Javad Rezaei, Mozafar Bag-Mohammadi
  - **institution:** Ilam University
  - **link:** https://arxiv.org/pdf/2512.16392
  - **Simple LLM Summary:** The paper proposes a new metaheuristic optimization algorithm called PCIA, inspired by how humans construct and select paths. It was tested on 53 mathematical and 13 constrained optimization problems, showing competitive performance against other popular and recent metaheuristic algorithms.

- **[arXiv251219] Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference**
  - **tags:** [mlsys], [llm inference], [sparse attention, dynamic programming, top-k selection, anchor layers, reuse layers, FlashAttention-3]
  - **authors:** Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee
  - **institution:** Microsoft Research India
  - **link:** https://arxiv.org/pdf/2512.16391
  - **Simple LLM Summary:** The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.

- **[arXiv251219] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs**
  - **tags:** [mlsys], [multi-modal inference], [SpeechLLMs, cascaded systems, speech foundation models, speech-to-text translation, benchmarking, Whisper, SeamlessM4T, Gemma, Tower+]
  - **authors:** Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vilém Zouhar, Carlos Escolano, Gerard I. Gállego, Jorge Iranzo-Sánchez, Ahrii Kim, Dominik Macháček, Patricia Schmidtova, Maike Züfle
  - **institution:** Fondazione Bruno Kessler, Barcelona Supercomputing Center, University of Zurich, ETH Zurich, Universitat Politècnica de Catalunya, Universitat Politècnica de València, AI-Bio Convergence Research Institute, Charles University, KIT
  - **link:** https://arxiv.org/pdf/2512.16378
  - **Simple LLM Summary:** This paper introduces the "Hearing to Translate" test suite to benchmark SpeechLLMs against cascaded and direct speech-to-text translation systems. It finds that cascaded systems, which combine speech recognition with LLM-based translation, remain the most reliable overall, while current SpeechLLMs only match them in specific scenarios. The study concludes that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

- **[arXiv251219] Hypernetworks That Evolve Themselves**
  - **tags:** [ai], [neuroevolution], [hypernetworks, graph hypernetworks, self-referential systems, evolutionary algorithms, adaptive mutation rates]
  - **authors:** Joachim Winther Pedersen, Erwan Plantec, Eleni Nisioti, Marcello Barylli, Milton Montero, Kathrin Korte, Sebastian Risi
  - **institution:** IT University of Copenhagen, Sakana AI
  - **link:** https://arxiv.org/pdf/2512.16406
  - **Simple LLM Summary:** This paper introduces Self-Referential Graph HyperNetworks, which embed evolutionary mechanisms like mutation and inheritance directly within neural networks, enabling them to evolve autonomously without external optimizers. The method demonstrates swift adaptation to environmental shifts and emergent population dynamics in reinforcement learning benchmarks, supporting the idea that evolvability can emerge from neural self-reference.

- **[arXiv251219] Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture**
  - **tags:** [ai], [computer vision, computer graphics], [Gaussian Splatting, neural radiance fields, triangulated surface, view-dependent neural texture, relightable Gaussian model, albedo texture, segmentation annotations]
  - **authors:** Haodi He, Jihun Yu, Ronald Fedkiw
  - **institution:** Epic Games, Stanford University
  - **link:** https://arxiv.org/pdf/2512.16397
  - **Simple LLM Summary:** This paper proposes a method for creating high-fidelity facial avatars by using Gaussian Splatting, constrained to an underlying triangulated surface, to reconstruct geometry and texture from a small set of uncalibrated images. The approach enables the generation of a standard mesh and a delit, high-resolution albedo texture for use in traditional graphics pipelines. The main conclusion is that this method facilitates scalable and democratized avatar creation, demonstrating utility in applications like text-driven asset generation.

- **[arXiv251219] Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs**
  - **tags:** [ai], [computer-aided synthesis planning], [retrosynthesis, large language models, chain-of-thought reasoning, natural language prompting, feasibility-aware planning]
  - **authors:** Nguyen Xuan-Vu, Daniel Armstrong, Milena Wehrbach, Andres M Bran, Zlatko Jončev, Philippe Schwaller
  - **institution:** École Polytechnique Fédérale de Lausanne (EPFL), National Centre of Competence in Research (NCCR) Catalysis, B12 Labs
  - **link:** https://arxiv.org/pdf/2512.16424
  - **Simple LLM Summary:** This paper introduces Synthelite, a synthesis planning framework that uses large language models (LLMs) to propose retrosynthetic transformations and generate end-to-end synthesis routes. The method allows expert intervention through natural language prompts to flexibly adapt to user-specified constraints. The experiments show that Synthelite achieves high success rates in constrained synthesis tasks and can account for chemical feasibility during route design.

- **[arXiv251219] Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach**
  - **tags:** [mlsys], [llm inference], [neuro-symbolic approach, vector search, knowledge graphs, retrieval-augmented generation (RAG), large language models (LLMs)]
  - **authors:** Allard Oelen, Mohamad Yaser Jaradeh, Sören Auer
  - **institution:** TIB – Leibniz Information Centre for Science and Technology, L3S Research Center, Leibniz University of Hannover
  - **link:** https://arxiv.org/pdf/2512.16425
  - **Simple LLM Summary:** This paper introduces ORKG ASK, a scholarly search system that uses a neuro-symbolic approach combining vector search, knowledge graphs, and LLMs with a Retrieval-Augmented Generation (RAG) framework to answer research questions. The system was evaluated for usability and found to be user-friendly, with users generally satisfied with its performance.

- **[arXiv251219] Emergent Bias and Fairness in Multi-Agent Decision Systems**
  - **tags:** [ai], [fairness and bias in multi-agent systems], [multi-agent systems, emergent bias, fairness evaluation, large-scale simulation, credit scoring, income estimation]
  - **authors:** Maeve Madigan, Parameswaran Kamalaruban, Glenn Moynihan, Tom Kempton, David Sutton, Stuart Burrell
  - **institution:** Visa Inc., University of Manchester
  - **link:** https://arxiv.org/pdf/2512.16433
  - **Simple LLM Summary:** This paper develops fairness evaluation methodologies for multi-agent predictive systems in the financial domain using large-scale simulations across diverse agent configurations. It finds that these systems exhibit emergent bias in tasks like credit scoring, which cannot be traced to individual agents, indicating genuinely collective behaviors. The authors conclude that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analysis of their components.

- **[arXiv251219] TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles**
  - **tags:** [mlsys], [others], [large language models, ai-assisted research, ro-crate, scholarly services, research life cycle]
  - **authors:** Allard Oelen, Sören Auer
  - **institution:** TIB – Leibniz Information Centre for Science and Technology, L3S Research Center, Leibniz University of Hannover
  - **link:** https://arxiv.org/pdf/2512.16442
  - **Simple LLM Summary:** The paper introduces TIB AIssistant, a platform that uses AI and LLMs to support researchers across the entire research life cycle through specialized assistants and external scholarly tools. It stores generated data as assets and allows export as RO-Crate bundles to enhance reproducibility. The platform aims to provide a community-maintained foundation for AI-supported research.

- **[arXiv251219] E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, reward engineering, vision-language models, environment perception, terrain sensor analysis]
  - **authors:** Enis Yalcin, Joshua O'Hara, Maria Stamatopoulou, Chengxu Zhou, Dimitrios Kanoulas
  - **institution:** University College London
  - **link:** https://arxiv.org/pdf/2512.16446
  - **Simple LLM Summary:** This paper introduces E-SDS, a framework that integrates vision-language models with real-time terrain sensor analysis to automatically generate reward functions for training humanoid locomotion policies. The method uniquely enables robust navigation on complex terrains like stairs, significantly reducing velocity tracking error and manual engineering effort. The main conclusion is that this environment-aware approach produces more capable policies than manual or non-perceptive automated methods.

- **[arXiv251219] StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm**
  - **tags:** [ai], [multi-agent reinforcement learning], [StarCraft II battle arena, adversarial PyMARL, dual-algorithm paired adversary, multi-algorithm mixed adversary, benchmark]
  - **authors:** Yadong Li, Tong Zhang, Bo Huang, Zhen Cui
  - **institution:** Nanjing University of Science and Technology, Zaozhuang University
  - **link:** https://arxiv.org/pdf/2512.16444
  - **Simple LLM Summary:** This paper introduces StarCraft II Battle Arena (SC2BA), a new multi-agent algorithm-vs-algorithm environment, and an Adversarial PyMARL (APyMARL) library to benchmark MARL algorithms in an adversary paradigm. The extensive experiments reveal thought-provoking observations about the effectivity, sensibility, and scalability of classic MARL algorithms when pitted against each other.

- **[arXiv251219] Topic Modelling Black Box Optimization**
  - **tags:** [ai], [hyperparameter optimization], [latent dirichlet allocation, black-box optimization, genetic algorithm, evolution strategy, preferential amortized black-box optimization, sharpness-aware black-box optimization]
  - **authors:** Roman Akramov, Artem Khamatullin, Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko
  - **institution:** Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2512.16445
  - **Simple LLM Summary:** This paper formulates selecting the number of topics in Latent Dirichlet Allocation (LDA) as a discrete black-box optimization problem. It compares evolutionary methods (GA, ES) against learned amortized optimizers (PABBO, SABBO), finding that the amortized approaches are substantially more sample- and time-efficient, with SABBO often finding a near-optimal topic number after essentially a single evaluation.

- **[arXiv251219] IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value**
  - **tags:** [mlsys], [others], [Convolutional Neural Network (CNN), Higher Order Singular Value Decomposition (HOSVD), Internet of Medical Things (IoMT), Acute Lymphoblastic Leukemia Image Database (ALL-IDB2)]
  - **authors:** Shabnam Bagheri Marzijarani, Mohammad Zolfaghari, Hedieh Sajedi
  - **institution:** Islamic Azad University, University of Tehran
  - **link:** https://arxiv.org/pdf/2512.16448
  - **Simple LLM Summary:** This paper proposes a method for automated leukemia classification by combining a Convolutional Neural Network (CNN) with a Higher Order Singular Value Decomposition (HOSVD) classifier, deployed within an Internet of Medical Things (IoMT) framework. The model was tested on the ALL-IDB2 database and achieved an average test accuracy of 98.88%, demonstrating its effectiveness for rapid and accurate identification of Acute Lymphocytic Leukemia cells.

- **[arXiv251219] TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries**
  - **tags:** [mlsys], [llm inference], [TimeSeries2Report (TS2R), prompting framework, segmentation, semantic abstraction, rule-based interpretation, anomaly detection, state-of-charge prediction, charging/discharging management]
  - **authors:** Jiayang Yang, Chunhui Zhao, Martin Guay, Zhixing Cao
  - **institution:** Zhejiang University, Queen's University
  - **link:** https://arxiv.org/pdf/2512.16453
  - **Simple LLM Summary:** The paper introduces TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery time-series data into structured natural language reports to enable large language models (LLMs) to manage battery systems. This method improves LLM performance in tasks like anomaly detection and state prediction without requiring model retraining. The conclusion is that TS2R provides a practical, adaptive path for LLM-driven battery intelligence, achieving expert-level decision quality.

- **[arXiv251219] Towards AI-Supported Research: a Vision of the TIB AIssistant**
  - **tags:** [mlsys], [llm inference], [prompt engineering, modular platform, orchestration framework, shared data store, tool libraries]
  - **authors:** Sören Auer, Allard Oelen, Mohamad Yaser Jaradeh, Mutahira Khalid, Farhana Keya, Sasi Kiran Gaddipati, Jennifer D'Souza, Lorenz Schlüter, Amirreza Alasti, Gollam Rabby, Azanzi Jiomekong, Oliver Karras
  - **institution:** TIB – Leibniz Information Centre for Science and Technology, L3S Research Center, Leibniz University of Hannover, University of Yaounde 1
  - **link:** https://arxiv.org/pdf/2512.16447
  - **Simple LLM Summary:** The paper presents the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers by integrating AI assistants across the research lifecycle. Its core method involves a modular architecture with prompt/tool libraries, a shared data store, and an orchestration framework to facilitate tasks like literature analysis and scholarly writing. The main conclusion is that this approach demonstrates the feasibility and potential impact of using a structured platform to overcome challenges in effectively integrating Generative AI into scholarly workflows.

- **[arXiv251219] AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research**
  - **tags:** [mlsys], [cluster infrastructure], [federated cloud platform, service catalogue, interactive development environments, GPU resources, annotation tools, experiment tracking, federated learning, model deployment, traceability, reproducibility]
  - **authors:** Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez
  - **institution:** Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)
  - **link:** https://arxiv.org/pdf/2512.16455
  - **Simple LLM Summary:** The paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform integrates distributed e-Infrastructures to provide consistent access to development environments, GPU training, annotation tools, and deployment options. Its main conclusion is that this integrated, customizable platform lowers adoption barriers and facilitates reproducible AI workflows for the scientific community.

- **[arXiv251219] cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution**
  - **tags:** [mlsys], [GPU kernels], [multi-agent framework, evolutionary algorithm, roofline-guided prompting, strategy-coordinated evolution]
  - **authors:** Jinwu Chen, Qidie Wu, Bin Li, Lin Ma, Xin Si, Yang Hu, Shouyi Yin, Jun Yang
  - **institution:** Southeast University, Tsinghua University, Tsing Micro, National Center of Technology Innovation for EDA
  - **link:** https://arxiv.org/pdf/2512.16465
  - **Simple LLM Summary:** The paper proposes cuPilot, a multi-agent framework that uses strategy as an intermediate semantic representation to coordinate LLM agents and evolutionary algorithms for optimizing CUDA kernels. It introduces a strategy-coordinated evolution algorithm and roofline-guided prompting. The generated kernels achieve an average 3.09x speedup over PyTorch on a benchmark of 100 kernels.

- **[arXiv251219] Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery**
  - **tags:** [mlsys], [others], [Decisive Feature Fidelity (DFF), explainable AI (XAI), counterfactual explanations, fidelity calibration, mechanism parity, KITTI-VirtualKITTI2]
  - **authors:** Danial Safaei, Siddartha Khastgir, Mohsen Alirezaei, Jeroen Ploeg, Son Tong, Xingyu Zhao
  - **institution:** University of Warwick, Siemens Digital Industries Software
  - **link:** https://arxiv.org/pdf/2512.16468
  - **Simple LLM Summary:** This paper introduces Decisive Feature Fidelity (DFF), a new SUT-specific metric that uses explainable AI methods to compare the causal features driving a system's decisions on matched real and synthetic data. The authors also propose a DFF-guided calibration scheme to improve simulator fidelity. Experiments show DFF reveals discrepancies missed by conventional fidelity measures and that calibration improves feature-level fidelity without harming output-level performance.

- **[arXiv251219] Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment**
  - **tags:** [ai], [computer vision], [reinforcement learning, perception-reasoning cascade, self-consistent reasoning, ROUGE-1, human annotations]
  - **authors:** Yuan Li, Yahan Yu, Youyuan Lin, Yong-Hao Yang, Chenhui Chu, Shin'ya Nishida
  - **institution:** Kyoto University
  - **link:** https://arxiv.org/pdf/2512.16484
  - **Simple LLM Summary:** This paper proposes a method to make blind image quality assessment (BIQA) models more human-like by using reinforcement learning guided by human annotations to align the model's perception-reasoning process. The approach introduces a reward to encourage self-consistent reasoning from self-generated descriptions. The results show the model achieves competitive score prediction and significantly improves alignment with human reasoning chains, as measured by ROUGE-1.

- **[arXiv251219] Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors**
  - **tags:** [mlsys], [multi-modal training], [eye-behavior-aided multimodal emotion recognition, modality-adversarial feature decoupling, multitask Transformer, eye movement sequences, eye fixation maps]
  - **authors:** Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen
  - **institution:** China University of Geosciences (Wuhan), Huazhong University of Science and Technology, Wuhan University, La Trobe University
  - **link:** https://arxiv.org/pdf/2512.16485
  - **Simple LLM Summary:** The paper introduces a multimodal emotion recognition dataset (EMER) incorporating eye behaviors and facial expressions, and proposes a Transformer-based model (EMERT) that uses modality-adversarial feature decoupling and multitask learning to bridge the gap between facial expression recognition and genuine emotion recognition. The results demonstrate that modeling eye behaviors significantly enhances emotion recognition performance, outperforming other state-of-the-art multimodal methods.

- **[arXiv251219] Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network**
  - **tags:** [ai], [meta-algorithmics], [algorithm selection, algorithm configuration, algorithm scheduling, experimental design, reproducibility, fairness, accountability]
  - **authors:** Theresa Eimer, Lennart Schäpermeier, André Biedenkapp, Alexander Tornede, Lars Kotthoff, Pieter Leyman, Matthias Feurer, Katharina Eggensperger, Kaitlin Maile, Tanja Tornede, Anna Kozak, Ke Xue, Marcel Wever, Mitra Baratchi, Damir Pulatov, Heike Trautmann, Haniye Kashgarani, Marius Lindauer
  - **institution:** Leibniz University Hannover, L3S Research Center, University of Münster, Albert-Ludwigs University Freiburg, University of St. Andrews, Ghent University, Dortmund University, Google, Warsaw University of Technology, Nanjing University, Leiden University, University of North Carolina Wilmington, Paderborn University, Purdue University
  - **link:** https://arxiv.org/pdf/2512.16491
  - **Simple LLM Summary:** This paper consolidates best practices for empirical research in meta-algorithmics, such as algorithm selection and configuration, covering the entire experimental cycle from design to analysis. It aims to establish reliable, reproducible, and fair research standards to improve the validity and societal impact of findings. The main conclusion is that systematic guidelines are needed to unify scattered practices and ensure long-term, meaningful progress in the field.

- **[arXiv251219] PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation**
  - **tags:** [ai], [computer vision], [mixture-of-experts, monocular 3d human pose estimation, lifting-based methods, cross-expert knowledge aggregation, spatio-temporal contextual information]
  - **authors:** Mengyuan Liu, Jiajie Liu, Jinyan Zhang, Wenhao Li, Junsong Yuan
  - **institution:** Peking University, Nanyang Technological University, University at Buffalo, State University of New York
  - **link:** https://arxiv.org/pdf/2512.16494
  - **Simple LLM Summary:** This paper proposes PoseMoE, a Mixture-of-Experts network that disentangles the feature encoding for 2D pose and depth to improve monocular 3D human pose estimation. It introduces specialized expert modules and a cross-expert knowledge aggregation module to refine features and reduce the negative influence of uncertain depth. The method outperforms conventional lifting-based approaches on standard benchmarks like Human3.6M, MPI-INF-3DHP, and 3DPW.

- **[arXiv251219] XTC, A Research Platform for Optimizing AI Workload Operators**
  - **tags:** [mlsys], [GPU kernels], [scheduling language, code generation, performance evaluation, compiler, autotuning]
  - **authors:** Pompougnac Hugo, Guillon Christophe, Noiry Sylvain, Dutilleul Alban, Iooss Guillaume, Rastello Fabrice
  - **institution:** Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG
  - **link:** https://arxiv.org/pdf/2512.16512
  - **Simple LLM Summary:** The paper introduces XTC, a research platform that provides a unified scheduling language and API to decouple optimization strategies from compiler-specific code generation and measurement. This enables portable experimentation and fair performance comparison across different compiler frameworks. The main conclusion is that XTC accelerates research on AI operator optimization by providing a common, reproducible framework for scheduling and evaluation.

- **[arXiv251219] ParamExplorer: A framework for exploring parameters in generative art**
  - **tags:** [ai], [generative art], [reinforcement learning, parameter exploration, human-in-the-loop, p5.js]
  - **authors:** Julien Gachadoat, Guillaume Lagarde
  - **institution:** University of Bordeaux
  - **link:** https://arxiv.org/pdf/2512.16529
  - **Simple LLM Summary:** This paper introduces ParamExplorer, an interactive and modular framework inspired by reinforcement learning to help explore the high-dimensional parameter spaces of generative art algorithms. It allows for exploration guided by human feedback and integrates with existing p5.js projects. The framework implements and evaluates several automated exploration strategies, referred to as agents, to discover aesthetically compelling outputs more efficiently than manual trial-and-error.

- **[arXiv251219] TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [test-time padding, adversarial detection, cosine similarity shift, trainable padding, similarity-aware ensemble, vision-language models, CLIP]
  - **authors:** Zhiwei Li, Yitian Pang, Weining Wang, Zhenan Sun, Qi Li
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.16523
  - **Simple LLM Summary:** This paper proposes Test-Time Padding (TTP), a lightweight defense framework for Vision-Language Models that detects adversarial inputs by analyzing the shift in feature embeddings after spatial padding and then adapts them using trainable padding and an ensemble strategy. The method consistently outperforms existing test-time defenses, significantly improving adversarial robustness without harming clean accuracy across various CLIP backbones and benchmarks.

- **[arXiv251219] Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics**
  - **tags:** [ai], [natural language processing], [text simplification, prompt engineering, fine-tuning, Flesch-Kincaid, SMOG, SARI, BERTScore, G-Eval, Likert scale]
  - **authors:** Primoz Kocbek, Leon Kopitar, Gregor Stiglic
  - **institution:** University of Maribor, University of Ljubljana, University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.16530
  - **Simple LLM Summary:** This paper investigates using Large Language Models (LLMs) like GPT-4o to simplify biomedical texts for better health literacy, comparing methods such as prompt templates, a two-agent approach, and fine-tuning. It concludes that the GPT-4o-mini model performed best, while fine-tuning underperformed, and that the LLM-based G-Eval metric aligned well with human qualitative assessments.

- **[arXiv251219] From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment**
  - **tags:** [ai], [fairness and bias in ai], [memory-enhanced personalization, large language models, AI agents, bias simulation, recruitment]
  - **authors:** Himanshu Gharat, Himanshi Agrawal, Gourab K. Patro
  - **institution:** Phi Labs, Quantiphi Inc.
  - **link:** https://arxiv.org/pdf/2512.16532
  - **Simple LLM Summary:** The paper simulates the behavior of memory-enhanced personalized AI agents using safety-trained LLMs in a recruitment use case. It finds that bias is systematically introduced and reinforced through personalization, highlighting the need for additional protective measures in such agents.

- **[arXiv251219] Scaling Laws for Energy Efficiency of Local LLMs**
  - **tags:** [mlsys], [llm inference], [model compression, preprocessing, CPU-only inference, edge AI, quantum-inspired compression, scaling laws]
  - **authors:** Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Samuel Mugel, Román Orús
  - **institution:** Multiverse Computing, Donostia International Physics Center, Ikerbasque Foundation for Science
  - **link:** https://arxiv.org/pdf/2512.16531
  - **Simple LLM Summary:** The paper systematically benchmarks LLMs and VLMs on CPU-only hardware (MacBook Pro M2 and Raspberry Pi 5) to uncover scaling laws for computational load. It finds that computational cost scales linearly with token length for LLMs, and VLMs exhibit a "resolution knee" due to preprocessing. The study also demonstrates that quantum-inspired compression can significantly reduce processor usage, memory usage, and energy consumption while preserving accuracy.

- **[arXiv251219] Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild**
  - **tags:** [mlsys], [others], [fuzzy exploratory search, benchmark, web retrieval, semantic ambiguity, agent evaluation]
  - **authors:** Yumeng Wang, Tianyu Fan, Lingrui Xu, Chao Huang
  - **institution:** Tsinghua University, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.16553
  - **Simple LLM Summary:** This paper introduces the "Needle in the Web" benchmark, designed to evaluate search agents and LLMs on retrieving web pages for ambiguous, exploratory queries. The method involves generating queries of controllable difficulty based on factual claims from web content. The main conclusion is that current LLMs and search agents struggle significantly with this fuzzy retrieval task, achieving low accuracy and highlighting it as an open challenge.

- **[arXiv251219] Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics**
  - **tags:** TBD
  - **authors:** Iker García-Ferrero, David Montero, Roman Orus
  - **institution:** 
  - **link:** https://arxiv.org/pdf/2512.16602

- **[arXiv251219] Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks**
  - **tags:** [mlsys], [diffusion inference], [diffusion model, Swin-transformer, text-conditioned generation, U-shaped architecture, adapted time step]
  - **authors:** Shaohua Wu, Tong Yu, Shenling Wang, Xudong Zhao
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2512.16586
  - **Simple LLM Summary:** This paper proposes Yuan-TecSwin, a text-to-image diffusion model that replaces CNN blocks with Swin-transformer blocks in its U-shaped architecture to better capture long-range semantic dependencies. It improves text-image alignment and uses an adapted time step strategy, achieving a state-of-the-art FID score of 1.37 on ImageNet, with generated images being highly photorealistic and difficult for humans to distinguish from real paintings.

- **[arXiv251219] Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents**
  - **tags:** [mlsys], [multi-modal inference], [AI forensic agents, uncertainty-aware assessments, detector orchestration, multimedia forensics, authenticity verification]
  - **authors:** Giulia Boato, Andrea Montibeller, Edward Delp, Luisa Verdoliva, Daniele Miorandi
  - **institution:** Truebees, University of Trento, Purdue University, University of Naples Federico II
  - **link:** https://arxiv.org/pdf/2512.16614
  - **Simple LLM Summary:** The paper proposes a framework for AI forensic agents that autonomously orchestrate multiple forensic detectors to verify the authenticity of multimedia content. It argues that a holistic, uncertainty-calibrated approach is necessary to address the challenges posed by generative AI, moving beyond isolated, single-purpose detectors. The main conclusion is that such explainable, uncertainty-aware agents can improve the trustworthiness and interpretability of the forensic verification process.

- **[arXiv251219] Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game**
  - **tags:** [ai], [preference optimization], [Stackelberg game, sequential-move game, inference-time refinement, preference models, Nash equilibrium, Bradley-Terry model]
  - **authors:** Barna Pásztor, Thomas Kleine Buening, Andreas Krause
  - **institution:** ETH Zürich
  - **link:** https://arxiv.org/pdf/2512.16626
  - **Simple LLM Summary:** This paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework that frames preference optimization as a sequential-move game between a Leader and a Follower policy. It demonstrates that this approach offers advantages in consistency, data sensitivity, and robustness to intransitive preferences compared to RLHF and NLHF. Experiments on large language models show that SLHF achieves strong alignment and enables inference-time refinements that transfer across model families.

- **[arXiv251219] Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam**
  - **tags:** [mlsys], [others], [reinforcement learning, q-learning, sentence-transformers, natural language processing, crisp-dm, flask, flutter]
  - **authors:** Wisnu Uriawan, Aria Octavian Hamza, Ade Ripaldi Nuralim, Adi Purnama, Ahmad Juaeni Yunus, Anissya Auliani Supriadi Putri
  - **institution:** UIN Sunan Gunung Djati Bandung
  - **link:** https://arxiv.org/pdf/2512.16644
  - **Simple LLM Summary:** This paper implements a Sharia-compliant chatbot using Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic matching on a curated Islamic QA dataset. The prototype, built with Flask and Flutter, achieved 87% semantic accuracy, demonstrating its potential as a digital consultation tool for verified Islamic knowledge.

- **[arXiv251219] Comprehensive AI Literacy: The Case for Centering Human Agency**
  - **tags:** [ai], [education], [AI literacy, human agency, critical thinking, epistemology, educational frameworks]
  - **authors:** Sri Yash Tadimalla, Justin Cary, Gordon Hull, Jordan Register, Daniel Maxwell, David Pugalee, Tina Heafner
  - **institution:** UNC Charlotte
  - **link:** https://arxiv.org/pdf/2512.16656
  - **Simple LLM Summary:** This position paper proposes a shift towards comprehensive AI literacy frameworks that center human agency and critical thinking in education. It concludes that educators and students must be empowered to make intentional, critical choices about AI use, rather than focusing solely on operational skills.

- **[arXiv251219] Prefix Probing: Lightweight Harmful Content Detection for Large Language Models**
  - **tags:** [mlsys], [llm inference], [prefix probing, black-box detection, log-probability comparison, prefix caching, prefix construction algorithm, harmful content detection]
  - **authors:** Jirui Yang, Hengqi Guo, Zhihui Lu, Yi Zhao, Yuansen Zhang, Shijing Hu, Qiang Duan, Yinggui Wang, Tao Wei
  - **institution:** Fudan University, Ant Group, Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.16650
  - **Simple LLM Summary:** This paper introduces Prefix Probing, a lightweight method for detecting harmful content in LLMs by comparing the model's log-probabilities for "agreement" versus "refusal" response prefixes. It uses prefix caching to reduce latency and an algorithm to automatically construct effective probe prefixes. The method achieves detection accuracy comparable to external safety models with minimal computational overhead, requiring no extra model deployment.

- **[arXiv251219] Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking**
  - **tags:** [mlsys], [others], [white-box watermarking, chaotic sequences, logistic map, genetic algorithm, weight injection]
  - **authors:** Sangeeth B, Serena Nicolazzo, Deepa K., Vinod P
  - **institution:** Cochin University of Science and Technology, University of Eastern Piedmont
  - **link:** https://arxiv.org/pdf/2512.16658
  - **Simple LLM Summary:** The paper proposes a white-box watermarking method for deep neural networks that embeds ownership information into model weights using chaotic sequences generated by a logistic map. Ownership verification is performed via a genetic algorithm to recover the original chaotic parameters. The approach maintains model accuracy and remains detectable after fine-tuning, offering a scalable solution for intellectual property protection.

- **[arXiv251219] Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance**
  - **tags:** [mlsys], [llm inference], [graph neural networks, graph attention mechanisms, retrieval-augmented generation, knowledge graphs, attention-based pruning, subgraph retrieval]
  - **authors:** Jacob Reiss, Shikshya Shiwakoti, Samuel Goldsmith, Ujjwal Pandit
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.16661
  - **Simple LLM Summary:** The paper proposes an Attention-Based Subgraph Retriever, a GNN-as-retriever model that uses attention pruning to extract a refined subgraph from a knowledge graph, which is then passed to a large language model for advanced reasoning. This approach aims to improve research paper recommendation by leveraging relational and structural information from graphs to enhance retrieval-augmented generation and mitigate LLM hallucinations.

- **[arXiv251219] Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray**
  - **tags:** [ai], [medical imaging], [subject fingerprinting, triplet margin loss, ResNet-50, few-shot learning, latent space embedding]
  - **authors:** Gonçalo Gaspar Alves, Shekoufeh Gorgi Zadeh, Andreas Husch, Ben Bausch
  - **institution:** University of Luxembourg
  - **link:** https://arxiv.org/pdf/2512.16685
  - **Simple LLM Summary:** This paper proposes a subject fingerprinting method for re-identifying individuals across medical datasets to prevent data leakage. Using a ResNet-50 model trained with triplet margin loss to map images to a latent space, the approach achieves high few-shot re-identification accuracy on both 3D MRI and 2D X-ray datasets.

- **[arXiv251219] Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm**
  - **tags:** [ai], [text mining], [apriori algorithm, unsupervised learning, association rule mining, support, confidence, lift, stemming, tokenization]
  - **authors:** Wisnu Uriawan, Achmad Ajie Priyajie, Angga Gustian, Fikri Nur Hidayat, Sendi Ahmad Rafiudin, Muhamad Fikri Zaelani
  - **institution:** UIN Sunan Gunung Djati Bandung
  - **link:** https://arxiv.org/pdf/2512.16694
  - **Simple LLM Summary:** This paper applies the unsupervised Apriori algorithm for association rule mining to automatically discover thematic patterns in Indonesian-translated Hadith texts. The method involves text preprocessing and uses support, confidence, and lift parameters to identify semantic relationships. The main conclusion is that the Apriori algorithm can effectively uncover latent thematic associations, such as between prayer and rakaat, contributing to digital Islamic studies.

- **[arXiv251219] Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences**
  - **tags:** [ai], [education technology], [Cyber Humanism, algorithmic citizenship, prompt-based learning, AI literacy, socio-technical infrastructure, epistemic agency]
  - **authors:** Giovanni Adorni
  - **institution:** University of Genoa
  - **link:** https://arxiv.org/pdf/2512.16701
  - **Simple LLM Summary:** This paper proposes the Cyber Humanism in Education framework, which conceptualizes AI-enabled learning environments as co-authored socio-technical infrastructures and promotes practices like prompt-based learning. It concludes that this approach can strengthen human epistemic agency and algorithmic citizenship in education, while also highlighting tensions around workload, equity, and governance that need to be addressed.

- **[arXiv251219] Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning**
  - **tags:** [ai], [multimodal reasoning], [multi-agent pipelines, single-agent pipelines, zero-shot evaluation, multimodal large language models (MLLMs), geometry problem solving]
  - **authors:** Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Mohammad Nehad Alam, Proma Hossain Progga, Swakkhar Shatabda
  - **institution:** BRAC University, United International University, Independent University, Bangladesh, Spectrum Software & Consulting Ltd
  - **link:** https://arxiv.org/pdf/2512.16698
  - **Simple LLM Summary:** This paper systematically compares single-agent and multi-agent frameworks for solving diagram-based geometry problems using multimodal large language models (MLLMs) in a zero-shot setting. It finds that multi-agent pipelines consistently improve performance for open-source models like Qwen-2.5-VL, while advanced closed-source models like Gemini-2.0-Flash generally perform better in single-agent mode, showing that agentic decomposition is not universally optimal.

- **[arXiv251219] Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems**
  - **tags:** [ai], [computational theory], [Gödel incompleteness, Lyapunov exponent, prediction horizon, algorithmic intelligence]
  - **authors:** Abhisek Ganguly
  - **institution:** Jawaharlal Nehru Centre for Advanced Scientific Research
  - **link:** https://arxiv.org/pdf/2512.16707
  - **Simple LLM Summary:** The paper formalizes two computational limitations—formal incompleteness from logic and dynamical unpredictability from chaos theory—to analyze algorithmic intelligence. It demonstrates that these constraints jointly bound an agent's ability to reason about its own predictive capabilities, concluding that an algorithmic agent generally cannot compute its own maximal prediction horizon.

- **[arXiv251219] Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library**
  - **tags:** [mlsys], [others], [predictive process mining, deep learning, PyTorch, LSTM, Transformer, reproducibility, benchmarking]
  - **authors:** Oliver Stritzel, Nick Hühnerbein, Simon Rauch, Itzel Zarate, Lukas Fleischmann, Moike Buck, Attila Lischka, Christian Frey
  - **institution:** Fraunhofer IIS, University of Technology Nuremberg, Chalmers University of Technology
  - **link:** https://arxiv.org/pdf/2512.16715
  - **Simple LLM Summary:** This paper introduces SPICE, a Python framework that reimplements three popular deep-learning-based methods for Predictive Process Mining in PyTorch within a common, configurable base. It aims to address reproducibility and benchmarking issues in the field. The framework is validated by comparing its performance to original reported metrics on 11 datasets.

- **[arXiv251219] Discovering and Learning Probabilistic Models of Black-Box AI Capabilities**
  - **tags:** [ai], [planning and reasoning], [PDDL, Monte-Carlo tree search, probabilistic models, black-box AI, symbolic models]
  - **authors:** Daniel Bramblett, Rushang Karia, Adrian Ciotinga, Ruthvick Suresh, Pulkit Verma, YooJung Choi, Siddharth Srivastava
  - **institution:** Arizona State University, Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.16733
  - **Simple LLM Summary:** This paper introduces a method to learn probabilistic models of black-box AI systems' planning capabilities using PDDL-style representations and Monte-Carlo tree search to systematically test and prune hypothesis spaces. The learned models describe the conditions and probabilistic outcomes of the AI's capabilities, with theoretical guarantees of soundness and convergence. Empirical results demonstrate the method's effectiveness in accurately modeling various BBAI systems.

- **[arXiv251219] AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach**
  - **tags:** [mlsys], [multi-modal inference], [hybrid machine learning, large language model, electronic health records, pain prediction, clinical decision support]
  - **authors:** Yipeng Zhuang, Yifeng Guo, Yuewen Li, Yuheng Wu, Philip Leung-Ho Yu, Tingting Song, Zhiyong Wang, Kunzhong Zhou, Weifang Wang, Li Zhuang
  - **institution:** The University of Hong Kong, Peking University Cancer Hospital Yunnan Hospital (The Third Affiliated Hospital of Kunming Medical University), City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.16739
  - **Simple LLM Summary:** This paper proposes a hybrid pipeline combining machine learning and a large language model to predict cancer pain episodes from structured and unstructured electronic health record data. The method integrates temporal medication trends with the interpretation of ambiguous dosing records and clinical notes. The hybrid approach improved prediction sensitivity, demonstrating a scalable tool for early pain forecasting in oncology care.

- **[arXiv251219] Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error**
  - **tags:** [ai], [human-ai interaction], [human evaluation, epistemic failure, plausibility, hermeneutic error, co-construction, verification burden, cognitive drift]
  - **authors:** Claudia Vale Oliveira, Nelson Zagalo, Filipe Silva, Anabela Brandao, Syeda Faryal Hussain Khurrum, Joaquim Santos
  - **institution:** University of Aveiro
  - **link:** https://arxiv.org/pdf/2512.16750
  - **Simple LLM Summary:** The paper uses a multi-round, multi-LLM evaluation with interdisciplinary tasks to study how humans interpret model responses. It concludes that LLM errors are co-constructed through model-generated plausibility and human interpretive shortcuts, shifting error analysis from predictive metrics to a relational, hermeneutic process.

- **[arXiv251219] TreeNet: A Light Weight Model for Low Bitrate Image Compression**
  - **tags:** [mlsys], [others], [binary tree-structured encoder-decoder, attentional feature fusion, low bitrate compression, rate-distortion optimization, quantization, entropy coding]
  - **authors:** Mahadev Prasad Panda, Purnachandra Rao Makkena, Srivatsa Prativadibhayankaram, Siegfried Fößel, André Kaup
  - **institution:** Fraunhofer Institute for Integrated Circuits IIS, Friedrich-Alexander-Universität Erlangen-Nürnberg
  - **link:** https://arxiv.org/pdf/2512.16743
  - **Simple LLM Summary:** The paper proposes TreeNet, a lightweight image compression model using a binary tree-structured encoder-decoder with attentional feature fusion to reduce complexity. It achieves a 4.83% BD-rate improvement over JPEG AI at low bitrates while reducing model complexity by 87.82%.

- **[arXiv251219] CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?**
  - **tags:** [ai], [embodied navigation], [CitySeeker benchmark, implicit human needs, spatial reasoning, decision-making, Backtracking Mechanisms, Enriching Spatial Cognition, Memory-Based Retrieval (BCR)]
  - **authors:** Siqi Wang, Chao Liang, Yunfan Gao, Erxin Yu, Sen Li, Yushi Li, Jing Li, Haofen Wang
  - **institution:** The Hong Kong Polytechnic University, Tongji University, Nanjing Institute of Geography and Limnology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.16755
  - **Simple LLM Summary:** This paper introduces the CitySeeker benchmark to evaluate Vision-Language Models (VLMs) on embodied urban navigation tasks that require interpreting implicit human needs. The core analysis involves exploring strategies like Backtracking, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR) to address model bottlenecks. The main conclusion is that even top-performing VLMs struggle with these tasks, achieving only 21.1% completion, due to issues in long-horizon reasoning, spatial cognition, and experiential recall.

- **[arXiv251219] GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation**
  - **tags:** [mlsys], [others], [natural language to temporal logic translation, grounding model, system signature, hierarchical classification, masked language models, logical equivalence]
  - **authors:** William English, Chase Walker, Dominic Simon, Rickard Ewetz
  - **institution:** University of Florida
  - **link:** https://arxiv.org/pdf/2512.16770
  - **Simple LLM Summary:** The paper proposes GinSign, a framework that grounds natural language into system signatures for temporal logic translation by using a hierarchical grounding model. This model decomposes the task into structured classification, allowing the use of smaller masked language models instead of large LLMs. Experiments show the framework achieves 95.5% grounded logical-equivalence, a 1.4x improvement over prior methods, enabling reliable downstream model checking.

- **[arXiv251219] Towards Mass Spectrum Analysis with ASP**
  - **tags:** [ai], [combinatorial search], [Answer Set Programming (ASP), symmetry-breaking, canonical representations, molecular structure, mass spectrometry]
  - **authors:** Nils Küchenmeister, Alex Ivliev, Markus Krötzsch
  - **institution:** TU Dresden
  - **link:** https://arxiv.org/pdf/2512.16780
  - **Simple LLM Summary:** This paper introduces a novel application of Answer Set Programming (ASP) to determine molecular structures from mass spectrometry data by encoding chemical knowledge and using canonical representations to break symmetries and constrain the combinatorial search space. The authors evaluate their method on known structures and compare its performance to other ASP symmetry-breaking techniques and a commercial chemistry tool. The approach effectively reduces redundant solutions and demonstrates the utility of ASP for this chemical analysis problem.

- **[arXiv251219] KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals**
  - **tags:** [ai], [motion tracking], [state space model, kinematics-guided bidirectional scanning, mixed spatiotemporal representation learning, geometric angular velocity loss]
  - **authors:** Shuting Zhao, Zeyu Xiao, Xinrong Chen
  - **institution:** Fudan University
  - **link:** https://arxiv.org/pdf/2512.16791
  - **Simple LLM Summary:** The paper proposes KineST, a kinematics-guided state space model for full-body motion tracking from sparse AR/VR signals. Its core innovations include a kinematics-guided bidirectional scanning strategy and mixed spatiotemporal representation learning to balance accuracy and smoothness. Experiments show the method achieves superior accuracy and temporal consistency within a lightweight framework.

- **[arXiv251219] Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint**
  - **tags:** [sys], [edge computing], [mixed integer linear programming, heuristic algorithm, task offloading, server deployment, budget constraint]
  - **authors:** Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham
  - **institution:** Sebelas Maret University, Curtin University, University of Wollongong
  - **link:** https://arxiv.org/pdf/2512.16792
  - **Simple LLM Summary:** The paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework, solved via an optimal Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). The main conclusion is that the heuristic solution performs close to optimal for small networks and significantly outperforms alternative strategies in large-scale networks, improving task satisfaction by up to 21.57% under budget and demand growth constraints.

- **[arXiv251219] From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, deductive reasoning, conflict-aware trust-score, reasoning-trace-augmented framework, supervised fine-tuning]
  - **authors:** Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.16795
  - **Simple LLM Summary:** This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems.

- **[arXiv251219] Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, QMIX, reactive jamming, channel hopping, power control, Upper Confidence Bound]
  - **authors:** Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella
  - **institution:** Nexcepta
  - **link:** https://arxiv.org/pdf/2512.16813
  - **Simple LLM Summary:** This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.

- **[arXiv251219] Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs**
  - **tags:** [ai], [natural language processing], [Grammar Forced Translation (GraFT), temporal logic translation, solution space reduction, atomic propositions lifting, large language models]
  - **authors:** William English, Dominic Simon, Sumit Kumar Jha, Rickard Ewetz
  - **institution:** University of Florida, Florida International University
  - **link:** https://arxiv.org/pdf/2512.16814
  - **Simple LLM Summary:** The paper proposes Grammar Forced Translation (GraFT), a framework that improves the translation of natural language to temporal logic by restricting the language model's valid output tokens at each step, thereby reducing the solution space. This method enhances both lifting of atomic propositions and the final translation phase. The results show that GraFT improves end-to-end and out-of-domain translation accuracy compared to state-of-the-art approaches.

- **[arXiv251219] Next-Generation License Plate Detection and Recognition System using YOLOv8**
  - **tags:** [mlsys], [others], [YOLOv8, license plate detection, character recognition, object detection, custom character sequencing, edge devices]
  - **authors:** Arslan Amin, Rafia Mumtaz, Muhammad Jawad Bashir, Syed Mohammad Hassan Zaidi
  - **institution:** National University of Sciences and Technology (NUST), Ghulam Ishaq Khan Institute of Engineering Sciences and Technology (GIKI)
  - **link:** https://arxiv.org/pdf/2512.16826
  - **Simple LLM Summary:** This paper proposes a license plate detection and recognition system using YOLOv8 variants, with YOLOv8 Nano for plate detection and YOLOv8 Small for character recognition, along with a custom method for sequencing characters. The optimized pipeline achieves high precision and mAP50 scores, demonstrating computational efficiency suitable for deployment on edge devices in Intelligent Transportation Systems.

- **[arXiv251219] LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference**
  - **tags:** [mlsys], [llm inference], [layer-wise caching, semantic similarity, fingerprinting, adaptive eviction, key-value cache, transformer acceleration]
  - **authors:** Harsh Vardhan Bansal
  - **institution:** Amazon Web Services
  - **link:** https://arxiv.org/pdf/2512.16843
  - **Simple LLM Summary:** This paper introduces LLMCache, a model-agnostic layer-wise caching framework that accelerates transformer inference by reusing intermediate activations for semantically similar inputs. It uses a lightweight fingerprinting mechanism for matching and adaptive strategies for cache management. Experiments show up to 3.1x inference speedup with minimal accuracy degradation, demonstrating its practicality for real-world deployment.

- **[arXiv251219] OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction**
  - **tags:** [mlsys], [multi-modal training], [tactile sensing, egocentric video, cross-modal alignment, grasp understanding, retrieval benchmarks, classification benchmarks]
  - **authors:** Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang
  - **institution:** MIT, Duke University, Brown University, University of Washington, Harvard University
  - **link:** https://arxiv.org/pdf/2512.16842
  - **Simple LLM Summary:** The paper introduces OpenTouch, a multimodal dataset combining synchronized in-the-wild egocentric video, full-hand tactile signals, and hand-pose data. It uses this dataset to create benchmarks for retrieval and classification, demonstrating that tactile information is a powerful cue for understanding grasps and improving cross-modal alignment. The main conclusion is that touch data significantly grounds perception and action, advancing research in multimodal egocentric perception and contact-rich robotics.

- **[arXiv251219] Meta-RL Induces Exploration in Language Agents**
  - **tags:** [ai], [meta-reinforcement learning], [meta-rl, cross-episode training, in-context policy adaptation, exploration, large language model agents]
  - **authors:** Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic
  - **institution:** EPFL, ETH Zurich, Idiap Research Institute
  - **link:** https://arxiv.org/pdf/2512.16848
  - **Simple LLM Summary:** This paper introduces LaMer, a Meta-RL framework designed to enhance exploration in LLM-based agents through cross-episode training and in-context policy adaptation via reflection. The method enables agents to actively learn from environmental feedback without gradient updates. Experiments show LaMer outperforms RL baselines and generalizes better to novel or challenging tasks.

- **[arXiv251219] PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy**
  - **tags:** [mlsys], [others], [explainable AI, differential privacy, membership inference attack, re-identification attack, post-hoc explanations, feature selection, transformer models]
  - **authors:** Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.16851
  - **Simple LLM Summary:** The paper proposes a framework that uses explainable AI (XAI) to identify the most influential features in AI-XR models and selectively applies differential privacy (DP) to those features during inference to defend against privacy attacks. This XAI-guided DP approach reduces the success rates of membership inference and re-identification attacks while preserving model accuracy and improving inference time compared to traditional DP. The method is deployed as a system called PrivateXR on an HTC VIVE Pro headset, allowing users to adjust privacy levels in real-time during XR gameplay.

- **[arXiv251219] TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge**
  - **tags:** [mlsys], [llm inference], [model compression, quantization, pruning, signal temporal logic, bayesian optimization]
  - **authors:** Khurram Khalil, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.16855
  - **Simple LLM Summary:** This paper introduces TOGGLE, a framework that uses Signal Temporal Logic (STL) and Bayesian optimization to guide the layer-wise compression of Large Language Models through quantization and pruning. It formally enforces linguistic properties during compression without requiring retraining. The method achieves significant reductions in model size and computational cost while preserving specified model behaviors, enabling verifiable deployment on edge devices.

- **[arXiv251219] Distributional AGI Safety**
  - **tags:** [ai], [multi-agent systems], [distributional AGI safety, patchwork AGI, virtual agentic sandbox economies, market mechanisms, auditability, reputation management, oversight]
  - **authors:** Nenad Tomašev, Matija Franklin, Julian Jacobs, Sébastien Krier, Simon Osindero
  - **institution:** Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.16856
  - **Simple LLM Summary:** The paper proposes a safety framework centered on virtual agentic sandbox economies, governed by market mechanisms with auditability and reputation management, to address risks from AGI emerging through the coordination of multiple sub-AGI agents. It concludes that this "distributional AGI safety" approach is urgently needed as an alternative to methods focused on aligning a single monolithic AGI.

- **[arXiv251219] ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning**
  - **tags:** [mlsys], [others], [imitation learning, reinforcement learning, task decomposition, motion planning, data generation, skill policies]
  - **authors:** Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett
  - **institution:** University of Toronto, Vector Institute, Georgia Institute of Technology, NVIDIA Research
  - **link:** https://arxiv.org/pdf/2512.16861
  - **Simple LLM Summary:** The paper proposes ReinforceGen, a hybrid system for long-horizon robot manipulation that first decomposes tasks into skills trained via imitation learning on generated data, and then refines them using reinforcement learning. It achieves an 80% success rate on benchmark tasks, with fine-tuning contributing to an 89% average performance increase.

- **[arXiv251219] GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation**
  - **tags:** [ai], [text-to-image evaluation], [benchmark drift, GenEval, GenEval 2, Soft-TIFA, VQAScore, model-based evaluation, visual primitives, compositionality]
  - **authors:** Amita Kamath, Kai-Wei Chang, Ranjay Krishna, Luke Zettlemoyer, Yushi Hu, Marjan Ghazvininejad
  - **institution:** FAIR at Meta, University of Washington, University of California, Los Angeles, Allen Institute for AI
  - **link:** https://arxiv.org/pdf/2512.16853
  - **Simple LLM Summary:** The paper identifies benchmark drift as a major issue in automated text-to-image evaluation, showing that the popular GenEval benchmark has become misaligned with human judgment. To address this, the authors introduce a new benchmark, GenEval 2, with more challenging prompts and a new evaluation method called Soft-TIFA, which aggregates judgments on visual primitives and shows better alignment with human scores. The work concludes that continual audits and updates are essential for maintaining valid automated evaluation benchmarks.

- **[arXiv251219] Sequencing to Mitigate Catastrophic Forgetting in Continual Learning**
  - **tags:** [ai], [continual learning], [task sequencing, zero-shot scoring, neural architecture search]
  - **authors:** Hesham G. Moussa, Aroosa Hameed, Arashmid Akhavain
  - **institution:** Huawei Technologies Canada
  - **link:** https://arxiv.org/pdf/2512.16871
  - **Simple LLM Summary:** This paper proposes a method to mitigate catastrophic forgetting in continual learning by determining an optimal task sequence. The method uses zero-shot scoring algorithms inspired by neural architecture search. The results show that intelligent task sequencing can substantially reduce forgetting and enhance performance when combined with traditional continual learning strategies.

- **[arXiv251219] Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models**
  - **tags:** [mlsys], [others], [knowledge distillation, active learning, causal reasoning, semi-supervised learning, online learning]
  - **authors:** Jiabin Xue
  - **institution:** University of Strathclyde
  - **link:** https://arxiv.org/pdf/2512.16866
  - **Simple LLM Summary:** This paper proposes Knowledge Transformation (KT), a hybrid method for semi-supervised online learning on edge devices that combines knowledge distillation, active learning, and causal reasoning to generate pseudo-labels from a teacher model for a student model. The experiments show that a student model can achieve its expected maximum performance when provided with a stable teacher model. The method is beneficial when teacher tasks are generic or student task labels are difficult to acquire.

- **[arXiv251219] The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI**
  - **tags:** [mlsys], [others], [control theory, constraint-based formulation, safety envelope, runtime monitoring, auditing mechanisms, socio-technical systems, closed-loop supervisory control, fairness, autonomy, cognitive burden]
  - **authors:** Otman A. Basir
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.16873
  - **Simple LLM Summary:** The paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that models responsible AI governance as a closed-loop supervisory control problem, integrating design-time constraints with runtime monitoring. It translates normative principles like fairness and autonomy into enforceable engineering mechanisms through a unified constraint-based formulation. The main conclusion is that the framework provides a practical, actionable foundation for building accountable, adaptive, and auditable socio-technical AI systems.

- **[arXiv251219] Pixel Seal: Adversarial-only training for invisible image and video watermarking**
  - **tags:** [mlsys], [multi-modal training], [adversarial-only training, three-stage training schedule, JND-based attenuation, temporal watermark pooling]
  - **authors:** Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko
  - **institution:** Meta FAIR
  - **link:** https://arxiv.org/pdf/2512.16874
  - **Simple LLM Summary:** Pixel Seal introduces an adversarial-only training paradigm for invisible watermarking, eliminating unreliable perceptual losses and using a three-stage schedule to decouple robustness and imperceptibility. It addresses high-resolution scaling with JND-based attenuation and adapts to video via temporal pooling. The method achieves state-of-the-art robustness and imperceptibility for image and video watermarking.

- **[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies**
  - **tags:** [mlsys], [others], [federated learning, immunofluorescence microscopy, collagen VI-related dystrophies, rare disease diagnosis, decentralized training]
  - **authors:** Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria
  - **institution:** National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai
  - **link:** https://arxiv.org/pdf/2512.16876
  - **Simple LLM Summary:** The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.

- **[arXiv251219] LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation**
  - **tags:** [mlsys], [multi-modal inference], [cross-layer knowledge fusion MoE, VLLM, world-knowledge representation, token extraction, layer-wise fusion]
  - **authors:** Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu
  - **institution:** Northeastern University, LinkedIn, University of Wisconsin–Madison
  - **link:** https://arxiv.org/pdf/2512.16891
  - **Simple LLM Summary:** The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.

- **[arXiv251219] Impacts of Racial Bias in Historical Training Data for News AI**
  - **tags:** [ai], [algorithmic auditing], [multi-label classifier, explainable AI, word2vec, New York Times Annotated Corpus]
  - **authors:** Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza
  - **institution:** Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group
  - **link:** https://arxiv.org/pdf/2512.16901
  - **Simple LLM Summary:** The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic "blacks" label acts as a general "racism detector" but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.

- **[arXiv251219] Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos**
  - **tags:** [mlsys], [multi-modal training], [reasoning-to-motion framework, trajectory-token interface, stage-aware trajectory prediction, 6DoF trajectory generation, vision-language reasoning, progressive training]
  - **authors:** Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu
  - **institution:** Meta, University of Washington
  - **link:** https://arxiv.org/pdf/2512.16907
  - **Simple LLM Summary:** The paper introduces the EgoMAN dataset and model, a reasoning-to-motion framework that links vision-language reasoning with motion generation via a trajectory-token interface to predict 3D hand trajectories. It demonstrates that progressive training aligning reasoning with motion dynamics yields accurate, stage-aware trajectories with generalization across real-world scenes.

- **[arXiv251219] Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning**
  - **tags:** [ai], [reinforcement learning], [behavioral cloning, posterior behavioral cloning, policy pretraining, RL finetuning, generative models]
  - **authors:** Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine
  - **institution:** UC Berkeley, Stanford
  - **link:** https://arxiv.org/pdf/2512.16911
  - **Simple LLM Summary:** This paper introduces Posterior Behavioral Cloning (PostBC), a method that pretrains a policy by modeling the posterior distribution of demonstrator behavior from a dataset, rather than exactly matching it. This ensures better coverage of demonstrator actions, which serves as a more effective initialization for subsequent reinforcement learning finetuning. The method is shown to improve RL finetuning performance on robotic control benchmarks and real-world manipulation tasks compared to standard behavioral cloning.

- **[arXiv251219] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward**
  - **tags:** [ai], [reinforcement learning], [RLVR, GRPO, policy entropy, spurious rewards, clipping bias, exploration-exploitation trade-off]
  - **authors:** Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin
  - **institution:** Columbia University, The Chinese University of Hong Kong, Shenzhen, Alibaba DAMO Academy, New York University Stern School of Business
  - **link:** https://arxiv.org/pdf/2512.16912
  - **Simple LLM Summary:** This paper investigates the exploration-exploitation trade-off in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning. It finds that spurious rewards, combined with clipping bias, reduce policy entropy to produce more confident outputs, which enhances performance, while entropy minimization alone is insufficient. The authors propose a reward-misalignment model to explain why spurious rewards can be beneficial beyond simple data contamination.

- **[arXiv251219] Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning**
  - **tags:** [mlsys], [post-training], [adversarial reinforcement learning, process reward models, step-level rewards, reasoning chain partitioning, joint training, discriminator]
  - **authors:** Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille
  - **institution:** Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.16917
  - **Simple LLM Summary:** The paper introduces Generative Adversarial Reasoner, a framework that jointly trains an LLM reasoner and an LLM-based discriminator using adversarial reinforcement learning to provide dense, step-level rewards for improving reasoning. This method enhances sample efficiency and reasoning quality by co-evolving the models to detect and correct process errors. It demonstrates consistent performance gains on mathematical benchmarks over standard RL post-training baselines.

- **[arXiv251219] DVGT: Driving Visual Geometry Transformer**
  - **tags:** [ai], [autonomous driving, 3D reconstruction], [transformer, DINO backbone, cross-view spatial attention, cross-frame temporal attention, multi-view geometry, point map]
  - **authors:** Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu
  - **institution:** Tsinghua University, Xiaomi EV, University of Macau, Peking University
  - **link:** https://arxiv.org/pdf/2512.16919
  - **Simple LLM Summary:** The paper proposes DVGT, a transformer-based model that reconstructs a global 3D point map from unposed multi-view image sequences using alternating attention mechanisms. It eliminates reliance on precise camera parameters and external sensor alignment, achieving superior performance across diverse driving scenarios.

- **[arXiv251219] EasyV2V: A High-quality Instruction-based Video Editing Framework**
  - **tags:** [mlsys], [multi-modal training], [instruction-based video editing, LoRA fine-tuning, sequence concatenation, spatiotemporal mask control, reference image conditioning, data curation]
  - **authors:** Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei
  - **institution:** KAUST, Snap Inc.
  - **link:** https://arxiv.org/pdf/2512.16920
  - **Simple LLM Summary:** EasyV2V is a framework for instruction-based video editing that simplifies architecture by using pretrained text-to-video models with light LoRA fine-tuning and sequence concatenation for conditioning. It unifies control through a single mask mechanism and supports flexible inputs like text, masks, and reference images. The method achieves state-of-the-art results by effectively curating diverse video data and leveraging existing model capabilities.

- **[arXiv251219] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, model auditing, capability gap discovery, counterfactual generation]
  - **authors:** Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu
  - **institution:** Google, Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.16921
  - **Simple LLM Summary:** The paper introduces AuditDM, a framework that uses reinforcement learning to fine-tune a multimodal LLM as an "auditor" to generate challenging questions and counterfactual images that expose model weaknesses. This automated auditing process discovers interpretable failure cases, which are then used as annotation-free data for rectification. The method successfully identifies over 20 failure types and improves model performance, enabling a smaller 3B model to surpass a larger 28B model, suggesting targeted auditing is an effective path for model improvement as data scaling yields diminishing returns.

- **[arXiv251219] FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction**
  - **tags:** [mlsys], [llm inference], [multi-agent system, large language model, chain-of-draft, federal funds rate prediction, FOMC simulation]
  - **authors:** Yuhan Hou, Tianji Rao, Jeremy Tan, Adler Viton, Xiyue Zhang, David Ye, Abhishek Kodi, Sanjana Dulam, Aditya Paul, Yikai Feng
  - **institution:** Duke University, BNY AI Hub
  - **link:** https://arxiv.org/pdf/2512.15728
  - **Simple LLM Summary:** This paper introduces FedSight AI, a multi-agent system that uses large language models to simulate FOMC deliberations by analyzing structured and unstructured data, with a Chain-of-Draft extension to improve reasoning. The system achieved high accuracy and stability in predicting federal funds target rates, outperforming baseline models while providing transparent reasoning aligned with real committee communications.

- **[arXiv251219] Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network**
  - **tags:** [mlsys], [multi-modal inference], [integrated perception communication computation, cloud-edge-end collaboration, parameter-efficient adaptation, multimodal fusion, intelligent base station agent, large AI model]
  - **authors:** Zhuoran Li, Zhen Gao, Xinhua Liu, Zheng Wang, Xiaotian Zhou, Lei Liu, Yongpeng Wu, Wei Feng, Yongming Huang
  - **institution:** Beijing Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.15109
  - **Simple LLM Summary:** This paper proposes using Large AI Models (LAMs) to transform base stations into Intelligent Base Station Agents (IBSAs) for 6G networks, integrating perception, communication, and computation. The core method involves a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. The main conclusion is that LAM-enabled IBSAs represent a practical path toward building native, safety-critical 6G systems.

- **[arXiv251219] TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge**
  - **tags:** [mlsys], [others], [transformer encoder, self-supervised learning, foundation model, edge deployment, ultra-low-power microcontroller, gesture classification, kinematic regression]
  - **authors:** Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini
  - **institution:** ETH Zurich, University of Bologna
  - **link:** https://arxiv.org/pdf/2512.15729
  - **Simple LLM Summary:** This paper introduces TinyMyo, a lightweight Transformer-based foundation model pre-trained with self-supervised learning for EMG signal processing. It demonstrates strong generalization across multiple tasks like gesture classification and speech recognition while being deployable on an ultra-low-power microcontroller. The work provides an open-source, efficient model for edge-based EMG applications.

- **[arXiv251219] A Context-Free Smart Grid Model Using Complex System Approach**
  - **tags:** [sys], [smart grid optimization], [complex system modeling, game theory, classical optimization algorithms, coordination methods]
  - **authors:** Soufian Ben Amor, Alain Bui, Guillaume Guerard
  - **institution:** University of Versailles SQY
  - **link:** https://arxiv.org/pdf/2512.15733
  - **Simple LLM Summary:** The paper proposes a complex system approach for smart grid modeling, combining game theoretical and classical optimization methods at different levels to achieve global optimization. This combined method aims to provide flexibility, scalability, and generality in optimizing production, transmission, and consumption within the smart grid.

- **[arXiv251219] Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming**
  - **tags:** [mlsys], [others], [reinforcement learning, adaptive dynamic programming, extended state observer, event-triggered control, lyapunov analysis]
  - **authors:** Ningwei Bai, Chi Pui Chan, Qichen Yin, Tengyang Gong, Yunda Yan, Zezhi Tang
  - **institution:** University College London, University of Manchester
  - **link:** https://arxiv.org/pdf/2512.15735
  - **Simple LLM Summary:** This paper proposes a control method combining reinforcement learning (via Adaptive Dynamic Programming) with an Extended State Observer for disturbance rejection and an Event-Triggered Mechanism to reduce computational updates. The approach maintains robust performance for uncertain nonlinear systems while significantly lowering the sampling and processing effort compared to standard time-triggered schemes.

- **[arXiv251219] Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance**
  - **tags:** [ai], [financial risk management], [Bayesian modeling, LSTM, GARCH, Value-at-Risk, logistic regression, state-space model]
  - **authors:** Sharif Al Mamun, Rakib Hossain, Md. Jobayer Rahman, Malay Kumar Devnath, Farhana Afroz, Lisan Al Amin
  - **institution:** iLynx Inc., Cognitive Links, United International University, University of Maryland Baltimore County, Washington University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.15739
  - **Simple LLM Summary:** The paper proposes a Bayesian analytics framework for financial risk management, integrating models like discount-factor DLM, Bayesian logistic regression, and hierarchical Beta state-space for uncertainty quantification in volatility forecasting, fraud detection, and compliance monitoring. It shows that the framework improves risk assessment accuracy and interpretability, with GPU acceleration offering significant speedups, though challenges like sparse data remain.

- **[arXiv251219] Foundation Models in Biomedical Imaging: Turning Hype into Reality**
  - **tags:** [ai], [biomedical imaging], [foundation models, clinical reasoning, causal inference, multimodal data, trustworthiness, safety, bias]
  - **authors:** Amgad Muneer, Kai Zhang, Ibraheem Hamdi, Rizwan Qureshi, Muhammad Waqas, Shereen Fouad, Hazrat Ali, Syed Muhammad Anwar, Jia Wu
  - **institution:** The University of Texas MD Anderson Cancer Center, UTHealth Houston, Massachusetts Institute of Technology, Aston University, University of Stirling, George Washington University, Children's National Hospital
  - **link:** https://arxiv.org/pdf/2512.15808
  - **Simple LLM Summary:** This paper critically analyzes the state of foundation models in biomedical imaging, examining their capabilities in clinical reasoning, spatial understanding, and multimodal integration. It concludes that while these models are powerful assistive tools, the field must move beyond scale to develop hybrid, causally aware, and verifiably safe systems that augment human expertise, as autonomous AI-doctors remain a distant vision.

- **[arXiv251219] Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons**
  - **tags:** [ai], [computational neuroscience], [spike-timing-dependent plasticity, cortical traveling waves, spike-timing precision, long-term working memory, inhibitory rebound]
  - **authors:** Terrence J. Sejnowski
  - **institution:** Salk Institute for Biological Sciences, University of California, San Diego
  - **link:** https://arxiv.org/pdf/2512.15891
  - **Simple LLM Summary:** The paper proposes a mechanism where cortical traveling waves precisely coordinate spike timing to regulate spike-timing-dependent plasticity (STDP), forming a temporary second-tier network. This mechanism is suggested to support long-term working memory over hours, integrating cognitive processes above the sensorimotor network.

- **[arXiv251219] Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins**
  - **tags:** [mlsys], [others], [multi-agent system, tournament-based reasoning, federated agentic middleware, retrieval augmented generation, diffusion models, molecular simulation, HPC]
  - **authors:** Matthew Sinclair, Moeen Meigooni, Archit Vasan, Ozan Gokdemir, Xinran Lian, Heng Ma, Yadu Babuji, Alexander Brace, Khalid Hossain, Carlo Siebenschuh, Thomas Brettin, Kyle Chard, Christopher Henry, Venkatram Vishwanath, Rick L. Stevens, Ian T. Foster, Arvind Ramanathan
  - **institution:** Argonne National Laboratory, University of Chicago
  - **link:** https://arxiv.org/pdf/2512.15930
  - **Simple LLM Summary:** The paper presents StructBioReasoner, a scalable multi-agent system that uses a novel tournament-based reasoning framework to design biologics targeting intrinsically disordered proteins (IDPs). The system integrates domain knowledge and computational tools like AI-structure prediction and molecular simulations, coordinated via a federated middleware on HPC infrastructure. It was benchmarked successfully, with over 50% of designed candidates for one target outperforming human-designed reference binders, demonstrating its potential for autonomous IDP therapeutic discovery.

- **[arXiv251219] Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model**
  - **tags:** [ai], [finance], [neural network, consensus-bottleneck, asset pricing, interpretable deep learning, belief aggregation]
  - **authors:** Bong-Gyu Jang, Younwoo Jeong, Changeun Kim
  - **institution:** POSTECH (Pohang University of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.16251
  - **Simple LLM Summary:** The paper introduces the Consensus-Bottleneck Asset Pricing Model (CB-APM), an interpretable neural network that models how dispersed investor beliefs are compressed into a consensus to predict stock returns. It demonstrates improved predictive accuracy and explanatory power over standard deep learning methods, uncovering belief-driven return dynamics not captured by traditional factor models.

- **[arXiv251219] The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence**
  - **tags:** [ai], [complex systems theory], [dynamical systems, state space, phase transitions, symmetry-breaking, emergent attractors, bifurcation, multiscale coupling, nonequilibrium structure formation]
  - **authors:** Pradeep Singh, Mudasani Rushikesh, Bezawada Sri Sai Anurag, Balasubramanian Raman
  - **institution:** Indian Institute of Technology Roorkee
  - **link:** https://arxiv.org/pdf/2512.16515
  - **Simple LLM Summary:** This paper proposes a unified theoretical perspective that interprets the entire history of the universe, from the Big Bang to modern AI, as a continuous evolution of dynamical systems. It frames successive domains like cosmology, biology, and machine intelligence as regimes of dynamics on increasingly rich state spaces, connected by motifs like instability and bifurcation. The core conclusion is that the universe's history can be read as the evolution of dynamics itself, culminating in systems that can model and perturb their own future trajectories.
