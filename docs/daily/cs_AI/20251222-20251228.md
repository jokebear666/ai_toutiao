---
slug: /daily/csai/20251222-20251228
---
# 20251222-20251228 (cs.AI)

## 2025-12-22

- **[arXiv251222] Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases**
  - **tags:** [ai], [knowledge representation and reasoning], [entity set expansion, expansion graph, logical formula, semantic inclusion, computational complexity]
  - **authors:** Pietro Cofone, Giovanni Amendola, Marco Manna, Aldo Ricioppo
  - **institution:** University of Calabria, University of Cyprus
  - **link:** https://arxiv.org/pdf/2512.16953
  - **Simple LLM Summary:** This paper proposes a logic-based framework using expansion graphs, which are rooted directed acyclic graphs, to support taxonomic expansions of entity sets from knowledge bases. To avoid the impracticality of fully materializing these potentially large graphs, the authors formalize efficient reasoning tasks to check relationships between entity tuples within the graph structure. Their main conclusion is that, under realistic assumptions like bounded input, these tasks can be implemented efficiently, enabling local and incremental navigation without full graph construction.

- **[arXiv251222] Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories**
  - **tags:** [mlsys], [multi-modal inference], [visual anchoring, asset-first mechanism, temporal bridge, diffusion models, large language model (LLM), text-to-video (T2V), character consistency, multi-stage pipeline]
  - **authors:** Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
  - **institution:** BITS Pilani
  - **link:** https://arxiv.org/pdf/2512.16954
  - **Simple LLM Summary:** This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method's necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.

- **[arXiv251222] Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections**
  - **tags:** [ai], [computer vision], [YOLOv8, Finer-CAM, saliency maps, cross-validation, TLS point cloud projections]
  - **authors:** Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke
  - **institution:** University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)
  - **link:** https://arxiv.org/pdf/2512.16950
  - **Simple LLM Summary:** This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models' perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.

- **[arXiv251222] Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach**
  - **tags:** [sys], [pattern matching algorithms], [Ukkonen's Algorithm, Suffix Trees, pattern recognition, text-search algorithms]
  - **authors:** Xinyu Guan, Shaohua Zhang
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2512.16927
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a novel pattern matching algorithm that combines Ukkonen's Algorithm for constructing Suffix Trees with a new search technique using Python's dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.

- **[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]
  - **authors:** SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju
  - **institution:** NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2512.16925
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp
  - **Simple LLM Summary:** V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.

- **[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework**
  - **tags:** [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]
  - **authors:** Kamer Ali Yuksel
  - **institution:** aiXplain Inc
  - **link:** https://arxiv.org/pdf/2512.16970
  - **Simple LLM Summary:** This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.

- **[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]
  - **authors:** Saksham Sahai Srivastava, Haoyu He
  - **institution:** University of Georgia
  - **link:** https://arxiv.org/pdf/2512.16962
  - **Simple LLM Summary:** This paper introduces MemoryGraft, a novel attack that poisons an LLM agent's long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent's semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT's DataInterpreter.

- **[arXiv251222] InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression**
  - **tags:** [mlsys], [multi-modal training], [discrete video tokenization, transformer-based adaptive compressor, evidence lower bound (ELBO), information-theoretic compression, adaptive tokenization]
  - **authors:** Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu
  - **institution:** NVIDIA, Stanford University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.16975
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.

- **[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows**
  - **tags:** [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]
  - **authors:** Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.16969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.

- **[arXiv251222] Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations**
  - **tags:** [mlsys], [others], [search engine audit, semantic alignment, topical annotation, trajectory analysis]
  - **authors:** Erica Coppolillo, Simone Mungari
  - **institution:** University of Calabria, ICAR-CNR, University of Southern California
  - **link:** https://arxiv.org/pdf/2512.17027
  - **Simple LLM Summary:** The paper conducts a comparative audit of search engine recommendations on Wikipedia and Grokipedia by analyzing over 70,000 results from nearly 10,000 neutral English word queries. It finds that both platforms frequently generate weakly related or unexpected results from innocuous queries, though their recommendation sets often differ substantially in topical distribution and exploration trajectories.

- **[arXiv251222] A Women's Health Benchmark for Large Language Models**
  - **tags:** [ai], [healthcare AI evaluation], [women's health benchmark, large language models, error types, model stumps, query types]
  - **authors:** Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard
  - **institution:** Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine
  - **link:** https://arxiv.org/pdf/2512.17028
  - **Simple LLM Summary:** The paper introduces the Women's Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women's health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women's health advice.

- **[arXiv251222] Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats**
  - **tags:** [mlsys], [others], [agentic AI, cross-layer threats, role-based architecture, severity matrix, attack-chain analysis, OWASP]
  - **authors:** Ali Eslami, Jiangbo Yu
  - **institution:** Unknown
  - **link:** https://arxiv.org/pdf/2512.17041
  - **Simple LLM Summary:** This paper introduces a role-based architecture for Agentic Vehicles to systematically analyze security threats, including cognitive vulnerabilities and cross-layer risks. It concludes by providing a structured framework for assessing how small distortions can escalate into unsafe behavior in both human-driven and autonomous vehicles.

- **[arXiv251222] Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation**
  - **tags:** [mlsys], [others], [adversarial attacks, deep learning, cybersickness detection, visual tunneling, MI-FGSM, PGD, C&W, DeepTCN, Transformer]
  - **authors:** Istiak Ahmed, Ripan Kumar Kundu, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.17029
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Adversarial-VR, an open-source Unity testbed that integrates DeepTCN and Transformer models for real-time cybersickness detection and mitigation, and evaluates their robustness against adversarial attacks like MI-FGSM, PGD, and C&W. The results show these attacks can successfully fool the system, significantly degrading model accuracy and preventing correct mitigation.

- **[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering**
  - **tags:** [ai], [knowledge graph question answering], [reinforcement learning, subgraph selection, graph pruning, llm fine-tuning, relation-centric reasoning]
  - **authors:** Yinxu Tang, Chengsong Huang, Jiaxin Huang, William Yeoh
  - **institution:** Washington University in St. Louis
  - **link:** https://arxiv.org/pdf/2512.17043
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/069fd74faaf7500b76c5bd6958a190a707d8ccbe86484c3026a357b38657a47a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces UniRel-R1, a framework for relation-centric knowledge graph question answering that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The method is designed to identify compact and informative subgraph answers by rewarding specific relations and lower-degree entities. Experiments show it outperforms baselines in connectivity and reward and generalizes well to unseen entities and relations.

- **[arXiv251222] Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations**
  - **tags:** [ai], [social simulation], [large language model, generative agents, causal analysis, intergroup conflict, threat perception]
  - **authors:** Suhaib Abdurahman, Farzan Karimi-Malekabadi, Chenxiao Yu, Nour S. Kteily, Morteza Dehghani
  - **institution:** University of Southern California, Northwestern University
  - **link:** https://arxiv.org/pdf/2512.17066
  - **Simple LLM Summary:** This paper uses simulations with LLM-driven generative agents in virtual societies to causally analyze intergroup conflict. It finds that realistic threat directly increases hostility, while symbolic threat has a weaker effect mediated by ingroup bias and only increases hostility when realistic threat is absent.

- **[arXiv251222] Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL**
  - **tags:** [mlsys], [llm training], [knowledge distillation, chain-of-thought, structured reasoning, query execution plan, text-to-sql]
  - **authors:** Khushboo Thaker, Yony Bresler
  - **institution:** Crater Labs
  - **link:** https://arxiv.org/pdf/2512.17053
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/906b49597857a3cad8e1c9c8d6cdbec46e7807fe819943d1e6d91facfb7f18bd_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Struct-SQL, a knowledge distillation framework that trains a small language model using a structured chain-of-thought derived from query execution plans, rather than unstructured reasoning traces. The distilled model achieves an 8.1% absolute improvement over an unstructured baseline, primarily due to a reduction in syntactic errors. This demonstrates that structured logical blueprints are beneficial for reliable SQL generation in small models.

- **[arXiv251222] Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution**
  - **tags:** [ai], [social media analysis], [Augmented Dickey-Fuller test, KPSS test, Spearman correlation, Chi-square test, time series analysis, stationarity testing]
  - **authors:** Ohoud Alzahrani, Russell Beale, Bob Hendley
  - **institution:** University of Birmingham
  - **link:** https://arxiv.org/pdf/2512.17067
  - **Simple LLM Summary:** This paper conducts a longitudinal study analyzing the temporal behavior of promotional Twitter bots using time series analysis and statistical tests on ten content-based meta-features. It finds that bot behavior is non-stationary, with individual features and their interdependencies evolving systematically over time and across bot generations. The conclusion is that bot-detection systems must account for this dynamic adaptation and avoid treating behavioral features as static.

- **[arXiv251222] On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues**
  - **tags:** [mlsys], [others], [multi-agent system, transactional analysis, ego states, information retrieval, vector stores, ablation test]
  - **authors:** Monika Zamojska, Jarosław A. Chudziak
  - **institution:** Warsaw University of Technology
  - **link:** https://arxiv.org/pdf/2512.17060
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ea02a6d58821ccc3cc89deee5daf014a7e650e133502a2a64e5cd69e54c8596_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-agent system architecture that integrates Transactional Analysis theory, dividing each agent into Parent, Adult, and Child ego states, and enhances their responses with contextual information retrieval from vector stores. The system is evaluated through ablation tests in a simulated dialogue scenario. The results show that this psychologically grounded structure improves the realism of LLM-based agent behavior.

- **[arXiv251222] Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?**
  - **tags:** [ai], [mathematical reasoning], [chain-of-thought prompting, reinforcement learning, GRPO, fine-tuning, error recovery]
  - **authors:** Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah
  - **institution:** MIT
  - **link:** https://arxiv.org/pdf/2512.17079
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp
  - **Simple LLM Summary:** The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.

- **[arXiv251222] When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation**
  - **tags:** [ai], [dialogue topic segmentation], [window-tolerant F1, boundary density, segment coherence, granularity-aware evaluation]
  - **authors:** Michael H. Coen
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17083
  - **Simple LLM Summary:** This paper introduces a new evaluation framework for dialogue topic segmentation that emphasizes boundary density and segment coherence alongside window-tolerant F1. It demonstrates through cross-dataset experiments that reported performance differences are often artifacts of annotation granularity mismatches, not model quality. The core conclusion is that topic segmentation should be viewed as selecting an appropriate granularity rather than predicting a single correct boundary set.

- **[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence**
  - **tags:** [ai], [reinforcement learning], [AIXI, Choquet integrals, imprecise probability theory, semimeasure loss, utility functions]
  - **authors:** Cole Wyeth, Marcus Hutter
  - **institution:** University of Waterloo, Google DeepMind, Australian National University
  - **link:** https://arxiv.org/pdf/2512.17086
  - **Simple LLM Summary:** This paper generalizes the AIXI reinforcement learning agent to handle a wider class of utility functions, confronting the ambiguity of finite history predictions by interpreting belief distributions as imprecise probabilities. It explores computing expected utilities using Choquet integrals from imprecise probability theory and investigates their computability. The authors show that the standard recursive value function is a special case, but the most general utilities under a "death interpretation" cannot be characterized by these integrals.

- **[arXiv251222] How to Square Tensor Networks and Circuits Without Squaring Them**
  - **tags:** [ai], [probabilistic modeling], [tensor networks, squared circuits, probabilistic circuits, marginalization, canonical forms, unitary matrices, distribution estimation]
  - **authors:** Lorenzo Loconte, Adrián Javaloy, Antonio Vergari
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.17090
  - **Simple LLM Summary:** This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.

- **[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]
  - **authors:** Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman
  - **institution:** Toyota Research Institute
  - **link:** https://arxiv.org/pdf/2512.17091
  - **Simple LLM Summary:** This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.

- **[arXiv251222] UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data**
  - **tags:** [ai], [interpretability], [counterfactual explanations, model-agnostic, time series, ECG, LIME, SHAP]
  - **authors:** Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun
  - **institution:** Boston University, Sandia National Laboratories, Boston Medical Center
  - **link:** https://arxiv.org/pdf/2512.17100
  - **Simple LLM Summary:** The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.

- **[arXiv251222] A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving**
  - **tags:** [mlsys], [llm training], [solver-in-the-loop, instruction-tuning, supervised fine-tuning, best-of-N sampling, answer set programming, semantic parsing]
  - **authors:** Timo Pierre Schrader, Lukas Lange, Tobias Kaminski, Simon Razniewski, Annemarie Friedrich
  - **institution:** Bosch Center for AI, University of Augsburg, ScaDS.AI & TU Dresden
  - **link:** https://arxiv.org/pdf/2512.17093
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38c83df2ce552270bc09f323934a96a0aad16af58e736a7049ccfd73afeed0d4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a solver-in-the-loop framework that uses an ASP solver to provide feedback on LLM-generated code, creating a dataset of chosen and rejected instances for supervised fine-tuning. The method improves LLM performance on generating Answer Set Programming code for logic puzzles, demonstrating consistent gains across different prompting settings and datasets.

- **[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library**
  - **tags:** [mlsys], [post-training], [reinforcement learning, skill library, sequential rollout, skill-integrated reward, GRPO, self-improving agent]
  - **authors:** Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong
  - **institution:** University of Wisconsin–Madison, AWS Agentic AI
  - **link:** https://arxiv.org/pdf/2512.17102
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fdb48e8df3d2e43c1e8301082bd3478f7894eef19c7194ccd54fb1c77738ef_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SAGE, a reinforcement learning framework that enhances LLM-based agents by integrating a skill library through sequential rollouts and a skill-integrated reward. This approach enables agents to accumulate and reuse skills across tasks for continual self-improvement. Experiments show SAGE improves task completion accuracy while significantly reducing interaction steps and token usage compared to existing methods.

- **[arXiv251222] Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs**
  - **tags:** [mlsys], [llm training], [Generalized Primal Averaging (GPA), DiLoCo, Schedule-Free, AdamW, Nesterov's method, primal averaging, optimizer, iterate averaging]
  - **authors:** Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao
  - **institution:** Meta Superintelligence Labs
  - **link:** https://arxiv.org/pdf/2512.17131
  - **Simple LLM Summary:** The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov's method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.

- **[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction**
  - **tags:** [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]
  - **authors:** Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang
  - **institution:** Johns Hopkins University, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.17137
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.

- **[arXiv251222] Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty**
  - **tags:** [ai], [algorithmic information theory], [Solomonoff induction, Bayesian Model Averaging, hypothesis ranking, systematic generalisation, uncertainty estimation]
  - **authors:** Josh Barber, Rourke Young, Cameron Coombe, Will Browne
  - **institution:** Queensland University of Technology, CSIRO
  - **link:** https://arxiv.org/pdf/2512.17145
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb373087fc2ec93e8e3a1729cbb4c2df71ab2a3e5c7a3936acdba21fa6ee2c9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that uses a Solomonoff-inspired scoring to weight hypotheses generated by a Large Language Model based on their simplicity and predictive fit. The method, applied to Mini-ARC tasks, produces uncertainty-aware predictions by spreading probability across multiple hypotheses, contrasting with Bayesian Model Averaging which tends to concentrate weight on a single candidate. The results highlight the value of algorithmic information-theoretic priors for robust, interpretable reasoning under uncertainty.

- **[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors**
  - **tags:** [ai], [reinforcement learning], [interactive reinforcement learning, multi-teacher learning, Q-learning, teacher selection, concept drift]
  - **authors:** Maher Mesto, Francisco Cruz
  - **institution:** University of New South Wales, Universidad Central de Chile
  - **link:** https://arxiv.org/pdf/2512.17180
  - **Simple LLM Summary:** This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.

- **[arXiv251222] PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases**
  - **tags:** [mlsys], [llm inference], [large language model, explainable ai, augmented reality, personalized explanations, real-time object detection, user study]
  - **authors:** Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.17172
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7767a7bc851a49f82148423782803913a5c377f60c4ce3a9cc7383c22a6d08a4_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PILAR, a framework that uses a pre-trained large language model (LLM) to generate unified, context-aware, and personalized explanations for AI-driven augmented reality systems. A user study on a recipe recommendation prototype showed that the LLM-based explanation interface significantly improved user task performance and perceived transparency compared to a traditional template-based approach.

- **[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, retrieval-augmented generation, multi-modal large language models, explainable AI]
  - **authors:** Shengwei Zhao, Jingwen Yao, Sitong Wei, Linhai Xu, Yuying Liu, Dong Zhang, Zhiqiang Tian, Shaoyi Du
  - **institution:** Xi’an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.17194
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e101261754ebc1d899bc11f5f5d9245217cf53bcbfc614d62f737f8cbd530473_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MMRAG-RFT, a two-stage reinforcement fine-tuning framework for explainable multi-modal retrieval-augmented generation. The method uses rule-based and reasoning-based reinforcement learning to filter documents and jointly optimize ranking and answer generation, achieving state-of-the-art results on benchmark datasets.

- **[arXiv251222] UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark**
  - **tags:** [mlsys], [multi-modal inference], [unified multimodal model, benchmark, omni-dimensional evaluation, understanding, generation, editing]
  - **authors:** Kai Liu, Leyang Chen, Wenbo Li, Zhikai Chen, Zhixin Wang, Renjing Pei, Linghe Kong, Yulun Zhang
  - **institution:** Shanghai Jiao Tong University, The Chinese University of Hong Kong, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.17196
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dffc23613309b3df00996da4dd30419c4aa81ea36355df55ab509eed8b7380c9_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces UmniBench, a benchmark designed to holistically evaluate Unified Multimodal Models (UMMs) by assessing their understanding, generation, and editing abilities within a single process, using the model's own understanding capability to judge its outputs. It covers 13 domains and over 200 concepts to provide comprehensive and fine-grained assessments. The authors benchmark 24 models and conclude that UmniBench offers a more integrated and objective evaluation framework compared to isolated, task-specific benchmarks.

- **[arXiv251222] Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening**
  - **tags:** [mlsys], [diffusion inference], [one-step distillation, lightweight ensemble blocks, four-stage training, pansharpening, diffusion model, end-to-end network]
  - **authors:** Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.17202
  - **Simple LLM Summary:** This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model's inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model.

- **[arXiv251222] Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines**
  - **tags:** [sys], [robotics and navigation], [extended kalman filter, inertial measurement unit, wheel odometer, dead reckoning]
  - **authors:** Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang
  - **institution:** Peking University
  - **link:** https://arxiv.org/pdf/2512.17215
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dead reckoning method for a self-propelled pipeline robot, using an IMU for initial attitude estimation, refining it with an Extended Kalman Filter, and combining it with wheel odometer data for localization. The method was tested in a rectangular loop pipeline, and the results verified the effectiveness of the proposed algorithm for navigating complex three-dimensional pipelines.

- **[arXiv251222] The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes**
  - **tags:** [ai], [ethics and society], [Systematic Literature Review (SLR), PRISMA, Maqasid al-Shariah, hifz al-ird, hifz al-nafs, adl, tabayyun]
  - **authors:** Wisnu Uriawan, Imany Fauzy Rahman, Muhamad Zidan, Irma Rohmatillah, Muhammad Arkan Raihan, Irma Dwiyanti
  - **institution:** UIN Sunan Gunung Djati Bandung
  - **link:** https://arxiv.org/pdf/2512.17218
  - **Simple LLM Summary:** This study employs a Systematic Literature Review (SLISMA) to formulate an Islamic ethical framework for preventing deepfake abuse. It concludes that principles from Maqasid al-Shariah, such as protecting honor and self, provide a normative basis for shifting from punitive to preventative approaches, focusing on human dignity and the common good in the digital age.

- **[arXiv251222] Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition**
  - **tags:** [mlsys], [llm inference], [error level noise embedding, n-best hypotheses, noise-aware modeling, whisper, llama-2, word error rate, fine-tuning]
  - **authors:** Zahra Rahmani, Hossein Sameti
  - **institution:** Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.17247
  - **Simple LLM Summary:** This paper proposes a robust noise-sensitive ASR error correction framework for Persian. It introduces Error Level Noise (ELN) embeddings, derived from disagreements in multiple ASR hypotheses, to condition a fine-tuned LLaMA-2 model, enabling it to reason about noise-induced uncertainty. The ELN-conditioned model significantly reduces Word Error Rate compared to text-only baselines, demonstrating the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust speech recognition in noisy environments.

- **[arXiv251222] Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics**
  - **tags:** [ai], [privacy-preserving data synthesis], [multi-objective optimization, origin-destination matrices, dwell-travel time quantiles, universal law of daily visited locations, synthetic trajectory generation]
  - **authors:** Jun'ichi Ozaki, Ryosuke Susuta, Takuhiro Moriyama, Yohei Shida
  - **institution:** Not explicitly provided; cannot infer from given information.
  - **link:** https://arxiv.org/pdf/2512.17239
  - **Simple LLM Summary:** This paper proposes a method to generate a privacy-preserving synthetic dataset of individual daily trajectories by integrating aggregated origin-destination flows with behavioral constraints in a multi-objective optimization framework. The method successfully reproduces realistic human mobility patterns in two Japanese regions, providing a practical pathway for high-resolution mobility analytics without using sensitive personal data.

- **[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs**
  - **tags:** [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]
  - **authors:** Madhava Gaikwad
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.17251
  - **Simple LLM Summary:** The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.

- **[arXiv251222] Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction**
  - **tags:** [mlsys], [multi-modal inference], [speculative execution, TD-MPC2, latent-space MPC, mismatch correction, action queue, transformer corrector]
  - **authors:** Ziyang Lin, Zixuan Sun, Sanhorn Chen, Xiaoyang Chen, Roy Zhao
  - **institution:** University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17250
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/801e0aab9fd9dcfb7d20ebc658880a9fa5c22a62c30b2127c1037ab48024b399_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a speculation-and-correction framework to reduce inference latency in real-time control agents. It uses a world model to predict future actions and latent states, then applies a lightweight learned corrector to adjust these actions when new observations arrive, reducing planning calls by 43.6% with minimal performance loss. The results show that correction is essential for reliable latency reduction in sequential control tasks.

- **[arXiv251222] Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems**
  - **tags:** [mlsys], [llm inference], [audit agents, attestation protocols, constrained reasoning, cryptographic attestation, symbolic methods, benchmark suite, verifiability]
  - **authors:** Abhivansh Gupta
  - **institution:** Indian Institute of Technology, Roorkee
  - **link:** https://arxiv.org/pdf/2512.17259
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a Verifiability-First architecture for LLM-based agents, integrating runtime attestations, lightweight audit agents for continuous verification, and challenge-response protocols for high-risk operations. It introduces the OPERA benchmark to evaluate the detectability and speed of remediation for misaligned behavior, shifting the focus from measuring the propensity for misalignment to ensuring reliable detection and control.

- **[arXiv251222] ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework**
  - **tags:** [mlsys], [others], [GPT-style transformer, autoregressive next-event prediction, player embeddings, counterfactual simulation, residual On-Ball Value (rOBV)]
  - **authors:** Miru Hong, Minho Lee, Geonhee Jo, Jae-Hee So, Pascal Bauer, Sang-Ki Ko
  - **institution:** University of Seoul, Saarland University, Bank of Korea
  - **link:** https://arxiv.org/pdf/2512.17266
  - **Simple LLM Summary:** The paper introduces ScoutGPT, a GPT-based autoregressive transformer model that treats football match events as discrete token sequences to predict the next action and its value, conditioned on player identity. It enables counterfactual simulations by swapping player embeddings to assess how a player's performance might change in a new tactical context. Evaluated on Premier League data, the model outperforms baselines in prediction accuracy and provides a principled framework for evaluating player transfer fit.

- **[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators**
  - **tags:** [ai], [evaluation framework], [LLM-as-a-Judge, regression, MetricBank, retrieval, human feedback correlation]
  - **authors:** Michael J. Ryan, Yanzhe Zhang, Amol Salunkhe, Yi Chu, Di Xu, Diyi Yang
  - **institution:** Stanford University, American Express
  - **link:** https://arxiv.org/pdf/2512.17267
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AutoMetrics, a framework that synthesizes evaluation metrics by combining retrieved metrics from a curated bank with automatically generated LLM-as-a-Judge criteria, composed via regression to maximize correlation with human feedback. It demonstrates that AutoMetrics significantly improves correlation with human judgments over standard LLM-as-a-Judge approaches while requiring minimal human feedback data. The method can serve as an effective proxy reward for optimizing AI applications.

- **[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory**
  - **tags:** [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]
  - **authors:** Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li
  - **institution:** Wuhan University, Tongyi Lab, Zhongguancun Academy
  - **link:** https://arxiv.org/pdf/2512.17270
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.

- **[arXiv251222] WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images**
  - **tags:** [ai], [medical image segmentation], [wavelet-guided enhancement, dual-attention feature fusion, U-shaped Mamba architecture, Wavelet-denoised High-Frequency-guided Feature (WHF), Dual Attention Feature Fusion (DAFF)]
  - **authors:** Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng
  - **institution:** Beijing Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.17278
  - **Simple LLM Summary:** The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications.

- **[arXiv251222] Subjective Question Generation and Answer Evaluation using NLP**
  - **tags:** [mlsys], [llm training], [large language models, instruct-tuning, bloom's taxonomy, subjective evaluation, question generation, answer evaluation]
  - **authors:** G. M. Refatul Islam, Safwan Shaheer, Yaseen Nur, Mohammad Rafid Hamid
  - **institution:** Brac University
  - **link:** https://arxiv.org/pdf/2512.17289
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f03ca77cc9ebde43ea5a7c83c936ce7c8843b9c39c6b6c58614cb92eb1ce8fc_w640_q70.webp
  - **Simple LLM Summary:** This research proposes a framework that uses instruct-tuned large language models (LLMs) to generate subjective questions and evaluate student answers, particularly for higher-order thinking skills. The study concludes that this approach can effectively automate the assessment of complex, subjective understanding, a task traditionally requiring human evaluators.

- **[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge**
  - **tags:** [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]
  - **authors:** Abdullah M. Zyarah, Dhireesha Kudithipudi
  - **institution:** University of Texas at San Antonio, University of Baghdad
  - **link:** https://arxiv.org/pdf/2512.17299
  - **Simple LLM Summary:** This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.

- **[arXiv251222] Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track**
  - **tags:** [mlsys], [diffusion training], [Self-Purifying Flow Matching (SPFM), flow matching, text-to-speech (TTS), Supertonic, fine-tuning, in-the-wild speech, label noise mitigation]
  - **authors:** June Young Yi, Hyeongju Kim, Juheon Lee
  - **institution:** Supertone Inc.
  - **link:** https://arxiv.org/pdf/2512.17293
  - **Simple LLM Summary:** This paper presents a lightweight TTS system that fine-tunes the Supertonic model using Self-Purifying Flow Matching (SPFM) to robustly adapt to noisy, in-the-wild speech data. SPFM handles label noise by comparing conditional and unconditional flow matching losses, routing suspicious samples for unconditional training while still using their acoustic information. The resulting model achieved the best word error rate in the WildSpoof 2026 challenge, demonstrating that open-weight architectures can be effectively adapted to real-world conditions with explicit noise-handling mechanisms.

- **[arXiv251222] Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation**
  - **tags:** [mlsys], [llm inference], [large language models, turn-based battle system, strategic decision-making, content generation, procedural generation, adaptive difficulty]
  - **authors:** Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani
  - **link:** https://arxiv.org/pdf/2512.17308
  - **Simple LLM Summary:** The paper develops a turn-based Pokémon battle system where LLMs act as agents, making tactical decisions based on a structured battle state without domain-specific training. The core method involves evaluating LLMs on strategic reasoning and their ability to generate novel game content. The main conclusion is that LLMs can function as dynamic game opponents and designers, offering a practical alternative to reinforcement learning for strategic games.

- **[arXiv251222] Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability**
  - **tags:** [ai], [explainable AI (XAI)], [graph theory, model decomposition, hypothesis-evidence structure, Cox proportional hazards model, PREDICT]
  - **authors:** Michael Merry, Pat Riddle, Jim Warren
  - **institution:** University of Auckland
  - **link:** https://arxiv.org/pdf/2512.17316
  - **Simple LLM Summary:** This paper proposes a formal, testable criterion for inherent explainability in AI, using graph theory to decompose models into verifiable structure-local explanations called annotations. The method is applied to demonstrate the inherent explainability of a clinical cardiovascular risk model (PREDICT). The work provides a rigorous foundation for regulators and formalizes the distinction between an explainable model and an explained one.

- **[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs**
  - **tags:** [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]
  - **authors:** Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao
  - **institution:** Nanjing University
  - **link:** https://arxiv.org/pdf/2512.17319
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.

- **[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs**
  - **tags:** [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]
  - **authors:** Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas
  - **institution:** University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17352
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.

- **[arXiv251222] Dialectics for Artificial Intelligence**
  - **tags:** [ai], [algorithmic information theory], [algorithmic information theory, Kolmogorov complexity, reversible consistency relation, excess information, dialectics optimization]
  - **authors:** Zhengmian Hu
  - **institution:** Adobe Research
  - **link:** https://arxiv.org/pdf/2512.17373
  - **Simple LLM Summary:** This paper proposes an algorithmic-information framework to define concepts as structural relations within an agent's total experience, using reversible consistency and excess information to enable dynamic concept revision. It formulates dialectics as an optimization process where concepts compete to explain new information, leading to expansion, splitting, or merging. The approach also formalizes low-cost concept transmission between agents through shared seeds, making concept alignment a concrete compute-bits trade-off.

- **[arXiv251222] TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data**
  - **tags:** [mlsys], [post-training], [imitation learning, dataset aggregation, direct preference optimization, closed-loop evaluation, expert takeover data]
  - **authors:** Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Science and Technology Beijing; Chongqing Chang'an Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.17370
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TakeAD, a framework that fine-tunes a pre-trained imitation learning policy for autonomous driving using expert takeover data. The method combines iterative Dataset Aggregation (DAgger) for imitation with Direct Preference Optimization (DPO) for preference alignment to improve closed-loop performance. Experiments show it effectively mitigates the open-loop gap and outperforms pure imitation learning methods.

- **[arXiv251222] Optimisation of Aircraft Maintenance Schedules**
  - **tags:** [ai], [evolutionary algorithms], [evolutionary algorithm, genetic operators, fitness function, maintenance scheduling, optimisation]
  - **authors:** Neil Urquhart, Amir Rahimi, Efstathios-Al. Tingas
  - **institution:** Edinburgh Napier University
  - **link:** https://arxiv.org/pdf/2512.17412
  - **Simple LLM Summary:** This paper applies an Evolutionary Algorithm to solve the aircraft maintenance scheduling problem, which involves assigning qualified staff to tasks within a turnaround window. The algorithm is benchmarked on 60 generated problem instances to evaluate its performance. The study demonstrates the proposed representation and genetic operators for this optimisation task.

- **[arXiv251222] Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques**
  - **tags:** [mlsys], [others], [FastText, NSFWJS, sentiment analysis, data restoration, classification]
  - **authors:** Xingyu Feng
  - **institution:** Hainan University
  - **link:** https://arxiv.org/pdf/2512.17411
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method for detecting sensitive and illegal content on the Ethereum blockchain using machine learning. It employs a data restoration algorithm and uses FastText for sentiment analysis and NSFWJS for image detection. The study concludes that harmful content, including personal data and explicit images, coexists with benign data on the blockchain, highlighting privacy and security concerns.

- **[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering**
  - **tags:** [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]
  - **authors:** Léo Butsanets, Charles Corbière, Julien Khlaut, Pierre Manceron, Corentin Dancette
  - **institution:** Raidium, Université de Paris Cité, Hôpital Européen Georges Pompidou, AP-HP, INSERM
  - **link:** https://arxiv.org/pdf/2512.17396
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs.

- **[arXiv251222] A Systematic Reproducibility Study of BSARec for Sequential Recommendation**
  - **tags:** [ai], [sequential recommendation], [BSARec, Transformer, Fourier transform, discrete wavelet transform, padding strategies, frequency rescaling]
  - **authors:** Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy, Yuanna Liu
  - **institution:** University of Amsterdam
  - **link:** https://arxiv.org/pdf/2512.17442
  - **Simple LLM Summary:** This paper reproduces and evaluates BSARec, a sequential recommendation method that enhances Transformer encoders with a frequency layer using Fourier transforms to capture high-frequency signals. The study finds that BSARec outperforms other methods on some datasets, but digital signal processing techniques like discrete wavelet transform offer only marginal improvements over Fourier transforms, and non-constant padding significantly boosts performance while constant padding hinders high-frequency signal capture.

- **[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories**
  - **tags:** [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]
  - **authors:** Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe
  - **institution:** Turing
  - **link:** https://arxiv.org/pdf/2512.17419
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements.

- **[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]
  - **authors:** Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni
  - **institution:** Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University
  - **link:** https://arxiv.org/pdf/2512.17444
  - **Simple LLM Summary:** This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.

- **[arXiv251222] Learning What to Write: Write-Gated KV for Efficient Long-Context Inference**
  - **tags:** [mlsys], [llm inference], [KV cache management, Write-Gated KV, KV Admission, KV cache eviction, KV cache selection, FlashAttention, paged-KV systems]
  - **authors:** Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu
  - **institution:** National Taiwan University, Academia Sinica
  - **link:** https://arxiv.org/pdf/2512.17452
  - **Simple LLM Summary:** This paper introduces Write-Gated KV, a learnable KV Admission mechanism that predicts token utility before it enters the KV cache to reduce memory usage and speed up inference. By filtering low-utility tokens early and maintaining a compact global cache, the method significantly reduces memory usage and improves prefill and decode speeds for long-context LLMs with minimal accuracy loss. The results demonstrate that proactive KV cache management is a practical solution for efficient long-context inference.

- **[arXiv251222] A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting**
  - **tags:** [ai], [time series forecasting], [spatial-temporal graph neural network, trend-seasonal decomposition, low-rank Top-K adjacency learning, horizon-wise gating, linear baseline]
  - **authors:** Henok Tenaw Moges, Deshendran Moodley
  - **institution:** University of Cape Town
  - **link:** https://arxiv.org/pdf/2512.17453
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Lite-STGNN, a lightweight model combining decomposition-based linear temporal modeling with a learnable sparse graph module for spatial corrections. It achieves state-of-the-art accuracy on long-term multivariate forecasting benchmarks while being parameter-efficient and faster than transformer-based methods. The learned adjacency matrices also provide interpretable insights into domain-specific variable interactions.

- **[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application**
  - **tags:** [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]
  - **authors:** Olivier Jeunen, Schaun Wheeler
  - **institution:** aampe
  - **link:** https://arxiv.org/pdf/2512.17462
  - **Simple LLM Summary:** This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.

- **[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding**
  - **tags:** [ai], [democratic systems], [fair voting methods, cumulative voting, equal shares, proportional representation, participatory budgeting, AI voting assistance]
  - **authors:** Evangelos Pournaras
  - **institution:** University of Leeds
  - **link:** https://arxiv.org/pdf/2512.17461
  - **Simple LLM Summary:** This paper proposes that combining expressive ballot formats like cumulative voting with proportional aggregation methods like equal shares constitutes a "fair voting method." It concludes that such methods enhance democratic legitimacy, accelerate impactful outcomes in areas like welfare and education, and serve as a safeguard against biases in emerging AI-assisted voting scenarios.

- **[arXiv251222] Translating the Rashomon Effect to Sequential Decision-Making Tasks**
  - **tags:** [ai], [sequential decision-making], [Rashomon effect, formal verification, policy ensembles, behavioral cloning, permissive policies]
  - **authors:** Dennis Gross, Jørn Eirik Betten, Helge Spieker
  - **institution:** University of Oslo
  - **link:** https://arxiv.org/pdf/2512.17470
  - **Simple LLM Summary:** This paper translates the Rashomon effect from classification to sequential decision-making by defining it for policies that behave identically but have different internal structures. It uses formal verification methods to compare the complete probabilistic behavior of policies in stochastic environments. The study concludes that the effect exists in this domain and that ensembles from the Rashomon set are more robust to distribution shifts.

- **[arXiv251222] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion**
  - **tags:** [mlsys], [diffusion inference], [4D scene geometry, diffusion-based video generation, occlusion consistency, illumination-aware dataset, mask generation]
  - **authors:** Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo
  - **institution:** KAIST AI, SK Telecom
  - **link:** https://arxiv.org/pdf/2512.17504
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp
  - **Simple LLM Summary:** The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments.

- **[arXiv251222] Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models**
  - **tags:** [mlsys], [llm inference], [LoRA, PEFT, orthonormal transform, hidden-state scrambling, access control, instruction tuning, 4-bit quantization]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17519
  - **Simple LLM Summary:** This paper proposes K-OTG, a method for secret-key access control in language models. It uses a training-time dual-path corpus and inference-time orthonormal transforms to scramble hidden states, making the model unusable without the correct key while preserving authorized performance. The method is compatible with LoRA and 4-bit quantization, showing effective locking with minimal utility loss for authorized users.

- **[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals**
  - **tags:** [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17527
  - **Simple LLM Summary:** This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at ≤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.

- **[arXiv251222] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding**
  - **tags:** [mlsys], [multi-modal inference], [degradation-aware reasoning, structured reasoning chains, supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling]
  - **authors:** Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen
  - **institution:** Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.17532
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities.

- **[arXiv251222] Towards Explainable Conversational AI for Early Diagnosis with Large Language Models**
  - **tags:** [mlsys], [llm inference], [GPT-4o, Retrieval-Augmented Generation, Chain-of-Thought prompting, similarity matching, adaptive questioning]
  - **authors:** Maliha Tabassum, M Shamim Kaiser
  - **institution:** Bangladesh University of Professionals, Jahangirnagar University
  - **link:** https://arxiv.org/pdf/2512.17559
  - **Simple LLM Summary:** This paper introduces a diagnostic chatbot powered by a Large Language Model (GPT-4o) that uses Retrieval-Augmented Generation, Chain-of-Thought prompting, and adaptive questioning to interactively extract symptoms and provide diagnoses. The system achieved 90% accuracy and 100% Top-3 accuracy, outperforming traditional machine learning models. The findings suggest that LLM-based systems can offer a more transparent, interactive, and clinically effective approach to early medical diagnosis.

- **[arXiv251222] When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems**
  - **tags:** [mlsys], [others], [MetricGAN-plus-voicebank, semantic WER, noise robustness, speech enhancement]
  - **authors:** Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati
  - **institution:** EkaCare (Orbi Health Private Limited)
  - **link:** https://arxiv.org/pdf/2512.17562
  - **Simple LLM Summary:** This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.

- **[arXiv251222] ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image**
  - **tags:** [ai], [computer vision], [clothing tailoring, body semantic estimation, body edge prediction, foundational human visual model (FHVM), 3D mesh recovery]
  - **authors:** Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen
  - **institution:** Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)
  - **link:** https://arxiv.org/pdf/2512.17545
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping.

- **[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping**
  - **tags:** [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]
  - **authors:** Yikang Yue, Yishu Yin, Xuehai Qian
  - **institution:** Tsinghua University, University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17570
  - **Simple LLM Summary:** The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.

- **[arXiv251222] A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points**
  - **tags:** [mlsys], [others], [Attention U-Net, FLAIR hyperintensity segmentation, Dice score, Raidionics]
  - **authors:** Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen
  - **institution:** SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Linköping University Hospital, Skåne University Hospital, Umeå University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital
  - **link:** https://arxiv.org/pdf/2512.17566
  - **Simple LLM Summary:** This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment.

- **[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification**
  - **tags:** [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]
  - **authors:** Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli
  - **institution:** The University of Texas at El Paso, University of North Carolina
  - **link:** https://arxiv.org/pdf/2512.17594
  - **Simple LLM Summary:** This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.

- **[arXiv251222] More Consistent Accuracy PINN via Alternating Easy-Hard Training**
  - **tags:** [ai], [scientific machine learning], [physics-informed neural networks, easy-hard prioritization, hybrid training strategy, alternating scheme]
  - **authors:** Zhaoqian Gao, Min Yanga
  - **institution:** Yantai University
  - **link:** https://arxiv.org/pdf/2512.17607
  - **Simple LLM Summary:** This paper proposes a hybrid training strategy for Physics-Informed Neural Networks (PINNs) that alternates between easy and hard prioritization to improve performance. The method achieves consistently high accuracy on challenging PDEs, significantly outperforming baseline approaches. The work demonstrates that this alternating scheme enhances the robustness and reliability of PINNs across diverse problem types.

- **[arXiv251222] MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration**
  - **tags:** [ai], [medical image registration], [mammography registration, anatomical landmarks, ANTs, VoxelMorph, TransMorph, IDIR, MammoRegNet, benchmark dataset]
  - **authors:** Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin
  - **institution:** Lomonosov Moscow State University, Third Opinion Platform
  - **link:** https://arxiv.org/pdf/2512.17605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration.

- **[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions**
  - **tags:** [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]
  - **authors:** Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt
  - **institution:** KU Leuven, Technical University of Munich (TUM)
  - **link:** https://arxiv.org/pdf/2512.17629
  - **Simple LLM Summary:** The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.

- **[arXiv251222] Trust-Region Adaptive Policy Optimization**
  - **tags:** [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]
  - **authors:** Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang
  - **institution:** Tsinghua University, Ant Group
  - **link:** https://arxiv.org/pdf/2512.17636
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.

- **[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines**
  - **tags:** [ai], [reinforcement learning], [timed reward machines, tabular Q-learning, timed automata, counterfactual-imagining]
  - **authors:** Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska
  - **institution:** Tata Institute of Fundamental Research, Max Planck Institute for Software Systems, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.17637
  - **Simple LLM Summary:** This paper proposes timed reward machines (TRMs), an extension of reward machines that incorporates timing constraints into the reward specification for reinforcement learning. The authors develop model-free RL algorithms, specifically using tabular Q-learning integrated with abstractions of timed automata and counterfactual-imagining heuristics, to learn optimal policies. The experimental results show that their approach successfully learns policies that achieve high rewards while satisfying the specified timing constraints.

- **[arXiv251222] STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting**
  - **tags:** [mlsys], [multi-modal inference], [zero-shot learning, cross-modal retrieval, dual-encoder architecture, contrastive learning, structure-aware augmentation, semantic-traffic alignment]
  - **authors:** Yifei Cheng, Yujia Zhu, Baiyang Li, Xinhao Deng, Yitong Cai, Yaochen Ren, Qingyun Liu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.17667
  - **Simple LLM Summary:** The paper introduces STAR, a method that reformulates website fingerprinting as a zero-shot cross-modal retrieval problem, using a dual-encoder architecture to learn a joint embedding space for encrypted traffic traces and website logic profiles. It achieves high accuracy on unseen websites by aligning semantic and traffic features, demonstrating that semantic leakage is a major privacy risk in encrypted HTTPS traffic.

- **[arXiv251222] You Only Train Once: Differentiable Subset Selection for Omics Data**
  - **tags:** [ai], [bioinformatics], [differentiable subset selection, multi-task learning, end-to-end training, sparsity, single-cell RNA-seq]
  - **authors:** Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt
  - **institution:** ETH Zurich, University Children’s Hospital Zurich
  - **link:** https://arxiv.org/pdf/2512.17678
  - **Simple LLM Summary:** The paper introduces YOTO, an end-to-end framework that jointly selects discrete gene subsets and performs prediction in a single differentiable model, using sparsity and multi-task learning. It demonstrates improved predictive performance and yields compact, meaningful gene subsets on single-cell RNA-seq datasets, advancing biomarker discovery.

- **[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation**
  - **tags:** [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]
  - **authors:** Alexandre Personnic, Mihai Bâce
  - **institution:** KU Leuven
  - **link:** https://arxiv.org/pdf/2512.17673
  - **Simple LLM Summary:** The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.

- **[arXiv251222] An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution**
  - **tags:** [mlsys], [diffusion inference], [Diffusion Posterior Sampling (DPS), Manifold Constrained Gradient (MCG), conditioning step size, diffusion step count, ablation study]
  - **authors:** Yudhistira Arief Wibowo
  - **institution:** Technical University of Munich, Korea Advanced Institute of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.17675
  - **Simple LLM Summary:** This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0].

- **[arXiv251222] Digital and Web Forensics Model Cards, V1**
  - **tags:** [ai], [knowledge representation], [model cards, controlled vocabularies, web-based generator, digital forensics, web forensics]
  - **authors:** Paola Di Maio
  - **institution:** Ronin Institute, W3C AI Knowledge Representation Community Group
  - **link:** https://arxiv.org/pdf/2512.17722
  - **Simple LLM Summary:** This paper introduces a web-based framework for generating standardized model cards to represent knowledge in digital and web forensics, featuring controlled vocabularies for classification, reasoning, bias, and error. The main conclusion is the presentation of this beta framework and tool to establish an emerging standard, inviting community feedback for refinement.

- **[arXiv251222] Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure**
  - **tags:** [ai], [recommendation systems], [causal deconfounding, LightGCN, Unbiased Asymmetric Co-purchase Relationship (UACR), counterfactual exposure, BPR loss]
  - **authors:** Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang
  - **institution:** Harbin Institute of Technology (Shenzhen), Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.17733
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0101cc9ad451bf594fefde69475b9ae7433ae4f0ec8954e841150d68840d01_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Cadence, a plug-and-play framework built on LightGCN that uses causal deconfounding to compute unbiased item-item relationships and counterfactual exposure simulation to enhance recommendation diversity. The method constructs a deconfounded directed item graph and identifies diverse, causally relevant items a user has not interacted with. Experiments show it outperforms state-of-the-art models in both diversity and accuracy.

- **[arXiv251222] AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora**
  - **tags:** [mlsys], [llm inference], [AncientBench, benchmark evaluation, ancient character comprehension, excavated documents, glyph comprehension, pronunciation comprehension, meaning comprehension, contextual comprehension]
  - **authors:** Zhihan Zhou, Daqian Shi, Rui Song, Lida Shi, Xiaolei Diao, Hao Xu
  - **institution:** Jilin University, Queen Mary University of London, University of Trento
  - **link:** https://arxiv.org/pdf/2512.17756
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a15f186f48b4cc77c0d16272783515a0f56f75b51cf745384fc582f7e77f37a_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AncientBench, a comprehensive benchmark designed to evaluate large language models' comprehension of ancient Chinese, particularly focusing on excavated documents. It assesses four competencies (glyph, pronunciation, meaning, and contextual) through ten tasks. The experimental results show that while LLMs have significant potential in ancient text scenarios, a performance gap remains compared to human experts.

- **[arXiv251222] Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments**
  - **tags:** [mlsys], [llm training], [Easy Adaptation, Parameter-Efficient Fine-Tuning, LoRA, Specific Small Models, task adaptation, resource-constrained]
  - **authors:** Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo
  - **institution:** Not explicitly provided in the given text.
  - **link:** https://arxiv.org/pdf/2512.17771
  - **Simple LLM Summary:** The paper proposes Easy Adaptation (EA), a method that uses Specific Small Models (SSMs) to complement the data distribution for Large Models, enabling task adaptation without accessing the LM's internal parameters. This approach matches the performance of Parameter-Efficient Fine-Tuning (PEFT) like LoRA on diverse tasks while requiring only minimal computational resources, making it suitable for resource-constrained environments.

- **[arXiv251222] Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity**
  - **tags:** [mlsys], [others], [BERT, DistilBERT, ELECTRA, RoBERTa, Multi-BERT Ensemble, transformer models, medical entity recognition]
  - **authors:** Tanjim Taharat Aurpa, Farzana Akter, Md. Mehedi Hasan, Shakil Ahmed, Shifat Ara Rafiq, Fatema Khan
  - **institution:** University of Frontier Technology, University of Liberal Arts Bangladesh
  - **link:** https://arxiv.org/pdf/2512.17769
  - **Simple LLM Summary:** This paper proposes a Multi-BERT Ensemble approach for Bangla Medical Entity Recognition (MedER), evaluating models like BERT, DistilBERT, ELECTRA, and RoBERTa. The ensemble method achieved 89.58% accuracy, an 11.80% improvement over single-layer BERT, and the authors also created a new annotated dataset for this low-resource language task.

- **[arXiv251222] Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image**
  - **tags:** [ai], [computer vision], [vision transformer, neural parametric head models, 3d morphable models, single-image 3d reconstruction, signed distance functions]
  - **authors:** Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner
  - **institution:** Technical University of Munich, Woven by Toyota, Toyota Motor Europe
  - **link:** https://arxiv.org/pdf/2512.17773
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data.

- **[arXiv251222] Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation**
  - **tags:** [mlsys], [others], [Knowledge Mining, Digital Preservation, Semantic Web, Data Integration, Dual-Stream Architecture]
  - **authors:** Binh Vu
  - **institution:** FernUniversität in Hagen
  - **link:** https://arxiv.org/pdf/2512.17795
  - **Simple LLM Summary:** The paper proposes the Intelligent Knowledge Mining Framework (IKMF), a conceptual dual-stream architecture that combines a horizontal AI-driven mining process with a parallel trustworthy archiving stream. It aims to bridge the gap between dynamic analysis and long-term preservation, transforming static data repositories into living, actionable knowledge ecosystems.

- **[arXiv251222] LLM-based Behaviour Driven Development for Hardware Design**
  - **tags:** [mlsys], [others], [Behavior Driven Development (BDD), Large Language Models (LLMs), hardware design, test and verification, natural language processing, Electronic Design Automation (EDA)]
  - **authors:** Rolf Drechsler, Qian Liu
  - **institution:** University of Bremen, DFKI
  - **link:** https://arxiv.org/pdf/2512.17814
  - **Simple LLM Summary:** This paper investigates the use of Large Language Models (LLMs) to automate the generation of behavioral scenarios from textual specifications for Behavior Driven Development (BDD) in hardware design. The core method involves applying LLM-based techniques to interpret specifications and produce high-level behavioral descriptions. The main conclusion is that LLMs offer a promising opportunity to support and automate BDD workflows in hardware design, addressing the manual effort and complexity of current verification practices.

- **[arXiv251222] ShareChat: A Dataset of Chatbot Conversations in the Wild**
  - **tags:** [mlsys], [others], [dataset collection, multi-turn conversations, platform affordances, source citations, temporal analysis, cross-platform corpus]
  - **authors:** Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le
  - **institution:** Indiana University
  - **link:** https://arxiv.org/pdf/2512.17843
  - **Simple LLM Summary:** The paper introduces ShareChat, a large-scale dataset of real-world chatbot conversations collected from five major platforms, preserving interface-specific features like reasoning traces and source links. It demonstrates the dataset's utility through analyses of user intent satisfaction, citation behaviors, and evolving usage patterns, providing a resource for studying authentic user-LLM interactions.

- **[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes**
  - **tags:** [ai], [reinforcement learning], [energy-based models, gradient-based refinement, hindsight goal relabeling, latent-space planning]
  - **authors:** Carlos Vélez García, Miguel Cazorla, Jorge Pomares
  - **institution:** INESCOP, University of Alicante
  - **link:** https://arxiv.org/pdf/2512.17846
  - **Simple LLM Summary:** The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data.

- **[arXiv251222] Animate Any Character in Any World**
  - **tags:** [mlsys], [multi-modal inference], [3DGS, conditional autoregressive video generation, pre-trained video generator, natural language control]
  - **authors:** Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu
  - **institution:** Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.17796
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips.

- **[arXiv251222] Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life**
  - **tags:** [ai], [computational social science], [computational text analysis, machine learning (ML), natural language processing (NLP), ethnography, in-depth interviews, mixed-methods]
  - **authors:** Corey M. Abramson
  - **institution:** Rice University, UC San Francisco
  - **link:** https://arxiv.org/pdf/2512.17850
  - **Simple LLM Summary:** This paper demonstrates how computational social science tools like machine learning and natural language processing can be integrated with traditional qualitative methods (e.g., ethnography, interviews) to study aging. It concludes that these computational methods can broaden qualitative research by streamlining workflows, scaling up projects, and enabling new multi-method insights, rather than replacing its foundational approaches.

- **[arXiv251222] InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models**
  - **tags:** [mlsys], [diffusion inference], [cross-attention maps, inference-time optimization, compound loss, denoising step, spatial alignment]
  - **authors:** Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad
  - **institution:** Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google
  - **link:** https://arxiv.org/pdf/2512.17851
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp
  - **Simple LLM Summary:** InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines.

- **[arXiv251222] Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN**
  - **tags:** [mlsys], [others], [Convolutional Neural Network (CNN), Attention Mechanism, CBAM, VGG16, Grad-CAM, Layer-wise Relevance Propagation (LRP)]
  - **authors:** Balram Singh, Ram Prakash Sharma, Somnath Dey
  - **institution:** National Institute of Technology Hamirpur, Indian Institute of Technology Indore
  - **link:** https://arxiv.org/pdf/2512.17864
  - **Simple LLM Summary:** This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques.

- **[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning**
  - **tags:** [mlsys], [multi-modal training], [ViPR, ViPR-Eureka, ViPR-RL, behavior cloning, VLM-in-the-loop Parallel Refinement, LLM-guided contact sampling, sim-to-real transfer, GPU simulation]
  - **authors:** Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper
  - **institution:** Robotics and AI Institute
  - **link:** https://arxiv.org/pdf/2512.17853
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks.

- **[arXiv251222] Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow**
  - **tags:** [ai], [generative modeling], [Wasserstein-Fisher-Rao gradient flow, weighted stochastic differential equations, Feynman-Kac representation, score-based diffusion models, Langevin dynamics]
  - **authors:** Herlock Rahimi
  - **institution:** Yale University
  - **link:** https://arxiv.org/pdf/2512.17878
  - **Simple LLM Summary:** The paper proposes a new sampling method for generative modeling by implementing Wasserstein-Fisher-Rao gradient flow via weighted stochastic differential equations, using the Feynman-Kac representation. This approach aims to overcome the slow mixing rates of traditional diffusion models in non-log-concave, multimodal target distributions by incorporating controlled mass reweighting. The study provides a rigorous geometric and operator-theoretic foundation for future developments in this area.

- **[arXiv251222] Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally**
  - **tags:** [ai], [human-computer interaction], [anthropomorphism, cross-national experiments, humanlike AI design, behavioral measures, cultural mediation]
  - **authors:** Robin Schimmelpfennig, Mark Díaz, Vinodkumar Prabhakaran, Aida Davani
  - **institution:** Max Planck Institute for Human Development, Google Research
  - **link:** https://arxiv.org/pdf/2512.17898
  - **Simple LLM Summary:** The paper conducts two large-scale cross-national experiments with 3,500 participants across 10 countries, involving real-time interactions with an AI system. It finds that humanlike design increases anthropomorphism but does not universally boost engagement and trust; instead, these outcomes are culturally mediated, with the same design choices having opposite effects in different populations like Brazil and Japan.

- **[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras**
  - **tags:** [mlsys], [multi-modal inference], [diffusion model, bird's-eye-view, radar cross section, Doppler, point cloud generation, foundation models]
  - **authors:** Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany
  - **institution:** Technion, MIT, NVIDIA, University of Toronto, Vector Institute
  - **link:** https://arxiv.org/pdf/2512.17897
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp
  - **Simple LLM Summary:** RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.

- **[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models**
  - **tags:** [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]
  - **authors:** Jonathon Fox, William J Buchanan, Pavlos Papadopoulos
  - **institution:** Edinburgh Napier University
  - **link:** https://arxiv.org/pdf/2512.17902
  - **Simple LLM Summary:** This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.

- **[arXiv251222] When Reasoning Meets Its Laws**
  - **tags:** [ai], [large reasoning models], [laws of reasoning, compute law, accuracy law, monotonicity, compositionality, LoRe-Bench, finetuning]
  - **authors:** Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang
  - **institution:** University of Illinois Urbana-Champaign, Massachusetts Institute of Technology, University of Pennsylvania, New York University, NTT Research
  - **link:** https://arxiv.org/pdf/2512.17901
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9db7b3665dbba1bcaed95897dff8a53103ef5bfe963b50f03c044189965a72_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Laws of Reasoning (LoRe), a framework that formalizes desired reasoning behaviors in large reasoning models, including compute and accuracy laws. It proposes LoRe-Bench to evaluate monotonicity and compositionality, and develops a finetuning method to improve compositionality. The study finds that better compliance with these laws leads to enhanced reasoning performance across benchmarks.

- **[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting**
  - **tags:** [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]
  - **authors:** Ananta R. Bhattarai, Helge Rhodin
  - **institution:** Bielefeld University
  - **link:** https://arxiv.org/pdf/2512.17908
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model's priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.

- **[arXiv251222] Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics**
  - **tags:** [ai], [uncertainty quantification], [conformal prediction, calibration, machine learning, high-energy physics, collider research, finite-sample guarantees, prediction sets, p-values]
  - **authors:** Jack Y. Araz, Michael Spannowsky
  - **institution:** University College London, City, University of London, Durham University
  - **link:** https://arxiv.org/pdf/2512.17048
  - **Simple LLM Summary:** This paper proposes using conformal prediction as a standard calibration layer for machine learning models in high-energy physics, providing rigorous uncertainty quantification and finite-sample coverage guarantees without retraining. It demonstrates the method's applicability across regression, classification, anomaly detection, and generative modeling using collider datasets. The authors conclude that adopting conformal calibration enables reliable statistical inference and principled decision-making in experimental analyses.

- **[arXiv251222] Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings**
  - **tags:** [mlsys], [others], [graph attention networks, electroencephalography, spatio-temporal graphs, edge analysis, low-cost hardware, RaspberryPi]
  - **authors:** Szymon Mazurek, Stephen Moore, Alessandro Crimi
  - **institution:** AGH University of Krakow, University of Cape Coast
  - **link:** https://arxiv.org/pdf/2507.15118
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a graph attention network (GAT) framework that models EEG signals as spatio-temporal graphs to detect epilepsy, with a focus on low-cost hardware for deployment in low-resource settings. The method adapts GATs to analyze edge connectivity for biomarker identification and is designed for lightweight training and deployment. The results demonstrate promising classification performance and highlight the potential for scalable, accessible diagnostic support in underserved regions.

- **[arXiv251222] Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning**
  - **tags:** [mlsys], [others], [multi-layer graph, GNN, temporal GNN, logistic regression, Random Forest, correlation-based, systemic risk]
  - **authors:** Sandeep Neela
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17185
  - **Simple LLM Summary:** This paper introduces the Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility. It demonstrates that graph-derived features from this model provide useful early-warning signals for market crashes, outperforming standard feature-based models.

- **[arXiv251222] From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework**
  - **tags:** [ai], [cognitive modeling], [Graph Neural Networks, Graph Theory, inductive biases, computational graph, systematic ablation]
  - **authors:** Quan Do, Caroline Ahn, Leah Bakst, Michael Pascale, Joseph T. McGuire, Chantal E. Stern, Michael E. Hasselmo
  - **institution:** Boston University
  - **link:** https://arxiv.org/pdf/2512.17255
  - **Simple LLM Summary:** The paper introduces a framework combining Graph Theory and Graph Neural Networks to formalize inductive biases as explicit priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus, it shows that differences in graph-based priors explain individual differences in human solutions, revealing how generalization depends on specific prior structures and why human-like errors arise from incorrect priors.

- **[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics**
  - **tags:** [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]
  - **authors:** Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton
  - **institution:** University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17534
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.

- **[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation**
  - **tags:** [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]
  - **authors:** Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein
  - **institution:** German Cancer Research Center (DKFZ), Heidelberg University
  - **link:** https://arxiv.org/pdf/2512.17774
  - **Simple LLM Summary:** This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network's architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.

- **[arXiv251222] Exploring the Effect of Basis Rotation on NQS Performance**
  - **tags:** [ai], [quantum machine learning], [neural quantum states, restricted boltzmann machine, quantum natural gradient, quantum fisher information, fubini-study distance, basis rotation, ising model]
  - **authors:** Sven Benjamin Kožić, Vinko Zlatić, Fabio Franchini, Salvatore Marco Giampaolo
  - **institution:** Institut Ruđer Bošković
  - **link:** https://arxiv.org/pdf/2512.17893
  - **Simple LLM Summary:** This paper uses an analytically solvable rotated Ising model to study how local basis rotations affect the optimization of Neural Quantum States (NQS). It finds that rotations relocate the target wavefunction in parameter space, exposing information-geometric barriers like saddle points that trap shallow architectures like RBMs, highlighting the need for landscape-aware model design.