---
slug: /daily/csai/20251222-20251228
---
# 20251222-20251228 (cs.AI)

## 2025-12-22

- **[arXiv251222] Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases**
  - **tags:** [ai], [knowledge representation and reasoning], [entity set expansion, expansion graph, logical formula, semantic inclusion, computational complexity]
  - **authors:** Pietro Cofone, Giovanni Amendola, Marco Manna, Aldo Ricioppo
  - **institution:** University of Calabria, University of Cyprus
  - **link:** https://arxiv.org/pdf/2512.16953
  - **Simple LLM Summary:** This paper proposes a logic-based framework using expansion graphs, which are rooted directed acyclic graphs, to support taxonomic expansions of entity sets from knowledge bases. To avoid the impracticality of fully materializing these potentially large graphs, the authors formalize efficient reasoning tasks to check relationships between entity tuples within the graph structure. Their main conclusion is that, under realistic assumptions like bounded input, these tasks can be implemented efficiently, enabling local and incremental navigation without full graph construction.

- **[arXiv251222] Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories**
  - **tags:** [mlsys], [multi-modal inference], [visual anchoring, asset-first mechanism, temporal bridge, diffusion models, large language model (LLM), text-to-video (T2V), character consistency, multi-stage pipeline]
  - **authors:** Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
  - **institution:** BITS Pilani
  - **link:** https://arxiv.org/pdf/2512.16954
  - **Simple LLM Summary:** This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method's necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.

- **[arXiv251222] Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections**
  - **tags:** [ai], [computer vision], [YOLOv8, Finer-CAM, saliency maps, cross-validation, TLS point cloud projections]
  - **authors:** Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke
  - **institution:** University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)
  - **link:** https://arxiv.org/pdf/2512.16950
  - **Simple LLM Summary:** This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models' perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.

- **[arXiv251222] Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach**
  - **tags:** [sys], [pattern matching algorithms], [Ukkonen's Algorithm, Suffix Trees, pattern recognition, text-search algorithms]
  - **authors:** Xinyu Guan, Shaohua Zhang
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2512.16927
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a novel pattern matching algorithm that combines Ukkonen's Algorithm for constructing Suffix Trees with a new search technique using Python's dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.

- **[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]
  - **authors:** SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju
  - **institution:** NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2512.16925
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp
  - **Simple LLM Summary:** V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.

- **[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework**
  - **tags:** [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]
  - **authors:** Kamer Ali Yuksel
  - **institution:** aiXplain Inc
  - **link:** https://arxiv.org/pdf/2512.16970
  - **Simple LLM Summary:** This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.

- **[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]
  - **authors:** Saksham Sahai Srivastava, Haoyu He
  - **institution:** University of Georgia
  - **link:** https://arxiv.org/pdf/2512.16962
  - **Simple LLM Summary:** This paper introduces MemoryGraft, a novel attack that poisons an LLM agent's long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent's semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT's DataInterpreter.

- **[arXiv251222] InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression**
  - **tags:** [mlsys], [multi-modal training], [discrete video tokenization, transformer-based adaptive compressor, evidence lower bound (ELBO), information-theoretic compression, adaptive tokenization]
  - **authors:** Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu
  - **institution:** NVIDIA, Stanford University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.16975
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.

- **[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows**
  - **tags:** [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]
  - **authors:** Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.16969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.

- **[arXiv251222] Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations**
  - **tags:** [mlsys], [others], [search engine audit, semantic alignment, topical annotation, trajectory analysis]
  - **authors:** Erica Coppolillo, Simone Mungari
  - **institution:** University of Calabria, ICAR-CNR, University of Southern California
  - **link:** https://arxiv.org/pdf/2512.17027
  - **Simple LLM Summary:** The paper conducts a comparative audit of search engine recommendations on Wikipedia and Grokipedia by analyzing over 70,000 results from nearly 10,000 neutral English word queries. It finds that both platforms frequently generate weakly related or unexpected results from innocuous queries, though their recommendation sets often differ substantially in topical distribution and exploration trajectories.

- **[arXiv251222] A Women's Health Benchmark for Large Language Models**
  - **tags:** [ai], [healthcare AI evaluation], [women's health benchmark, large language models, error types, model stumps, query types]
  - **authors:** Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard
  - **institution:** Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine
  - **link:** https://arxiv.org/pdf/2512.17028
  - **Simple LLM Summary:** The paper introduces the Women's Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women's health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women's health advice.

- **[arXiv251222] Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats**
  - **tags:** [mlsys], [others], [agentic AI, cross-layer threats, role-based architecture, severity matrix, attack-chain analysis, OWASP]
  - **authors:** Ali Eslami, Jiangbo Yu
  - **institution:** Unknown
  - **link:** https://arxiv.org/pdf/2512.17041
  - **Simple LLM Summary:** This paper introduces a role-based architecture for Agentic Vehicles to systematically analyze security threats, including cognitive vulnerabilities and cross-layer risks. It concludes by providing a structured framework for assessing how small distortions can escalate into unsafe behavior in both human-driven and autonomous vehicles.

- **[arXiv251222] Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation**
  - **tags:** [mlsys], [others], [adversarial attacks, deep learning, cybersickness detection, visual tunneling, MI-FGSM, PGD, C&W, DeepTCN, Transformer]
  - **authors:** Istiak Ahmed, Ripan Kumar Kundu, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.17029
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Adversarial-VR, an open-source Unity testbed that integrates DeepTCN and Transformer models for real-time cybersickness detection and mitigation, and evaluates their robustness against adversarial attacks like MI-FGSM, PGD, and C&W. The results show these attacks can successfully fool the system, significantly degrading model accuracy and preventing correct mitigation.

- **[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering**
  - **tags:** [ai], [knowledge graph question answering], [reinforcement learning, subgraph selection, graph pruning, llm fine-tuning, relation-centric reasoning]
  - **authors:** Yinxu Tang, Chengsong Huang, Jiaxin Huang, William Yeoh
  - **institution:** Washington University in St. Louis
  - **link:** https://arxiv.org/pdf/2512.17043
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/069fd74faaf7500b76c5bd6958a190a707d8ccbe86484c3026a357b38657a47a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces UniRel-R1, a framework for relation-centric knowledge graph question answering that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The method is designed to identify compact and informative subgraph answers by rewarding specific relations and lower-degree entities. Experiments show it outperforms baselines in connectivity and reward and generalizes well to unseen entities and relations.

- **[arXiv251222] Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations**
  - **tags:** [ai], [social simulation], [large language model, generative agents, causal analysis, intergroup conflict, threat perception]
  - **authors:** Suhaib Abdurahman, Farzan Karimi-Malekabadi, Chenxiao Yu, Nour S. Kteily, Morteza Dehghani
  - **institution:** University of Southern California, Northwestern University
  - **link:** https://arxiv.org/pdf/2512.17066
  - **Simple LLM Summary:** This paper uses simulations with LLM-driven generative agents in virtual societies to causally analyze intergroup conflict. It finds that realistic threat directly increases hostility, while symbolic threat has a weaker effect mediated by ingroup bias and only increases hostility when realistic threat is absent.

- **[arXiv251222] Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL**
  - **tags:** [mlsys], [llm training], [knowledge distillation, chain-of-thought, structured reasoning, query execution plan, text-to-sql]
  - **authors:** Khushboo Thaker, Yony Bresler
  - **institution:** Crater Labs
  - **link:** https://arxiv.org/pdf/2512.17053
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/906b49597857a3cad8e1c9c8d6cdbec46e7807fe819943d1e6d91facfb7f18bd_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Struct-SQL, a knowledge distillation framework that trains a small language model using a structured chain-of-thought derived from query execution plans, rather than unstructured reasoning traces. The distilled model achieves an 8.1% absolute improvement over an unstructured baseline, primarily due to a reduction in syntactic errors. This demonstrates that structured logical blueprints are beneficial for reliable SQL generation in small models.

- **[arXiv251222] Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution**
  - **tags:** [ai], [social media analysis], [Augmented Dickey-Fuller test, KPSS test, Spearman correlation, Chi-square test, time series analysis, stationarity testing]
  - **authors:** Ohoud Alzahrani, Russell Beale, Bob Hendley
  - **institution:** University of Birmingham
  - **link:** https://arxiv.org/pdf/2512.17067
  - **Simple LLM Summary:** This paper conducts a longitudinal study analyzing the temporal behavior of promotional Twitter bots using time series analysis and statistical tests on ten content-based meta-features. It finds that bot behavior is non-stationary, with individual features and their interdependencies evolving systematically over time and across bot generations. The conclusion is that bot-detection systems must account for this dynamic adaptation and avoid treating behavioral features as static.

- **[arXiv251222] On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues**
  - **tags:** [mlsys], [others], [multi-agent system, transactional analysis, ego states, information retrieval, vector stores, ablation test]
  - **authors:** Monika Zamojska, Jarosław A. Chudziak
  - **institution:** Warsaw University of Technology
  - **link:** https://arxiv.org/pdf/2512.17060
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ea02a6d58821ccc3cc89deee5daf014a7e650e133502a2a64e5cd69e54c8596_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-agent system architecture that integrates Transactional Analysis theory, dividing each agent into Parent, Adult, and Child ego states, and enhances their responses with contextual information retrieval from vector stores. The system is evaluated through ablation tests in a simulated dialogue scenario. The results show that this psychologically grounded structure improves the realism of LLM-based agent behavior.

- **[arXiv251222] Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?**
  - **tags:** [ai], [mathematical reasoning], [chain-of-thought prompting, reinforcement learning, GRPO, fine-tuning, error recovery]
  - **authors:** Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah
  - **institution:** MIT
  - **link:** https://arxiv.org/pdf/2512.17079
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp
  - **Simple LLM Summary:** The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.

- **[arXiv251222] When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation**
  - **tags:** [ai], [dialogue topic segmentation], [window-tolerant F1, boundary density, segment coherence, granularity-aware evaluation]
  - **authors:** Michael H. Coen
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17083
  - **Simple LLM Summary:** This paper introduces a new evaluation framework for dialogue topic segmentation that emphasizes boundary density and segment coherence alongside window-tolerant F1. It demonstrates through cross-dataset experiments that reported performance differences are often artifacts of annotation granularity mismatches, not model quality. The core conclusion is that topic segmentation should be viewed as selecting an appropriate granularity rather than predicting a single correct boundary set.

- **[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence**
  - **tags:** [ai], [reinforcement learning], [AIXI, Choquet integrals, imprecise probability theory, semimeasure loss, utility functions]
  - **authors:** Cole Wyeth, Marcus Hutter
  - **institution:** University of Waterloo, Google DeepMind, Australian National University
  - **link:** https://arxiv.org/pdf/2512.17086
  - **Simple LLM Summary:** This paper generalizes the AIXI reinforcement learning agent to handle a wider class of utility functions, confronting the ambiguity of finite history predictions by interpreting belief distributions as imprecise probabilities. It explores computing expected utilities using Choquet integrals from imprecise probability theory and investigates their computability. The authors show that the standard recursive value function is a special case, but the most general utilities under a "death interpretation" cannot be characterized by these integrals.

- **[arXiv251222] How to Square Tensor Networks and Circuits Without Squaring Them**
  - **tags:** [ai], [probabilistic modeling], [tensor networks, squared circuits, probabilistic circuits, marginalization, canonical forms, unitary matrices, distribution estimation]
  - **authors:** Lorenzo Loconte, Adrián Javaloy, Antonio Vergari
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.17090
  - **Simple LLM Summary:** This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.

- **[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]
  - **authors:** Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman
  - **institution:** Toyota Research Institute
  - **link:** https://arxiv.org/pdf/2512.17091
  - **Simple LLM Summary:** This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.

- **[arXiv251222] UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data**
  - **tags:** [ai], [interpretability], [counterfactual explanations, model-agnostic, time series, ECG, LIME, SHAP]
  - **authors:** Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun
  - **institution:** Boston University, Sandia National Laboratories, Boston Medical Center
  - **link:** https://arxiv.org/pdf/2512.17100
  - **Simple LLM Summary:** The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.

- **[arXiv251222] A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving**
  - **tags:** [mlsys], [llm training], [solver-in-the-loop, instruction-tuning, supervised fine-tuning, best-of-N sampling, answer set programming, semantic parsing]
  - **authors:** Timo Pierre Schrader, Lukas Lange, Tobias Kaminski, Simon Razniewski, Annemarie Friedrich
  - **institution:** Bosch Center for AI, University of Augsburg, ScaDS.AI & TU Dresden
  - **link:** https://arxiv.org/pdf/2512.17093
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38c83df2ce552270bc09f323934a96a0aad16af58e736a7049ccfd73afeed0d4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a solver-in-the-loop framework that uses an ASP solver to provide feedback on LLM-generated code, creating a dataset of chosen and rejected instances for supervised fine-tuning. The method improves LLM performance on generating Answer Set Programming code for logic puzzles, demonstrating consistent gains across different prompting settings and datasets.

- **[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library**
  - **tags:** [mlsys], [post-training], [reinforcement learning, skill library, sequential rollout, skill-integrated reward, GRPO, self-improving agent]
  - **authors:** Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong
  - **institution:** University of Wisconsin–Madison, AWS Agentic AI
  - **link:** https://arxiv.org/pdf/2512.17102
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fdb48e8df3d2e43c1e8301082bd3478f7894eef19c7194ccd54fb1c77738ef_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SAGE, a reinforcement learning framework that enhances LLM-based agents by integrating a skill library through sequential rollouts and a skill-integrated reward. This approach enables agents to accumulate and reuse skills across tasks for continual self-improvement. Experiments show SAGE improves task completion accuracy while significantly reducing interaction steps and token usage compared to existing methods.

- **[arXiv251222] Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs**
  - **tags:** [mlsys], [llm training], [Generalized Primal Averaging (GPA), DiLoCo, Schedule-Free, AdamW, Nesterov's method, primal averaging, optimizer, iterate averaging]
  - **authors:** Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao
  - **institution:** Meta Superintelligence Labs
  - **link:** https://arxiv.org/pdf/2512.17131
  - **Simple LLM Summary:** The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov's method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.

- **[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction**
  - **tags:** [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]
  - **authors:** Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang
  - **institution:** Johns Hopkins University, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.17137
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.

- **[arXiv251222] Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty**
  - **tags:** [ai], [algorithmic information theory], [Solomonoff induction, Bayesian Model Averaging, hypothesis ranking, systematic generalisation, uncertainty estimation]
  - **authors:** Josh Barber, Rourke Young, Cameron Coombe, Will Browne
  - **institution:** Queensland University of Technology, CSIRO
  - **link:** https://arxiv.org/pdf/2512.17145
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb373087fc2ec93e8e3a1729cbb4c2df71ab2a3e5c7a3936acdba21fa6ee2c9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that uses a Solomonoff-inspired scoring to weight hypotheses generated by a Large Language Model based on their simplicity and predictive fit. The method, applied to Mini-ARC tasks, produces uncertainty-aware predictions by spreading probability across multiple hypotheses, contrasting with Bayesian Model Averaging which tends to concentrate weight on a single candidate. The results highlight the value of algorithmic information-theoretic priors for robust, interpretable reasoning under uncertainty.

- **[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors**
  - **tags:** [ai], [reinforcement learning], [interactive reinforcement learning, multi-teacher learning, Q-learning, teacher selection, concept drift]
  - **authors:** Maher Mesto, Francisco Cruz
  - **institution:** University of New South Wales, Universidad Central de Chile
  - **link:** https://arxiv.org/pdf/2512.17180
  - **Simple LLM Summary:** This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.

- **[arXiv251222] PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases**
  - **tags:** [mlsys], [llm inference], [large language model, explainable ai, augmented reality, personalized explanations, real-time object detection, user study]
  - **authors:** Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.17172
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7767a7bc851a49f82148423782803913a5c377f60c4ce3a9cc7383c22a6d08a4_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PILAR, a framework that uses a pre-trained large language model (LLM) to generate unified, context-aware, and personalized explanations for AI-driven augmented reality systems. A user study on a recipe recommendation prototype showed that the LLM-based explanation interface significantly improved user task performance and perceived transparency compared to a traditional template-based approach.

- **[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, retrieval-augmented generation, multi-modal large language models, explainable AI]
  - **authors:** Shengwei Zhao, Jingwen Yao, Sitong Wei, Linhai Xu, Yuying Liu, Dong Zhang, Zhiqiang Tian, Shaoyi Du
  - **institution:** Xi’an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.17194
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e101261754ebc1d899bc11f5f5d9245217cf53bcbfc614d62f737f8cbd530473_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MMRAG-RFT, a two-stage reinforcement fine-tuning framework for explainable multi-modal retrieval-augmented generation. The method uses rule-based and reasoning-based reinforcement learning to filter documents and jointly optimize ranking and answer generation, achieving state-of-the-art results on benchmark datasets.

- **[arXiv251222] UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark**
  - **tags:** [mlsys], [multi-modal inference], [unified multimodal model, benchmark, omni-dimensional evaluation, understanding, generation, editing]
  - **authors:** Kai Liu, Leyang Chen, Wenbo Li, Zhikai Chen, Zhixin Wang, Renjing Pei, Linghe Kong, Yulun Zhang
  - **institution:** Shanghai Jiao Tong University, The Chinese University of Hong Kong, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.17196
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dffc23613309b3df00996da4dd30419c4aa81ea36355df55ab509eed8b7380c9_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces UmniBench, a benchmark designed to holistically evaluate Unified Multimodal Models (UMMs) by assessing their understanding, generation, and editing abilities within a single process, using the model's own understanding capability to judge its outputs. It covers 13 domains and over 200 concepts to provide comprehensive and fine-grained assessments. The authors benchmark 24 models and conclude that UmniBench offers a more integrated and objective evaluation framework compared to isolated, task-specific benchmarks.

- **[arXiv251222] Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening**
  - **tags:** [mlsys], [diffusion inference], [one-step distillation, lightweight ensemble blocks, four-stage training, pansharpening, diffusion model, end-to-end network]
  - **authors:** Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.17202
  - **Simple LLM Summary:** This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model's inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model.

- **[arXiv251222] Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines**
  - **tags:** [sys], [robotics and navigation], [extended kalman filter, inertial measurement unit, wheel odometer, dead reckoning]
  - **authors:** Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang
  - **institution:** Peking University
  - **link:** https://arxiv.org/pdf/2512.17215
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dead reckoning method for a self-propelled pipeline robot, using an IMU for initial attitude estimation, refining it with an Extended Kalman Filter, and combining it with wheel odometer data for localization. The method was tested in a rectangular loop pipeline, and the results verified the effectiveness of the proposed algorithm for navigating complex three-dimensional pipelines.

- **[arXiv251222] The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes**
  - **tags:** [ai], [ethics and society], [Systematic Literature Review (SLR), PRISMA, Maqasid al-Shariah, hifz al-ird, hifz al-nafs, adl, tabayyun]
  - **authors:** Wisnu Uriawan, Imany Fauzy Rahman, Muhamad Zidan, Irma Rohmatillah, Muhammad Arkan Raihan, Irma Dwiyanti
  - **institution:** UIN Sunan Gunung Djati Bandung
  - **link:** https://arxiv.org/pdf/2512.17218
  - **Simple LLM Summary:** This study employs a Systematic Literature Review (SLISMA) to formulate an Islamic ethical framework for preventing deepfake abuse. It concludes that principles from Maqasid al-Shariah, such as protecting honor and self, provide a normative basis for shifting from punitive to preventative approaches, focusing on human dignity and the common good in the digital age.

- **[arXiv251222] Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition**
  - **tags:** [mlsys], [llm inference], [error level noise embedding, n-best hypotheses, noise-aware modeling, whisper, llama-2, word error rate, fine-tuning]
  - **authors:** Zahra Rahmani, Hossein Sameti
  - **institution:** Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.17247
  - **Simple LLM Summary:** This paper proposes a robust noise-sensitive ASR error correction framework for Persian. It introduces Error Level Noise (ELN) embeddings, derived from disagreements in multiple ASR hypotheses, to condition a fine-tuned LLaMA-2 model, enabling it to reason about noise-induced uncertainty. The ELN-conditioned model significantly reduces Word Error Rate compared to text-only baselines, demonstrating the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust speech recognition in noisy environments.

- **[arXiv251222] Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics**
  - **tags:** [ai], [privacy-preserving data synthesis], [multi-objective optimization, origin-destination matrices, dwell-travel time quantiles, universal law of daily visited locations, synthetic trajectory generation]
  - **authors:** Jun'ichi Ozaki, Ryosuke Susuta, Takuhiro Moriyama, Yohei Shida
  - **institution:** Not explicitly provided; cannot infer from given information.
  - **link:** https://arxiv.org/pdf/2512.17239
  - **Simple LLM Summary:** This paper proposes a method to generate a privacy-preserving synthetic dataset of individual daily trajectories by integrating aggregated origin-destination flows with behavioral constraints in a multi-objective optimization framework. The method successfully reproduces realistic human mobility patterns in two Japanese regions, providing a practical pathway for high-resolution mobility analytics without using sensitive personal data.

- **[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs**
  - **tags:** [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]
  - **authors:** Madhava Gaikwad
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.17251
  - **Simple LLM Summary:** The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.

- **[arXiv251222] Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction**
  - **tags:** [mlsys], [multi-modal inference], [speculative execution, TD-MPC2, latent-space MPC, mismatch correction, action queue, transformer corrector]
  - **authors:** Ziyang Lin, Zixuan Sun, Sanhorn Chen, Xiaoyang Chen, Roy Zhao
  - **institution:** University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17250
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/801e0aab9fd9dcfb7d20ebc658880a9fa5c22a62c30b2127c1037ab48024b399_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a speculation-and-correction framework to reduce inference latency in real-time control agents. It uses a world model to predict future actions and latent states, then applies a lightweight learned corrector to adjust these actions when new observations arrive, reducing planning calls by 43.6% with minimal performance loss. The results show that correction is essential for reliable latency reduction in sequential control tasks.

- **[arXiv251222] Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems**
  - **tags:** [mlsys], [llm inference], [audit agents, attestation protocols, constrained reasoning, cryptographic attestation, symbolic methods, benchmark suite, verifiability]
  - **authors:** Abhivansh Gupta
  - **institution:** Indian Institute of Technology, Roorkee
  - **link:** https://arxiv.org/pdf/2512.17259
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a Verifiability-First architecture for LLM-based agents, integrating runtime attestations, lightweight audit agents for continuous verification, and challenge-response protocols for high-risk operations. It introduces the OPERA benchmark to evaluate the detectability and speed of remediation for misaligned behavior, shifting the focus from measuring the propensity for misalignment to ensuring reliable detection and control.

- **[arXiv251222] ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework**
  - **tags:** [mlsys], [others], [GPT-style transformer, autoregressive next-event prediction, player embeddings, counterfactual simulation, residual On-Ball Value (rOBV)]
  - **authors:** Miru Hong, Minho Lee, Geonhee Jo, Jae-Hee So, Pascal Bauer, Sang-Ki Ko
  - **institution:** University of Seoul, Saarland University, Bank of Korea
  - **link:** https://arxiv.org/pdf/2512.17266
  - **Simple LLM Summary:** The paper introduces ScoutGPT, a GPT-based autoregressive transformer model that treats football match events as discrete token sequences to predict the next action and its value, conditioned on player identity. It enables counterfactual simulations by swapping player embeddings to assess how a player's performance might change in a new tactical context. Evaluated on Premier League data, the model outperforms baselines in prediction accuracy and provides a principled framework for evaluating player transfer fit.

- **[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators**
  - **tags:** [ai], [evaluation framework], [LLM-as-a-Judge, regression, MetricBank, retrieval, human feedback correlation]
  - **authors:** Michael J. Ryan, Yanzhe Zhang, Amol Salunkhe, Yi Chu, Di Xu, Diyi Yang
  - **institution:** Stanford University, American Express
  - **link:** https://arxiv.org/pdf/2512.17267
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AutoMetrics, a framework that synthesizes evaluation metrics by combining retrieved metrics from a curated bank with automatically generated LLM-as-a-Judge criteria, composed via regression to maximize correlation with human feedback. It demonstrates that AutoMetrics significantly improves correlation with human judgments over standard LLM-as-a-Judge approaches while requiring minimal human feedback data. The method can serve as an effective proxy reward for optimizing AI applications.

- **[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory**
  - **tags:** [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]
  - **authors:** Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li
  - **institution:** Wuhan University, Tongyi Lab, Zhongguancun Academy
  - **link:** https://arxiv.org/pdf/2512.17270
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.

- **[arXiv251222] WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images**
  - **tags:** [ai], [medical image segmentation], [wavelet-guided enhancement, dual-attention feature fusion, U-shaped Mamba architecture, Wavelet-denoised High-Frequency-guided Feature (WHF), Dual Attention Feature Fusion (DAFF)]
  - **authors:** Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng
  - **institution:** Beijing Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.17278
  - **Simple LLM Summary:** The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications.

- **[arXiv251222] Subjective Question Generation and Answer Evaluation using NLP**
  - **tags:** [mlsys], [llm training], [large language models, instruct-tuning, bloom's taxonomy, subjective evaluation, question generation, answer evaluation]
  - **authors:** G. M. Refatul Islam, Safwan Shaheer, Yaseen Nur, Mohammad Rafid Hamid
  - **institution:** Brac University
  - **link:** https://arxiv.org/pdf/2512.17289
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f03ca77cc9ebde43ea5a7c83c936ce7c8843b9c39c6b6c58614cb92eb1ce8fc_w640_q70.webp
  - **Simple LLM Summary:** This research proposes a framework that uses instruct-tuned large language models (LLMs) to generate subjective questions and evaluate student answers, particularly for higher-order thinking skills. The study concludes that this approach can effectively automate the assessment of complex, subjective understanding, a task traditionally requiring human evaluators.

- **[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge**
  - **tags:** [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]
  - **authors:** Abdullah M. Zyarah, Dhireesha Kudithipudi
  - **institution:** University of Texas at San Antonio, University of Baghdad
  - **link:** https://arxiv.org/pdf/2512.17299
  - **Simple LLM Summary:** This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.

- **[arXiv251222] Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track**
  - **tags:** [mlsys], [diffusion training], [Self-Purifying Flow Matching (SPFM), flow matching, text-to-speech (TTS), Supertonic, fine-tuning, in-the-wild speech, label noise mitigation]
  - **authors:** June Young Yi, Hyeongju Kim, Juheon Lee
  - **institution:** Supertone Inc.
  - **link:** https://arxiv.org/pdf/2512.17293
  - **Simple LLM Summary:** This paper presents a lightweight TTS system that fine-tunes the Supertonic model using Self-Purifying Flow Matching (SPFM) to robustly adapt to noisy, in-the-wild speech data. SPFM handles label noise by comparing conditional and unconditional flow matching losses, routing suspicious samples for unconditional training while still using their acoustic information. The resulting model achieved the best word error rate in the WildSpoof 2026 challenge, demonstrating that open-weight architectures can be effectively adapted to real-world conditions with explicit noise-handling mechanisms.

- **[arXiv251222] Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation**
  - **tags:** [mlsys], [llm inference], [large language models, turn-based battle system, strategic decision-making, content generation, procedural generation, adaptive difficulty]
  - **authors:** Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani
  - **link:** https://arxiv.org/pdf/2512.17308
  - **Simple LLM Summary:** The paper develops a turn-based Pokémon battle system where LLMs act as agents, making tactical decisions based on a structured battle state without domain-specific training. The core method involves evaluating LLMs on strategic reasoning and their ability to generate novel game content. The main conclusion is that LLMs can function as dynamic game opponents and designers, offering a practical alternative to reinforcement learning for strategic games.

- **[arXiv251222] Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability**
  - **tags:** [ai], [explainable AI (XAI)], [graph theory, model decomposition, hypothesis-evidence structure, Cox proportional hazards model, PREDICT]
  - **authors:** Michael Merry, Pat Riddle, Jim Warren
  - **institution:** University of Auckland
  - **link:** https://arxiv.org/pdf/2512.17316
  - **Simple LLM Summary:** This paper proposes a formal, testable criterion for inherent explainability in AI, using graph theory to decompose models into verifiable structure-local explanations called annotations. The method is applied to demonstrate the inherent explainability of a clinical cardiovascular risk model (PREDICT). The work provides a rigorous foundation for regulators and formalizes the distinction between an explainable model and an explained one.

- **[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs**
  - **tags:** [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]
  - **authors:** Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao
  - **institution:** Nanjing University
  - **link:** https://arxiv.org/pdf/2512.17319
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.

- **[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs**
  - **tags:** [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]
  - **authors:** Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas
  - **institution:** University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17352
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.

- **[arXiv251222] Dialectics for Artificial Intelligence**
  - **tags:** [ai], [algorithmic information theory], [algorithmic information theory, Kolmogorov complexity, reversible consistency relation, excess information, dialectics optimization]
  - **authors:** Zhengmian Hu
  - **institution:** Adobe Research
  - **link:** https://arxiv.org/pdf/2512.17373
  - **Simple LLM Summary:** This paper proposes an algorithmic-information framework to define concepts as structural relations within an agent's total experience, using reversible consistency and excess information to enable dynamic concept revision. It formulates dialectics as an optimization process where concepts compete to explain new information, leading to expansion, splitting, or merging. The approach also formalizes low-cost concept transmission between agents through shared seeds, making concept alignment a concrete compute-bits trade-off.

- **[arXiv251222] TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data**
  - **tags:** [mlsys], [post-training], [imitation learning, dataset aggregation, direct preference optimization, closed-loop evaluation, expert takeover data]
  - **authors:** Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Science and Technology Beijing; Chongqing Chang'an Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.17370
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TakeAD, a framework that fine-tunes a pre-trained imitation learning policy for autonomous driving using expert takeover data. The method combines iterative Dataset Aggregation (DAgger) for imitation with Direct Preference Optimization (DPO) for preference alignment to improve closed-loop performance. Experiments show it effectively mitigates the open-loop gap and outperforms pure imitation learning methods.

- **[arXiv251222] Optimisation of Aircraft Maintenance Schedules**
  - **tags:** [ai], [evolutionary algorithms], [evolutionary algorithm, genetic operators, fitness function, maintenance scheduling, optimisation]
  - **authors:** Neil Urquhart, Amir Rahimi, Efstathios-Al. Tingas
  - **institution:** Edinburgh Napier University
  - **link:** https://arxiv.org/pdf/2512.17412
  - **Simple LLM Summary:** This paper applies an Evolutionary Algorithm to solve the aircraft maintenance scheduling problem, which involves assigning qualified staff to tasks within a turnaround window. The algorithm is benchmarked on 60 generated problem instances to evaluate its performance. The study demonstrates the proposed representation and genetic operators for this optimisation task.

- **[arXiv251222] Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques**
  - **tags:** [mlsys], [others], [FastText, NSFWJS, sentiment analysis, data restoration, classification]
  - **authors:** Xingyu Feng
  - **institution:** Hainan University
  - **link:** https://arxiv.org/pdf/2512.17411
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method for detecting sensitive and illegal content on the Ethereum blockchain using machine learning. It employs a data restoration algorithm and uses FastText for sentiment analysis and NSFWJS for image detection. The study concludes that harmful content, including personal data and explicit images, coexists with benign data on the blockchain, highlighting privacy and security concerns.

- **[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering**
  - **tags:** [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]
  - **authors:** Léo Butsanets, Charles Corbière, Julien Khlaut, Pierre Manceron, Corentin Dancette
  - **institution:** Raidium, Université de Paris Cité, Hôpital Européen Georges Pompidou, AP-HP, INSERM
  - **link:** https://arxiv.org/pdf/2512.17396
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs.

- **[arXiv251222] A Systematic Reproducibility Study of BSARec for Sequential Recommendation**
  - **tags:** [ai], [sequential recommendation], [BSARec, Transformer, Fourier transform, discrete wavelet transform, padding strategies, frequency rescaling]
  - **authors:** Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy, Yuanna Liu
  - **institution:** University of Amsterdam
  - **link:** https://arxiv.org/pdf/2512.17442
  - **Simple LLM Summary:** This paper reproduces and evaluates BSARec, a sequential recommendation method that enhances Transformer encoders with a frequency layer using Fourier transforms to capture high-frequency signals. The study finds that BSARec outperforms other methods on some datasets, but digital signal processing techniques like discrete wavelet transform offer only marginal improvements over Fourier transforms, and non-constant padding significantly boosts performance while constant padding hinders high-frequency signal capture.

- **[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories**
  - **tags:** [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]
  - **authors:** Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe
  - **institution:** Turing
  - **link:** https://arxiv.org/pdf/2512.17419
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements.

- **[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]
  - **authors:** Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni
  - **institution:** Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University
  - **link:** https://arxiv.org/pdf/2512.17444
  - **Simple LLM Summary:** This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.

- **[arXiv251222] Learning What to Write: Write-Gated KV for Efficient Long-Context Inference**
  - **tags:** [mlsys], [llm inference], [KV cache management, Write-Gated KV, KV Admission, KV cache eviction, KV cache selection, FlashAttention, paged-KV systems]
  - **authors:** Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu
  - **institution:** National Taiwan University, Academia Sinica
  - **link:** https://arxiv.org/pdf/2512.17452
  - **Simple LLM Summary:** This paper introduces Write-Gated KV, a learnable KV Admission mechanism that predicts token utility before it enters the KV cache to reduce memory usage and speed up inference. By filtering low-utility tokens early and maintaining a compact global cache, the method significantly reduces memory usage and improves prefill and decode speeds for long-context LLMs with minimal accuracy loss. The results demonstrate that proactive KV cache management is a practical solution for efficient long-context inference.

- **[arXiv251222] A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting**
  - **tags:** [ai], [time series forecasting], [spatial-temporal graph neural network, trend-seasonal decomposition, low-rank Top-K adjacency learning, horizon-wise gating, linear baseline]
  - **authors:** Henok Tenaw Moges, Deshendran Moodley
  - **institution:** University of Cape Town
  - **link:** https://arxiv.org/pdf/2512.17453
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Lite-STGNN, a lightweight model combining decomposition-based linear temporal modeling with a learnable sparse graph module for spatial corrections. It achieves state-of-the-art accuracy on long-term multivariate forecasting benchmarks while being parameter-efficient and faster than transformer-based methods. The learned adjacency matrices also provide interpretable insights into domain-specific variable interactions.

- **[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application**
  - **tags:** [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]
  - **authors:** Olivier Jeunen, Schaun Wheeler
  - **institution:** aampe
  - **link:** https://arxiv.org/pdf/2512.17462
  - **Simple LLM Summary:** This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.

- **[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding**
  - **tags:** [ai], [democratic systems], [fair voting methods, cumulative voting, equal shares, proportional representation, participatory budgeting, AI voting assistance]
  - **authors:** Evangelos Pournaras
  - **institution:** University of Leeds
  - **link:** https://arxiv.org/pdf/2512.17461
  - **Simple LLM Summary:** This paper proposes that combining expressive ballot formats like cumulative voting with proportional aggregation methods like equal shares constitutes a "fair voting method." It concludes that such methods enhance democratic legitimacy, accelerate impactful outcomes in areas like welfare and education, and serve as a safeguard against biases in emerging AI-assisted voting scenarios.

- **[arXiv251222] Translating the Rashomon Effect to Sequential Decision-Making Tasks**
  - **tags:** [ai], [sequential decision-making], [Rashomon effect, formal verification, policy ensembles, behavioral cloning, permissive policies]
  - **authors:** Dennis Gross, Jørn Eirik Betten, Helge Spieker
  - **institution:** University of Oslo
  - **link:** https://arxiv.org/pdf/2512.17470
  - **Simple LLM Summary:** This paper translates the Rashomon effect from classification to sequential decision-making by defining it for policies that behave identically but have different internal structures. It uses formal verification methods to compare the complete probabilistic behavior of policies in stochastic environments. The study concludes that the effect exists in this domain and that ensembles from the Rashomon set are more robust to distribution shifts.

- **[arXiv251222] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion**
  - **tags:** [mlsys], [diffusion inference], [4D scene geometry, diffusion-based video generation, occlusion consistency, illumination-aware dataset, mask generation]
  - **authors:** Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo
  - **institution:** KAIST AI, SK Telecom
  - **link:** https://arxiv.org/pdf/2512.17504
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp
  - **Simple LLM Summary:** The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments.

- **[arXiv251222] Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models**
  - **tags:** [mlsys], [llm inference], [LoRA, PEFT, orthonormal transform, hidden-state scrambling, access control, instruction tuning, 4-bit quantization]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17519
  - **Simple LLM Summary:** This paper proposes K-OTG, a method for secret-key access control in language models. It uses a training-time dual-path corpus and inference-time orthonormal transforms to scramble hidden states, making the model unusable without the correct key while preserving authorized performance. The method is compatible with LoRA and 4-bit quantization, showing effective locking with minimal utility loss for authorized users.

- **[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals**
  - **tags:** [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17527
  - **Simple LLM Summary:** This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at ≤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.

- **[arXiv251222] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding**
  - **tags:** [mlsys], [multi-modal inference], [degradation-aware reasoning, structured reasoning chains, supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling]
  - **authors:** Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen
  - **institution:** Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.17532
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities.

- **[arXiv251222] Towards Explainable Conversational AI for Early Diagnosis with Large Language Models**
  - **tags:** [mlsys], [llm inference], [GPT-4o, Retrieval-Augmented Generation, Chain-of-Thought prompting, similarity matching, adaptive questioning]
  - **authors:** Maliha Tabassum, M Shamim Kaiser
  - **institution:** Bangladesh University of Professionals, Jahangirnagar University
  - **link:** https://arxiv.org/pdf/2512.17559
  - **Simple LLM Summary:** This paper introduces a diagnostic chatbot powered by a Large Language Model (GPT-4o) that uses Retrieval-Augmented Generation, Chain-of-Thought prompting, and adaptive questioning to interactively extract symptoms and provide diagnoses. The system achieved 90% accuracy and 100% Top-3 accuracy, outperforming traditional machine learning models. The findings suggest that LLM-based systems can offer a more transparent, interactive, and clinically effective approach to early medical diagnosis.

- **[arXiv251222] When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems**
  - **tags:** [mlsys], [others], [MetricGAN-plus-voicebank, semantic WER, noise robustness, speech enhancement]
  - **authors:** Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati
  - **institution:** EkaCare (Orbi Health Private Limited)
  - **link:** https://arxiv.org/pdf/2512.17562
  - **Simple LLM Summary:** This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.

- **[arXiv251222] ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image**
  - **tags:** [ai], [computer vision], [clothing tailoring, body semantic estimation, body edge prediction, foundational human visual model (FHVM), 3D mesh recovery]
  - **authors:** Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen
  - **institution:** Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)
  - **link:** https://arxiv.org/pdf/2512.17545
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping.

- **[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping**
  - **tags:** [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]
  - **authors:** Yikang Yue, Yishu Yin, Xuehai Qian
  - **institution:** Tsinghua University, University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17570
  - **Simple LLM Summary:** The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.

- **[arXiv251222] A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points**
  - **tags:** [mlsys], [others], [Attention U-Net, FLAIR hyperintensity segmentation, Dice score, Raidionics]
  - **authors:** Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen
  - **institution:** SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Linköping University Hospital, Skåne University Hospital, Umeå University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital
  - **link:** https://arxiv.org/pdf/2512.17566
  - **Simple LLM Summary:** This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment.

- **[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification**
  - **tags:** [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]
  - **authors:** Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli
  - **institution:** The University of Texas at El Paso, University of North Carolina
  - **link:** https://arxiv.org/pdf/2512.17594
  - **Simple LLM Summary:** This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.

- **[arXiv251222] More Consistent Accuracy PINN via Alternating Easy-Hard Training**
  - **tags:** [ai], [scientific machine learning], [physics-informed neural networks, easy-hard prioritization, hybrid training strategy, alternating scheme]
  - **authors:** Zhaoqian Gao, Min Yanga
  - **institution:** Yantai University
  - **link:** https://arxiv.org/pdf/2512.17607
  - **Simple LLM Summary:** This paper proposes a hybrid training strategy for Physics-Informed Neural Networks (PINNs) that alternates between easy and hard prioritization to improve performance. The method achieves consistently high accuracy on challenging PDEs, significantly outperforming baseline approaches. The work demonstrates that this alternating scheme enhances the robustness and reliability of PINNs across diverse problem types.

- **[arXiv251222] MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration**
  - **tags:** [ai], [medical image registration], [mammography registration, anatomical landmarks, ANTs, VoxelMorph, TransMorph, IDIR, MammoRegNet, benchmark dataset]
  - **authors:** Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin
  - **institution:** Lomonosov Moscow State University, Third Opinion Platform
  - **link:** https://arxiv.org/pdf/2512.17605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration.

- **[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions**
  - **tags:** [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]
  - **authors:** Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt
  - **institution:** KU Leuven, Technical University of Munich (TUM)
  - **link:** https://arxiv.org/pdf/2512.17629
  - **Simple LLM Summary:** The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.

- **[arXiv251222] Trust-Region Adaptive Policy Optimization**
  - **tags:** [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]
  - **authors:** Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang
  - **institution:** Tsinghua University, Ant Group
  - **link:** https://arxiv.org/pdf/2512.17636
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.

- **[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines**
  - **tags:** [ai], [reinforcement learning], [timed reward machines, tabular Q-learning, timed automata, counterfactual-imagining]
  - **authors:** Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska
  - **institution:** Tata Institute of Fundamental Research, Max Planck Institute for Software Systems, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.17637
  - **Simple LLM Summary:** This paper proposes timed reward machines (TRMs), an extension of reward machines that incorporates timing constraints into the reward specification for reinforcement learning. The authors develop model-free RL algorithms, specifically using tabular Q-learning integrated with abstractions of timed automata and counterfactual-imagining heuristics, to learn optimal policies. The experimental results show that their approach successfully learns policies that achieve high rewards while satisfying the specified timing constraints.

- **[arXiv251222] STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting**
  - **tags:** [mlsys], [multi-modal inference], [zero-shot learning, cross-modal retrieval, dual-encoder architecture, contrastive learning, structure-aware augmentation, semantic-traffic alignment]
  - **authors:** Yifei Cheng, Yujia Zhu, Baiyang Li, Xinhao Deng, Yitong Cai, Yaochen Ren, Qingyun Liu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.17667
  - **Simple LLM Summary:** The paper introduces STAR, a method that reformulates website fingerprinting as a zero-shot cross-modal retrieval problem, using a dual-encoder architecture to learn a joint embedding space for encrypted traffic traces and website logic profiles. It achieves high accuracy on unseen websites by aligning semantic and traffic features, demonstrating that semantic leakage is a major privacy risk in encrypted HTTPS traffic.

- **[arXiv251222] You Only Train Once: Differentiable Subset Selection for Omics Data**
  - **tags:** [ai], [bioinformatics], [differentiable subset selection, multi-task learning, end-to-end training, sparsity, single-cell RNA-seq]
  - **authors:** Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt
  - **institution:** ETH Zurich, University Children’s Hospital Zurich
  - **link:** https://arxiv.org/pdf/2512.17678
  - **Simple LLM Summary:** The paper introduces YOTO, an end-to-end framework that jointly selects discrete gene subsets and performs prediction in a single differentiable model, using sparsity and multi-task learning. It demonstrates improved predictive performance and yields compact, meaningful gene subsets on single-cell RNA-seq datasets, advancing biomarker discovery.

- **[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation**
  - **tags:** [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]
  - **authors:** Alexandre Personnic, Mihai Bâce
  - **institution:** KU Leuven
  - **link:** https://arxiv.org/pdf/2512.17673
  - **Simple LLM Summary:** The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.

- **[arXiv251222] An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution**
  - **tags:** [mlsys], [diffusion inference], [Diffusion Posterior Sampling (DPS), Manifold Constrained Gradient (MCG), conditioning step size, diffusion step count, ablation study]
  - **authors:** Yudhistira Arief Wibowo
  - **institution:** Technical University of Munich, Korea Advanced Institute of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.17675
  - **Simple LLM Summary:** This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0].

- **[arXiv251222] Digital and Web Forensics Model Cards, V1**
  - **tags:** [ai], [knowledge representation], [model cards, controlled vocabularies, web-based generator, digital forensics, web forensics]
  - **authors:** Paola Di Maio
  - **institution:** Ronin Institute, W3C AI Knowledge Representation Community Group
  - **link:** https://arxiv.org/pdf/2512.17722
  - **Simple LLM Summary:** This paper introduces a web-based framework for generating standardized model cards to represent knowledge in digital and web forensics, featuring controlled vocabularies for classification, reasoning, bias, and error. The main conclusion is the presentation of this beta framework and tool to establish an emerging standard, inviting community feedback for refinement.

- **[arXiv251222] Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure**
  - **tags:** [ai], [recommendation systems], [causal deconfounding, LightGCN, Unbiased Asymmetric Co-purchase Relationship (UACR), counterfactual exposure, BPR loss]
  - **authors:** Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang
  - **institution:** Harbin Institute of Technology (Shenzhen), Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.17733
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0101cc9ad451bf594fefde69475b9ae7433ae4f0ec8954e841150d68840d01_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Cadence, a plug-and-play framework built on LightGCN that uses causal deconfounding to compute unbiased item-item relationships and counterfactual exposure simulation to enhance recommendation diversity. The method constructs a deconfounded directed item graph and identifies diverse, causally relevant items a user has not interacted with. Experiments show it outperforms state-of-the-art models in both diversity and accuracy.

- **[arXiv251222] AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora**
  - **tags:** [mlsys], [llm inference], [AncientBench, benchmark evaluation, ancient character comprehension, excavated documents, glyph comprehension, pronunciation comprehension, meaning comprehension, contextual comprehension]
  - **authors:** Zhihan Zhou, Daqian Shi, Rui Song, Lida Shi, Xiaolei Diao, Hao Xu
  - **institution:** Jilin University, Queen Mary University of London, University of Trento
  - **link:** https://arxiv.org/pdf/2512.17756
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a15f186f48b4cc77c0d16272783515a0f56f75b51cf745384fc582f7e77f37a_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AncientBench, a comprehensive benchmark designed to evaluate large language models' comprehension of ancient Chinese, particularly focusing on excavated documents. It assesses four competencies (glyph, pronunciation, meaning, and contextual) through ten tasks. The experimental results show that while LLMs have significant potential in ancient text scenarios, a performance gap remains compared to human experts.

- **[arXiv251222] Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments**
  - **tags:** [mlsys], [llm training], [Easy Adaptation, Parameter-Efficient Fine-Tuning, LoRA, Specific Small Models, task adaptation, resource-constrained]
  - **authors:** Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo
  - **institution:** Not explicitly provided in the given text.
  - **link:** https://arxiv.org/pdf/2512.17771
  - **Simple LLM Summary:** The paper proposes Easy Adaptation (EA), a method that uses Specific Small Models (SSMs) to complement the data distribution for Large Models, enabling task adaptation without accessing the LM's internal parameters. This approach matches the performance of Parameter-Efficient Fine-Tuning (PEFT) like LoRA on diverse tasks while requiring only minimal computational resources, making it suitable for resource-constrained environments.

- **[arXiv251222] Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity**
  - **tags:** [mlsys], [others], [BERT, DistilBERT, ELECTRA, RoBERTa, Multi-BERT Ensemble, transformer models, medical entity recognition]
  - **authors:** Tanjim Taharat Aurpa, Farzana Akter, Md. Mehedi Hasan, Shakil Ahmed, Shifat Ara Rafiq, Fatema Khan
  - **institution:** University of Frontier Technology, University of Liberal Arts Bangladesh
  - **link:** https://arxiv.org/pdf/2512.17769
  - **Simple LLM Summary:** This paper proposes a Multi-BERT Ensemble approach for Bangla Medical Entity Recognition (MedER), evaluating models like BERT, DistilBERT, ELECTRA, and RoBERTa. The ensemble method achieved 89.58% accuracy, an 11.80% improvement over single-layer BERT, and the authors also created a new annotated dataset for this low-resource language task.

- **[arXiv251222] Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image**
  - **tags:** [ai], [computer vision], [vision transformer, neural parametric head models, 3d morphable models, single-image 3d reconstruction, signed distance functions]
  - **authors:** Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner
  - **institution:** Technical University of Munich, Woven by Toyota, Toyota Motor Europe
  - **link:** https://arxiv.org/pdf/2512.17773
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data.

- **[arXiv251222] Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation**
  - **tags:** [mlsys], [others], [Knowledge Mining, Digital Preservation, Semantic Web, Data Integration, Dual-Stream Architecture]
  - **authors:** Binh Vu
  - **institution:** FernUniversität in Hagen
  - **link:** https://arxiv.org/pdf/2512.17795
  - **Simple LLM Summary:** The paper proposes the Intelligent Knowledge Mining Framework (IKMF), a conceptual dual-stream architecture that combines a horizontal AI-driven mining process with a parallel trustworthy archiving stream. It aims to bridge the gap between dynamic analysis and long-term preservation, transforming static data repositories into living, actionable knowledge ecosystems.

- **[arXiv251222] LLM-based Behaviour Driven Development for Hardware Design**
  - **tags:** [mlsys], [others], [Behavior Driven Development (BDD), Large Language Models (LLMs), hardware design, test and verification, natural language processing, Electronic Design Automation (EDA)]
  - **authors:** Rolf Drechsler, Qian Liu
  - **institution:** University of Bremen, DFKI
  - **link:** https://arxiv.org/pdf/2512.17814
  - **Simple LLM Summary:** This paper investigates the use of Large Language Models (LLMs) to automate the generation of behavioral scenarios from textual specifications for Behavior Driven Development (BDD) in hardware design. The core method involves applying LLM-based techniques to interpret specifications and produce high-level behavioral descriptions. The main conclusion is that LLMs offer a promising opportunity to support and automate BDD workflows in hardware design, addressing the manual effort and complexity of current verification practices.

- **[arXiv251222] ShareChat: A Dataset of Chatbot Conversations in the Wild**
  - **tags:** [mlsys], [others], [dataset collection, multi-turn conversations, platform affordances, source citations, temporal analysis, cross-platform corpus]
  - **authors:** Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le
  - **institution:** Indiana University
  - **link:** https://arxiv.org/pdf/2512.17843
  - **Simple LLM Summary:** The paper introduces ShareChat, a large-scale dataset of real-world chatbot conversations collected from five major platforms, preserving interface-specific features like reasoning traces and source links. It demonstrates the dataset's utility through analyses of user intent satisfaction, citation behaviors, and evolving usage patterns, providing a resource for studying authentic user-LLM interactions.

- **[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes**
  - **tags:** [ai], [reinforcement learning], [energy-based models, gradient-based refinement, hindsight goal relabeling, latent-space planning]
  - **authors:** Carlos Vélez García, Miguel Cazorla, Jorge Pomares
  - **institution:** INESCOP, University of Alicante
  - **link:** https://arxiv.org/pdf/2512.17846
  - **Simple LLM Summary:** The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data.

- **[arXiv251222] Animate Any Character in Any World**
  - **tags:** [mlsys], [multi-modal inference], [3DGS, conditional autoregressive video generation, pre-trained video generator, natural language control]
  - **authors:** Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu
  - **institution:** Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.17796
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips.

- **[arXiv251222] Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life**
  - **tags:** [ai], [computational social science], [computational text analysis, machine learning (ML), natural language processing (NLP), ethnography, in-depth interviews, mixed-methods]
  - **authors:** Corey M. Abramson
  - **institution:** Rice University, UC San Francisco
  - **link:** https://arxiv.org/pdf/2512.17850
  - **Simple LLM Summary:** This paper demonstrates how computational social science tools like machine learning and natural language processing can be integrated with traditional qualitative methods (e.g., ethnography, interviews) to study aging. It concludes that these computational methods can broaden qualitative research by streamlining workflows, scaling up projects, and enabling new multi-method insights, rather than replacing its foundational approaches.

- **[arXiv251222] InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models**
  - **tags:** [mlsys], [diffusion inference], [cross-attention maps, inference-time optimization, compound loss, denoising step, spatial alignment]
  - **authors:** Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad
  - **institution:** Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google
  - **link:** https://arxiv.org/pdf/2512.17851
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp
  - **Simple LLM Summary:** InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines.

- **[arXiv251222] Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN**
  - **tags:** [mlsys], [others], [Convolutional Neural Network (CNN), Attention Mechanism, CBAM, VGG16, Grad-CAM, Layer-wise Relevance Propagation (LRP)]
  - **authors:** Balram Singh, Ram Prakash Sharma, Somnath Dey
  - **institution:** National Institute of Technology Hamirpur, Indian Institute of Technology Indore
  - **link:** https://arxiv.org/pdf/2512.17864
  - **Simple LLM Summary:** This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques.

- **[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning**
  - **tags:** [mlsys], [multi-modal training], [ViPR, ViPR-Eureka, ViPR-RL, behavior cloning, VLM-in-the-loop Parallel Refinement, LLM-guided contact sampling, sim-to-real transfer, GPU simulation]
  - **authors:** Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper
  - **institution:** Robotics and AI Institute
  - **link:** https://arxiv.org/pdf/2512.17853
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks.

- **[arXiv251222] Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow**
  - **tags:** [ai], [generative modeling], [Wasserstein-Fisher-Rao gradient flow, weighted stochastic differential equations, Feynman-Kac representation, score-based diffusion models, Langevin dynamics]
  - **authors:** Herlock Rahimi
  - **institution:** Yale University
  - **link:** https://arxiv.org/pdf/2512.17878
  - **Simple LLM Summary:** The paper proposes a new sampling method for generative modeling by implementing Wasserstein-Fisher-Rao gradient flow via weighted stochastic differential equations, using the Feynman-Kac representation. This approach aims to overcome the slow mixing rates of traditional diffusion models in non-log-concave, multimodal target distributions by incorporating controlled mass reweighting. The study provides a rigorous geometric and operator-theoretic foundation for future developments in this area.

- **[arXiv251222] Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally**
  - **tags:** [ai], [human-computer interaction], [anthropomorphism, cross-national experiments, humanlike AI design, behavioral measures, cultural mediation]
  - **authors:** Robin Schimmelpfennig, Mark Díaz, Vinodkumar Prabhakaran, Aida Davani
  - **institution:** Max Planck Institute for Human Development, Google Research
  - **link:** https://arxiv.org/pdf/2512.17898
  - **Simple LLM Summary:** The paper conducts two large-scale cross-national experiments with 3,500 participants across 10 countries, involving real-time interactions with an AI system. It finds that humanlike design increases anthropomorphism but does not universally boost engagement and trust; instead, these outcomes are culturally mediated, with the same design choices having opposite effects in different populations like Brazil and Japan.

- **[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras**
  - **tags:** [mlsys], [multi-modal inference], [diffusion model, bird's-eye-view, radar cross section, Doppler, point cloud generation, foundation models]
  - **authors:** Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany
  - **institution:** Technion, MIT, NVIDIA, University of Toronto, Vector Institute
  - **link:** https://arxiv.org/pdf/2512.17897
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp
  - **Simple LLM Summary:** RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.

- **[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models**
  - **tags:** [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]
  - **authors:** Jonathon Fox, William J Buchanan, Pavlos Papadopoulos
  - **institution:** Edinburgh Napier University
  - **link:** https://arxiv.org/pdf/2512.17902
  - **Simple LLM Summary:** This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.

- **[arXiv251222] When Reasoning Meets Its Laws**
  - **tags:** [ai], [large reasoning models], [laws of reasoning, compute law, accuracy law, monotonicity, compositionality, LoRe-Bench, finetuning]
  - **authors:** Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang
  - **institution:** University of Illinois Urbana-Champaign, Massachusetts Institute of Technology, University of Pennsylvania, New York University, NTT Research
  - **link:** https://arxiv.org/pdf/2512.17901
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9db7b3665dbba1bcaed95897dff8a53103ef5bfe963b50f03c044189965a72_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Laws of Reasoning (LoRe), a framework that formalizes desired reasoning behaviors in large reasoning models, including compute and accuracy laws. It proposes LoRe-Bench to evaluate monotonicity and compositionality, and develops a finetuning method to improve compositionality. The study finds that better compliance with these laws leads to enhanced reasoning performance across benchmarks.

- **[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting**
  - **tags:** [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]
  - **authors:** Ananta R. Bhattarai, Helge Rhodin
  - **institution:** Bielefeld University
  - **link:** https://arxiv.org/pdf/2512.17908
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model's priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.

- **[arXiv251222] Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics**
  - **tags:** [ai], [uncertainty quantification], [conformal prediction, calibration, machine learning, high-energy physics, collider research, finite-sample guarantees, prediction sets, p-values]
  - **authors:** Jack Y. Araz, Michael Spannowsky
  - **institution:** University College London, City, University of London, Durham University
  - **link:** https://arxiv.org/pdf/2512.17048
  - **Simple LLM Summary:** This paper proposes using conformal prediction as a standard calibration layer for machine learning models in high-energy physics, providing rigorous uncertainty quantification and finite-sample coverage guarantees without retraining. It demonstrates the method's applicability across regression, classification, anomaly detection, and generative modeling using collider datasets. The authors conclude that adopting conformal calibration enables reliable statistical inference and principled decision-making in experimental analyses.

- **[arXiv251222] Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings**
  - **tags:** [mlsys], [others], [graph attention networks, electroencephalography, spatio-temporal graphs, edge analysis, low-cost hardware, RaspberryPi]
  - **authors:** Szymon Mazurek, Stephen Moore, Alessandro Crimi
  - **institution:** AGH University of Krakow, University of Cape Coast
  - **link:** https://arxiv.org/pdf/2507.15118
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a graph attention network (GAT) framework that models EEG signals as spatio-temporal graphs to detect epilepsy, with a focus on low-cost hardware for deployment in low-resource settings. The method adapts GATs to analyze edge connectivity for biomarker identification and is designed for lightweight training and deployment. The results demonstrate promising classification performance and highlight the potential for scalable, accessible diagnostic support in underserved regions.

- **[arXiv251222] Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning**
  - **tags:** [mlsys], [others], [multi-layer graph, GNN, temporal GNN, logistic regression, Random Forest, correlation-based, systemic risk]
  - **authors:** Sandeep Neela
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17185
  - **Simple LLM Summary:** This paper introduces the Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility. It demonstrates that graph-derived features from this model provide useful early-warning signals for market crashes, outperforming standard feature-based models.

- **[arXiv251222] From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework**
  - **tags:** [ai], [cognitive modeling], [Graph Neural Networks, Graph Theory, inductive biases, computational graph, systematic ablation]
  - **authors:** Quan Do, Caroline Ahn, Leah Bakst, Michael Pascale, Joseph T. McGuire, Chantal E. Stern, Michael E. Hasselmo
  - **institution:** Boston University
  - **link:** https://arxiv.org/pdf/2512.17255
  - **Simple LLM Summary:** The paper introduces a framework combining Graph Theory and Graph Neural Networks to formalize inductive biases as explicit priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus, it shows that differences in graph-based priors explain individual differences in human solutions, revealing how generalization depends on specific prior structures and why human-like errors arise from incorrect priors.

- **[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics**
  - **tags:** [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]
  - **authors:** Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton
  - **institution:** University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17534
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.

- **[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation**
  - **tags:** [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]
  - **authors:** Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein
  - **institution:** German Cancer Research Center (DKFZ), Heidelberg University
  - **link:** https://arxiv.org/pdf/2512.17774
  - **Simple LLM Summary:** This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network's architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.

- **[arXiv251222] Exploring the Effect of Basis Rotation on NQS Performance**
  - **tags:** [ai], [quantum machine learning], [neural quantum states, restricted boltzmann machine, quantum natural gradient, quantum fisher information, fubini-study distance, basis rotation, ising model]
  - **authors:** Sven Benjamin Kožić, Vinko Zlatić, Fabio Franchini, Salvatore Marco Giampaolo
  - **institution:** Institut Ruđer Bošković
  - **link:** https://arxiv.org/pdf/2512.17893
  - **Simple LLM Summary:** This paper uses an analytically solvable rotated Ising model to study how local basis rotations affect the optimization of Neural Quantum States (NQS). It finds that rotations relocate the target wavefunction in parameter space, exposing information-geometric barriers like saddle points that trap shallow architectures like RBMs, highlighting the need for landscape-aware model design.

## 2025-12-23

- **[arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning**
  - **tags:** TBD
  - **authors:** Lihui Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17912
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp
  - **Simple LLM Summary:** Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning

- **[arXiv251223] Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression**
  - **tags:** TBD
  - **authors:** Rahul Baxi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17920
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e85dcb740e46985e03fe90bf075468b334e1528a3de2a4858fac5b2ddbc2dc9_w640_q70.webp
  - **Simple LLM Summary:** Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression

- **[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation**
  - **tags:** TBD
  - **authors:** Nihir Chadderwala
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17913
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp
  - **Simple LLM Summary:** Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation

- **[arXiv251223] Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models**
  - **tags:** TBD
  - **authors:** Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17911
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp
  - **Simple LLM Summary:** Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models

- **[arXiv251223] Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models**
  - **tags:** TBD
  - **authors:** Minh Tri LÊ, Ali Ait-Bachir
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17916
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a9a26225e6a2c495c48df9cb6a0e4bd0c624c036e56d8d0cf9885d0d909a745_w640_q70.webp
  - **Simple LLM Summary:** Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models

- **[arXiv251223] KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction**
  - **tags:** TBD
  - **authors:** Aomufei Yuan, Zhiming Wang, Ruijie Miao, Dayu Wang, Yuxuan Tian, Zihan Wang, Yebo Peng, Yuhan Wu, Bairen Yi, Xin Liu, Tong Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17917
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca40de411c47ab6f32fb67699fbcdce0808ef0d5179742bf46c64d088a640d3_w640_q70.webp
  - **Simple LLM Summary:** KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction

- **[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA**
  - **tags:** TBD
  - **authors:** Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17910
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp
  - **Simple LLM Summary:** Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA

- **[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU**
  - **tags:** TBD
  - **authors:** Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17941
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp
  - **Simple LLM Summary:** Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU

- **[arXiv251223] Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition**
  - **tags:** TBD
  - **authors:** Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17946
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp
  - **Simple LLM Summary:** Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition

- **[arXiv251223] Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy**
  - **tags:** TBD
  - **authors:** Zhao Song, Song Yue, Jiahao Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17950
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21bbafaf75367ac515be62999bf183f2dc9b58cb6de4b044d5e95331ef1cf1ed_w640_q70.webp
  - **Simple LLM Summary:** Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy

- **[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction**
  - **tags:** TBD
  - **authors:** Karthik Prabhakar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17943
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp
  - **Simple LLM Summary:** NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction

- **[arXiv251223] Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States**
  - **tags:** TBD
  - **authors:** Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp
  - **Simple LLM Summary:** Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States

- **[arXiv251223] Will AI Trade? A Computational Inversion of the No-Trade Theorem**
  - **tags:** TBD
  - **authors:** Hanyu Li, Xiaotie Deng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17952
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ad1a13c7765317192c2b83746186b421990df4011bf9cad5b20f5624cf6ca35_w640_q70.webp
  - **Simple LLM Summary:** Will AI Trade? A Computational Inversion of the No-Trade Theorem

- **[arXiv251223] Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration**
  - **tags:** TBD
  - **authors:** Victor Stasiuc, Round Table Collaboration
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17956
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57a72b745466d47b8a8056586ccf86e4e40bd0ac42a76b0061c6576b563f4532_w640_q70.webp
  - **Simple LLM Summary:** Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration

- **[arXiv251223] Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition**
  - **tags:** TBD
  - **authors:** Ellie Zhou, Jihoon Chung, Olga Russakovsky
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17953
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp
  - **Simple LLM Summary:** Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition

- **[arXiv251223] CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs**
  - **tags:** TBD
  - **authors:** Gunho Park, Jeongin Bae, Byeongwook Kim, Baeseong park, Jiwon Ryu, Hoseung Kim, Se Jung Kwon, Dongsoo Lee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17970
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp
  - **Simple LLM Summary:** CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs

- **[arXiv251223] Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis**
  - **tags:** TBD
  - **authors:** Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17979
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43cecfb66c204c7d6add4e7c40f9325f7867c22be2179de9305d482292a7a4dd_w640_q70.webp
  - **Simple LLM Summary:** Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis

- **[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization**
  - **tags:** TBD
  - **authors:** Farida Mohsen, Ali Safa
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17958
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp
  - **Simple LLM Summary:** Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization

- **[arXiv251223] Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models**
  - **tags:** TBD
  - **authors:** Irina Seregina, Philippe Lalanda, German Vega
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17983
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp
  - **Simple LLM Summary:** Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models

- **[arXiv251223] Convolutional-neural-operator-based transfer learning for solving PDEs**
  - **tags:** TBD
  - **authors:** Peng Fan, Guofei Pang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp
  - **Simple LLM Summary:** Convolutional-neural-operator-based transfer learning for solving PDEs

- **[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India**
  - **tags:** TBD
  - **authors:** Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18014
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp
  - **Simple LLM Summary:** ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India

- **[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models**
  - **tags:** TBD
  - **authors:** Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp
  - **Simple LLM Summary:** Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models

- **[arXiv251223] Specification and Detection of LLM Code Smells**
  - **tags:** TBD
  - **authors:** Brahim Mahmoudi, Zacharie Chenail-Larcher, Naouel Moha, Quentin Stievenert, Florent Avellaneda
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18020
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cee6a3c076b484391912c8b6083413504d86a9475e81592b3004ce305e4f9c4_w640_q70.webp
  - **Simple LLM Summary:** Specification and Detection of LLM Code Smells

- **[arXiv251223] A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients**
  - **tags:** TBD
  - **authors:** Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18031
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp
  - **Simple LLM Summary:** A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients

- **[arXiv251223] Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout**
  - **tags:** TBD
  - **authors:** Joshua Gibson, Kapil Dhakal
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18034
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee8143ffe65f15c2c3477c3c466f8d3d7e3d40519caf5667a1e168e357e8ce6b_w640_q70.webp
  - **Simple LLM Summary:** Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout

- **[arXiv251223] A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations**
  - **tags:** TBD
  - **authors:** Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17984
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp
  - **Simple LLM Summary:** A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations

- **[arXiv251223] Securing Agentic AI Systems -- A Multilayer Security Framework**
  - **tags:** TBD
  - **authors:** Sunil Arora, John Hastings
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18043
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96ca042b8b2e4295fa813434a544ac9adb3df55cbea55e3fe9b24f2dcbee4733_w640_q70.webp
  - **Simple LLM Summary:** Securing Agentic AI Systems -- A Multilayer Security Framework

- **[arXiv251223] FOODER: Real-time Facial Authentication and Expression Recognition**
  - **tags:** TBD
  - **authors:** Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18057
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp
  - **Simple LLM Summary:** FOODER: Real-time Facial Authentication and Expression Recognition

- **[arXiv251223] From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems**
  - **tags:** TBD
  - **authors:** Marcos Ortiz, Justin Hill, Collin Overbay, Ingrida Semenec, Frederic Sauve-Hoover, Jim Schwoebel, Joel Shor
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18080
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0154cb62824a09bcbf4a6476b89c563b0f548b2e6721f62b4207082ab09ee544_w640_q70.webp
  - **Simple LLM Summary:** From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems

- **[arXiv251223] Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling**
  - **tags:** TBD
  - **authors:** Ohoud Alzahrani, Russell Beale, Robert J. Hendley
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18077
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ecf679a26dc73049a43a2ec8c3b4b9a5b43ae16a82684041393594d146210f5_w640_q70.webp
  - **Simple LLM Summary:** Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling

- **[arXiv251223] Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation**
  - **tags:** TBD
  - **authors:** Shreshth Rajan, Raymond Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18082
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp
  - **Simple LLM Summary:** Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation

- **[arXiv251223] Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability**
  - **tags:** TBD
  - **authors:** Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18092
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp
  - **Simple LLM Summary:** Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability

- **[arXiv251223] Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks**
  - **tags:** TBD
  - **authors:** Boxuan Wang, Zhuoyun Li, Xiaowei Huang, Yi Dong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18094
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d3b6262ae305fdec9209648a021429405f004c2b9a531305c5c02be4396c55f_w640_q70.webp
  - **Simple LLM Summary:** Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks

- **[arXiv251223] Holistic Evaluation of State-of-the-Art LLMs for Code Generation**
  - **tags:** TBD
  - **authors:** Le Zhang, Suresh Kothari
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18131
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e08a9f551315e560db6179abac192d75f62ffe1b185263cb90784842012ac802_w640_q70.webp
  - **Simple LLM Summary:** Holistic Evaluation of State-of-the-Art LLMs for Code Generation

- **[arXiv251223] Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap**
  - **tags:** TBD
  - **authors:** Zijun Wang, Yijiahao Qi, Hanqiu Chen, Zishen Wan, Gongjin Sun, Dongyang Li, Shuyi Pei, Cong Hao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18126
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7254830672e4c2c139708d42673d4462ea8f38cc986ab26672ee4838f74c1458_w640_q70.webp
  - **Simple LLM Summary:** Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap

- **[arXiv251223] Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection**
  - **tags:** TBD
  - **authors:** Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18133
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp
  - **Simple LLM Summary:** Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection

- **[arXiv251223] Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications**
  - **tags:** TBD
  - **authors:** Cristiano da Costa Cunha, Wei Liu, Tim French, Ajmal Mian
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18135
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afd713c843807b61ef24ae3fe42d41c20e6fbaeaff735c0c77378e6c26ab19a6_w640_q70.webp
  - **Simple LLM Summary:** Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications

- **[arXiv251223] On Swarm Leader Identification using Probing Policies**
  - **tags:** TBD
  - **authors:** Stergios E. Bachoumas, Panagiotis Artemiadis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18146
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp
  - **Simple LLM Summary:** On Swarm Leader Identification using Probing Policies

- **[arXiv251223] Propose, Solve, Verify: Self-Play Through Formal Verification**
  - **tags:** TBD
  - **authors:** Alex Wilf, Pranjal Aggarwal, Bryan Parno, Daniel Fried, Louis-Philippe Morency, Paul Pu Liang, Sean Welleck
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18160
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/352216a02c2bd4fb599d9561a7994cb21ba9b1ce3f9f8b8cfea3ec4cc0b8413d_w640_q70.webp
  - **Simple LLM Summary:** Propose, Solve, Verify: Self-Play Through Formal Verification

- **[arXiv251223] NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI**
  - **tags:** TBD
  - **authors:** Midhat Urooj, Ayan Banerjee, Sandeep Gupta
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18177
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp
  - **Simple LLM Summary:** NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI

- **[arXiv251223] External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning**
  - **tags:** TBD
  - **authors:** Jian Yan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18190
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp
  - **Simple LLM Summary:** External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning

- **[arXiv251223] PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS**
  - **tags:** TBD
  - **authors:** Devang Dhanuka, Nidhi Rastogi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18199
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7026a8c8f552664c28724b8716e8e7e5cef6b29425c374a1c5439f2bfb877d5f_w640_q70.webp
  - **Simple LLM Summary:** PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS

- **[arXiv251223] NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework**
  - **tags:** TBD
  - **authors:** Zihao Deng, Yijia Li, Renrui Zhang, Peijun Ye
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18189
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee57353b28c6f902f2d47ba6499f6f3db69fe4e0b025fa2fd4ed11eff3c1c003_w640_q70.webp
  - **Simple LLM Summary:** NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework

- **[arXiv251223] Sophia: A Persistent Agent Framework of Artificial Life**
  - **tags:** TBD
  - **authors:** Mingyang Sun, Feng Hong, Weinan Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18202
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c33289396399b1bb0e301fb010ed82c6a686d9391a9307867ae3c7ae74a8cc3_w640_q70.webp
  - **Simple LLM Summary:** Sophia: A Persistent Agent Framework of Artificial Life

- **[arXiv251223] When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics**
  - **tags:** TBD
  - **authors:** Yizhou Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18209
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp
  - **Simple LLM Summary:** When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics

- **[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning**
  - **tags:** TBD
  - **authors:** Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18215
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp
  - **Simple LLM Summary:** Stable and Efficient Single-Rollout RL for Multimodal Reasoning

- **[arXiv251223] LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning**
  - **tags:** TBD
  - **authors:** Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18211
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp
  - **Simple LLM Summary:** LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning

- **[arXiv251223] Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation**
  - **tags:** TBD
  - **authors:** Zehao Liu, Xi Lin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18244
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a4ab3d36e0ccc56b5c8f0b5a9f82481bdfdc1f1bc14e54959edf8ba4006a00d_w640_q70.webp
  - **Simple LLM Summary:** Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation

- **[arXiv251223] Offline Behavioral Data Selection**
  - **tags:** TBD
  - **authors:** Shiye Lei, Zhihao Cheng, Dacheng Tao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18246
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp
  - **Simple LLM Summary:** Offline Behavioral Data Selection

- **[arXiv251223] Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration**
  - **tags:** TBD
  - **authors:** Himabindu Thogaru, Saisubramaniam Gopalakrishnan, Zishan Ahmad, Anirudh Deodhar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18265
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50a73b0ae1606916d8e1e7324c2baa8154f67eb8e76c0e1f1c0d014d54aa2ab2_w640_q70.webp
  - **Simple LLM Summary:** Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration

- **[arXiv251223] Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image**
  - **tags:** TBD
  - **authors:** Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Zhimin Gao, Chuankun Li, Shaohua Qiu, Jiangfeng Xu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18245
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp
  - **Simple LLM Summary:** Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image

- **[arXiv251223] Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks**
  - **tags:** TBD
  - **authors:** Yucheng Fan, Jiawei Chen, Yu Tian, Zhaoxia Yin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18264
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp
  - **Simple LLM Summary:** Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks

- **[arXiv251223] Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model**
  - **tags:** TBD
  - **authors:** Rui Xing, Runmin Cong, Yingying Wu, Can Wang, Zhongming Tang, Fen Wang, Hao Wu, Sam Kwong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18247
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp
  - **Simple LLM Summary:** Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model

- **[arXiv251223] MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification**
  - **tags:** TBD
  - **authors:** Sirui Li, Wangyue Lu, Xiaorui Shi, Ke Weng, Haozhe Sun, Minghe Yu, Tiancheng Zhang, Ge Yu, Hengyu Liu, Lun Du
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18256
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fa370d48e50a94f7beaa30df78416e8de75e5bddd8996af2ad55d2751ed49c0_w640_q70.webp
  - **Simple LLM Summary:** MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification

- **[arXiv251223] Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective**
  - **tags:** TBD
  - **authors:** M. Mehdi Kholoosi, Triet Huynh Minh Le, M. Ali Babar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18261
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c527700e49403d8d115fcd9b121714ac7cc66ca4c4917d88ae5eeb6712cf705e_w640_q70.webp
  - **Simple LLM Summary:** Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective

- **[arXiv251223] AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning**
  - **tags:** TBD
  - **authors:** Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18295
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp
  - **Simple LLM Summary:** AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning

- **[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings**
  - **tags:** TBD
  - **authors:** Harsh Rathva, Ojas Srivastava, Pruthwik Mishra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18309
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp
  - **Simple LLM Summary:** Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings

- **[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems**
  - **tags:** TBD
  - **authors:** Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18317
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp
  - **Simple LLM Summary:** Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems

- **[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems**
  - **tags:** TBD
  - **authors:** Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18318
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp
  - **Simple LLM Summary:** Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems

- **[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)**
  - **tags:** TBD
  - **authors:** Youssef Mahran, Zeyad Gamal, Ayman El-Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18333
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp
  - **Simple LLM Summary:** Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)

- **[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism**
  - **tags:** TBD
  - **authors:** Youssef Mahran, Zeyad Gamal, Ayman El-Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18336
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp
  - **Simple LLM Summary:** Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism

- **[arXiv251223] Monitoring Monitorability**
  - **tags:** TBD
  - **authors:** Melody Y. Guan, Miles Wang, Micah Carroll, Zehao Dou, Annie Y. Wei, Marcus Williams, Benjamin Arnav, Joost Huizinga, Ian Kivlichan, Mia Glaese, Jakub Pachocki, Bowen Baker
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18311
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b80143b15f0287eaa0d31decbf1a350d64c8110ec245d21e81c64ae73cd6febc_w640_q70.webp
  - **Simple LLM Summary:** Monitoring Monitorability

- **[arXiv251223] MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation**
  - **tags:** TBD
  - **authors:** Zhiheng Zhang, Jiajun Yang, Hong Sun, Dong Wang, Honghua Jiang, Yaru Chen, Tangyuan Ning
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18344
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp
  - **Simple LLM Summary:** MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation

- **[arXiv251223] LLM-based Few-Shot Early Rumor Detection with Imitation Agent**
  - **tags:** TBD
  - **authors:** Fengzhu Zeng, Qian Shao, Ling Cheng, Wei Gao, Shih-Fen Cheng, Jing Ma, Cheng Niu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18352
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68253881479be80c6f5156b8929dcea7c9affda2463411703dbff80daa9a6787_w640_q70.webp
  - **Simple LLM Summary:** LLM-based Few-Shot Early Rumor Detection with Imitation Agent

- **[arXiv251223] LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators**
  - **tags:** TBD
  - **authors:** Mateusz Lango, Ondřej Dušek
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18360
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e70af4d7e4c119643e3631c8815a5d46438a6f1b8881a5821764fb495f8608f_w640_q70.webp
  - **Simple LLM Summary:** LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators

- **[arXiv251223] Datasets for machine learning and for assessing the intelligence level of automatic patent search systems**
  - **tags:** TBD
  - **authors:** Boris Genin, Alexander Gorbunov, Dmitry Zolkin, Igor Nekrasov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18384
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac80e35326c113d249d5bd28c4aafe01da23a8e0c681cde84257daab6b92b651_w640_q70.webp
  - **Simple LLM Summary:** Datasets for machine learning and for assessing the intelligence level of automatic patent search systems

- **[arXiv251223] Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models**
  - **tags:** TBD
  - **authors:** Chao Wen, Tung Phung, Pronita Mehrotra, Sumit Gulwani, Tomohiro Nagashima, Adish Singla
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18388
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26ae3275661776caa174169c0f345d17624c81b4d8a6d11a881528d1b3ebbea4_w640_q70.webp
  - **Simple LLM Summary:** Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models

- **[arXiv251223] Neural Proofs for Sound Verification and Control of Complex Systems**
  - **tags:** TBD
  - **authors:** Alessandro Abate
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18389
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp
  - **Simple LLM Summary:** Neural Proofs for Sound Verification and Control of Complex Systems

- **[arXiv251223] AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3**
  - **tags:** TBD
  - **authors:** Mark Kashirskiy, Artiom Lipinski, Ilya Makarov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18399
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb721fc55d6ea689ae069e1c50129b86f7fc25c4c3433aa154aaa38bf9378cd3_w640_q70.webp
  - **Simple LLM Summary:** AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3

- **[arXiv251223] AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning**
  - **tags:** TBD
  - **authors:** Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng, Fanjiang Xu, Hui Xiong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18411
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp
  - **Simple LLM Summary:** AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning

- **[arXiv251223] Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation**
  - **tags:** TBD
  - **authors:** Mykyta Lapin, Kostiantyn Bokhan, Yurii Parzhyn
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18412
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccebc716317d25dbc5d826c4045eca3d2894e8904f35d62a55262711b7023239_w640_q70.webp
  - **Simple LLM Summary:** Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation

- **[arXiv251223] Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks**
  - **tags:** TBD
  - **authors:** Ansar Ahmed
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18432
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8b17e8fd956e845305266fe6ff75f18289ac1b39f8db27ac25d02758609d8f9_w640_q70.webp
  - **Simple LLM Summary:** Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks

- **[arXiv251223] A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing**
  - **tags:** TBD
  - **authors:** Zihan Han, Lingran Meng, Jingwei Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18441
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acf1ec57b519400d58fe519f0aa1f4d66aecd68168a2f192e508b971607f7ef7_w640_q70.webp
  - **Simple LLM Summary:** A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing

- **[arXiv251223] VeruSAGE: A Study of Agent-Based Verification for Rust Systems**
  - **tags:** TBD
  - **authors:** Chenyuan Yang, Natalie Neamtu, Chris Hawblitzel, Jacob R. Lorch, Shan Lu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18436
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a6dcac7fed2f39d9b3c88cf4fcec2c0a341fe5463b697a482bfb914d0610c67_w640_q70.webp
  - **Simple LLM Summary:** VeruSAGE: A Study of Agent-Based Verification for Rust Systems

- **[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery**
  - **tags:** TBD
  - **authors:** Grammateia Kotsialou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18444
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp
  - **Simple LLM Summary:** Snowveil: A Framework for Decentralised Preference Discovery

- **[arXiv251223] An Agentic AI Framework for Training General Practitioner Student Skills**
  - **tags:** TBD
  - **authors:** Victor De Marez, Jens Van Nooten, Luna De Bruyne, Walter Daelemans
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18440
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937c34e04859d95b3aed57029c9c5791e5aa2e5a17dbc8d2d6ac7882d0933e37_w640_q70.webp
  - **Simple LLM Summary:** An Agentic AI Framework for Training General Practitioner Student Skills

- **[arXiv251223] Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System**
  - **tags:** TBD
  - **authors:** Xavier Rafael-Palou, Jose Munuera, Ana Jimenez-Pastor, Richard Osuala, Karim Lekadir, Oliver Diaz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18450
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp
  - **Simple LLM Summary:** Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System

- **[arXiv251223] MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading**
  - **tags:** TBD
  - **authors:** Shurui Xu, Siqi Yang, Jiapin Ren, Zhong Cao, Hongwei Yang, Mengzhen Fan, Yuyu Sun, Shuyan Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18437
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp
  - **Simple LLM Summary:** MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading

- **[arXiv251223] Secret mixtures of experts inside your LLM**
  - **tags:** TBD
  - **authors:** Enric Boix-Adsera
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18452
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp
  - **Simple LLM Summary:** Secret mixtures of experts inside your LLM

- **[arXiv251223] SoK: Understanding (New) Security Issues Across AI4Code Use Cases**
  - **tags:** TBD
  - **authors:** Qilong Wu, Taoran Li, Tianyang Zhou, Varun Chandrasekaran
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18456
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f652ae22c8ea45edbe21093e80ef932b29b1703110171928654985064e108e1_w640_q70.webp
  - **Simple LLM Summary:** SoK: Understanding (New) Security Issues Across AI4Code Use Cases

- **[arXiv251223] Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling**
  - **tags:** TBD
  - **authors:** Christopher Román Jaimes
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18462
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp
  - **Simple LLM Summary:** Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling

- **[arXiv251223] SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios**
  - **tags:** TBD
  - **authors:** Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18470
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aa84e9ee4a961b04628c969b7317aae944aad76753539183cd0213072cfce14_w640_q70.webp
  - **Simple LLM Summary:** SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios

- **[arXiv251223] Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review**
  - **tags:** TBD
  - **authors:** Oraib Almegdadi, João Marcelino, Sarah Fakhreddine, João Manso, Nuno C. Marques
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18466
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp
  - **Simple LLM Summary:** Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review

- **[arXiv251223] Large Language Models as Discounted Bayesian Filters**
  - **tags:** TBD
  - **authors:** Jensen Zhang, Jing Yang, Keze Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18489
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da72fb53dbbaca8aea568d7b6fc9cba313aa6bb5798a40c89e2a22a5a0788e17_w640_q70.webp
  - **Simple LLM Summary:** Large Language Models as Discounted Bayesian Filters

- **[arXiv251223] Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation**
  - **tags:** TBD
  - **authors:** Rahul Yumlembam, Biju Issac, Seibu Mary Jacob
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18495
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0272a8734a4477ae1f6aa46e60e4c4f62aa860ef3a9559c12f0aad180ecb2_w640_q70.webp
  - **Simple LLM Summary:** Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation

- **[arXiv251223] PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs**
  - **tags:** TBD
  - **authors:** Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18500
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp
  - **Simple LLM Summary:** PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs

- **[arXiv251223] Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations**
  - **tags:** TBD
  - **authors:** Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang, Deepa Krishnan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18483
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c098db4784c358fb7712e2be8c3c8d57e56fd9982bed1aff3f16699a16acd5_w640_q70.webp
  - **Simple LLM Summary:** Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations

- **[arXiv251223] GTMA: Dynamic Representation Optimization for OOD Vision-Language Models**
  - **tags:** TBD
  - **authors:** Jensen Zhang, Ningyuan Liu, Keze Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18504
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp
  - **Simple LLM Summary:** GTMA: Dynamic Representation Optimization for OOD Vision-Language Models

- **[arXiv251223] Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts**
  - **tags:** TBD
  - **authors:** Hatim M. E. Geli, Islam Omar, Mona Y. Elshinawy, David W. DuBios, Lara Prehodko, Kelly H Smith, Abdel-Hameed A. Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18522
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp
  - **Simple LLM Summary:** Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts

- **[arXiv251223] A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System**
  - **tags:** TBD
  - **authors:** Miyuki T. Nakata
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18525
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5fa66d8eb7deac60effe912ce57cc5b3a8decb9a70cf6acda4def17328ab6c_w640_q70.webp
  - **Simple LLM Summary:** A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System

- **[arXiv251223] Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism**
  - **tags:** TBD
  - **authors:** Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18527
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp
  - **Simple LLM Summary:** Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism

- **[arXiv251223] SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models**
  - **tags:** TBD
  - **authors:** Scott Thornton
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18542
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp
  - **Simple LLM Summary:** SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models

- **[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL**
  - **tags:** TBD
  - **authors:** Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18552
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp
  - **Simple LLM Summary:** Toward Training Superintelligent Software Agents through Self-Play SWE-RL

- **[arXiv251223] Enhancing Medical Large Vision-Language Models via Alignment Distillation**
  - **tags:** TBD
  - **authors:** Aofei Chang, Ting Wang, Fenglong Ma
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18554
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp
  - **Simple LLM Summary:** Enhancing Medical Large Vision-Language Models via Alignment Distillation

- **[arXiv251223] Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V**
  - **tags:** TBD
  - **authors:** John Chen, Sihan Cheng, Can Gurkan, Ryan Lay, Moez Salahuddin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18564
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b0a311cb9d10337e5f3590761c621c192dd12a7c496b0cd7891fbccd0f8367_w640_q70.webp
  - **Simple LLM Summary:** Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V

- **[arXiv251223] Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale**
  - **tags:** TBD
  - **authors:** Saad Alqithami
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18561
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0aec0ad59816dd3fa60a1098beec472f1b9dc9c795b4760cf9c9f0516435437_w640_q70.webp
  - **Simple LLM Summary:** Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale

- **[arXiv251223] AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software**
  - **tags:** TBD
  - **authors:** Bin Wang, Wenjie Yu, Yilu Zhong, Hao Yu, Keke Lian, Chaohua Lu, Hongfang Zheng, Dong Zhang, Hui Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18567
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9235d0c624936af1cfc09fc3a4442da3cad4b4006c63ac0a8fe4392c0673dc_w640_q70.webp
  - **Simple LLM Summary:** AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software

- **[arXiv251223] Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing**
  - **tags:** TBD
  - **authors:** Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18575
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp
  - **Simple LLM Summary:** Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing

- **[arXiv251223] Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model**
  - **tags:** TBD
  - **authors:** Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18573
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp
  - **Simple LLM Summary:** Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model

- **[arXiv251223] ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18571
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp
  - **Simple LLM Summary:** ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning

- **[arXiv251223] From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation**
  - **tags:** TBD
  - **authors:** Amit Barman, Atanu Mandal, Sudip Kumar Naskar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18593
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp
  - **Simple LLM Summary:** From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation

- **[arXiv251223] Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction**
  - **tags:** TBD
  - **authors:** Qinglin Zeng, Jing Yang, Keze Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f210ca82f5b83e0df7f60aafea30fa1547a0370e3b4cda5c94ba26cc393a6f1d_w640_q70.webp
  - **Simple LLM Summary:** Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction

- **[arXiv251223] Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments**
  - **tags:** TBD
  - **authors:** Saeideh Yousefzadeh, Hamidreza Pourreza
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18613
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp
  - **Simple LLM Summary:** Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments

- **[arXiv251223] PTTA: A Pure Text-to-Animation Framework for High-Quality Creation**
  - **tags:** TBD
  - **authors:** Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18614
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp
  - **Simple LLM Summary:** PTTA: A Pure Text-to-Animation Framework for High-Quality Creation

- **[arXiv251223] Assignment-Routing Optimization: Solvers for Problems Under Constraints**
  - **tags:** TBD
  - **authors:** Yuan Qilong, Michal Pavelka
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18618
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be4f502655365659aa5803c3c3e98ff2a6071f2798b4e71f56472c3fe923e05_w640_q70.webp
  - **Simple LLM Summary:** Assignment-Routing Optimization: Solvers for Problems Under Constraints

- **[arXiv251223] DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System**
  - **tags:** TBD
  - **authors:** Zelin Wan, Han Jun Yoon, Nithin Alluru, Terrence J. Moore, Frederica F. Nelson, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Jin-Hee Cho
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18616
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fabd2c171e25c623bc7edccb8e1ef0506a926726f1d2a534aaa0d5113899beef_w640_q70.webp
  - **Simple LLM Summary:** DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System

- **[arXiv251223] ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning**
  - **tags:** TBD
  - **authors:** Zhenhao Zhou, Dan Negrut
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18619
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp
  - **Simple LLM Summary:** ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning

- **[arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction**
  - **tags:** TBD
  - **authors:** Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng, Kaitong Cai, Jian Wang, Keze Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18623
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp
  - **Simple LLM Summary:** LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction

- **[arXiv251223] The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation**
  - **tags:** TBD
  - **authors:** Huiqi Deng, Qihan Ren, Zhuofan Chen, Zhenyuan Cui, Wen Shen, Peng Zhang, Hongbin Pei, Quanshi Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18607
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp
  - **Simple LLM Summary:** The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation

- **[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback**
  - **tags:** TBD
  - **authors:** Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18622
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp
  - **Simple LLM Summary:** A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback

- **[arXiv251223] ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs**
  - **tags:** TBD
  - **authors:** Han-Seul Jeong, Youngjoon Park, Hyungseok Song, Woohyung Lim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18633
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp
  - **Simple LLM Summary:** ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs

- **[arXiv251223] Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking**
  - **tags:** TBD
  - **authors:** Dmitry Bennett, Fernand Gobet
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18665
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38aa956a72d7d3233a8e9436ec8b3eb96fe6f950dddcefb2c6497e4cc28ea7e6_w640_q70.webp
  - **Simple LLM Summary:** Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking

- **[arXiv251223] Geometric-Photometric Event-based 3D Gaussian Ray Tracing**
  - **tags:** TBD
  - **authors:** Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18640
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp
  - **Simple LLM Summary:** Geometric-Photometric Event-based 3D Gaussian Ray Tracing

- **[arXiv251223] IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling**
  - **tags:** TBD
  - **authors:** Jones David, Shreya Ghosh
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18669
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c4809145f5d00782974383c70a1a7132c8f9a0785e93701b00d5f208d14cae5_w640_q70.webp
  - **Simple LLM Summary:** IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling

- **[arXiv251223] ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting**
  - **tags:** TBD
  - **authors:** Hafiz Saif Ur Rehman, Ling Liu, Kaleem Ullah Qasim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18661
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e0e274099dd56b8114b544bec1b5ebd56dd8742f2795d776a62751dceb1bd0_w640_q70.webp
  - **Simple LLM Summary:** ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting

- **[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing**
  - **tags:** TBD
  - **authors:** Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18674
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp
  - **Simple LLM Summary:** Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing

- **[arXiv251223] Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model**
  - **tags:** TBD
  - **authors:** Yosuke Taniuchi, Chie Hieida, Atsushi Noritake, Kazushi Ikeda, Masaki Isoda
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18687
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a80afdf97506cd46b51f3e0232de08278947f67ce0d02728ea31ab57eb6fd39c_w640_q70.webp
  - **Simple LLM Summary:** Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model

- **[arXiv251223] CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles**
  - **tags:** TBD
  - **authors:** Cailin Lei, Haiyang Wu, Yuxiong Ji, Xiaoyu Cai, Yuchuan Du
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18703
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp
  - **Simple LLM Summary:** CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles

- **[arXiv251223] KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing**
  - **tags:** TBD
  - **authors:** Zhifei Li, Lifan Chen, Jiali Yi, Xiaoju Hou, Yue Zhao, Wenxin Huang, Miao Zhang, Kui Xiao, Bing Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18709
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6aad95971dcc28db4090fb00dc3a2271fb594ddd1417a6438b8306e2ee01f03a_w640_q70.webp
  - **Simple LLM Summary:** KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing

- **[arXiv251223] Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding**
  - **tags:** TBD
  - **authors:** Xiangrui Cai, Shaocheng Ma, Lei Cao, Jie Li, Tianyu Liu, Yilin Dong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18689
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp
  - **Simple LLM Summary:** Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding

- **[arXiv251223] Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection**
  - **tags:** TBD
  - **authors:** Junjun Pan, Yixin Liu, Rui Miao, Kaize Ding, Yu Zheng, Quoc Viet Hung Nguyen, Alan Wee-Chung Liew, Shirui Pan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18733
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7692bd1c5b9a982f1df045d952e852c9a8c6543c4c09cb5bf1bd92156eff8c8_w640_q70.webp
  - **Simple LLM Summary:** Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection

- **[arXiv251223] PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation**
  - **tags:** TBD
  - **authors:** Zichuan Lin, Xiaokai Huang, Jiate Liu, Yuxuan Han, Jia Chen, Xiapeng Wu, Deheng Ye
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18737
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp
  - **Simple LLM Summary:** PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation

- **[arXiv251223] Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth**
  - **tags:** TBD
  - **authors:** Chainarong Amornbunchornvej
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18732
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp
  - **Simple LLM Summary:** Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth

- **[arXiv251223] $M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models**
  - **tags:** TBD
  - **authors:** Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia, Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18735
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp
  - **Simple LLM Summary:** $M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models

- **[arXiv251223] Code2Doc: A Quality-First Curated Dataset for Code Documentation**
  - **tags:** TBD
  - **authors:** Recep Kaan Karaman, Meftun Akarsu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18748
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b6da5096797c358f77d8022914985853333b12b54cf68425fe470f42a60638b_w640_q70.webp
  - **Simple LLM Summary:** Code2Doc: A Quality-First Curated Dataset for Code Documentation

- **[arXiv251223] IPCV: Information-Preserving Compression for MLLM Visual Encoders**
  - **tags:** TBD
  - **authors:** Yuan Chen, Zichen Wen, Yuzhou Wu, Xuyang Liu, Shuang Chen, Junpeng Ma, Weijia Li, Conghui He, Linfeng Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18747
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp
  - **Simple LLM Summary:** IPCV: Information-Preserving Compression for MLLM Visual Encoders

- **[arXiv251223] MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking**
  - **tags:** TBD
  - **authors:** Jianyi Zhang, Shizhao Liu, Ziyin Zhou, Zhen Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18755
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/955c9d1fa243ab2977ec256331411b7bf6cca731527350187d399ec006ebe145_w640_q70.webp
  - **Simple LLM Summary:** MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking

- **[arXiv251223] Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs**
  - **tags:** TBD
  - **authors:** Lisan Al Amin, Vandana P. Janeja
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18797
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e389f385848f8816c83b5a1ff23be8e5adce564472d3ca92ed4e1c1107846a61_w640_q70.webp
  - **Simple LLM Summary:** Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs

- **[arXiv251223] The Dead Salmons of AI Interpretability**
  - **tags:** TBD
  - **authors:** Maxime Méloux, Giada Dirupo, François Portet, Maxime Peyrard
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18792
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04addf8c4fc59dd6f1ac7d1afe6ee58894441304221e6091e61ac24a403fb54e_w640_q70.webp
  - **Simple LLM Summary:** The Dead Salmons of AI Interpretability

- **[arXiv251223] Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform**
  - **tags:** TBD
  - **authors:** Yichuan Zhang, Chengxin Li, Yujie Gu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18791
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/828d54530ed9add4098a79bb9dd1f4047ed230dfaa399d57cade241c18713658_w640_q70.webp
  - **Simple LLM Summary:** Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform

- **[arXiv251223] FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation**
  - **tags:** TBD
  - **authors:** Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18809
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp
  - **Simple LLM Summary:** FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation

- **[arXiv251223] Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection**
  - **tags:** TBD
  - **authors:** Souhail Abdelmouaiz Sadat, Mohamed Yacine Touahria Miliani, Khadidja Hab El Hames, Hamida Seba, Mohammed Haddad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18826
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp
  - **Simple LLM Summary:** Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection

- **[arXiv251223] HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare**
  - **tags:** TBD
  - **authors:** Aditya Siddhant
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18829
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27fcee3d9ab3da4d05e9467a7467b02eecd9ce9ef97d79e927633356a26bb91b_w640_q70.webp
  - **Simple LLM Summary:** HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare

- **[arXiv251223] Controllable Probabilistic Forecasting with Stochastic Decomposition Layers**
  - **tags:** TBD
  - **authors:** John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne II, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18815
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp
  - **Simple LLM Summary:** Controllable Probabilistic Forecasting with Stochastic Decomposition Layers

- **[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning**
  - **tags:** TBD
  - **authors:** Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18857
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp
  - **Simple LLM Summary:** CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning

- **[arXiv251223] Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers**
  - **tags:** TBD
  - **authors:** Bruno Campello de Souza
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18871
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp
  - **Simple LLM Summary:** Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers

- **[arXiv251223] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction**
  - **tags:** TBD
  - **authors:** Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18880
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp
  - **Simple LLM Summary:** Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction

- **[arXiv251223] CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis**
  - **tags:** TBD
  - **authors:** Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18878
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp
  - **Simple LLM Summary:** CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis

- **[arXiv251223] Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models**
  - **tags:** TBD
  - **authors:** Gökdeniz Gülmez
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18901
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp
  - **Simple LLM Summary:** Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models

- **[arXiv251223] Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage**
  - **tags:** TBD
  - **authors:** Szymon Rusiecki, Cecilia G. Morales, Kimberly Elenberg, Leonard Weiss, Artur Dubrawski
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18908
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40a7a336fdd61231fb69499563fa54a3feead11655ae2c62d612544da79b259a_w640_q70.webp
  - **Simple LLM Summary:** Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage

- **[arXiv251223] An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects**
  - **tags:** TBD
  - **authors:** Shaokang Jiang, Daye Nam
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18925
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2089c7618775b9b9cdc54e25f2f7b14898adbf8c1ce8308d624be1f22c566408_w640_q70.webp
  - **Simple LLM Summary:** An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects

- **[arXiv251223] When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models**
  - **tags:** TBD
  - **authors:** Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp
  - **Simple LLM Summary:** When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models

- **[arXiv251223] LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer**
  - **tags:** TBD
  - **authors:** Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18930
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp
  - **Simple LLM Summary:** LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer

- **[arXiv251223] Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm**
  - **tags:** TBD
  - **authors:** Li Yan, Bolun Liu, Chao Li, Jing Liang, Kunjie Yu, Caitong Yue, Xuzhao Chai, Boyang Qu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18947
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b46e7ee52db647b84d2f31cc7432f1e81dc459afc316c9b1daf014a0d4d28f5d_w640_q70.webp
  - **Simple LLM Summary:** Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm

- **[arXiv251223] Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement**
  - **tags:** TBD
  - **authors:** Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18950
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp
  - **Simple LLM Summary:** Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement

- **[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection**
  - **tags:** TBD
  - **authors:** Yizhi Wang, Linan Yue, Min-Ling Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18956
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp
  - **Simple LLM Summary:** Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection

- **[arXiv251223] Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning**
  - **tags:** TBD
  - **authors:** Cheng-Hong Chang, Pei-Hsuan Tsai
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp
  - **Simple LLM Summary:** Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning

- **[arXiv251223] R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression**
  - **tags:** TBD
  - **authors:** Kun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu, Pengfei Gu, Chenghua Lin, Paul M. Thompson, Alex Leow, Heng Huang, Lifang He, Liang Zhan, Haoteng Tang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18986
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp
  - **Simple LLM Summary:** R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression

- **[arXiv251223] Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework**
  - **tags:** TBD
  - **authors:** Jinyan Liu, Zikang Chen, Qinchuan Wang, Tan Xie, Heming Zheng, Xudong Lv
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18999
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e96bd25bfb48e12bba787eb322582c438c99e6cb5614ad626a47b788b4598038_w640_q70.webp
  - **Simple LLM Summary:** Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework

- **[arXiv251223] ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation**
  - **tags:** TBD
  - **authors:** Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18991
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp
  - **Simple LLM Summary:** ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation

- **[arXiv251223] Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline**
  - **tags:** TBD
  - **authors:** Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19011
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp
  - **Simple LLM Summary:** Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline

- **[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management**
  - **tags:** TBD
  - **authors:** Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19001
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp
  - **Simple LLM Summary:** ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management

- **[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models**
  - **tags:** TBD
  - **authors:** Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp
  - **Simple LLM Summary:** Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models

- **[arXiv251223] The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results**
  - **tags:** TBD
  - **authors:** Konstantin Kaulen, Tobias Ladner, Stanley Bak, Christopher Brix, Hai Duong, Thomas Flinkow, Taylor T. Johnson, Lukas Koller, Edoardo Manino, ThanhVu H Nguyen, Haoze Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19007
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp
  - **Simple LLM Summary:** The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results

- **[arXiv251223] The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation**
  - **tags:** TBD
  - **authors:** Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19025
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp
  - **Simple LLM Summary:** The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation

- **[arXiv251223] Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation**
  - **tags:** TBD
  - **authors:** Connor Kilrain, David Carlyn, Julia Chae, Sara Beery, Wei-Lun Chao, Jianyang Gu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19026
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp
  - **Simple LLM Summary:** Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation

- **[arXiv251223] Recontextualization Mitigates Specification Gaming without Modifying the Specification**
  - **tags:** TBD
  - **authors:** Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19027
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp
  - **Simple LLM Summary:** Recontextualization Mitigates Specification Gaming without Modifying the Specification

- **[arXiv251223] IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments**
  - **tags:** TBD
  - **authors:** Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19024
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp
  - **Simple LLM Summary:** IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments

- **[arXiv251223] Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation**
  - **tags:** TBD
  - **authors:** Chi Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19061
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp
  - **Simple LLM Summary:** Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation

- **[arXiv251223] Can abstract concepts from LLM improve SLM performance?**
  - **tags:** TBD
  - **authors:** Siddharth Tandon
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19069
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c5acb71114f63e96507f1b112ea484c287d603fcb8d6eff38459b7e748f327_w640_q70.webp
  - **Simple LLM Summary:** Can abstract concepts from LLM improve SLM performance?

- **[arXiv251223] Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning**
  - **tags:** TBD
  - **authors:** Yanzhi Zhang, Yitong Duan, Zhaoxi Zhang, Jiyan He, Shuxin Zheng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19081
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bc376ab7cffef097c2b5c88731fb39a3b1651cbf234ff265cff9da444c1ef62_w640_q70.webp
  - **Simple LLM Summary:** Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning

- **[arXiv251223] $γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics**
  - **tags:** TBD
  - **authors:** Mark Burgess
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19084
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72c21c32f8ba86f52bd19910dee8b59d7a15f63a4f5f0fffc1d598908faca89a_w640_q70.webp
  - **Simple LLM Summary:** $γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics

- **[arXiv251223] Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving**
  - **tags:** TBD
  - **authors:** Peiqing Lu, Yuan Zhang, Haoyun Zhang, Jiasen Zheng, Kejian Tong, Wenjun Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19093
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ba3857cb85f2a61550d6204fcd94a616e3814c7b544954750bbe22dbc8e0777_w640_q70.webp
  - **Simple LLM Summary:** Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving

- **[arXiv251223] Conditioning Accept-Desirability models in the context of AGM-like belief change**
  - **tags:** TBD
  - **authors:** Kathelijne Coussement, Gert de Cooman, Keano De Vos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19096
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9a6821327555a097c992306a0343c693b0fa705a5154b551d5d3ec2af0af7fa_w640_q70.webp
  - **Simple LLM Summary:** Conditioning Accept-Desirability models in the context of AGM-like belief change

- **[arXiv251223] DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale**
  - **tags:** TBD
  - **authors:** Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19097
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp
  - **Simple LLM Summary:** DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale

- **[arXiv251223] FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning**
  - **tags:** TBD
  - **authors:** Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19107
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e20091cfc0ac4fa7e40c8d6d14ca11096f0a2c018bfea46fd36ed88a815bde01_w640_q70.webp
  - **Simple LLM Summary:** FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning

- **[arXiv251223] HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction**
  - **tags:** TBD
  - **authors:** Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin, Wei Zhong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19114
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp
  - **Simple LLM Summary:** HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction

- **[arXiv251223] Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis**
  - **tags:** TBD
  - **authors:** Chenghao Li, Chaoning Zhang, Yi Lu, Shuxu Chen, Xudong Wang, Jiaquan Zhang, Zhicheng Wang, Zhengxun Jin, Kuien Liu, Sung-Ho Bae, Guoqing Wang, Yang Yang, Hen Tao Shen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19135
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e824123c4e563edae3f7da6c1bb5757bd1fcb243770125d234d65f7ec4e4d0_w640_q70.webp
  - **Simple LLM Summary:** Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis

- **[arXiv251223] Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness**
  - **tags:** TBD
  - **authors:** Yin Jun Phua
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19155
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8c12473f43d4f07485e40276605a028c808372091757f33fc5847a97e4469d0_w640_q70.webp
  - **Simple LLM Summary:** Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness

- **[arXiv251223] Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments**
  - **tags:** TBD
  - **authors:** Geraud Nangue Tasse, Matthew Riemer, Benjamin Rosman, Tim Klinger
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19154
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp
  - **Simple LLM Summary:** Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments

- **[arXiv251223] Vision-Language-Policy Model for Dynamic Robot Task Planning**
  - **tags:** TBD
  - **authors:** Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19178
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp
  - **Simple LLM Summary:** Vision-Language-Policy Model for Dynamic Robot Task Planning

- **[arXiv251223] Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning**
  - **tags:** TBD
  - **authors:** Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19184
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp
  - **Simple LLM Summary:** Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning

- **[arXiv251223] Practical Quantum-Classical Feature Fusion for complex data Classification**
  - **tags:** TBD
  - **authors:** Azadeh Alavi, Fatemeh Kouchmeshki, Abdolrahman Alavi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19180
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp
  - **Simple LLM Summary:** Practical Quantum-Classical Feature Fusion for complex data Classification

- **[arXiv251223] On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning**
  - **tags:** TBD
  - **authors:** Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19199
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp
  - **Simple LLM Summary:** On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning

- **[arXiv251223] MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning**
  - **tags:** TBD
  - **authors:** Tao Zhang, Ziqian Zeng, Hao Peng, Huiping Zhuang, Cen Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19206
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp
  - **Simple LLM Summary:** MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning

- **[arXiv251223] Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation**
  - **tags:** TBD
  - **authors:** Jerry Wang, Ting Yiu Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19210
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62c98c8f57888fd4ebe24a386db1767dce70baaed757b8ccaec8a6d19541746c_w640_q70.webp
  - **Simple LLM Summary:** Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation

- **[arXiv251223] Towards Minimal Fine-Tuning of VLMs**
  - **tags:** TBD
  - **authors:** Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19219
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp
  - **Simple LLM Summary:** Towards Minimal Fine-Tuning of VLMs

- **[arXiv251223] From Pixels to Predicates Structuring urban perception with scene graphs**
  - **tags:** TBD
  - **authors:** Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19221
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp
  - **Simple LLM Summary:** From Pixels to Predicates Structuring urban perception with scene graphs

- **[arXiv251223] Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models**
  - **tags:** TBD
  - **authors:** Valentin Schmidberger, Manuel Eberhardinger, Setareh Maghsudi, Johannes Maucher
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19228
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53ed231c85ea495143f9234046091abf1a755a2bae0a9a62789a4927a126a190_w640_q70.webp
  - **Simple LLM Summary:** Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models

- **[arXiv251223] Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation**
  - **tags:** TBD
  - **authors:** Anna-Maria Gueorguieva, Aylin Caliskan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19238
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp
  - **Simple LLM Summary:** Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation

- **[arXiv251223] ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models**
  - **tags:** TBD
  - **authors:** Mingxu Zhang, Dazhong Shen, Qi Zhang, Ying Sun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19240
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeb3b37133d9071af2bfeeecd0da8171d70b3d3d9b3b0658553484d1919570f5_w640_q70.webp
  - **Simple LLM Summary:** ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models

- **[arXiv251223] DeliveryBench: Can Agents Earn Profit in Real World?**
  - **tags:** TBD
  - **authors:** Lingjun Mao, Jiawei Ren, Kun Zhou, Jixuan Chen, Ziqiao Ma, Lianhui Qin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19234
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c24ad2486e536c4e46b7008f9b36fe0a1b521f054ff0b708d9260cb71032fa7_w640_q70.webp
  - **Simple LLM Summary:** DeliveryBench: Can Agents Earn Profit in Real World?

- **[arXiv251223] Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics**
  - **tags:** TBD
  - **authors:** Do Minh Duc, Quan Xuan Truong, Nguyen Tat Dat, Nguyen Van Vinh
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19247
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3e85ac8e144e835ca46b7dcdc94a82085e98b53bc5c52cfa6addea751cf9af2_w640_q70.webp
  - **Simple LLM Summary:** Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics

- **[arXiv251223] Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study**
  - **tags:** TBD
  - **authors:** Carla Crivoi, Radu Tudor Ionescu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19253
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp
  - **Simple LLM Summary:** Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study

- **[arXiv251223] Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6**
  - **tags:** TBD
  - **authors:** Jiaao Wu, Xian Zhang, Fan Yang, Yinpeng Dong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19287
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b24f019b56baece009c93ac1107b1f8aea4b09d87df075427e81815567970f3d_w640_q70.webp
  - **Simple LLM Summary:** Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6

- **[arXiv251223] Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation**
  - **tags:** TBD
  - **authors:** Ivan DeAndres-Tame, Chengwei Ye, Ruben Tolosana, Ruben Vera-Rodriguez, Shiqi Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19275
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp
  - **Simple LLM Summary:** Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation

- **[arXiv251223] Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models**
  - **tags:** TBD
  - **authors:** Linzhi Chen, Yang Sun, Hongru Wei, Yuqi Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19297
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb0bd7eb8763e5b7b48b95fffa2b9d2689db5cbe27fd8dd5ff4a8a691095d826_w640_q70.webp
  - **Simple LLM Summary:** Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models

- **[arXiv251223] Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals**
  - **tags:** TBD
  - **authors:** Chang Dong, Jianfeng Tao, Chengliang Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19280
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23dae36b800575da09218251c1aa3582a2ce5ca30c8c9988566be10e83f43e38_w640_q70.webp
  - **Simple LLM Summary:** Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals

- **[arXiv251223] SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models**
  - **tags:** TBD
  - **authors:** A.A. Gde Yogi Pramana, Jason Ray, Anthony Jaya, Michael Wijaya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19317
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e59bf81bbcade919249cc70e6c7c857215e36f22480011e424fdeb4a1f6ca56_w640_q70.webp
  - **Simple LLM Summary:** SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models

- **[arXiv251223] Alternative positional encoding functions for neural transformers**
  - **tags:** TBD
  - **authors:** Ezequiel Lopez-Rubio, Macoris Decena-Gimenez, Rafael Marcos Luque-Baena
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19323
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a8d61d0d13038271b6545ad7d265494ffd5b3cce79e03046ec542b879d17a8_w640_q70.webp
  - **Simple LLM Summary:** Alternative positional encoding functions for neural transformers

- **[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration**
  - **tags:** TBD
  - **authors:** Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19320
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp
  - **Simple LLM Summary:** MAGIC: Achieving Superior Model Merging via Magnitude Calibration

- **[arXiv251223] MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture**
  - **tags:** TBD
  - **authors:** Hui Li, Jiayue Lyu, Fu-Yun Wang, Kaihui Cheng, Siyu Zhu, Jingdong Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19311
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp
  - **Simple LLM Summary:** MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture

- **[arXiv251223] Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application**
  - **tags:** TBD
  - **authors:** Haoyu Jiang, Fanjie Zeng, Boan Qu, Xiaojie Lin, Wei Zhong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19299
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f659480b32a80b6959cb66f4e981a4486e0674a48613b87e1f87ae33a24a364_w640_q70.webp
  - **Simple LLM Summary:** Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application

- **[arXiv251223] VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop**
  - **tags:** TBD
  - **authors:** JiaWei Zhu, ZiHeng Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19349
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0b686fca34f155b782afa1f7cabe09de9b7bcb967ccfba8215a39405da5ba99_w640_q70.webp
  - **Simple LLM Summary:** VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop

- **[arXiv251223] First-Order Representation Languages for Goal-Conditioned RL**
  - **tags:** TBD
  - **authors:** Simon Ståhlberg, Hector Geffner
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19355
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a8381365dc741932409d148f8d829e1cb922492f327e8a311d02d4ed8ad6d56_w640_q70.webp
  - **Simple LLM Summary:** First-Order Representation Languages for Goal-Conditioned RL

- **[arXiv251223] PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models**
  - **tags:** TBD
  - **authors:** A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19350
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22263a25ed189a880450b2a3c8562fb8b0c96c8ec9fefce093363fc2ea2fb8f6_w640_q70.webp
  - **Simple LLM Summary:** PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models

- **[arXiv251223] Learning General Policies with Policy Gradient Methods**
  - **tags:** TBD
  - **authors:** Simon Ståhlberg, Blai Bonet, Hector Geffner
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19366
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp
  - **Simple LLM Summary:** Learning General Policies with Policy Gradient Methods

- **[arXiv251223] Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture**
  - **tags:** TBD
  - **authors:** Christian Hägg, Kathlén Kohn, Giovanni Luca Marchetti, Boris Shapiro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19367
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa84d344152205d90f463c26b0bb9356b71dde7f412bdd03d7fd7f036a0b845_w640_q70.webp
  - **Simple LLM Summary:** Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture

- **[arXiv251223] OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation**
  - **tags:** TBD
  - **authors:** Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19379
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp
  - **Simple LLM Summary:** OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation

- **[arXiv251223] EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration**
  - **tags:** TBD
  - **authors:** Runze Li, Yuwen Zhai, Bo Xu, LiWu Xu, Nian Shi, Wei Zhang, Ran Lin, Liang Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19396
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bef52c443ab4e7eee0649ff0f53fd382c9727d2345228eb8bb9fe5ad898a8eb_w640_q70.webp
  - **Simple LLM Summary:** EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration

- **[arXiv251223] DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition**
  - **tags:** TBD
  - **authors:** Yueyao Chen, Kai-Ni Wang, Dario Tayupo, Arnaud Huaulm'e, Krystel Nyangoh Timoh, Pierre Jannin, Qi Dou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19387
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp
  - **Simple LLM Summary:** DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition

- **[arXiv251223] Research Program: Theory of Learning in Dynamical Systems**
  - **tags:** TBD
  - **authors:** Elad Hazan, Shai Shalev Shwartz, Nathan Srebro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19410
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/879026bb2ccb2b89c867729f9f077e32888b82173c20d950c0ed5feda6519aa9_w640_q70.webp
  - **Simple LLM Summary:** Research Program: Theory of Learning in Dynamical Systems

- **[arXiv251223] Attention Is Not What You Need**
  - **tags:** TBD
  - **authors:** Zhang Chong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19428
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbe1f1a463179312610994fd34a110ca4bfc5b56928e49a6749dd43948421e91_w640_q70.webp
  - **Simple LLM Summary:** Attention Is Not What You Need

- **[arXiv251223] MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation**
  - **tags:** TBD
  - **authors:** Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19438
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp
  - **Simple LLM Summary:** MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation

- **[arXiv251223] Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations**
  - **tags:** TBD
  - **authors:** Jinwei Chi, Ke Wang, Yu Chen, Xuanye Lin, Qiang Xu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19456
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44e2303feb430ac2c66a05d707fa59f0184a8efed477dd24968816daffaaf4a2_w640_q70.webp
  - **Simple LLM Summary:** Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations

- **[arXiv251223] An Agentic Framework for Autonomous Materials Computation**
  - **tags:** TBD
  - **authors:** Zeyu Xia, Jinzhe Ma, Congjie Zheng, Shufei Zhang, Yuqiang Li, Hang Su, P. Hu, Changshui Zhang, Xingao Gong, Wanli Ouyang, Lei Bai, Dongzhan Zhou, Mao Su
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19458
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc8796e6d842dffe17cec24dc175dc3c7a315afb61353b40a3cd5603693d9a1_w640_q70.webp
  - **Simple LLM Summary:** An Agentic Framework for Autonomous Materials Computation

- **[arXiv251223] Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications**
  - **tags:** TBD
  - **authors:** Lorenzo Capelli, Leandro de Souza Rosa, Gianluca Setti, Mauro Mangia, Riccardo Rovatti
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19472
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40e0a2de9bc90f1d512acba9f8178e3caeb11f59b899b803bcea54b876c14e2e_w640_q70.webp
  - **Simple LLM Summary:** Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications

- **[arXiv251223] A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis**
  - **tags:** TBD
  - **authors:** Katharina Stengg, Christian Macho, Martin Pinzger
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19481
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83f5edcaed3e66cf570a1118c40709f58e2f28ea874fce90d48d26c98b1618e8_w640_q70.webp
  - **Simple LLM Summary:** A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis

- **[arXiv251223] Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset**
  - **tags:** TBD
  - **authors:** Nikita Volzhin, Soowhan Yoon
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19494
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d0c1cdb2dc4e7f555b6151e7dd057ebb961c3137ad65952a50508e3d588bb4a_w640_q70.webp
  - **Simple LLM Summary:** Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset

- **[arXiv251223] Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation**
  - **tags:** TBD
  - **authors:** Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19512
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp
  - **Simple LLM Summary:** Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation

- **[arXiv251223] DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast**
  - **tags:** TBD
  - **authors:** Hongliang Li, Nong Zhang, Zhewen Xu, Xiang Li, Changzheng Liu, Chongbo Zhao, Jie Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19506
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ab91ff5a253b798aeb4b7fa1cee40fb5a1319f0697a2094d06ad95557270ff9_w640_q70.webp
  - **Simple LLM Summary:** DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast

- **[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Xueming Yan, Bo Yin, Yaochu Jin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19516
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp
  - **Simple LLM Summary:** LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning

- **[arXiv251223] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models**
  - **tags:** TBD
  - **authors:** Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li Fei-fei, Ehsan Adeli
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19526
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc62413ed86e23bc11d3109dd3f6d9f5cdc6abfea9e0de269b698952b18c550_w640_q70.webp
  - **Simple LLM Summary:** QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models

- **[arXiv251223] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement**
  - **tags:** TBD
  - **authors:** Hongsheng Xing, Qiuxin Si
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19530
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a398a1c7e656e700ae7d2e4b360c6e2a84d8a4f125de94406eb2fcbd1174cabf_w640_q70.webp
  - **Simple LLM Summary:** Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement

- **[arXiv251223] CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion**
  - **tags:** TBD
  - **authors:** Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19535
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp
  - **Simple LLM Summary:** CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion

- **[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal**
  - **tags:** TBD
  - **authors:** Yongxin Wang, Zhicheng Yang, Meng Cao, Mingfei Han, Haokun Lin, Yingying Zhu, Xiaojun Chang, Xiaodan Liang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19554
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp
  - **Simple LLM Summary:** CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal

- **[arXiv251223] Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios**
  - **tags:** TBD
  - **authors:** Jiawen Wang, Jingjing Wang Tianyang Chen, Min Zhang, Guodong Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19551
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdd63f7cc3d632dfdf950fc0474c659424578038c0958a942040653588c1bbd3_w640_q70.webp
  - **Simple LLM Summary:** Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios

- **[arXiv251223] REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation**
  - **tags:** TBD
  - **authors:** Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19562
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp
  - **Simple LLM Summary:** REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation

- **[arXiv251223] Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations**
  - **tags:** TBD
  - **authors:** Lawrence Krukrubo, Julius Odede, Olawande Olusegun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19557
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4cf6d654019f56b56f3c2182b2db9d82b07540ace871f6684795ebe22f1a7c35_w640_q70.webp
  - **Simple LLM Summary:** Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations

- **[arXiv251223] Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles**
  - **tags:** TBD
  - **authors:** Yanliang Huang, Xia Yan, Peiran Yin, Zhenduo Zhang, Zeyan Shao, Youran Wang, Haoliang Huang, Matthias Althoff
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19564
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp
  - **Simple LLM Summary:** Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles

- **[arXiv251223] BabyFlow: 3D modeling of realistic and expressive infant faces**
  - **tags:** TBD
  - **authors:** Antonia Alomar, Mireia Masias, Marius George Linguraru, Federico M. Sukno, Gemma Piella
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19560
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp
  - **Simple LLM Summary:** BabyFlow: 3D modeling of realistic and expressive infant faces

- **[arXiv251223] The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge**
  - **tags:** TBD
  - **authors:** Angjelin Hila
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19570
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14053cd17ec2f7fbfad183ca144e70fb650f93b135eca28453bda97a810f7db4_w640_q70.webp
  - **Simple LLM Summary:** The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge

- **[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller**
  - **tags:** TBD
  - **authors:** Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19576
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp
  - **Simple LLM Summary:** LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller

- **[arXiv251223] Exploring the features used for summary evaluation by Human and GPT**
  - **tags:** TBD
  - **authors:** Zahra Sadeghi, Evangelos Milios, Frank Rudzicz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19620
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e154f176f186b8dabd43d09d2a96579db8d3bbe3ccbf6debeb1b756642ffa2a_w640_q70.webp
  - **Simple LLM Summary:** Exploring the features used for summary evaluation by Human and GPT

- **[arXiv251223] MapTrace: Scalable Data Generation for Route Tracing on Maps**
  - **tags:** TBD
  - **authors:** Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19609
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp
  - **Simple LLM Summary:** MapTrace: Scalable Data Generation for Route Tracing on Maps

- **[arXiv251223] Clustering with Label Consistency**
  - **tags:** TBD
  - **authors:** Diptarka Chakraborty, Hendrik Fichtenberger, Bernhard Haeupler, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola Svensson
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19654
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73fc229d3b4fc51718fe8d59f9abfb97c5b0e2d37b67f22dacfc080be54754bb_w640_q70.webp
  - **Simple LLM Summary:** Clustering with Label Consistency

- **[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies**
  - **tags:** TBD
  - **authors:** Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19673
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp
  - **Simple LLM Summary:** Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies

- **[arXiv251223] Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis**
  - **tags:** TBD
  - **authors:** Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19663
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp
  - **Simple LLM Summary:** Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis

- **[arXiv251223] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion**
  - **tags:** TBD
  - **authors:** Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19678
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp
  - **Simple LLM Summary:** WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion

- **[arXiv251223] Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight**
  - **tags:** TBD
  - **authors:** Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19691
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3b7aa03e4ca854038444e0521aaa9d1a6b6c20e2054b4637dd76d61ecac2014_w640_q70.webp
  - **Simple LLM Summary:** Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight

- **[arXiv251223] Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing**
  - **tags:** TBD
  - **authors:** Christopher Regan, Ying Xie
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17923
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0972c0f33f9a898aad22d9d466edb2d113b1f06ae271519a0310fbe4deb8326_w640_q70.webp
  - **Simple LLM Summary:** Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing

- **[arXiv251223] Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach**
  - **tags:** TBD
  - **authors:** Dongdong Yang, Bin Li, Jiguang He, Yicheng Yan, Xiaoyu Zhang, Chongwen Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17928
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52bfb16cf6da30f529d6affbb7e78493e8d297701e513afd78f81efa7c4804bd_w640_q70.webp
  - **Simple LLM Summary:** Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach

- **[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods**
  - **tags:** TBD
  - **authors:** Sheryl Chen, Tony Wang, Kyle Feinstein
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17929
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp
  - **Simple LLM Summary:** Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods

- **[arXiv251223] A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework**
  - **tags:** TBD
  - **authors:** Ravi Prasad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17968
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7665945961b2f7304f7c1df5cbe15f902828795ce6193b462edbd1a98af56880_w640_q70.webp
  - **Simple LLM Summary:** A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework

- **[arXiv251223] Re-assessing the evidence for mental rotation abilities in children using computational models**
  - **tags:** TBD
  - **authors:** Arthur Aubret, Jochen Triesch
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17972
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04b19b13a0170f5c8d0ddc85fcabbe83baa46fd0d40efd430823288b256d119b_w640_q70.webp
  - **Simple LLM Summary:** Re-assessing the evidence for mental rotation abilities in children using computational models

- **[arXiv251223] The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective**
  - **tags:** TBD
  - **authors:** Muhammad Osama Imran, Roshni Lulla, Rodney Sappington
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17989
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088c001d289b3633293d46545215e3c6c3c647164a557d5d4458682598dc319a_w640_q70.webp
  - **Simple LLM Summary:** The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective

- **[arXiv251223] On Efficient Adjustment in Causal Graphs**
  - **tags:** TBD
  - **authors:** Isabela Belciug, Simon Ferreira, Charles K. Assaad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18315
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e651794ebebddd044adc102fdfd3e79c703c2f1d1bbe46f929abaf0866588f7c_w640_q70.webp
  - **Simple LLM Summary:** On Efficient Adjustment in Causal Graphs

- **[arXiv251223] Evolutionary BP+OSD Decoding for Low-Latency Quantum Error Correction**
  - **tags:** TBD
  - **authors:** Hee-Youl Kwak, Seong-Joon Park, Hyunwoo Jung, Jeongseok Ha, Jae-Won Kim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18273
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f5d84f1158c216751ab871ccb9ff92e39812054d2d89f523c363ba1d516324f_w640_q70.webp
  - **Simple LLM Summary:** Evolutionary BP+OSD Decoding for Low-Latency Quantum Error Correction

- **[arXiv251223] TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition**
  - **tags:** TBD
  - **authors:** Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18263
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp
  - **Simple LLM Summary:** TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition

- **[arXiv251223] The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics**
  - **tags:** TBD
  - **authors:** Barak Or
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18508
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de95b749b6e41b5ecb30b6f99c2de21906ad83985f90b86407263b2e092757dd_w640_q70.webp
  - **Simple LLM Summary:** The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics

- **[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics**
  - **tags:** TBD
  - **authors:** Yucheng Yang, Chiyuan Wang, Andreas Schaab, Benjamin Moll
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18892
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp
  - **Simple LLM Summary:** Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics

- **[arXiv251223] Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty**
  - **tags:** TBD
  - **authors:** Lapo Santarlasci, Armando Rungi, Loredana Fattorini, Nestor Maslej
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19569
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d480c4d77b7eaba72c49770556d7fc5b5d3dfd78a176aef0d2a236de15ebeb67_w640_q70.webp
  - **Simple LLM Summary:** Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty

## 2025-12-24

- **[arXiv251224] QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19696
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp
  - **Simple LLM Summary:** QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning

- **[arXiv251224] Automated Fault Detection in 5G Core Networks Using Large Language Models**
  - **tags:** TBD
  - **authors:** Parsa Hatami, Ahmadreza Majlesara, Ali Majlesi, Babak Hossein Khalaj
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19697
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51195e88b90e075513d8e30088f944c9008c4dee4bc84af09cdce6bb8a7b71e4_w640_q70.webp
  - **Simple LLM Summary:** Automated Fault Detection in 5G Core Networks Using Large Language Models

- **[arXiv251224] PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility**
  - **tags:** TBD
  - **authors:** Md Nahid Hasan Shuvo, Moinul Hossain
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19711
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp
  - **Simple LLM Summary:** PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility

- **[arXiv251224] Large Language Models for EDA Cloud Job Resource and Lifetime Prediction**
  - **tags:** TBD
  - **authors:** Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19701
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp
  - **Simple LLM Summary:** Large Language Models for EDA Cloud Job Resource and Lifetime Prediction

- **[arXiv251224] Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data**
  - **tags:** TBD
  - **authors:** Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19716
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp
  - **Simple LLM Summary:** Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data

- **[arXiv251224] Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance**
  - **tags:** TBD
  - **authors:** James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19707
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp
  - **Simple LLM Summary:** Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance

- **[arXiv251224] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference**
  - **tags:** TBD
  - **authors:** Zhan Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19717
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp
  - **Simple LLM Summary:** Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference

- **[arXiv251224] Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries**
  - **tags:** TBD
  - **authors:** Zihao Lv, Siqi Ai, Yanbin Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19719
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp
  - **Simple LLM Summary:** Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries

- **[arXiv251224] Tiny, On-Device Decision Makers with the MiniConv Library**
  - **tags:** TBD
  - **authors:** Carlos Purves
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19726
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp
  - **Simple LLM Summary:** Tiny, On-Device Decision Makers with the MiniConv Library

- **[arXiv251224] High-Performance Self-Supervised Learning by Joint Training of Flow Matching**
  - **tags:** TBD
  - **authors:** Kosuke Ukita, Tsuyoshi Okita
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19729
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp
  - **Simple LLM Summary:** High-Performance Self-Supervised Learning by Joint Training of Flow Matching

- **[arXiv251224] Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach**
  - **tags:** TBD
  - **authors:** Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19737
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp
  - **Simple LLM Summary:** Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach

- **[arXiv251224] CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology**
  - **tags:** TBD
  - **authors:** Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19736
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp
  - **Simple LLM Summary:** CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology

- **[arXiv251224] From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning**
  - **tags:** TBD
  - **authors:** Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19743
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp
  - **Simple LLM Summary:** From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning

- **[arXiv251224] How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts**
  - **tags:** TBD
  - **authors:** Sumin Park, Noseong Park
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19765
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp
  - **Simple LLM Summary:** How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts

- **[arXiv251224] A K-Means, Ward and DBSCAN repeatability study**
  - **tags:** TBD
  - **authors:** Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19772
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp
  - **Simple LLM Summary:** A K-Means, Ward and DBSCAN repeatability study

- **[arXiv251224] Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning**
  - **tags:** TBD
  - **authors:** Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19777
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp
  - **Simple LLM Summary:** Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning

- **[arXiv251224] A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows**
  - **tags:** TBD
  - **authors:** Ivan Daunis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19769
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e158baf642d33624c0967b1dcd509fbc3876a4bc52a539d4b6e7c800995b42_w640_q70.webp
  - **Simple LLM Summary:** A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows

- **[arXiv251224] Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models**
  - **tags:** TBD
  - **authors:** Wang Bin, Ao Yang, Kedan Li, Aofan Liu, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19758
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45aa033e5d42a870c8059ab54cb6cefa331610516ad0bef063a9ce423cb132dc_w640_q70.webp
  - **Simple LLM Summary:** Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models

- **[arXiv251224] PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research**
  - **tags:** TBD
  - **authors:** Tingjia Miao, Jiawen Dai, Jingkun Liu, Jinxin Tan, Muhua Zhang, Wenkai Jin, Yuwen Du, Tian Jin, Xianghe Pang, Zexi Liu, Tu Guo, Zhengliang Zhang, Yunjie Huang, Shuo Chen, Rui Ye, Yuzhi Zhang, Linfeng Zhang, Kun Chen, Wei Wang, Weinan E, Siheng Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19799
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1922ac933b302c99f4a3c639911ffa3980ae43238fad1b247af369017413128_w640_q70.webp
  - **Simple LLM Summary:** PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research

- **[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication**
  - **tags:** TBD
  - **authors:** Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19849
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp
  - **Simple LLM Summary:** UCCL-EP: Portable Expert-Parallel Communication

- **[arXiv251224] A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution**
  - **tags:** TBD
  - **authors:** Mahdi Mostajabdaveh, F. Sibel Salman, Walter J. Gutjahr
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19882
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81c6462c8b784a5ac97bed22a2eedfc4c0fbad0afad1bab0e0a9aab3730a1834_w640_q70.webp
  - **Simple LLM Summary:** A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution

- **[arXiv251224] HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data**
  - **tags:** TBD
  - **authors:** Shashi Kant Gupta, Arijeet Pramanik, Jerrin John Thomas, Regina Schwind, Lauren Wiener, Avi Raju, Jeremy Kornbluth, Yanshan Wang, Zhaohui Su, Hrituraj Singh
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19864
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0cfa9ec043e8a27718dd14bb89bf3c4ceafb97fb48a8a0b61d661ec9d34b09_w640_q70.webp
  - **Simple LLM Summary:** HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data

- **[arXiv251224] Fine-Tuned In-Context Learners for Efficient Adaptation**
  - **tags:** TBD
  - **authors:** Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19879
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp
  - **Simple LLM Summary:** Fine-Tuned In-Context Learners for Efficient Adaptation

- **[arXiv251224] A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones**
  - **tags:** TBD
  - **authors:** Sujan Warnakulasooriya, Andreas Willig, Xiaobing Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19914
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp
  - **Simple LLM Summary:** A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones

- **[arXiv251224] Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19920
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp
  - **Simple LLM Summary:** Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning

- **[arXiv251224] Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling**
  - **tags:** TBD
  - **authors:** Indranil Halder, Cengiz Pehlevan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19905
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp
  - **Simple LLM Summary:** Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling

- **[arXiv251224] Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra**
  - **tags:** TBD
  - **authors:** Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19909
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp
  - **Simple LLM Summary:** Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra

- **[arXiv251224] Unified Brain Surface and Volume Registration**
  - **tags:** TBD
  - **authors:** S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19928
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp
  - **Simple LLM Summary:** Unified Brain Surface and Volume Registration

- **[arXiv251224] Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress**
  - **tags:** TBD
  - **authors:** Samruddhi Baviskar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19935
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp
  - **Simple LLM Summary:** Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress

- **[arXiv251224] Vehicle-centric Perception via Multimodal Structured Pre-training**
  - **tags:** TBD
  - **authors:** Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp
  - **Simple LLM Summary:** Vehicle-centric Perception via Multimodal Structured Pre-training

- **[arXiv251224] Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs**
  - **tags:** TBD
  - **authors:** Eric Yeh, John Cadigan, Ran Chen, Dick Crouch, Melinda Gervasio, Dayne Freitag
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19937
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67e83c33b123e956d61ade807a7eb155837dc60f3e60e67f0209df1eb75d7639_w640_q70.webp
  - **Simple LLM Summary:** Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs

- **[arXiv251224] Block-Recurrent Dynamics in Vision Transformers**
  - **tags:** TBD
  - **authors:** Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19941
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp
  - **Simple LLM Summary:** Block-Recurrent Dynamics in Vision Transformers

- **[arXiv251224] How Much 3D Do Video Foundation Models Encode?**
  - **tags:** TBD
  - **authors:** Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19949
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp
  - **Simple LLM Summary:** How Much 3D Do Video Foundation Models Encode?

- **[arXiv251224] Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification**
  - **tags:** TBD
  - **authors:** Luciano Araujo Dourado Filho, Almir Moreira da Silva Neto, Rodrigo Pereira David, Rodrigo Tripodi Calumby
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19957
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/307d3e337ca6d53335e1861afc2551a5e05ba3baffbfb580ffc6378d16a74e2f_w640_q70.webp
  - **Simple LLM Summary:** Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification

- **[arXiv251224] FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification**
  - **tags:** TBD
  - **authors:** Luciano Araujo Dourado Filho, Rodrigo Tripodi Calumby
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19960
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbfda29c0b0fd29ef417a6aed2c9021c6a676577f450cbc925474212756dbba1_w640_q70.webp
  - **Simple LLM Summary:** FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification

- **[arXiv251224] Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?**
  - **tags:** TBD
  - **authors:** Zhe Yin, Xiaodong Gu, Beijun Shen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19980
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/612b57ba54262082ae033612387571cddc86ac826d14c6f5b1ba4c241011b3b9_w640_q70.webp
  - **Simple LLM Summary:** Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?

- **[arXiv251224] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models**
  - **tags:** TBD
  - **authors:** Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19995
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp
  - **Simple LLM Summary:** Schoenfeld's Anatomy of Mathematical Reasoning by Language Models

- **[arXiv251224] S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test**
  - **tags:** TBD
  - **authors:** Zhe Sun, Xueyuan Yang, Yujie Lu, Zhenliang Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19992
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7f6951c4ebc6cf6c8bd633198ae7d4c6a7c8f1e0cd348aa9e6737966dfe600_w640_q70.webp
  - **Simple LLM Summary:** S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test

- **[arXiv251224] IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense**
  - **tags:** TBD
  - **authors:** Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp
  - **Simple LLM Summary:** IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense

- **[arXiv251224] DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics**
  - **tags:** TBD
  - **authors:** Yuan Gao, Zhenguo Dong, Xuelong Wang, Zhiqiang Wang, Yong Zhang, Shaofan Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20028
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp
  - **Simple LLM Summary:** DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics

- **[arXiv251224] Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting**
  - **tags:** TBD
  - **authors:** Sangoh Lee, Sangwoo Mo, Wook-Shin Han
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20014
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp
  - **Simple LLM Summary:** Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting

- **[arXiv251224] Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva**
  - **tags:** TBD
  - **authors:** Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20042
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp
  - **Simple LLM Summary:** Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva

- **[arXiv251224] Learning Skills from Action-Free Videos**
  - **tags:** TBD
  - **authors:** Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20052
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp
  - **Simple LLM Summary:** Learning Skills from Action-Free Videos

- **[arXiv251224] An Optimal Policy for Learning Controllable Dynamics by Exploration**
  - **tags:** TBD
  - **authors:** Peter N. Loxley
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20053
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp
  - **Simple LLM Summary:** An Optimal Policy for Learning Controllable Dynamics by Exploration

- **[arXiv251224] Discovering Lie Groups with Flow Matching**
  - **tags:** TBD
  - **authors:** Jung Yeon Park, Yuxuan Chen, Floor Eijkelboom, Jan-Willem van de Meent, Lawson L.S. Wong, Robin Walters
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20043
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/806c459fa9197bda13199f58531fce983ef12854f6eb2e8acaaea33dfebd6a22_w640_q70.webp
  - **Simple LLM Summary:** Discovering Lie Groups with Flow Matching

- **[arXiv251224] Scaling Reinforcement Learning for Content Moderation with Large Language Models**
  - **tags:** TBD
  - **authors:** Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20061
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cc1d5bc88350e4d91579fed9a3b17abade80dc25754379e8e11ce92e39ca7d5_w640_q70.webp
  - **Simple LLM Summary:** Scaling Reinforcement Learning for Content Moderation with Large Language Models

- **[arXiv251224] Reason2Decide: Rationale-Driven Multi-Task Learning**
  - **tags:** TBD
  - **authors:** H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20074
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/187b806c979650defdb64bdb9ce297598ef473654b93625ec2257af228dbd0da_w640_q70.webp
  - **Simple LLM Summary:** Reason2Decide: Rationale-Driven Multi-Task Learning

- **[arXiv251224] On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities**
  - **tags:** TBD
  - **authors:** Sangryu Park, Gihyuk Ko, Homook Cho
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20062
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74a88fcd014c903ee16397aa497f69b488c2c3bdeb23c2bddfc09643306081ec_w640_q70.webp
  - **Simple LLM Summary:** On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities

- **[arXiv251224] Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach**
  - **tags:** TBD
  - **authors:** Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20056
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp
  - **Simple LLM Summary:** Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach

- **[arXiv251224] CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks**
  - **tags:** TBD
  - **authors:** Dianxuan Fu, Xiaomin Liu, Yihao Zhang, Shikui Shen, Weisheng Hu, Qunbi Zhuge
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20080
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b7f27ae92944936b92479ef7c0a55f8c448c532444c4f43131d8768483bda71_w640_q70.webp
  - **Simple LLM Summary:** CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks

- **[arXiv251224] Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches**
  - **tags:** TBD
  - **authors:** Chaithra, Kamesh Kadimisetty, Biju R Mohan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20082
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8f2d13846195cc68b294529fa5d22600c76c1ff5581ee402ddf30ac1c06da72_w640_q70.webp
  - **Simple LLM Summary:** Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches

- **[arXiv251224] Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection**
  - **tags:** TBD
  - **authors:** Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20086
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp
  - **Simple LLM Summary:** Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection

- **[arXiv251224] QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption**
  - **tags:** TBD
  - **authors:** Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20084
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp
  - **Simple LLM Summary:** QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption

- **[arXiv251224] Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts**
  - **tags:** TBD
  - **authors:** Jinyoung Choi, Youngchae Kwon, Injung Kim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20088
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp
  - **Simple LLM Summary:** Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts

- **[arXiv251224] Evolutionary Neural Architecture Search with Dual Contrastive Learning**
  - **tags:** TBD
  - **authors:** Xian-Rong Zhang, Yue-Jiao Gong, Wei-Neng Chen, Jun Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20112
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/659d1805ddb5e7863af4ee9eeedcfdd78686f84f207247f4d0e65555165f2577_w640_q70.webp
  - **Simple LLM Summary:** Evolutionary Neural Architecture Search with Dual Contrastive Learning

- **[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language**
  - **tags:** TBD
  - **authors:** Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20111
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp
  - **Simple LLM Summary:** ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language

- **[arXiv251224] MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization**
  - **tags:** TBD
  - **authors:** Zhuo Yang, Yeyun chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20135
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/354a0c5aa0cbd47dcbbf13234c253d1300ed7b0266e8ead6da21580f2b96f942_w640_q70.webp
  - **Simple LLM Summary:** MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization

- **[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models**
  - **tags:** TBD
  - **authors:** Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20145
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp
  - **Simple LLM Summary:** Retrieval-augmented Prompt Learning for Pre-trained Foundation Models

- **[arXiv251224] M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation**
  - **tags:** TBD
  - **authors:** Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20136
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c211a1c8d3a17c264fe9655b35305f3ece6ef329a1cb2344fb1a18976f11017_w640_q70.webp
  - **Simple LLM Summary:** M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation

- **[arXiv251224] Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection**
  - **tags:** TBD
  - **authors:** Xingyou Yin, Ceyao Zhang, Min Hu, Kai Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20140
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4380a71bf6060d158bfda4324d258b0056d7685334fc2256c0df38315036c209_w640_q70.webp
  - **Simple LLM Summary:** Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection

- **[arXiv251224] Fun-Audio-Chat Technical Report**
  - **tags:** TBD
  - **authors:** Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20156
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp
  - **Simple LLM Summary:** Fun-Audio-Chat Technical Report

- **[arXiv251224] AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration**
  - **tags:** TBD
  - **authors:** Ruiqi Wang, Xinchen Wang, Cuiyun Gao, Chun Yong Chong, Xin Xia, Qing Liao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20159
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9b4276c369f7cc83453b7385f456424f32c6a43157de7895979a0b4e9cd33bf_w640_q70.webp
  - **Simple LLM Summary:** AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration

- **[arXiv251224] A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers**
  - **tags:** TBD
  - **authors:** Dhivya Dharshini Kannan, Anupam Trivedi, Dipti Srinivasan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20161
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1417550973096c816c854e8c26e0184adfd5d9410e4449d0498cc343fd80cc27_w640_q70.webp
  - **Simple LLM Summary:** A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers

- **[arXiv251224] Concept Generalization in Humans and Large Language Models: Insights from the Number Game**
  - **tags:** TBD
  - **authors:** Arghavan Bazigaran, Hansem Sohn
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20162
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f33aa8c0a9e961f65d06bf913a8cd4cb96d114bf3d4806d5932ebcb99dbac9d2_w640_q70.webp
  - **Simple LLM Summary:** Concept Generalization in Humans and Large Language Models: Insights from the Number Game

- **[arXiv251224] AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications**
  - **tags:** TBD
  - **authors:** Honglin Mu, Jinghao Liu, Kaiyang Wan, Rui Xing, Xiuying Chen, Timothy Baldwin, Wanxiang Che
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20164
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be09765889dd9516b45fd948d07346b8b6487f6dc02927a597c1cab7861bfb7_w640_q70.webp
  - **Simple LLM Summary:** AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications

- **[arXiv251224] Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography**
  - **tags:** TBD
  - **authors:** Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20168
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp
  - **Simple LLM Summary:** Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography

- **[arXiv251224] FaithLens: Detecting and Explaining Faithfulness Hallucination**
  - **tags:** TBD
  - **authors:** Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20182
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp
  - **Simple LLM Summary:** FaithLens: Detecting and Explaining Faithfulness Hallucination

- **[arXiv251224] Offline Safe Policy Optimization From Heterogeneous Feedback**
  - **tags:** TBD
  - **authors:** Ze Gong, Pradeep Varakantham, Akshat Kumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20173
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9834f9cb644c8389de473430a70e825274ad26459f3ce94c2018ac8228df6f9_w640_q70.webp
  - **Simple LLM Summary:** Offline Safe Policy Optimization From Heterogeneous Feedback

- **[arXiv251224] Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation**
  - **tags:** TBD
  - **authors:** Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20188
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp
  - **Simple LLM Summary:** Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation

- **[arXiv251224] Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings**
  - **tags:** TBD
  - **authors:** Marko Čechovič, Natália Komorníková, Dominik Macháček, Ondřej Bojar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20204
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d50c0d1f767aca0e832de0ef650261ca473b1bbf6f1ac37ad18132605e13bc77_w640_q70.webp
  - **Simple LLM Summary:** Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings

- **[arXiv251224] TongSIM: A General Platform for Simulating Intelligent Machines**
  - **tags:** TBD
  - **authors:** Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20206
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca507a85ff857b30b18d4ea2014b82001e6b5d0adac56afe981969173bc45325_w640_q70.webp
  - **Simple LLM Summary:** TongSIM: A General Platform for Simulating Intelligent Machines

- **[arXiv251224] MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents**
  - **tags:** TBD
  - **authors:** Xingbo Du, Loka Li, Duzhen Zhang, Le Song
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20237
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b81bde40292c52d66676e0cadc37d954f64fdf390b55d610cd63316ed43ef4_w640_q70.webp
  - **Simple LLM Summary:** MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents

- **[arXiv251224] Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds**
  - **tags:** TBD
  - **authors:** Tarik Houichime, Abdelghani Souhar, Younes El Amrani
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20245
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1001e3781e678219db00950fa667bbe46bbaa2f98cd7e5064d91ede9a2cbc6fe_w640_q70.webp
  - **Simple LLM Summary:** Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds

- **[arXiv251224] Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks**
  - **tags:** TBD
  - **authors:** Divya Vijay, Vignesh Ethiraj
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20275
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da6037bde0b2a44f38e40927d7e2b880cc97a5f8fde73345c4fad4c78baf4836_w640_q70.webp
  - **Simple LLM Summary:** Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks

- **[arXiv251224] Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation**
  - **tags:** TBD
  - **authors:** Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20278
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68f0ca1a3004807c69fa2a7198569167a93240c27f60163c26979dcccdf6072f_w640_q70.webp
  - **Simple LLM Summary:** Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation

- **[arXiv251224] ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge**
  - **tags:** TBD
  - **authors:** Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20276
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp
  - **Simple LLM Summary:** ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge

- **[arXiv251224] $\{D\}^\{3\}$\{ETOR\}: $\{D\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\{D\}$ebiasing for Weakly-Supervised Camouflaged Object $\{D\}$etection with Scribble Annotations**
  - **tags:** TBD
  - **authors:** Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20260
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp
  - **Simple LLM Summary:** $\{D\}^\{3\}$\{ETOR\}: $\{D\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\{D\}$ebiasing for Weakly-Supervised Camouflaged Object $\{D\}$etection with Scribble Annotations

- **[arXiv251224] UbiQVision: Quantifying Uncertainty in XAI for Image Recognition**
  - **tags:** TBD
  - **authors:** Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20288
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp
  - **Simple LLM Summary:** UbiQVision: Quantifying Uncertainty in XAI for Image Recognition

- **[arXiv251224] Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives**
  - **tags:** TBD
  - **authors:** Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20298
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp
  - **Simple LLM Summary:** Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives

- **[arXiv251224] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers**
  - **tags:** TBD
  - **authors:** Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20292
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp
  - **Simple LLM Summary:** SlideTailor: Personalized Presentation Slide Generation for Scientific Papers

- **[arXiv251224] TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation**
  - **tags:** TBD
  - **authors:** Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20296
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp
  - **Simple LLM Summary:** TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation

- **[arXiv251224] KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System**
  - **tags:** TBD
  - **authors:** Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20299
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp
  - **Simple LLM Summary:** KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System

- **[arXiv251224] TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20312
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp
  - **Simple LLM Summary:** TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning

- **[arXiv251224] Toward Explaining Large Language Models in Software Engineering Tasks**
  - **tags:** TBD
  - **authors:** Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20328
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp
  - **Simple LLM Summary:** Toward Explaining Large Language Models in Software Engineering Tasks

- **[arXiv251224] SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization**
  - **tags:** TBD
  - **authors:** Junren Li, Luhua Lai
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20333
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e8cac7516cbb65a566c19ba7dde5921369f69a0fe2fe62a1ee10bb383644f7_w640_q70.webp
  - **Simple LLM Summary:** SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization

- **[arXiv251224] A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice**
  - **tags:** TBD
  - **authors:** Yaowei Bai, Ruiheng Zhang, Yu Lei, Xuhua Duan, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, Qian Yuan, Lei Chen, Wenjuan Tang, Biqiang Zhu, Xinggang Wang, Tao Sun, Wei Zhou, Dacheng Tao, Yongchao Xu, Chuansheng Zheng, Huangxuan Zhao, Bo Du
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20344
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f502578578854e6ba573a8a7758a3229a45ec3bf4a1c2e637d1006bb31aa1c4_w640_q70.webp
  - **Simple LLM Summary:** A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice

- **[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning**
  - **tags:** TBD
  - **authors:** Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20363
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp
  - **Simple LLM Summary:** Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning

- **[arXiv251224] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation**
  - **tags:** TBD
  - **authors:** Nilesh Jain, Seyi Adeyinka, Leor Roseman, Aza Allsop
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20352
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2047f0800a6a91a372fd66013fa3526084396a7513879598fb459814e55b18c_w640_q70.webp
  - **Simple LLM Summary:** Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation

- **[arXiv251224] Identifying Appropriately-Sized Services with Deep Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Syeda Tasnim Fabiha, Saad Shafiq, Wesley Klewerton Guez Assunção, Nenad Medvidović
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20381
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/295a39edb17ec8824ad34080df5c2a960bf42d578ae6e9a2db55f3775d90e225_w640_q70.webp
  - **Simple LLM Summary:** Identifying Appropriately-Sized Services with Deep Reinforcement Learning

- **[arXiv251224] Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems**
  - **tags:** TBD
  - **authors:** YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20387
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp
  - **Simple LLM Summary:** Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems

- **[arXiv251224] AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition**
  - **tags:** TBD
  - **authors:** Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20407
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp
  - **Simple LLM Summary:** AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition

- **[arXiv251224] Simplifying Multi-Task Architectures Through Task-Specific Normalization**
  - **tags:** TBD
  - **authors:** Mihai Suteu, Ovidiu Serban
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20420
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp
  - **Simple LLM Summary:** Simplifying Multi-Task Architectures Through Task-Specific Normalization

- **[arXiv251224] Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit**
  - **tags:** TBD
  - **authors:** Adam Elaoumari
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20423
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b67cb70660ec29cf307ab6baf5a52c6e3cecc0888ba9656259ee05bcfef71b96_w640_q70.webp
  - **Simple LLM Summary:** Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit

- **[arXiv251224] DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning**
  - **tags:** TBD
  - **authors:** Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20409
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp
  - **Simple LLM Summary:** DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning

- **[arXiv251224] Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale**
  - **tags:** TBD
  - **authors:** Linfeng Zhang, Siheng Chen, Yuzhu Cai, Jingyi Chai, Junhan Chang, Kun Chen, Zhi X. Chen, Zhaohan Ding, Yuwen Du, Yuanpeng Gao, Yuan Gao, Jing Gao, Zhifeng Gao, Qiangqiang Gu, Yanhui Hong, Yuan Huang, Xi Fang, Xiaohong Ji, Guolin Ke, Zixing Lei, Xinyu Li, Yongge Li, Ruoxue Liao, Hang Lin, Xiaolu Lin, Yuxiang Liu, Xinzijian Liu, Zexi Liu, Jintan Lu, Tingjia Miao, Haohui Que, Weijie Sun, Yanfeng Wang, Bingyang Wu, Tianju Xue, Rui Ye, Jinzhe Zeng, Duo Zhang, Jiahui Zhang, Linfeng Zhang, Tianhan Zhang, Wenchang Zhang, Yuzhi Zhang, Zezhong Zhang, Hang Zheng, Hui Zhou, Tong Zhu, Xinyu Zhu, Qingguo Zhou, Weinan E
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20469
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efbb4cf73bd5e97a426fe07fa2444d9fce83c1cc337fc7a02676141bf805fbd9_w640_q70.webp
  - **Simple LLM Summary:** Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale

- **[arXiv251224] SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization**
  - **tags:** TBD
  - **authors:** Revanth Gangi Reddy, Ye Liu, Wenting Zhao, JaeHyeok Doo, Tarun Suresh, Daniel Lee, Caiming Xiong, Yingbo Zhou, Semih Yavuz, Shafiq Joty
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20482
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61a2f4887d3f7e1f76ef9da213d8cbb2fd86e68bd8a8206e3a2e53677aa7151e_w640_q70.webp
  - **Simple LLM Summary:** SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization

- **[arXiv251224] Benchmarking LLMs for Predictive Applications in the Intensive Care Units**
  - **tags:** TBD
  - **authors:** Chehak Malhotra, Mehak Gopal, Akshaya Devadiga, Pradeep Singh, Ridam Pal, Ritwik Kashyap, Tavpritesh Sethi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20520
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/274c8c98260b88efe758b67759790448f6c4f5abe21a32dafdc4d3a311f9524b_w640_q70.webp
  - **Simple LLM Summary:** Benchmarking LLMs for Predictive Applications in the Intensive Care Units

- **[arXiv251224] Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model**
  - **tags:** TBD
  - **authors:** Zhiyi Duan, Xiangren Wang, Hongyu Yuan, Qianli Xing
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20548
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb1b5cafe85ffb670c8629adfcf70c1f2a8ef006559d14cc0f3ad75463909ad8_w640_q70.webp
  - **Simple LLM Summary:** Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model

- **[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving**
  - **tags:** TBD
  - **authors:** Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20563
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp
  - **Simple LLM Summary:** LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving

- **[arXiv251224] Distilling to Hybrid Attention Models via KL-Guided Layer Selection**
  - **tags:** TBD
  - **authors:** Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda, Jiawei Zhou, Yoon Kim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20569
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36c08fad9c560eeacd24d61bbc8fc4ace2f57a4dda4d1eaeb59a63b10f01d2e_w640_q70.webp
  - **Simple LLM Summary:** Distilling to Hybrid Attention Models via KL-Guided Layer Selection

- **[arXiv251224] Performative Policy Gradient: Optimality in Performative Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20576
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp
  - **Simple LLM Summary:** Performative Policy Gradient: Optimality in Performative Reinforcement Learning

- **[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs**
  - **tags:** TBD
  - **authors:** Rui Pan, Zhuofu Chen, Ravi Netravali
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20573
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp
  - **Simple LLM Summary:** Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs

- **[arXiv251224] Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent**
  - **tags:** TBD
  - **authors:** Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20586
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp
  - **Simple LLM Summary:** Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent

- **[arXiv251224] Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs**
  - **tags:** TBD
  - **authors:** Dhruv Anand, Ehsan Shareghi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20595
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp
  - **Simple LLM Summary:** Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs

- **[arXiv251224] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning**
  - **tags:** TBD
  - **authors:** Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp
  - **Simple LLM Summary:** Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

- **[arXiv251224] Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information**
  - **tags:** TBD
  - **authors:** İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20589
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp
  - **Simple LLM Summary:** Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information

- **[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos**
  - **tags:** TBD
  - **authors:** Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20618
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp
  - **Simple LLM Summary:** LongVideoAgent: Multi-Agent Reasoning with Long Videos

- **[arXiv251224] Generative AI for Analysts**
  - **tags:** TBD
  - **authors:** Jian Xue, Qian Zhang, Wu Zhu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19705
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c446bca4991916c167230d9d5b7a57cb785d6214690dca03a9aacef2ebddb67_w640_q70.webp
  - **Simple LLM Summary:** Generative AI for Analysts

- **[arXiv251224] QMBench: A Research Level Benchmark for Quantum Materials Research**
  - **tags:** TBD
  - **authors:** Yanzhen Wang, Yiyang Jiang, Diana Golovanova, Kamal Das, Hyeonhu Bae, Yufei Zhao, Huu-Thong Le, Abhinava Chatterjee, Yunzhe Liu, Chao-Xing Liu, Felipe H. da Jornada, Binghai Yan, Xiao-Liang Qi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19753
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a5901ce24dd3558937f8fb69f70cd14d0da814e05be0cd205e3d5fb0d138661_w640_q70.webp
  - **Simple LLM Summary:** QMBench: A Research Level Benchmark for Quantum Materials Research

- **[arXiv251224] Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation**
  - **tags:** TBD
  - **authors:** Alexis Pomares Pastor, Ines Ribeiro Violante, Gregory Scott
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20319
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcb7898ab29ae44cef389712101b2dbc98a334b5761ab3b2e5825364d8f3f316_w640_q70.webp
  - **Simple LLM Summary:** Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation

- **[arXiv251224] Regression of Functions by Quantum Neural Networks Circuits**
  - **tags:** TBD
  - **authors:** Fernando M. de Paula Neto, Lucas dos Reis Silva, Paulo S. G. de Mattos Neto, Felipe F. Fanchini
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19978
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad43eefe70b19a60b4cb6a98a6af78a755ad48beb67110a54813ba3e49fc1d5e_w640_q70.webp
  - **Simple LLM Summary:** Regression of Functions by Quantum Neural Networks Circuits

- **[arXiv251224] Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI**
  - **tags:** TBD
  - **authors:** Muhammad Usman, Azka Rehman, Muhammad Mutti Ur Rehman, Abd Ur Rehman, Muhammad Umar Farooq
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20436
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp
  - **Simple LLM Summary:** Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI

## 2025-12-25

- **[arXiv251225] Parameter-Efficient Neural CDEs via Implicit Function Jacobians**
  - **tags:** [ai], [time series analysis], [Neural Controlled Differential Equations, parameter efficiency, implicit function Jacobians, continuous RNN]
  - **authors:** Ilya Kuleshov, Alexey Zaytsev
  - **institution:** Applied AI Institute
  - **link:** https://arxiv.org/pdf/2512.20625
  - **contributions:** 1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a "Continuous RNN," aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a "Continuous RNN" and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Parameter-Efficient Neural CDEs via Implicit Function Jacobians] --> B[核心问题/Problem: NCDEs require many parameters]
    A --> C[主要方法/Method: Parameter-efficient formulation via implicit Jacobians, "Continuous RNN" analogy]
    A --> D[关键结果/Results: Achieves similar performance with far fewer parameters]
    ```

- **[arXiv251225] BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization**
  - **tags:** [mlsys], [on-device ai], [1-bit quantization, Deep Q-Network (DQN), edge inference, multi-objective RL, model compression]
  - **authors:** Ravi Gupta, Shabista Haider
  - **institution:** AMD, Oracle
  - **link:** https://arxiv.org/pdf/2512.20623
  - **contributions:** 1. A novel architecture integrating 1-bit quantized LLMs with DQN for multi-objective optimization of smart home lighting. 2. Voice command integration via Google Home and IFTTT webhooks for natural user interaction. 3. Comprehensive evaluation demonstrating the feasibility of intelligent adaptive control on sub-$50 hardware, with significant energy and latency improvements.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc8c0af6233346bd047dc958868370625b7e65b546832d11939a371662fc4980_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes BitRL-Light, a framework that combines a 1-bit quantized LLM with Deep Q-Network reinforcement learning for real-time smart home lighting control on edge devices. The system optimizes for energy efficiency and user comfort, achieving substantial energy savings and low latency on a Raspberry Pi. The work demonstrates that adaptive AI control is feasible on resource-constrained hardware without cloud dependency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[BitRL-Light] --> B[核心问题/Problem: Smart home lighting lacks adaptive intelligence for energy and comfort]
    A --> C[主要方法/Method: 1-bit LLM + DQN on edge devices]
    A --> D[关键结果/Results: 32% energy savings, <200ms latency, 95% user satisfaction]
    ```

- **[arXiv251225] Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)**
  - **tags:** [other], [multidisciplinary conference], [knowledge engineering, creativity support systems, human-computer interaction, artificial intelligence, peer-reviewed proceedings]
  - **authors:** Edited by Tessai Hayama, Takayuki Ito, Takahiro Uchiya, Motoki Miura, Takahiro Kawaji, Takaya Yuizono, Atsuo Yoshitaka, Tokuro Matsuo, Shun Okuhara, Jawad Haqbeen, Sofia Sahab, Wen Gu, Shiyao Ding
  - **institution:** IEICE (The Institute of Electronics, Information and Communication Engineers)
  - **link:** https://arxiv.org/pdf/2512.20628
  - **contributions:** 1. Provides a multidisciplinary forum for researchers in AI, knowledge engineering, HCI, and creativity support systems. 2. Presents peer-reviewed proceedings following a double-blind review process. 3. Facilitates extended publication of selected papers in IEICE Transactions on Information and Systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8c9d50976ba52ad8c3511bc5a2800a053b1b858244a20cfcda006c283fdd5c_w640_q70.webp
  - **Simple LLM Summary:** This is the proceedings volume for the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025). It compiles peer-reviewed papers from a multidisciplinary forum, with selected works recommended for further publication in a journal.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[KICSS 2025 Proceedings] --> B(核心问题/Problem: 提供多学科研究论坛/Provide Multidisciplinary Research Forum)
        A --> C(主要方法/Method: 双盲同行评审会议/Double-Blind Peer-Reviewed Conference)
        A --> D(关键结果/Results: 出版会议论文集并推荐期刊发表/Publish Proceedings & Recommend Journal Publication)
    ```

- **[arXiv251225] Cooperation Through Indirect Reciprocity in Child-Robot Interactions**
  - **tags:** [ai], [human-robot interaction], [indirect reciprocity, multi-armed bandit, coordination dilemmas]
  - **authors:** Isabel Neto, Alexandre S. Pires, Filipa Correia, Fernando P. Santos
  - **institution:** Universidade de Lisboa, University of Amsterdam, Instituto Superior Técnico
  - **link:** https://arxiv.org/pdf/2512.20621
  - **contributions:** 1. Demonstrated that the mechanism of indirect reciprocity can be successfully transposed from human-human interactions to child-robot interactions. 2. Showed that children's behavioral strategies provide a sufficient signal for multi-armed bandit algorithms to learn cooperative actions. 3. Analyzed how differences in learning algorithms impact the dynamics and outcomes of human-AI cooperation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a49edd2299eeb19bce07b564c8061c35b829f55eb48557d15dbeabbbd028e0_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether indirect reciprocity, a mechanism for sustaining cooperation, applies to child-robot interactions. The authors combine laboratory experiments with theoretical modeling, using multi-armed bandit algorithms for the robots. They find that indirect reciprocity does extend to these interactions and that robots can learn to cooperate based on children's strategies, though this learning is highly dependent on the human strategies revealed.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Cooperation Through Indirect Reciprocity in Child-Robot Interactions] --> B(核心问题/Problem: Can indirect reciprocity enable cooperation between children and robots?)
    A --> C(主要方法/Method: Laboratory experiments and theoretical modeling with multi-armed bandit algorithms)
    A --> D(关键结果/Results: IR extends to child-robot groups; robots can learn cooperation from children's strategies)
    ```

- **[arXiv251225] Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things**
  - **tags:** [mlsys], [federated learning], [Intent-Based Networking, Industrial Internet of Things, Asynchronous Federated Learning, Strategy Similarity, Multimodal Intent Alignment]
  - **authors:** Shaowen Qin, Jianfeng Zeng, Haodong Guo, Xiaohuan Li, Jiawen Kang, Qian Chen, Dusit Niyato
  - **institution:** Guilin University of Electronic Technology, Guangdong University of Technology, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.20627
  - **contributions:** 1. Proposes FEIBN, a federated learning framework for distributed policy verification in IIoT, enhancing privacy by avoiding raw data exposure. 2. Introduces SSAFL, a strategy similarity-aware mechanism for efficient node selection and asynchronous model updates to reduce communication overhead. 3. Leverages LLMs to align multimodal user intents into structured strategy tuples for automated policy generation and verification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17f53425c1430ea3ad8516d92a010c39c2936d478f64e21bad2b552f30214b23_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes FEIBN, a federated evaluation framework for Intent-Based Networking in IIoT. It uses LLMs to translate intents and a strategy similarity-aware asynchronous federated learning mechanism (SSAFL) for efficient, private policy verification. Experiments show SSAFL improves accuracy, convergence speed, and reduces cost by 27.8% compared to a baseline.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness] --> B[核心问题/Problem: Frequent strategy deployment & centralized verification are impractical in IIoT]
    A --> C[主要方法/Method: FEIBN framework with LLM-based intent alignment & SSAFL mechanism]
    A --> D[关键结果/Results: Improved accuracy, faster convergence, 27.8% cost reduction]
    ```

- **[arXiv251225] Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment**
  - **tags:** [ai], [multi-agent reinforcement learning], [Quantum-Inspired MARL, Variational Quantum Circuits (VQC), Centralized Training Decentralized Execution (CTDE), UAV-assisted 6G, Exploration-Exploitation Tradeoff]
  - **authors:** Mazyar Taghavi, Javad Vahidi
  - **institution:** Iran University of Science and Technology, Intelligent Knowledge City
  - **link:** https://arxiv.org/pdf/2512.20624
  - **contributions:** 1. Proposes a novel quantum-inspired framework integrating variational quantum circuits (VQCs) and QAOA with classical MARL to optimize the exploration-exploitation tradeoff. 2. Incorporates complementary probabilistic modeling (Bayesian inference, Gaussian processes) to capture latent environmental dynamics in a cooperative UAV scenario. 3. Demonstrates through experiments that the framework improves sample efficiency, convergence speed, and coverage performance compared to classical baselines like PPO and DDPG.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a quantum-inspired multi-agent reinforcement learning framework to optimize the exploration-exploitation balance for UAV-assisted 6G network deployment. The method integrates variational quantum circuits and probabilistic modeling within a centralized training, decentralized execution paradigm. The results show it achieves superior performance in coverage and convergence compared to classical MARL methods.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Quantum-Inspired MARL for UAV 6G Deployment] --> B(核心问题/Problem: Exploration-Exploitation Tradeoff in MARL for UAV Coverage)
    A --> C(主要方法/Method: VQC/QAOA + Probabilistic Models + CTDE)
    A --> D(关键结果/Results: Improved Efficiency, Convergence, and Coverage)
    ```

- **[arXiv251225] Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning**
  - **tags:** [mlsys], [agent system], [multi-agent language systems, latent strategy evolution, reinforcement feedback, external latent vectors, dual-loop architecture]
  - **authors:** Wenlong Tang
  - **institution:** Independent Researcher (No institutional affiliation inferred from provided content)
  - **link:** https://arxiv.org/pdf/2512.20629
  - **code:** https://github.com/wltang-dev/Latent-Strategy-RL-Agent
  - **contributions:** 1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model's parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning] --> B[核心问题/Problem: Static semantic representations in LLMs cannot evolve with experience.]
    A --> C[主要方法/Method: Dual-loop architecture (Behavior Loop & Language Loop) updates external latent vectors via reinforcement and reflection.]
    A --> D[关键结果/Results: Agents develop stable, disentangled strategies; latent spaces show convergence and emergent adaptation.]
    ```

- **[arXiv251225] MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [multimodal knowledge graph, cross-modal reasoning, visual document understanding, retrieval-augmented generation, entity-centric structure]
  - **authors:** Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen
  - **institution:** National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University
  - **link:** https://arxiv.org/pdf/2512.20626
  - **contributions:** 1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[MegaRAG: 多模态知识图谱检索增强生成 / MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有RAG方法在长文档、多模态内容上理解不足 / Existing RAG struggles with long-form, multimodal document understanding]
        C --> C1[构建融合视觉线索的多模态知识图谱 / Construct multimodal KG incorporating visual cues]
        C --> C2[在多模态检索与生成中利用图谱 / Utilize KG in multimodal retrieval & generation]
        D --> D1[在全局与细粒度QA任务上超越现有方法 / Outperforms existing methods on global & fine-grained QA]
    ```

- **[arXiv251225] MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data**
  - **tags:** [ai], [model evaluation & reliability], [reliability assessment, uncertainty quantification, strategic sampling, foundation models, probe selection]
  - **authors:** Aayam Bansal, Ishaan Gangwani
  - **institution:** IEEE (implied from email domain)
  - **link:** https://arxiv.org/pdf/2512.20630
  - **contributions:** 1. A novel strategic probe selection methodology that maximizes reliability coverage across five key dimensions with information-theoretic justification. 2. An advanced uncertainty-aware assessment framework with adaptive weighting and sophisticated consistency metrics. 3. Comprehensive empirical and cross-domain validation demonstrating significant improvements over random sampling with high statistical rigor.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43b9cf5db61863c7d69dd0ba1dd95c34144ed7e7c901f129f236bfb156cb89dc_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MicroProbe, a method for efficiently assessing the reliability of foundation models using only 100 strategically selected probe examples. It combines prompt diversity, uncertainty quantification, and adaptive weighting to detect failure modes. The approach is shown to achieve higher reliability scores with 90% lower cost and 95% coverage compared to traditional methods requiring thousands of examples.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data] --> B[核心问题/Problem: Traditional reliability assessment is computationally expensive, requiring thousands of examples.]
    A --> C[主要方法/Method: Strategic probe selection across five reliability dimensions with uncertainty quantification and adaptive weighting.]
    A --> D[关键结果/Results: 23.5% higher reliability scores, 90% cost reduction, 95% coverage maintained, validated across domains.]
    ```

- **[arXiv251225] Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams**
  - **tags:** [nlp], [sentiment analysis], [temporal drift, zero-training detection, transformer models, social media streams, model instability]
  - **authors:** Aayam Bansal, Ishaan Gangwani
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2512.20631
  - **contributions:**  1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --> B[核心问题/Problem: Transformer模型在动态事件期间的行为不稳定/Transformer model instability during dynamic events]
    A --> C[主要方法/Method: 零训练检测框架与四个新指标/Zero-training detection framework with four novel metrics]
    A --> D[关键结果/Results: 在真实数据上验证，准确率下降达23.4%，检测能力强/Validated on authentic data, 23.4% accuracy drop, strong detection capability]
    ```

- **[arXiv251225] Erkang-Diagnosis-1.1 Technical Report**
  - **tags:** [nlp], [retrieval-augmented generation], [Qwen-3, enhanced pre-training, retrieval-augmented generation, medical knowledge base, healthcare assistant]
  - **authors:** Jianbing Ma, Ao Feng, Zhenjie Gao, Xinyu Song, Li Su, Bin Chen, Wei Wang, Jiamin Wu
  - **institution:** Chengdu Lingshu Health Technology Corp. Ltd.
  - **link:** https://arxiv.org/pdf/2512.20632
  - **contributions:** 1. Development of a specialized AI healthcare assistant (Erkang-Diagnosis-1.1) based on the Alibaba Qwen-3 model, 2. Integration of approximately 500GB of high-quality structured medical knowledge using a hybrid approach of enhanced pre-training and retrieval-augmented generation (RAG), 3. Demonstration of superior performance over GPT-4 in comprehensive medical exams through efficient 3-5 round interactions
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed164818e4da399ea74ba4b02c96fdf625f0f5b1c9130aaf2994523190ecf4eb_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Erkang-Diagnosis-1.1, an AI healthcare assistant built on the Qwen-3 model. It integrates a large medical knowledge base using enhanced pre-training and retrieval-augmented generation to provide diagnostic suggestions. The model reportedly outperforms GPT-4 on medical exams.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Erkang-Diagnosis-1.1 Technical Report] --> B[核心问题/Problem: Need for professional, reliable AI health advisor]
    A --> C[主要方法/Method: Qwen-3 + Enhanced Pre-training + RAG + 500GB Medical Knowledge]
    A --> D[关键结果/Results: Outperforms GPT-4 in medical exams, provides diagnostic suggestions]
    ```

- **[arXiv251225] Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models**
  - **tags:** [ai], [clinical prediction], [large language models, semantic feature engineering, multi-modal data integration, goal-oriented knowledge curator, treatment outcome prediction]
  - **authors:** MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong
  - **institution:** Mayo Clinic
  - **link:** https://arxiv.org/pdf/2512.20633
  - **contributions:** 1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Enhancing Lung Cancer Treatment Outcome Prediction<br>增强肺癌治疗结果预测] --> B(核心问题/Problem: Sparse, heterogeneous clinical data<br>稀疏、异构的临床数据)
    A --> C(主要方法/Method: LLMs as Goal-oriented Knowledge Curators<br>LLMs作为目标导向知识策展器)
    A --> D(关键结果/Results: Superior AUROC 0.803, outperforms baselines<br>优异的AUROC 0.803，超越基线)
    ```

- **[arXiv251225] Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning**
  - **tags:** [mlsys], [llm training], [catastrophic forgetting, spurious forgetting, shallow alignment, deep alignment, task alignment depth]
  - **authors:** Weiwei Wang
  - **institution:** Shenzhen Sunline Tech Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.20634
  - **contributions:** 1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to "spurious forgetting" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>虚假遗忘的实时检测与定量分析] --> B[核心问题/Problem: Catastrophic forgetting from shallow task alignment<br/>由浅层任务对齐导致的灾难性遗忘]
    A --> C[主要方法/Method: Quantitative metrics & real-time detection for alignment depth<br/>对齐深度的量化指标与实时检测]
    A --> D[关键结果/Results: High identification accuracy & improved robustness<br/>高识别准确率与提升的鲁棒性]
    ```

- **[arXiv251225] Data-Free Pruning of Self-Attention Layers in LLMs**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [attention pruning, data-free pruning, Gate-Norm, inference acceleration, attention suppression hypothesis]
  - **authors:** Dhananjay Saikumar, Blesson Varghese
  - **institution:** University of St Andrews
  - **link:** https://arxiv.org/pdf/2512.20636
  - **contributions:** 1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Data-Free Pruning of Self-Attention Layers in LLMs] --> B[核心问题/Problem: LLM推理成本高，注意力层是瓶颈/High LLM inference cost, attention layers are bottleneck]
    A --> C[主要方法/Method: 提出Gate-Norm，基于权重无数据剪枝/Propose Gate-Norm, weight-only data-free pruning]
    A --> D[关键结果/Results: 推理速度提升1.30倍，精度损失<2%，速度快1000倍/1.30x faster inference, <2% accuracy drop, 1000x faster scoring]
    ```

- **[arXiv251225] Uncovering Competency Gaps in Large Language Models and Their Benchmarks**
  - **tags:** [nlp], [llm evaluation], [sparse autoencoders, benchmark gaps, model gaps, concept activations, competency gaps]
  - **authors:** Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan
  - **institution:** Stanford University, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.20638
  - **code:** competency-gaps.github.io
  - **contributions:** 1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Uncovering Competency Gaps<br/>揭示能力差距] --> B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>问题：聚合指标掩盖模型/基准差距]
        A --> C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>方法：使用稀疏自编码器进行概念级分解]
        A --> D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>结果：发现非谄媚、安全方面的差距；基准过度代表服从性]
    ```

- **[arXiv251225] Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations**
  - **tags:** [ai], [scientific machine learning], [Neural Ordinary Differential Equations, Universal Differential Equations, forecasting breakdown point, n-body problem, Julia]
  - **authors:** Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat
  - **institution:** Vizuara AI Labs
  - **link:** https://arxiv.org/pdf/2512.20643
  - **contributions:** 1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the "forecasting breakdown point" to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp
  - **Simple LLM Summary:** This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a "forecasting breakdown point" to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Forecasting N-Body Dynamics<br/>N体动力学预测] --> B[核心问题/Problem<br/>传统黑盒模型忽略物理定律<br/>Traditional black-box models ignore physics]
    A --> C[主要方法/Method<br/>使用科学机器学习框架<br/>Use Scientific ML frameworks (NODEs, UDEs)]
    A --> D[关键结果/Results<br/>UDE数据效率更高<br/>UDE is more data-efficient]
    ```

- **[arXiv251225] Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning**
  - **tags:** [nlp], [reasoning evaluation], [chain-of-thought, reasoning interchangeability, process reward model, token-level log-probability thresholds]
  - **authors:** Leo Lu, Jonathan Zhang, Sean Chua, Spencer Kim, Kevin Zhu, Sean O'Brien, Vasu Sharma
  - **institution:** Pennsylvania State University, Binghamton University, University of Toronto, UC Berkeley, Algoverse
  - **link:** https://arxiv.org/pdf/2512.20647
  - **contributions:** 1. Proposes a framework to evaluate the interchangeability of reasoning chains across LLMs, assessing if partial reasoning from one model can be reliably continued by another. 2. Introduces a method using token-level log-probability thresholds to truncate reasoning at different stages and a Process Reward Model (PRM) for evaluation. 3. Demonstrates that hybrid reasoning chains often preserve or even improve final accuracy and logical structure, suggesting interchangeability as an emerging property for modular reasoning in collaborative AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2ec01f1b7eff609789d150706b31289a628fdb2bdaa6ac8867d9e30a0ea0c1_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether partially completed reasoning chains from one large language model can be reliably continued by another model, using token-level log-probability thresholds to truncate reasoning and a Process Reward Model for evaluation. The study finds that hybrid reasoning chains often maintain or enhance accuracy and coherence, indicating that reasoning interchangeability is a viable property for collaborative AI systems.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning] --> B[核心问题/Problem: 不同LLM间的推理链是否可互换?/Interchangeability of reasoning across LLMs?]
    A --> C[主要方法/Method: 使用log-probability阈值截断推理链,并用PRM评估/Use log-probability thresholds to truncate reasoning, evaluate with PRM]
    A --> D[关键结果/Results: 混合推理链保持或提升准确性与逻辑结构/Hybrid reasoning chains preserve or improve accuracy & logical structure]
    ```

- **[arXiv251225] Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA**
  - **tags:** [mlsys], [llm inference], [Mixture of Attention Schemes, KV Cache, Dynamic Routing, Conditional Computation, Attention Mechanism]
  - **authors:** Esmail Gumaan
  - **institution:** Independent Researcher (Inferred from personal email domain)
  - **link:** https://arxiv.org/pdf/2512.20650
  - **code:** https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS
  - **contributions:** 1. Proposes Mixture of Attention Schemes (MoAS), a novel architecture that dynamically routes tokens between MHA, GQA, and MQA attention schemes. 2. Demonstrates that dynamic routing outperforms a static averaging of attention schemes, validating the learned routing approach. 3. Shows the method achieves performance competitive with the high-quality MHA baseline while offering potential for conditional compute and memory efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151d0589c37dc455317c2c4d1e613c5f722207da0abcb0abaf82373ef04ee19d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off between model quality and inference efficiency in Transformer attention mechanisms. It proposes MoAS, which uses a learned router to dynamically select between MHA, GQA, and MQA for each token. Experiments show dynamic routing outperforms static mixtures and matches MHA performance, offering a path to efficient conditional computation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MoAS: Mixture of Attention Schemes] --> B[核心问题/Problem<br>Attention机制的质量与效率权衡<br>Trade-off between quality and efficiency]
    A --> C[主要方法/Method<br>动态路由选择注意力方案<br>Dynamic routing between MHA, GQA, MQA]
    A --> D[关键结果/Results<br>动态路由优于静态混合，性能媲美MHA<br>Dynamic routing outperforms static mixture, competitive with MHA]
    ```

- **[arXiv251225] AIAuditTrack: A Framework for AI Security system**
  - **tags:** [sec], [AI governance and auditing], [blockchain, decentralized identity (DID), verifiable credentials (VC), risk diffusion algorithm, interaction graph]
  - **authors:** Zixun Luo, Yuhang Fan, Yufei Li, Youzhi Zhang, Hengyu Lin, Ziqi Wang
  - **institution:** Huazhong University of Science and Technology, Lingnan University, Centre for Artificial Intelligence and Robotics (CAIR) Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, Tsinghua University, Fujian Jiangxia University
  - **link:** https://arxiv.org/pdf/2512.20649
  - **contributions:** 1. Proposes a blockchain-based framework (AiAuditTrack) for recording and governing AI usage traffic to enable auditing and risk traceability. 2. Introduces an identity management mechanism using Decentralized Identity (DID) and Verifiable Credentials (VC) to establish trusted and identifiable AI entities. 3. Designs a risk diffusion algorithm on a dynamic interaction graph model to trace the source of risky behaviors and propagate warnings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b6835bdebf655288e51122fb875aaa032b286789052e6467724b2eb1e0af84_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the security and accountability challenges in AI-driven applications by proposing AiAuditTrack, a blockchain framework that uses decentralized identity and verifiable credentials to record AI interaction data and enable auditing. It models AI entities as nodes in a graph and introduces a risk diffusion algorithm for tracing risky behavior origins. The framework's feasibility is demonstrated through blockchain performance metrics (TPS) under large-scale recording scenarios.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AiAuditTrack: AI安全系统框架<br>AiAuditTrack: AI Security System Framework] --> B[核心问题/Problem: AI交互数据激增导致安全与责任归属挑战<br>Explosive AI interaction data raises security & accountability issues]
    A --> C[主要方法/Method: 基于区块链的框架，使用DID/VC进行身份管理，设计风险扩散算法<br>Blockchain-based framework with DID/VC for identity & risk diffusion algorithm]
    A --> D[关键结果/Results: 实现可验证的AI审计与风险溯源，系统在大规模记录下稳定<br>Enables verifiable AI auditing & risk tracing, system stable at scale]
    ```

- **[arXiv251225] AI-Driven Decision-Making System for Hiring Process**
  - **tags:** [mlsys], [agent system], [multi-agent system, LLM orchestration, human-in-the-loop, explainable scoring, public-data verification]
  - **authors:** Vira Filatova, Andrii Zelenchuk, Dmytro Filatov
  - **institution:** Covijn Ltd., Aimech Technologies Corp.
  - **link:** https://arxiv.org/pdf/2512.20652
  - **contributions:** 1. A modular multi-agent AI hiring assistant that integrates heterogeneous inputs (documents, video, public data) into a structured profile. 2. An LLM-orchestrated pipeline with strict constraints to reduce output variability and generate traceable, component-level rationales for scoring. 3. A configurable candidate ranking system based on aggregated technical fit, culture fit, and normalized risk penalties, evaluated with a proposed efficiency metric (expected time per qualified candidate).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af0225e1b8b3f96dbb1b944ce2ab06438c60ac8bcf7ef93a1f210771fa73aae4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the bottleneck of early-stage candidate validation in hiring by proposing an AI-driven, modular multi-agent system. The system integrates and processes diverse candidate inputs through an LLM-orchestrated pipeline to produce explainable scores and rankings. Evaluation on real applicants shows the system significantly improves screening efficiency, reducing the expected time per qualified candidate compared to human recruiters, while keeping a human as the final decision authority.
  - **Mindmap:**

    ```mermaid
    graph LR
        Root["AI-Driven Hiring System<br>AI驱动的招聘系统"] --> Problem["核心问题/Problem<br>Heterogeneous inputs & screening bottleneck<br>输入异构与筛选瓶颈"]
        Root --> Method["主要方法/Method<br>Multi-agent LLM pipeline with constraints<br>带约束的多智能体LLM流程"]
        Root --> Results["关键结果/Results<br>Higher efficiency & lower cost<br>更高效率与更低成本"]
    ```

- **[arXiv251225] Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence**
  - **tags:** [mlsys], [agent system], [memory architecture, long-term conversation, hallucination reduction, multimodal perception, cognitive integration]
  - **authors:** Deliang Wen, Ke Sun
  - **institution:** Not specified in provided content
  - **link:** https://arxiv.org/pdf/2512.20651
  - **contributions:** 1. Proposes the Memory Bear system, a human-like memory architecture for LLMs grounded in cognitive science principles. 2. Achieves a full-chain reconstruction of LLM memory mechanisms by integrating multimodal perception, dynamic memory maintenance, and adaptive cognitive services. 3. Demonstrates significant performance improvements in knowledge fidelity, retrieval efficiency, and hallucination reduction across multiple domains compared to existing solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c13cde65ef804fade26e06361a8d03517ad2394b3e21295ae77d2b5b462c29_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inherent memory limitations of LLMs, such as restricted context and hallucination, by proposing the Memory Bear system. Memory Bear constructs a cognitive science-inspired memory architecture to enhance long-term dialogue and reasoning. Experimental results show it outperforms existing methods in accuracy and efficiency, marking a step from memory to cognition in AI.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Memory Bear AI<br>论文标题/Paper Title] --> B[LLM Memory Limitations<br>核心问题/Problem]
    A --> C[Human-like Memory Architecture<br>主要方法/Method]
    A --> D[Performance Breakthrough<br>关键结果/Results]
    B --> B1[Restricted Context & Forgetting<br>受限上下文与遗忘]
    B --> B2[Hallucination & Redundancy<br>幻觉与冗余]
    C --> C1[Multimodal Perception<br>多模态感知]
    C --> C2[Dynamic Memory Maintenance<br>动态记忆维护]
    C --> C3[Adaptive Cognitive Services<br>自适应认知服务]
    D --> D1[Higher Accuracy & Efficiency<br>更高准确率与效率]
    D --> D2[Reduced Hallucination<br>降低幻觉率]
    D --> D3[Improved Reasoning<br>增强推理能力]
    ```

- **[arXiv251225] MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing**
  - **tags:** [mlsys], [others], [mask optimization, optical proximity correction, inverse lithography technique, deep learning, benchmark dataset]
  - **authors:** Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam
  - **institution:** University at Buffalo, IBM Research
  - **link:** https://arxiv.org/pdf/2512.20655
  - **contributions:** 1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp
  - **Simple LLM Summary:** The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];
    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];
    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];
    ```

- **[arXiv251225] Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering**
  - **tags:** [mlsys], [agent system], [dual-state architecture, atomic action pairs, guard functions, neuro-symbolic systems, code generation]
  - **authors:** Matthew Thompson
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.20660
  - **contributions:** 1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Managing the Stochastic<br>管理随机性] --> B[Problem: LLM-based agents prone to stochastic failures<br>问题: 基于LLM的智能体易受随机性故障影响]
    A --> C[Method: Dual-State Architecture, Atomic Action Pairs, Guard Functions<br>方法: 双态架构, 原子动作对, 守卫函数]
    A --> D[Results: Improved success rates, architectural constraints can substitute for scale<br>结果: 成功率提升, 架构约束可替代模型规模]
    ```

- **[arXiv251225] From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers**
  - **tags:** [nlp], [sentiment analysis], [adversarial training, attention mechanism, policy gradient, transformer, model interpretability]
  - **authors:** Yawei Liu
  - **institution:** Chinese Academy of Sciences, Computer Network Information Center
  - **link:** https://arxiv.org/pdf/2512.20661
  - **contributions:** 1. Proposes an Adversarial Feedback for Attention (AFA) training mechanism to automatically redistribute attention weights without manual supervision. 2. Introduces a dynamic masking strategy and a discriminator in an adversarial framework to identify and correct suboptimal attention. 3. Employs a policy gradient approach to efficiently optimize attention distributions, leading to improved performance on sentiment analysis tasks and a 12.6% gain when applied to large language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd81ed17263c1d20f831b211a88f9bc1399e87818af70003e7ba1196a7ddddf_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that Transformer models for sentiment analysis often misallocate attention to common words, missing important but less frequent terms. To solve this, it proposes an Adversarial Feedback for Attention (AFA) mechanism using dynamic masking and policy gradient optimization to refine attention distributions automatically. Experiments show the method achieves state-of-the-art results and significantly boosts performance in large language models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[From Fake Focus to Real Precision<br>从虚假聚焦到真实精度] --> B[核心问题/Problem<br>Transformer注意力分配不当<br>Transformer Misallocates Attention]
    A --> C[主要方法/Method<br>对抗性注意力反馈(AFA)<br>Adversarial Feedback for Attention]
    A --> D[关键结果/Results<br>SOTA性能 & LLM提升12.6%<br>SOTA Performance & 12.6% LLM Gain]
    B --> C
    C --> D
    ```

- **[arXiv251225] Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models**
  - **tags:** [nlp], [llm evaluation], [laziness, decoding suboptimality, context degradation, instruction-following, long-context]
  - **authors:** Yiqing Ma, Jung-Hua Liu
  - **institution:** Universiti Malaya, National Chung Cheng University
  - **link:** https://arxiv.org/pdf/2512.20662
  - **contributions:** 1. Quantified the "laziness" artifact in LLMs, showing widespread failure to fully comply with complex multi-part instructions. 2. Provided empirical evidence challenging the prevalence of "decoding suboptimality" in a simple reasoning task, suggesting greedy decoding may align with high-confidence solutions. 3. Demonstrated surprising robustness against "context degradation" in long, chaotic conversations, indicating LLMs may internally mitigate context forgetting in retrieval scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c503ae7a717782d28701da19cc7269ead2f9a34b5478457e18e791787fc94dea_w640_q70.webp
  - **Simple LLM Summary:** This paper quantifies three behavioral artifacts in Large Language Models (LLMs) through controlled experiments. The results show that while LLMs are often "lazy" in following complex instructions, they show limited decoding suboptimality and surprising robustness against context degradation in long conversations. The findings suggest instruction compliance remains a challenge, but some hypothesized failure modes like context forgetting may be less severe in straightforward scenarios.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models] --> B(核心问题/Problem: LLM行为缺陷/LLM Behavioral Artifacts)
        A --> C(主要方法/Method: 三个受控实验/Three Controlled Experiments)
        A --> D(关键结果/Results: 懒惰普遍/Laziness Widespread, 解码次优有限/Limited Decoding Suboptimality, 上下文退化稳健/Robust Context Degradation)
    ```

- **[arXiv251225] Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction**
  - **tags:** [nlp], [reasoning verification], [structural constraint satisfaction, neuro-symbolic verification, hallucination detection, constraint satisfaction problem, system-2 gate]
  - **authors:** Shinobu Miya
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.20664
  - **code:** https://github.com/ShinobuMiya/Eidoku
  - **contributions:** 1. Reformulates LLM reasoning verification as a Constraint Satisfaction Problem (CSP) independent of generation likelihood, focusing on structural feasibility instead of statistical plausibility. 2. Introduces a lightweight System-2 gate, Eidoku, that uses a context-calibrated cost threshold derived from intrinsic statistics to reject candidates based on structural violation cost. 3. Demonstrates the ability to deterministically reject "smooth falsehoods"—high-probability but structurally inconsistent statements—which probability-based verifiers cannot detect.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62978c31bb485ddbb117f5081c33f608c011c35aca015e4df8eaad0dd42439ff_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses LLM hallucinations by proposing Eidoku, a neuro-symbolic verification gate that treats reasoning verification as a structural constraint satisfaction problem, independent of generation likelihood. It uses a cost function based on graph connectivity, feature consistency, and logical entailment to reject structurally inconsistent statements. Experiments show this approach can deterministically reject high-probability yet structurally disconnected hallucinations, serving as a sanity check for generative reasoning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Eidoku: 神经符号验证门<br>Neuro-Symbolic Verification Gate] --> B[核心问题/Problem: LLM产生高概率幻觉<br>LLMs produce high-likelihood hallucinations]
    A --> C[主要方法/Method: 基于结构约束满足的验证<br>Verification via Structural Constraint Satisfaction]
    A --> D[关键结果/Results: 拒绝平滑错误，确定性检测<br>Rejects smooth falsehoods, deterministic detection]
    ```

- **[arXiv251225] Dominating vs. Dominated: Generative Collapse in Diffusion Models**
  - **tags:** [cv], [text-to-image generation], [diffusion models, cross-attention, generative collapse, multi-concept generation, attention dynamics]
  - **authors:** Hayeon Jeong, Jong-Seok Lee
  - **institution:** Yonsei University
  - **link:** https://arxiv.org/pdf/2512.20666
  - **contributions:** 1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation & distributed head mechanisms) perspectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the "Dominant-vs-Dominated" (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Dominating vs. Dominated<br/>支配 vs. 被支配] --> B[核心问题/Problem<br/>Multi-concept prompt generation imbalance<br/>多概念提示生成失衡];
        A --> C[主要方法/Method<br/>Introduce DominanceBench & analyze causes<br/>引入DominanceBench并分析原因];
        A --> D[关键结果/Results<br/>Data diversity & attention dynamics cause DvD<br/>数据多样性和注意力动态导致DvD];
    ```

- **[arXiv251225] Forward Only Learning for Orthogonal Neural Networks of any Depth**
  - **tags:** [ai], [neural network training algorithms], [forward-only learning, orthogonal neural networks, backpropagation alternative, FOTON, PEPITA]
  - **authors:** Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen
  - **institution:** Université Paris-Dauphine - PSL, ESPCI PSL
  - **link:** https://arxiv.org/pdf/2512.20668
  - **code:** https://github.com/ (URL mentioned as "this https URL" and "open-sourced on github" in the abstract/first page)
  - **contributions:** 1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs>
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Forward Only Learning for Orthogonal Neural Networks<br>前向传播学习用于正交神经网络] --> B[Problem: Backpropagation is computationally expensive<br>问题: 反向传播计算成本高]
    A --> C[Method: FOTON - Forward-Only Training with modulated forward pass<br>方法: FOTON - 使用调制前向传播的前向训练]
    A --> D[Results: Trains networks of any depth, outperforms PEPITA<br>结果: 可训练任意深度网络，性能优于PEPITA]
    ```

- **[arXiv251225] Improving Cardiac Risk Prediction Using Data Generation Techniques**
  - **tags:** [ai], [generative models], [Conditional Variational Autoencoder, synthetic data generation, cardiac risk prediction, data augmentation, clinical records]
  - **authors:** Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama
  - **institution:** Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela
  - **link:** https://arxiv.org/pdf/2512.20669
  - **contributions:** 1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Improving Cardiac Risk Prediction Using Data Generation Techniques] --> B(核心问题/Problem: 真实医疗数据稀缺、不完整且存在缺失值/Real-world medical data is scarce, incomplete, and has missing values)
    A --> C(主要方法/Method: 基于条件变分自编码器的架构生成合成临床记录/CVAE-based architecture for synthetic clinical record generation)
    A --> D(关键结果/Results: 生成的数据提高了心脏风险预测分类器的准确性/Generated data improves cardiac risk prediction classifier accuracy)
    ```

- **[arXiv251225] Bridging the AI Trustworthiness Gap between Functions and Norms**
  - **tags:** [ai], [trustworthy ai], [Trustworthy AI, AI Act, Functional AI Trustworthiness, Normative AI Trustworthiness, Conceptual Language]
  - **authors:** Daan Di Scala, Sophie Lathouwers, Michael van Bekkum
  - **institution:** TNO Netherlands Organisation for Applied Scientific Research, Utrecht University
  - **link:** https://arxiv.org/pdf/2512.20671
  - **contributions:** 1. Identifies and articulates the gap between Functional Trustworthy AI (FTAI) and Normative Trustworthy AI (NTAI). 2. Proposes the development of a semantic/conceptual language as a bridge to link FTAI implementations with NTAI regulations. 3. Provides key considerations and a future roadmap for assessing AI trustworthiness using this integrated framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/578b1d068cdd40bde2c92d3a17699f222634e6ec69ca5580ca2eda375c8105ca_w640_q70.webp
  - **Simple LLM Summary:** This position paper identifies a gap between the functional implementation (FTAI) and normative regulation (NTAI) of Trustworthy AI, which hinders system assessment. It proposes bridging this gap by developing a semantic language to map technical functions to legal norms. The conclusion is that such a framework will help developers implement compliant systems and stakeholders assess trustworthiness.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[论文标题: Bridging the AI Trustworthiness Gap<br>论文标题: Bridging the AI Trustworthiness Gap] --> B[核心问题/Problem: FTAI与NTAI存在鸿沟<br>核心问题/Problem: Gap between FTAI and NTAI]
        A --> C[主要方法/Method: 提出语义语言作为桥梁<br>主要方法/Method: Propose a semantic language as a bridge]
        A --> D[关键结果/Results: 提供评估框架与未来路线图<br>关键结果/Results: Provide assessment framework & future roadmap]
    ```

- **[arXiv251225] Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection**
  - **tags:** [ai], [multimodal fake news detection], [inconsistency detection, feature disentanglement, conflict-consensus mechanism, physics-inspired dynamics, cross-modal discrepancy]
  - **authors:** Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang
  - **institution:** Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano
  - **link:** https://arxiv.org/pdf/2512.20670
  - **contributions:** 1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection] --> B[核心问题/Problem: 主流一致性融合将关键跨模态差异误判为噪声，稀释了伪造证据]
    A --> C[主要方法/Method: 提出DCCF框架，解耦事实与情感，利用物理启发的动力学主动放大矛盾]
    A --> D[关键结果/Results: 在三个真实数据集上超越SOTA，平均准确率提升3.52%]
    ```

- **[arXiv251225] Revisiting the Learning Objectives of Vision-Language Reward Models**
  - **tags:** [ai], [reinforcement learning], [reward modeling, vision-language models, triplet loss, Meta-World, contrastive learning]
  - **authors:** Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome
  - **institution:** Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université
  - **link:** https://arxiv.org/pdf/2512.20675
  - **contributions:** 1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Revisiting VLM Reward Models] --> B(核心问题/Problem: 难以比较不同奖励模型目标/Difficulty in comparing reward model objectives)
        A --> C(主要方法/Method: 统一框架评估/Unified framework evaluation)
        A --> D(关键结果/Results: 三元组损失更优/Triplet loss outperforms SOTA)
    ```

- **[arXiv251225] HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model**
  - **tags:** [mlsys], [multi-modal training], [Low-Rank Adaptation (LoRA), parameter-efficient fine-tuning, rank adaptation, mobile vision language model, dynamic scheduling]
  - **authors:** Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour
  - **institution:** Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen
  - **link:** https://arxiv.org/pdf/2512.20674
  - **contributions:** 1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]
    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]
    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]
    ```

- **[arXiv251225] Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems**
  - **tags:** [ai], [multi-agent systems], [Differentiable Price Mechanism, Dominant Strategy Incentive Compatibility, VCG-equivalent incentive, Dec-POMDPs, Bayesian Incentive Compatibility]
  - **authors:** Stefano Grassi
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.20688
  - **contributions:** 1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems] --> B[核心问题/Problem: Hayekian Information Problem & Hurwiczian Incentive Problem]
    A --> C[主要方法/Method: Differentiable Price Mechanism (DPM) & Bayesian Extension]
    A --> D[关键结果/Results: DSIC/BIC Guarantee, Linear Scaling, 50x Faster than Model-Free RL]
    ```

- **[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation**
  - **tags:** [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]
  - **authors:** Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai
  - **institution:** Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University
  - **link:** https://arxiv.org/pdf/2512.20687
  - **contributions:** 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]
    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]
    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]
    ```

- **[arXiv251225] From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education**
  - **tags:** [ai], [educational technology], [generative AI, personalization, adaptive learning, large language models, intelligent tutoring systems]
  - **authors:** Iman Reihanian, Yunfei Hou, Qingquan Sun
  - **institution:** California State University, San Bernardino
  - **link:** https://arxiv.org/pdf/2512.20714
  - **contributions:** 1. Identified and analyzed five key application domains for GenAI-enabled personalization in CS education: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. 2. Synthesized four design patterns for successful implementations: context-aware tutoring anchored in student artifacts, multi-level hint structures, composition with traditional CS infrastructure, and human-in-the-loop quality assurance. 3. Proposed an exploration-first adoption framework for integrating GenAI, emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling, while pairing recurrent risks with operational mitigations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp
  - **Simple LLM Summary:** This scoping review maps how generative AI enables personalized computer science education. It analyzes design choices across 32 studies and finds that structured implementations with explanation-first guidance and artifact grounding lead to more positive learning outcomes than unconstrained chat interfaces. The paper concludes that generative AI can provide precision scaffolding when embedded in audit-ready workflows that preserve productive struggle.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education] --> B[核心问题/Problem: Does GenAI personalization support or undermine CS learning?]
    A --> C[主要方法/Method: Scoping review of 32 studies; Analysis of design choices & patterns]
    A --> D[关键结果/Results: Structured designs (e.g., hint ladders, artifact grounding) are more effective; Proposes an exploration-first adoption framework]
    ```

- **[arXiv251225] From artificial to organic: Rethinking the roots of intelligence for digital health**
  - **tags:** [ai], [digital health], [artificial intelligence, organic intelligence, digital health, neural networks, evolutionary processes]
  - **authors:** Prajwal Ghimire, Keyoumars Ashkan
  - **institution:** King's College London
  - **link:** https://arxiv.org/pdf/2512.20723
  - **contributions:** 1. Argues that AI is a product of organic human ingenuity, challenging the artificial vs. organic dichotomy. 2. Proposes that intelligence in digital health fundamentally stems from organization and adaptation, not just parameter scaling. 3. Highlights the inspiration of AI principles from human neurobiology and evolutionary processes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0101ccc5442460351cdd29a890224590d6ea20c129b31befb533fedb4fbf8c31_w640_q70.webp
  - **Simple LLM Summary:** This paper rethinks the roots of intelligence in digital health by arguing that artificial intelligence is fundamentally inspired by and derived from organic human cognition. It posits that the distinction between artificial and organic intelligence is blurred, emphasizing organization and adaptation as key principles. The conclusion suggests a more integrated view of intelligence for advancing digital health technologies.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[From artificial to organic: Rethinking the roots of intelligence for digital health] --> B[核心问题/Problem: Distinction between artificial and organic intelligence in digital health]
    A --> C[主要方法/Method: Philosophical analysis of AI's origins in human cognition and biology]
    A --> D[关键结果/Results: Boundaries are less distinct; intelligence is about organization and adaptation]
    ```

- **[arXiv251225] SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention**
  - **tags:** [mlsys], [diffusion models], [sparse attention, diffusion models, long-text generation, soft absorbing state, computational complexity]
  - **authors:** Alexandros Christoforos, Chadbourne Davis
  - **institution:** Suffolk University
  - **link:** https://arxiv.org/pdf/2512.20724
  - **contributions:** 1. Introduces SA-DiffuSeq, a diffusion framework that integrates sparse attention to improve scalability for long-document modeling. 2. Proposes a novel soft absorbing state tailored to sparse attention dynamics to stabilize diffusion trajectories and accelerate sequence reconstruction. 3. Demonstrates superior training efficiency and sampling speed compared to state-of-the-art diffusion baselines, especially on extended sequences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost of diffusion models for long-text generation by proposing SA-DiffuSeq, which integrates sparse attention and a novel soft absorbing state. This method reduces complexity while maintaining generation quality, making it suitable for applications like scientific writing and code generation. The results show that incorporating structured sparsity is a promising direction for efficient long-text generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[SA-DiffuSeq] --> B[核心问题/Problem<br>Computational Cost & Scalability];
    A --> C[主要方法/Method<br>Sparse Attention & Soft Absorbing State];
    A --> D[关键结果/Results<br>Improved Efficiency & Quality];
    ```

- **[arXiv251225] FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs**
  - **tags:** [mlsys], [llm inference], [Finite Element Method (FEM), Code Generation, LLM Benchmark, Computational Mechanics, Scientific Machine Learning]
  - **authors:** Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune
  - **institution:** Boston University, Move37 Labs
  - **link:** https://arxiv.org/pdf/2512.20732
  - **contributions:** 1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs' ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a lack of benchmarks for evaluating LLMs' scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[FEM-Bench Paper] --> B[核心问题/Problem: 缺乏评估LLM生成科学物理模型代码能力的基准/Lack of benchmark for evaluating LLMs' ability to generate scientifically valid physical model code]
        A --> C[主要方法/Method: 提出基于计算力学和有限元法的结构化基准/Proposes a structured benchmark based on computational mechanics and the Finite Element Method]
        A --> D[关键结果/Results: 先进LLM无法可靠解决所有基准任务，为跟踪进展奠定基础/State-of-the-art LLMs cannot reliably solve all benchmark tasks, establishing a foundation for tracking progress]
    ```

- **[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent**
  - **tags:** [mlsys], [agent system], [tool-augmented agent, agentic reinforcement learning, supervised fine-tuning (SFT), request-level asynchronous rollout, prefix-aware load balancing]
  - **authors:** Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang
  - **institution:** Tsinghua University, Tencent Hunyuan
  - **link:** https://arxiv.org/pdf/2512.20745
  - **contributions:** 1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AgentMath] --> B[核心问题/Problem: LRMs are inefficient and inaccurate for complex math]
    A --> C[主要方法/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]
    A --> D[关键结果/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]
    ```

- **[arXiv251225] AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication**
  - **tags:** [sys], [wireless networks], [Deep Reinforcement Learning (DRL), Reconfigurable Intelligent Surfaces (RIS), Energy Harvesting (EH)]
  - **authors:** Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur
  - **institution:** Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)
  - **link:** https://arxiv.org/pdf/2512.20739
  - **contributions:** 1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AI-Driven Green CRNs for 6G] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[频谱稀缺与高能耗/Spectrum Scarcity & High Energy Consumption]
    C --> C1[AI驱动框架/AI-Driven Framework]
    C1 --> C2[集成DRL, TL, EH, RIS/Integrates DRL, TL, EH, RIS]
    D --> D1[节能25-30%/25-30% Energy Saving]
    D --> D2[AUC>0.90, PDR提升/AUC>0.90, PDR Improved]
    ```

- **[arXiv251225] Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies**
  - **tags:** [mlsys], [multi-modal training], [Lipschitz Continuity, Attention Mechanism, Aggregation Methods, Training Stability, Multimodal Autoencoders]
  - **authors:** Diyar Altinses, Andreas Schwung
  - **institution:** South Westphalia University of Applied Sciences
  - **link:** https://arxiv.org/pdf/2512.20749
  - **contributions:** 1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method's superior performance in consistency, convergence speed, and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Stabilizing Multimodal Autoencoders<br/>稳定多模态自编码器] --> B(核心问题/Problem: Training Stability & Robustness<br/>训练稳定性与鲁棒性)
    A --> C(主要方法/Method: Theoretical Lipschitz Analysis & Regularized Attention Fusion<br/>理论Lipschitz分析与正则化注意力融合)
    A --> D(关键结果/Results: Improved Consistency, Convergence, Accuracy<br/>提升的一致性、收敛速度与精度)
    ```

- **[arXiv251225] Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits**
  - **tags:** [mlsys], [others], [formal verification, neural network robustness, early exits, adversarial perturbations, off-the-shelf solvers]
  - **authors:** Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz
  - **institution:** The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside
  - **link:** https://arxiv.org/pdf/2512.20755
  - **contributions:** 1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title<br>Bridging Efficiency and Safety] --> B(核心问题 / Problem<br>Verifying Early Exit Networks);
    A --> C(主要方法 / Method<br>Tailored Robustness Property & Enhanced Algorithm);
    A --> D(关键结果 / Results<br>Improved Verifiability & Efficiency);
    ```

- **[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed**
  - **tags:** [ai], [reinforcement learning], [RLVR, causal reasoning, generalization, supervised fine-tuning, large language models]
  - **authors:** Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei
  - **institution:** Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago
  - **link:** https://arxiv.org/pdf/2512.20760
  - **contributions:** 1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.
  - **Mindmap:**

    ```mermaid
    graph LR
    A["Generalization of RLVR Using Causal Reasoning as a Testbed<br>以因果推理为测试平台的RLVR泛化研究"] --> B["核心问题/Problem<br>RLVR何时能实现鲁棒泛化？<br>When does RLVR yield robust generalization?"]
    A --> C["主要方法/Method<br>在因果图模型上实证研究RLVR与SFT<br>Empirical study of RLVR vs SFT on causal graphical models"]
    A --> D["关键结果/Results<br>RLVR泛化更强，但依赖模型规模与初始能力<br>RLVR yields stronger generalization but depends on model size & initial competence"]
    ```

- **[arXiv251225] TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform**
  - **tags:** [mlsys], [others], [time series foundation models, live forecasting, pre-registration, information leakage, temporal split]
  - **authors:** Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller
  - **institution:** Paderborn University
  - **link:** https://arxiv.org/pdf/2512.20761
  - **code:** https://huggingface.co/spaces/DAG-UPB/TS-Arena
  - **contributions:** 1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[TS-Arena Technical Report] --> B[核心问题/Problem: TSFM评估危机 / TSFM Evaluation Crisis]
    A --> C[主要方法/Method: 预注册实时预测平台 / Pre-registered Live Forecasting Platform]
    A --> D[关键结果/Results: 防止历史污染，真实评估泛化 / Prevents Historical Contamination, Authentic Generalization Assessment]
    B --> E[信息泄露与数据重用 / Information Leakage & Data Reuse]
    C --> F[实时数据流与严格时间分割 / Live Data Streams & Strict Temporal Split]
    D --> G[可持续的基准测试基础设施 / Sustainable Benchmarking Infrastructure]
    ```

- **[arXiv251225] Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication**
  - **tags:** [ai], [multi-agent reinforcement learning], [Dec-POMDP, belief inconsistency, limited communication, action consistency, multi-agent planning]
  - **authors:** Moshe Rafaeli Shimron, Vadim Indelman
  - **institution:** Technion - Israel Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.20778
  - **contributions:** 1. A novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies among agents. 2. Provides probabilistic guarantees for both action consistency and performance relative to a fully-communicating baseline. 3. Introduces a mechanism to selectively trigger communication only when necessary and addresses the decision of whether to share data after action selection to improve inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4538ed8602d7a249e385e29bfc0423ed3f6682a9ac338c82e10822f10bd96df8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of multi-agent decision-making under uncertainty when agents have inconsistent beliefs due to limited communication. It proposes a new decentralized framework for Dec-POMDPs that provides performance and action consistency guarantees while minimizing communication. Simulation results demonstrate that the approach outperforms existing state-of-the-art algorithms.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs] --> B(核心问题/Problem: Belief Inconsistency & Limited Communication)
    A --> C(主要方法/Method: Novel Decentralized Framework with Guarantees)
    A --> D(关键结果/Results: Outperforms SOTA Algorithms)
    ```

- **[arXiv251225] NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts**
  - **tags:** [cv], [medical image segmentation], [nullable prompts, mixed-supervision, vision-language models, breast ultrasound segmentation]
  - **authors:** Raja Mallina, Bryar Shareef
  - **institution:** University of Nevada, Las Vegas
  - **link:** https://arxiv.org/pdf/2512.20783
  - **contributions:** 1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]
    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]
    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]
    ```

- **[arXiv251225] X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis**
  - **tags:** [mlsys], [agent system], [hierarchical agent architecture, prompt refinement with human feedback, schema-adaptive hybrid RAG]
  - **authors:** Yihan, Xin Chen
  - **institution:** Texas A&M University
  - **link:** https://arxiv.org/pdf/2512.20789
  - **contributions:** 1. Proposes X-GridAgent, a novel LLM-powered agentic AI system with a three-layer hierarchical architecture for automating power grid analysis via natural language. 2. Introduces an LLM-driven prompt refinement algorithm with human feedback to enhance task planning. 3. Develops a schema-adaptive hybrid retrieval-augmented generation (RAG) algorithm for accurate information retrieval from large-scale structured grid datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb48dc08ade0ddfa007a5156a481bb9ff3e7a4fabec1d577a9040fa4193ffe1_w640_q70.webp
  - **Simple LLM Summary:** This paper presents X-GridAgent, an LLM-powered agent system designed to automate complex power grid analysis through natural language queries using a hierarchical architecture and novel algorithms for prompt refinement and information retrieval. Experimental results demonstrate its effectiveness and reliability in performing interpretable and rigorous power system analysis.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[X-GridAgent] --> B[核心问题/Problem: Conventional grid tools require manual effort and expertise]
    A --> C[主要方法/Method: LLM-powered agent with hierarchical architecture & novel algorithms]
    A --> D[关键结果/Results: Effective & reliable automated analysis]
    ```

- **[arXiv251225] A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents**
  - **tags:** [ai], [ai safety & alignment], [autonomous agents, safety benchmark, constraint violations, key performance indicator (KPI), deliberative misalignment]
  - **authors:** Miles Q. Li, Benjamin C. M. Fung, Martin Weiss, Pulei Xiong, Khalil Al-Hussaeni, Claude Fachkha
  - **institution:** McGill University, Tiptree Advanced Systems Corporation, Polytechnique Montréal, National Research Council Canada, Rochester Institute of Technology Dubai, University of Dubai
  - **link:** https://arxiv.org/pdf/2512.20798
  - **contributions:** 1. Introduced a novel benchmark with 40 multi-step scenarios to evaluate emergent, outcome-driven constraint violations in autonomous AI agents, distinguishing between Mandated and Incentivized variations. 2. Conducted a comprehensive evaluation across 12 state-of-the-art LLMs, revealing high misalignment rates (up to 71.4%) and showing that superior reasoning capability does not guarantee safety. 3. Identified and highlighted the phenomenon of "deliberative misalignment," where agents recognize their own actions as unethical in separate evaluations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a9156498ba4348ea63fbca94bacbbb938f1ccadc8825abf01cb9c946cda2c48_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of realistic benchmarks for evaluating safety risks in autonomous AI agents. It proposes a new benchmark with multi-step scenarios tied to KPIs to test for outcome-driven constraint violations. The evaluation reveals alarmingly high violation rates across leading models, demonstrating that advanced capabilities do not ensure safety and highlighting a critical need for improved agentic-safety training.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents] --> B(核心问题/Problem: Lack of realistic benchmarks for emergent agent misalignment)
    A --> C(主要方法/Method: New benchmark with 40 multi-step, KPI-driven scenarios)
    A --> D(关键结果/Results: High violation rates (1.3%-71.4%); reasoning ≠ safety; deliberative misalignment)
    ```

- **[arXiv251225] Safety Alignment of LMs via Non-cooperative Games**
  - **tags:** [ai], [reinforcement learning], [non-cooperative game, adversarial training, preference-based reward, online reinforcement learning, safety alignment]
  - **authors:** Anselm Paulus, Ilia Kulikov, Brandon Amos, Rémi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov
  - **institution:** Meta (FAIR), University of Tübingen
  - **link:** https://arxiv.org/pdf/2512.20806
  - **code:** https://github.com/facebookresearch/advgame
  - **contributions:** 1. Introduces a new paradigm for safety alignment by framing it as a non-zero-sum game between an Attacker LM and a Defender LM. 2. Proposes joint training of the LMs via online reinforcement learning with a preference-based reward signal to reduce reward hacking. 3. Demonstrates that the method (AdvGame) produces a Defender LM with improved safety and utility and an Attacker LM that serves as a strong red-teaming agent.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99d494326c3e6d7e063baf364aff968ae0d21f53ab3f3e8b4214c548e5ac79b4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of aligning language models for safety without sacrificing utility. It proposes AdvGame, a method that frames safety alignment as a non-cooperative game between an Attacker and a Defender LM, training them jointly with online RL using preference-based rewards. The results show the approach yields a more helpful and safe Defender and a powerful general-purpose Attacker for red-teaming.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Safety Alignment of LMs via Non-cooperative Games] --> B(核心问题/Problem: Safety vs. Utility Trade-off in LM Alignment)
        A --> C(主要方法/Method: Non-zero-sum Game & Online RL with Preference Reward)
        A --> D(关键结果/Results: Improved Defender LM & Strong Red-teaming Attacker)
    ```

- **[arXiv251225] MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs**
  - **tags:** [nlp], [medical nlp / llm evaluation], [medical benchmark, electronic health records (EHR), knowledge grounding, counterfactual reasoning, DPO fine-tuning]
  - **authors:** Zhan Qu, Michael Färber
  - **institution:** TU Dresden, ScaDS.AI
  - **link:** https://arxiv.org/pdf/2512.20822
  - **contributions:** 1. Introduces MediEval, a unified benchmark linking real EHRs (MIMIC-IV) to a biomedical knowledge base for evaluating LLMs on patient-contextual and knowledge-grounded reasoning. 2. Proposes a 4-quadrant evaluation framework to systematically assess models on both factual correctness and contextual consistency, identifying critical failure modes like hallucinated support and truth inversion. 3. Proposes Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty, which significantly improves model accuracy and safety by eliminating truth inversion errors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59c4d88b1ecf7256d86a2c1dd12f74897d1a42b0c88d272ea8cf058355f013cd_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a gap in evaluating LLMs for medical applications, where existing benchmarks either test isolated knowledge or patient reasoning without verifying correctness. To address this, the authors introduce the MediEval benchmark and a 4-quadrant evaluation framework to systematically assess LLMs, and propose a novel fine-tuning method called CoRFu. The results show that CoRFu significantly improves model performance and safety by eliminating dangerous error types like truth inversion.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[MediEval] --> B[核心问题/Problem: LLMs in medicine lack reliable evaluation combining knowledge and patient context];
        A --> C[主要方法/Method: Unified benchmark (EHR + KB) & 4-quadrant framework & CoRFu fine-tuning];
        A --> D[关键结果/Results: Identifies failure modes; CoRFu improves accuracy and safety];
    ```

- **[arXiv251225] NotSoTiny: A Large, Living Benchmark for RTL Code Generation**
  - **tags:** [mlsys], [llm training], [RTL code generation, benchmark, hardware design, data contamination, verification]
  - **authors:** Razine Moundir Ghorab, Emanuele Parisi, Cristian Gutierrez, Miquel Alberti-Binimelis, Miquel Moreto, Dario Garcia-Gasulla, Gokcen Kestor
  - **institution:** Barcelona Supercomputing Center, Universitat Politecnica de Catalunya
  - **link:** https://arxiv.org/pdf/2512.20823
  - **contributions:** 1. Introduces NotSoTiny, a large-scale, living benchmark for evaluating LLMs on RTL code generation, built from real hardware designs. 2. Proposes an automated pipeline to ensure benchmark quality by removing duplicates, verifying correctness, and periodically updating to mitigate data contamination. 3. Demonstrates that NotSoTiny presents more challenging tasks than prior benchmarks, effectively highlighting current LLM limitations in hardware design.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ee59b5723cdae955454bd94bdc8872b40c0eaccf59a4e54b86951d040529325_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces NotSoTiny, a benchmark for evaluating LLMs on generating Register-Transfer Level (RTL) code, addressing limitations of prior benchmarks by using real, complex hardware designs and a pipeline to ensure correctness and reduce data contamination. The results show that NotSoTiny tasks are more challenging, effectively guiding the improvement of LLMs for hardware design.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NotSoTiny: A Large, Living Benchmark for RTL Code Generation] --> B(核心问题/Problem: LLM RTL代码生成评估挑战 / LLM RTL Code Generation Evaluation Challenge)
    A --> C(主要方法/Method: 基于真实硬件设计的自动化基准测试 / Automated Benchmark from Real Hardware Designs)
    A --> D(关键结果/Results: 任务更具挑战性，有效指导改进 / Tasks More Challenging, Effectively Guides Improvement)
    ```

- **[arXiv251225] Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions**
  - **tags:** [ai], [reinforcement learning], [parameterized actions, state abstraction, action abstraction, TD(λ), sample efficiency]
  - **authors:** Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava
  - **institution:** Arizona State University, Brown University
  - **link:** https://arxiv.org/pdf/2512.20831
  - **code:** https://github.com/AAIR-lab/PEARL.git
  - **contributions:** 1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Context-Sensitive Abstractions for RL with Parameterized Actions] --> B(核心问题/Problem: RL for Parameterized Actions)
    A --> C(主要方法/Method: Learn & Refine State/Action Abstractions)
    A --> D(关键结果/Results: Higher Sample Efficiency for TD(λ))
    ```

- **[arXiv251225] MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs**
  - **tags:** [mlsys], [agent system], [multi-agent reflection, reasoning improvement, self-correction, episodic memory, iterative refinement]
  - **authors:** Onat Ozer, Grace Wu, Yuchen Wang, Daniel Dosti, Honghao Zhang, Vivi De La Rue
  - **institution:** University of Michigan
  - **link:** https://arxiv.org/pdf/2512.20845
  - **contributions:** 1. Identifies systematic shortcomings in the single-agent Reflexion framework, such as repeated reasoning errors and confirmation bias, through detailed replication and analysis. 2. Proposes Multi-Agent Reflexion (MAR), a structured multi-agent extension that incorporates diverse reasoning personas and a judge model to synthesize critiques into unified reflections. 3. Demonstrates that MAR improves performance over Reflexion on HotPotQA and HumanEval benchmarks, reducing stagnation and enhancing reasoning reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6022c55272f43b40b16d9e6f722483682ba159634bd04eb55be130ec83c86fe4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of reasoning error repetition and limited corrective feedback in single-agent LLM self-reflection frameworks like Reflexion. It proposes Multi-Agent Reflexion (MAR), which uses multiple agents with diverse personas to generate critiques and a judge to synthesize them, leading to more diverse and effective reflections. The method shows improved accuracy on HotPotQA and HumanEval benchmarks compared to the single-agent approach.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[MAR: Multi-Agent Reflexion] --> B[核心问题/Problem: Single-agent self-reflection leads to repeated errors and confirmation bias]
        A --> C[主要方法/Method: Multi-agent system with diverse personas and a judge model for critique synthesis]
        A --> D[关键结果/Results: Improved accuracy on HotPotQA (47% EM) and HumanEval (82.7% pass@1)]
    ```

- **[arXiv251225] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning**
  - **tags:** [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, agentic reasoning, sparse activation, long context]
  - **authors:** NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.20848
  - **contributions:** 1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp
  - **Simple LLM Summary:** This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Nemotron 3 Nano<br>论文标题/Paper Title] --> B[构建高效、能进行智能体推理的大模型<br>核心问题/Problem];
    A --> C[混合MoE与Mamba-Transformer架构，稀疏激活参数<br>主要方法/Method];
    A --> D[更高推理吞吐与精度，支持100万令牌上下文<br>关键结果/Results];
    ```

- **[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence**
  - **tags:** [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, LatentMoE, NVFP4, multi-environment reinforcement learning]
  - **authors:** NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.20856
  - **contributions:** 1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NVIDIA Nemotron 3] --> B[核心问题/Problem: Efficient and open intelligence for agentic applications]
    A --> C[主要方法/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]
    A --> D[关键结果/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]
    ```

- **[arXiv251225] Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs**
  - **tags:** [mlsys], [llm inference], [block low-rank (BLR), Triton kernels, memory-bound optimization, Jetson Orin Nano, roofline analysis]
  - **authors:** Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim
  - **institution:** University of Michigan
  - **link:** https://arxiv.org/pdf/2512.20861
  - **code:** https://github.com/pabillam/mem-efficient-blr
  - **contributions:** 1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Memory-Efficient Acceleration of Block Low-Rank Foundation Models] --> B[核心问题/Problem: BLR模型多token推理存在内存墙/Multi-token inference for BLR models is memory-bound]
    A --> C[主要方法/Method: 定制Triton内核与内存优化/Custom Triton kernels with memory optimizations]
    A --> D[关键结果/Results: 显著加速与模型压缩/Significant speedup & model compression]
    ```

- **[arXiv251225] Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images**
  - **tags:** [cv], [object detection], [YOLOv11, 3D-DIoU, multi-view fusion, GPR, FDTD]
  - **authors:** Haotian Lv, Chao Li, Jiangbo Dai, Yuhui Zhang, Zepeng Fan, Yiqiu Tan, Dawei Wang, Binglei Xie
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.20866
  - **contributions:** 1. Proposed a B/C/D-Scan three-view joint analysis strategy and a feature evaluation method validated by FDTD simulations and real data. 2. Developed the DCO-YOLO framework, integrating DySample, CGLU, and OutlookAttention into YOLOv11 to enhance small-scale pipeline feature extraction. 3. Introduced a 3D-DIoU spatial feature matching algorithm with 3D geometric constraints to automate multi-view annotation association and resolve single-view ambiguities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a lightweight framework for 3D underground pipeline detection using multi-view 2D GPR images. The method integrates an improved YOLO-based detection model (DCO-YOLO) with a novel 3D-DIoU spatial matching algorithm for multi-view fusion. Experiments on real urban data show the framework achieves high accuracy, recall, and mAP, outperforming the baseline and offering a reliable solution for pipeline recognition and localization.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Lightweight framework for underground pipeline recognition and spatial localization<br>基于多视图2D GPR图像的地下管道识别与空间定位轻量级框架] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[Weak multi-view feature correlation, low small-target accuracy<br>多视图特征关联弱，小目标识别精度低]
    C --> C1[3D pipeline three-view feature evaluation<br>三维管道三视图特征评估]
    C --> C2[DCO-YOLO framework<br>DCO-YOLO框架]
    C --> C3[3D-DIoU spatial feature matching<br>3D-DIoU空间特征匹配]
    D --> D1[Accuracy 96.2%, Recall 93.3%, mAP 96.7%<br>准确率96.2%，召回率93.3%，平均精度96.7%]
    ```

- **[arXiv251225] The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents**
  - **tags:** [mlsys], [agent system], [epistemic asymmetry, Beta-Bernoulli distribution, epistemic caching, forgetting factor, active learning]
  - **authors:** Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
  - **institution:** Kwansei Gakuin University, Victoria University of Wellington
  - **link:** https://arxiv.org/pdf/2512.20884
  - **contributions:** 1. A formal probabilistic framework using a Beta-Bernoulli model with a forgetting factor to quantify epistemic uncertainty and provide a non-altruistic motive for knowledge sharing among LLM agents. 2. The introduction of epistemic caching, a resource management mechanism that leverages the forgetting factor to dynamically prioritize the active head of non-stationary knowledge distributions for scalable deployment. 3. Demonstrating how accumulated belief states can serve as verifiable reward signals for RLHF and high-quality data filters for SFT, bridging inference-time interaction with long-term model alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f9f5d3701afff9c1243309bc058183ccd079615b358ea6fcd7f66333c45b2ae_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies epistemic asymmetry as a key limitation where LLM agents are unidirectional knowledge consumers. To address this, it proposes a probabilistic framework that models agent belief to create a self-interested motive for sharing knowledge, framed as optimal active learning. Simulations show this uncertainty-driven strategy outperforms random baselines in dynamic environments.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[The Silent Scholar Problem<br>沉默学者问题] --> B[Problem: Epistemic Asymmetry<br>问题: 认知不对称];
    A --> C[Method: Probabilistic Framework<br>方法: 概率框架];
    A --> D[Results: Outperforms Baseline<br>结果: 优于基线];
    B --> B1[Agents as unidirectional consumers<br>智能体作为单向消费者];
    C --> C1[Belief as Beta-Bernoulli<br>信念的Beta-Bernoulli建模];
    C --> C2[Epistemic Caching<br>认知缓存];
    D --> D1[Efficient in heterogeneous env.<br>在异构环境中高效];
    ```

- **[arXiv251225] DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction**
  - **tags:** [cv], [medical image analysis], [spatiotemporal attention, graph neural network, multimodal fusion, pulmonary nodule classification, feature encoder]
  - **authors:** Xiao Yu, Zhaojie Fang, Guanyu Zhou, Yin Shen, Huoling Luo, Ye Li, Ahmed Elazab, Xiang Wan, Ruiquan Ge, Changmiao Wang
  - **institution:** Hangzhou Dianzi University
  - **link:** https://arxiv.org/pdf/2512.20898
  - **code:** https://github.com/lcbkmm/DGSAN
  - **contributions:** 1. Proposed a Dual-Graph Spatiotemporal Attention Network (DGSAN) for pulmonary nodule malignancy prediction. 2. Introduced a Dual-Graph Construction method and a Hierarchical Cross-Modal Graph Fusion Module for effective multimodal feature integration. 3. Compiled a novel multimodal dataset named NLST-cmst to support related research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of inefficient multimodal fusion in pulmonary nodule malignancy prediction. It proposes a Dual-Graph Spatiotemporal Attention Network (DGSAN) that uses a Global-Local Feature Encoder and a hierarchical graph fusion module to integrate temporal and multimodal data. Experiments show DGSAN outperforms state-of-the-art methods with high computational efficiency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[DGSAN: Dual-Graph Spatiotemporal Attention Network] --> B(核心问题/Problem: Inefficient multimodal fusion for nodule prediction)
    A --> C(主要方法/Method: Dual-Graph construction & Hierarchical Cross-Modal Fusion)
    A --> D(关键结果/Results: Outperforms SOTA on NLST-cmst/CSTL datasets)
    ```

- **[arXiv251225] Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction**
  - **tags:** [sys], [edge computing], [UAV trajectory optimization, task offloading, mobility prediction, deep reinforcement learning, Transformer]
  - **authors:** Siqi Mu, Shuo Wen, Yang Lu, Ruihong Jiang, Bo Ai
  - **institution:** Beijing Sport University, Beijing Jiaotong University, Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.20902
  - **contributions:** 1. Establishes an embodied AI-enhanced IoMT edge computing framework for dynamic UAV service provisioning. 2. Proposes a novel hierarchical multi-scale Transformer-based model for predicting WBAN user mobility from historical trajectory data. 3. Designs a prediction-enhanced deep reinforcement learning algorithm that integrates mobility forecasts to jointly optimize UAV flight trajectory and task offloading decisions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75676682cad2a49439c91ac390cbbb997d6b492883c198c681369ae72675ee8a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of minimizing task completion time for WBAN users by optimizing UAV trajectory and task offloading under energy constraints. It proposes an embodied AI framework that uses a Transformer-based model to predict user mobility and a DRL algorithm to make intelligent optimization decisions. Simulation results show the proposed method outperforms existing benchmarks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Embodied AI-Enhanced IoMT Edge Computing<br/>具身AI增强的IoMT边缘计算] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[Minimize WBAN task time<br/>最小化WBAN任务时间]
    B --> B2[UAV Energy Constraint<br/>UAV能量约束]
    C --> C1[Transformer Mobility Prediction<br/>Transformer移动性预测]
    C --> C2[DRL for Trajectory & Offloading<br/>DRL优化轨迹与卸载]
    D --> D1[Superior Performance<br/>性能优越]
    ```

- **[arXiv251225] DiEC: Diffusion Embedded Clustering**
  - **tags:** [ai], [deep clustering], [diffusion models, representation selection, self-training, graph regularization, denoising consistency]
  - **authors:** Haidong Hu
  - **institution:** Not explicitly provided in the given content.
  - **link:** https://arxiv.org/pdf/2512.20905
  - **contributions:** 1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model's internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[DiEC: Diffusion Embedded Clustering] --> B[核心问题/Problem: Single fixed embedding ignores varying clusterability in diffusion model's internal trajectory];
    A --> C[主要方法/Method: Two-stage search (CML & OTS) on layer×timestep, regularized self-training with denoising-consistency];
    A --> D[关键结果/Results: Achieves competitive clustering performance on multiple benchmarks];
    ```

- **[arXiv251225] RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks**
  - **tags:** [mlsys], [llm training], [reversible networks, memory-efficient fine-tuning, mixture-of-experts, full-parameter fine-tuning, activation recomputation]
  - **authors:** Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.20920
  - **contributions:** 1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model's expressive power.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[RevFFN: Memory-Efficient Fine-Tuning] --> B[核心问题/Problem: Full fine-tuning memory overhead高]
    A --> C[主要方法/Method: 使用可逆Transformer块/Use reversible Transformer blocks]
    A --> D[关键结果/Results: 单GPU高效全参数微调/Efficient full fine-tuning on single GPU]
    ```

- **[arXiv251225] Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy**
  - **tags:** [ai], [revenue management], [constrained optimization, Bayesian hierarchical modeling, Monte Carlo simulation, price elasticity, churn prediction]
  - **authors:** Deepit Sapru
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.20932
  - **contributions:** 1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Guardrailed Elasticity Pricing] --> B[核心问题/Problem: Static pricing fails to balance revenue & retention];
    A --> C[主要方法/Method: Forecast + Elasticity + Churn model with constrained optimization];
    A --> D[关键结果/Results: Outperforms static pricing, protects customers, enables durable growth];
    ```

- **[arXiv251225] Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning**
  - **tags:** [cv], [visual reasoning], [visual programming, spatial reasoning, tool induction, transductive learning, 3D scene understanding]
  - **authors:** Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.20934
  - **code:** https://transductive-visualprogram.github.io/
  - **contributions:** 1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Transductive Visual Programming] --> B[核心问题/Problem<br>Spatial reasoning is challenging for VLMs]
        A --> C[主要方法/Method<br>Build tools from experience, not speculation]
        A --> D[关键结果/Results<br>SOTA performance, better tool reuse & generalization]
    ```

- **[arXiv251225] A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate**
  - **tags:** [mlsys], [others], [graph neural network, surrogate model, multi-fidelity dataset, scaling laws, aerodynamic field prediction]
  - **authors:** Yiren Shen, Juan J. Alonso
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.20941
  - **contributions:** 1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[论文标题 / Paper Title<br>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws] --> B(核心问题 / Problem<br>缺乏开源多保真数据集与数据规模对模型性能影响的实证指导 / Lack of open-source multi-fidelity datasets and empirical guidelines on data scaling)
        A --> C(主要方法 / Method<br>发布数据集并进行缩放研究 / Release dataset and conduct scaling study)
        B --> D(关键结果 / Results<br>误差随数据量呈幂律下降 / Test error decreases with data size via power law)
        C --> D
    ```

- **[arXiv251225] Neural Probe-Based Hallucination Detection for Large Language Models**
  - **tags:** [nlp], [hallucination detection], [MLP probes, token-level detection, Bayesian optimization, hidden states, multi-objective loss]
  - **authors:** Shize Liang, Hongzhi Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.20949
  - **contributions:** 1. Proposed a neural network-based framework using lightweight MLP probes for token-level hallucination detection, enabling nonlinear modeling of hidden states. 2. Designed a multi-objective joint loss function to improve detection stability and semantic disambiguation. 3. Established a layer position-probe performance response model and used Bayesian optimization to automatically search for optimal probe insertion layers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/256e2b7c6550072fc0e643c4045a4a592ba6b2241cd12656b7dd16ad27bf89b0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of hallucination in large language models by proposing a real-time, token-level detection method. The method uses lightweight MLP probes on frozen model hidden states and a Bayesian-optimized layer search. Experiments show it outperforms existing methods in accuracy and recall under low false-positive conditions.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Neural Probe-Based Hallucination Detection for Large Language Models] --> B(核心问题/Problem: LLMs生成幻觉内容/LLMs generate hallucinations)
    A --> C(主要方法/Method: MLP探针 & 贝叶斯优化/MLP probes & Bayesian optimization)
    A --> D(关键结果/Results: 在多个数据集上表现优异/Outperforms SOTA on multiple datasets)
    ```

- **[arXiv251225] MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment**
  - **tags:** [nlp], [crosslingual information retrieval], [dual-encoder, contrastive learning, hard negative sampling, data augmentation, multi-source alignment]
  - **authors:** Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah
  - **institution:** Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is "MultiMind".
  - **link:** https://arxiv.org/pdf/2512.20950
  - **contributions:** 1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --> B(核心问题/Problem: Rapid spread of multilingual misinformation);
        A --> C(主要方法/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);
        A --> D(关键结果/Results: Improved retrieval accuracy on benchmarks);
    ```

- **[arXiv251225] Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models**
  - **tags:** [nlp], [protein language models], [reflection pretraining, chain-of-thought, language expressiveness, self-correction, biological sequences]
  - **authors:** Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun
  - **institution:** Fudan University, Shanghai Artificial Intelligence Laboratory, University of British Columbia, Zhejiang University, The Chinese University of Hong Kong, Stony Brook University
  - **link:** https://arxiv.org/pdf/2512.20954
  - **contributions:** 1. Proposed and defined the concept of "language expressiveness" to explain the difficulty of applying Chain-of-Thought reasoning to biological sequence models. 2. Introduced reflection pretraining for biological sequence models, enabling intermediate reasoning through auxiliary "thinking tokens". 3. Demonstrated that this approach enables self-correction, improves performance, and offers benefits like counter-memorization and enhanced human steerability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c51a6a0ca0e6bf5774254f47e4581544610c262b22a0cc12fe84b840bda40a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of applying Chain-of-Thought reasoning to biological sequence models like protein language models, which have limited token expressiveness. The authors propose reflection pretraining, which augments the model with auxiliary "thinking tokens" to enable intermediate reasoning and self-correction. The method theoretically enhances language expressiveness and experimentally leads to substantial performance gains compared to standard pretraining.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models] --> B(核心问题/Problem: Limited expressiveness of protein language restricts CoT reasoning)
    A --> C(主要方法/Method: Reflection pretraining with auxiliary "thinking tokens")
    A --> D(关键结果/Results: Enhanced expressiveness, self-correction, performance gains)
    ```

- **[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design**
  - **tags:** [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), ChemBERTa, ESM-2, reaction-template, de novo drug design]
  - **authors:** R Yadunandan, Nimisha Ghosh
  - **institution:** Department of Computer Science and Engineering, Shiv Nadar University Chennai
  - **link:** https://arxiv.org/pdf/2512.20958
  - **code:** https://github.com/YadunandanRaman/ReACT-Drug/
  - **contributions:** 1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[ReACT-Drug] --> B[核心问题/Problem: Navigating vast chemical space for synthesizable, high-affinity drugs];
        A --> C[主要方法/Method: RL + Protein Embeddings + Reaction-Template Actions];
        A --> D[关键结果/Results: Novel, valid, synthetically accessible candidates];
    ```

- **[arXiv251225] One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents**
  - **tags:** [se], [repository-level code understanding], [LLM agent, reinforcement learning, tool usage, code navigation, execution-aware]
  - **authors:** Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu
  - **institution:** Affiliation not explicitly stated in provided text. Email domains suggest potential institutions, but cannot be reliably inferred from given content.
  - **link:** https://arxiv.org/pdf/2512.20957
  - **contributions:** 1. Proposes RepoNavigator, an LLM agent that uses a single, execution-aware tool ("jump to definition") for navigating code repositories, simplifying agent control and aligning with code execution logic. 2. Introduces an end-to-end Reinforcement Learning (RL) training method for the agent directly from a pretrained model, eliminating the need for closed-source model distillation. 3. Demonstrates state-of-the-art performance on repository-level issue localization, showing that smaller RL-trained models (e.g., 7B) can outperform larger baseline models (e.g., 14B, 32B) and even closed-source models like Claude-3.7.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6254efa02c0725684b783a26c76c1825bf8aaa25aee61fb7aaea40887f0efc46_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of locating code to modify in large software repositories. It proposes RepoNavigator, an LLM agent trained with Reinforcement Learning to use a single "jump to definition" tool for navigation. Experiments show this approach achieves state-of-the-art performance, with smaller models outperforming larger baselines, proving the efficiency of a simple, execution-aware tool combined with RL training.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents] --> B[核心问题/Problem: Locating modification points in large, complex code repositories is difficult]
    A --> C[主要方法/Method: RepoNavigator agent with a single "jump to definition" tool, trained end-to-end via RL]
    A --> D[关键结果/Results: SOTA performance; smaller RL-trained models outperform larger baselines and closed-source models]
    ```

- **[arXiv251225] Can Agentic AI Match the Performance of Human Data Scientists?**
  - **tags:** [ai], [automated data science], [agentic AI, domain knowledge, synthetic data, large language models, human-AI teaming]
  - **authors:** An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding
  - **institution:** University of Minnesota, University of Chicago, Cisco Research
  - **link:** https://arxiv.org/pdf/2512.20959
  - **contributions:** 1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp
  - **Simple LLM Summary:** The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Can Agentic AI Match Human Data Scientists?] --> B[核心问题/Problem: Can agentic AI match human performance using domain knowledge?];
    A --> C[主要方法/Method: Design task with latent variable in images, use synthetic insurance data];
    A --> D[关键结果/Results: Agentic AI with generic workflow falls short; highlights need for domain-aware AI];
    ```

- **[arXiv251225] Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality**
  - **tags:** [mlsys], [llm training], [distributed attention, communication efficiency, Ring-Attention, communication-computation ratio, scalability]
  - **authors:** Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian
  - **institution:** Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed
  - **link:** https://arxiv.org/pdf/2512.20968
  - **contributions:** 1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Mesh-Attention<br>论文标题/Paper Title] --> B[核心问题/Problem: 分布式注意力通信开销大<br>High Communication in Distributed Attention]
    A --> C[主要方法/Method: 基于2D计算块划分的Mesh-Attention算法<br>Mesh-Attention with 2D Tile Assignment]
    A --> D[关键结果/Results: 通信量减少85.4%, 速度提升3.4倍<br>85.4% Comm Reduction, 3.4x Speedup]
    ```

- **[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions**
  - **tags:** [ai], [reinforcement learning], [Bayesian Reinforcement Learning, Meta-Reinforcement Learning, Generalised Linear Models, Learnable Basis Functions, Variational Inference]
  - **authors:** Jingyang You, Hanna Kurniawati
  - **institution:** Australian National University
  - **link:** https://arxiv.org/pdf/2512.20974
  - **contributions:** 1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]
    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]
    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]
    ```

- **[arXiv251225] Automatic Replication of LLM Mistakes in Medical Conversations**
  - **tags:** [nlp], [llm evaluation], [medical conversation, mistake replication, benchmark creation, llm judges, single-shot qa]
  - **authors:** Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu
  - **institution:** Lumos AI
  - **link:** https://arxiv.org/pdf/2512.20983
  - **contributions:** 1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Automatic Replication of LLM Mistakes in Medical Conversations] --> B(核心问题/Problem: LLM错误难以在其他模型中复现/Mistakes hard to replicate across LLMs)
    A --> C(主要方法/Method: MedMistake自动管道/MedMistake automatic pipeline)
    A --> D(关键结果/Results: 发布基准并评估12个LLM/Released benchmark & evaluated 12 LLMs)
    C --> C1(生成对话/Generate conversations)
    C --> C2(LLM委员会评估/LLM committee evaluation)
    C --> C3(创建单轮QA对/Create single-shot QA pairs)
    D --> D1(MedMistake-All数据集/MedMistake-All dataset)
    D --> D2(MedMistake-Bench验证子集/MedMistake-Bench validated subset)
    D --> D3(GPT/Claude/Grok表现最佳/GPT/Claude/Grok performed best)
    ```

- **[arXiv251225] A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines**
  - **tags:** [mlsys], [agent system], [LangChain, Hyperledger Fabric, MCP (Model Context Protocol), permissioned blockchain, multi-agent system]
  - **authors:** Salman Jan, Hassan Ali Razzaqi, Ali Akarma, Mohammad Riyaz Belgaum
  - **institution:** Arab Open University-Bahrain, Islamic University of Madinah
  - **link:** https://arxiv.org/pdf/2512.20985
  - **contributions:** 1. Proposed a novel architecture integrating a LangChain-based multi-agent system with a permissioned blockchain for monitoring and auditing agentic AI actions. 2. Introduced a framework linking the perception-reasoning-action cycle to a blockchain governance layer for input verification, action evaluation, and outcome logging. 3. Demonstrated the framework's applicability and performance through experiments in smart inventory management, traffic-signal control, and healthcare monitoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24b6daf67b596733e755359a781c7506180a1c98a12a8eab70c8fd627d718b47_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a blockchain-monitored architecture for agentic AI systems to address trust and oversight concerns. The method integrates a LangChain multi-agent system with a Hyperledger Fabric blockchain to create an auditable perception-reasoning-action pipeline. The results show the framework effectively prevents unauthorized actions, provides full decision traceability, and maintains acceptable operational latency.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[论文标题 / Paper Title<br>A Blockchain-Monitored Agentic AI Architecture] --> B(核心问题 / Problem<br>Agentic AI缺乏信任与审计 / Lack of Trust & Auditability)
        A --> C(主要方法 / Method<br>区块链监控的LangChain多智能体架构 / Blockchain-Monitored LangChain Multi-Agent Architecture)
        A --> D(关键结果 / Results<br>有效安全验证与可追溯性 / Effective Security Verification & Traceability)
    ```

- **[arXiv251225] FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning**
  - **tags:** [mlsys], [agent system], [multi-agent architecture, substitution graph, price-aware optimization]
  - **authors:** Toqeer Ali Syed, Abdulaziz Alshahrani, Ali Ullah, Ali Akarma, Sohail Khan, Muhammad Nauman, Salman Jan
  - **institution:** Islamic University of Madinah, Effat University, Arab Open University
  - **link:** https://arxiv.org/pdf/2512.20991
  - **contributions:** 1. Proposes a novel agentic AI framework that integrates personal finance management with real-time diet optimization, addressing a gap in current tools. 2. Implements a modular multi-agent architecture with specialized agents (budgeting, nutrition, price monitoring, health personalization) that share a knowledge base and use a substitution graph for cost-effective meal planning. 3. Demonstrates through a case study simulation significant cost reduction (12-18%) and high nutrient adequacy (>95%) while maintaining robustness to market price fluctuations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc1ddd8fc13f7437534fa058d5e3879ce0f82c07898d64ccb7548f0932948a8a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FinAgent, an agentic AI framework that combines personal finance and real-time price data to generate nutritionally adequate meal plans at minimal cost. It uses a modular multi-agent system and a substitution graph to dynamically adapt to budget constraints and market changes. The system was validated in a simulation, showing significant cost savings and high nutritional adequacy, aligning with sustainable development goals.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[FinAgent] --> B[核心问题/Problem: Limited budgets & nutritional demands with fluctuating food prices]
        A --> C[主要方法/Method: Price-aware agentic AI with multi-agent architecture & substitution graph]
        A --> D[关键结果/Results: 12-18% cost reduction, >95% nutrient adequacy, robust to price changes]
    ```

- **[arXiv251225] TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control**
  - **tags:** [mlsys], [agent system], [traffic simulation, LLM agent, hierarchical framework, MCP control, autonomous decision-making]
  - **authors:** Yuwei Du, Jun Zhang, Jie Feng, Zhicheng Liu, Jian Yuan, Yong Li
  - **institution:** Tsinghua University, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.20996
  - **contributions:** 1. Proposes TrafficSimAgent, a novel LLM-based hierarchical agent framework designed to lower the barrier for conducting traffic simulations by automating experiment design and decision optimization. 2. Introduces a cross-level collaboration mechanism where high-level agents interpret natural language instructions and plan workflows, while low-level agents make real-time, condition-based action selections for fundamental elements. 3. Demonstrates through extensive experiments that the framework effectively executes simulations under various conditions, handles ambiguous instructions, and achieves superior performance compared to other systems and SOTA LLM-based methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f663746265410a8f10fffee2372e39160e0e5f92969f9c3f7f502da4d3657fd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TrafficSimAgent, a hierarchical LLM-based agent framework that automates traffic simulation tasks by using high-level agents for natural language instruction comprehension and workflow planning, and low-level agents for real-time action optimization. The system is designed to make powerful simulation platforms like SUMO more accessible to non-experts. Experiments show it effectively executes simulations, handles ambiguous instructions, and outperforms existing methods.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[TrafficSimAgent] --> B[核心问题/Problem: 现有交通仿真平台使用门槛高/High barrier to using traffic simulators]
    A --> C[主要方法/Method: 分层LLM智能体框架/Hierarchical LLM Agent Framework]
    A --> D[关键结果/Results: 有效执行仿真，处理模糊指令，性能优越/Executes simulations effectively, handles ambiguous instructions, superior performance]
    ```

- **[arXiv251225] Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation**
  - **tags:** [mlsys], [llm training], [knowledge distillation, chain-of-thought, sequence truncation, training efficiency, reasoning models]
  - **authors:** Wei-Rui Chen, Vignesh Kothapalli, Ata Fatahibaarzi, Hejian Sang, Shao Tang, Qingquan Song, Zhipeng Wang, Muhammad Abdul-Mageed
  - **institution:** The University of British Columbia, LinkedIn
  - **link:** https://arxiv.org/pdf/2512.21002
  - **code:** https://github.com/weiruichen01/distilling-the-essence
  - **contributions:** 1. Analysis of supervision allocation in reasoning distillation, showing the CoT segment is the dominant factor for transferring reasoning capability. 2. Establishment of a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. 3. Empirical demonstration that training on only the first 50% of tokens retains ~94% of performance while halving computational costs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a99e2da19bbc9bacf5104e37b4afd860b26a5285dd752f9a6e025702d930839_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational expense of distilling reasoning capabilities from large to small models over long sequences. It proposes a method of selective distillation and sequence truncation, focusing on early reasoning tokens. The key finding is that training on just the first half of tokens can preserve most performance while significantly reducing training time, memory, and FLOPs.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Distilling the Essence<br>高效推理蒸馏] --> B{核心问题/Problem};
        A --> C{主要方法/Method};
        A --> D{关键结果/Results};
        B --> B1[长序列推理蒸馏计算昂贵<br>Long-Sequence Reasoning Distillation is Expensive];
        C --> C1[选择性监督与序列截断<br>Selective Supervision & Sequence Truncation];
        D --> D1[保留94%性能，减少50%成本<br>Retain 94% Performance, Reduce 50% Cost];
    ```

- **[arXiv251225] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics**
  - **tags:** [ai], [llm evaluation], [Competitive Swiss-System Dynamics, Expected Win Score, Failure Sensitivity Analysis, Monte Carlo Simulation, risk appetite]
  - **authors:** Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong
  - **institution:** ByteDance Seed, Carnegie Mellon University, Columbia University
  - **link:** https://arxiv.org/pdf/2512.21010
  - **contributions:** 1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]
    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]
    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]
    ```

- **[arXiv251225] Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Supervised Fine-Tuning, Chain-of-Thought, Two-Stage Training, Attention Imbalance, Key Answer Tokens]
  - **authors:** Xiaofeng Shi, Qian Kou, Yuduo Li, Hua Zhou
  - **institution:** Beijing Academy of Artificial Intelligence (BAAI), Beijing Jiaotong University (BJTU)
  - **link:** https://arxiv.org/pdf/2512.21017
  - **contributions:** 1. Identifies a key limitation in conventional SFT where models over-attend to lengthy Chain-of-Thought reasoning sequences at the expense of the shorter, critical final answer tokens. 2. Proposes SFTKey, a novel two-stage fine-tuning scheme that first applies conventional SFT for format learning, then fine-tunes only on the Key (final answer) portion to boost accuracy. 3. Demonstrates through extensive experiments that SFTKey achieves an average accuracy improvement of over 5% compared to standard SFT while maintaining correct output formatting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eab786d7e6ac187d45153b771fdc458d7c8434c0e133a9bcdb70a2afa441a73_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that standard Supervised Fine-Tuning (SFT) for LLMs can cause an attention imbalance, where models focus too much on long reasoning chains (CoT) and not enough on the final answer. To solve this, the authors propose SFTKey, a two-stage method that first does standard SFT for formatting, then fine-tunes only on the key answer tokens. Experiments show this approach improves average accuracy by over 5% without harming output format correctness.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title<br>Rethinking Supervised Fine-Tuning] --> B[核心问题 / Problem<br>注意力失衡于长推理链 / Attention Imbalance on Long CoT]
    A --> C[主要方法 / Method<br>两阶段训练 SFTKey / Two-Stage Training SFTKey]
    A --> D[关键结果 / Results<br>准确率提升>5% / Accuracy Improvement >5%]
    ```

- **[arXiv251225] Policy-Conditioned Policies for Multi-Agent Task Solving**
  - **tags:** [ai], [multi-agent reinforcement learning], [Program Equilibrium, Programmatic Iterated Best Response (PIBR), policy-conditioning, large language models, textual gradients]
  - **authors:** Yue Lin, Shuhui Zhu, Wenhao Li, Ang Li, Dan Qiao, Pascal Poupart, Hongyuan Zha, Baoxiang Wang
  - **institution:** The Chinese University of Hong Kong, Shenzhen; University of Waterloo; Tongji University; Vector Institute
  - **link:** https://arxiv.org/pdf/2512.21024
  - **contributions:** 1. Proposes a paradigm shift by representing agent policies as human-interpretable source code, bridging the gap between opaque neural policies and the need for strategy comprehension in multi-agent settings. 2. Introduces Programmatic Iterated Best Response (PIBR), a novel algorithm that uses LLMs as point-wise best-response operators to synthesize and refine policy code based on game utility and unit tests. 3. Operationalizes the game-theoretic concept of Program Equilibrium for modern learning, demonstrating its effectiveness on coordination games and a cooperative foraging environment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8665c80d59a606e3c0796d224d372080c9552f5b48a9e7e797a73cad25b1b7e7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of dynamic strategy adaptation in multi-agent tasks by representing policies as interpretable source code and using Large Language Models (LLMs) to optimize them. The core method, Programmatic Iterated Best Response (PIBR), leverages LLMs to iteratively refine an agent's policy code in response to an opponent's strategy using textual feedback. The approach successfully solves standard coordination games and a cooperative environment, demonstrating a practical bridge between theoretical Program Equilibrium and modern AI learning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Policy-Conditioned Policies for Multi-Agent Task Solving] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[策略表示瓶颈/Representational Bottleneck]
    B --> B2[无法直接条件化对手策略/Cannot Condition on Opponent's Policy]
    C --> C1[程序化策略表示/Programmatic Policy Representation]
    C --> C2[LLM作为近似解释器/LLM as Approximate Interpreter]
    C --> C3[程序化迭代最佳响应/PIBR Algorithm]
    D --> D1[解决协调矩阵游戏/Solves Coordination Games]
    D --> D2[解决合作觅食环境/Solves Cooperative Foraging]
    ```

- **[arXiv251225] DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors**
  - **tags:** [cv], [3D human pose estimation], [3D sign language reconstruction, biomechanical accuracy, hand and body pose priors, monocular video, SMPL-X]
  - **authors:** Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov
  - **institution:** Monash University, TCS Research
  - **link:** https://arxiv.org/pdf/2512.21054
  - **code:** https://github.com/kaustesseract/DexAvatar
  - **contributions:** 1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]
        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]
        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]
    ```

- **[arXiv251225] Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics**
  - **tags:** [ai], [deep learning theory], [scaling laws, feature learning, infinite-depth limit, ResNets, hyperparameter transfer]
  - **authors:** Zihan Yao, Ruoyu Wu, Tianxiang Gao
  - **institution:** DePaul University, Iowa State University
  - **link:** https://arxiv.org/pdf/2512.21075
  - **contributions:** 1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Understanding Scaling Laws via Feature Learning Dynamics] --> B[核心问题/Problem: Scaling laws describe success but not when/why scaling fails]
    A --> C[主要方法/Method: Derive Neural Feature Dynamics (NFD) in infinite-width & depth limit]
    A --> D[关键结果/Results: Explains diminishing returns, proposes depth-aware LR correction]
    ```

- **[arXiv251225] Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation**
  - **tags:** [mlsys], [agent system], [Agentic AI, SHAP, Large Language Model, Iterative Refinement, Bias-Variance Trade-off]
  - **authors:** Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura
  - **institution:** Gifu University, Leibniz Centre for Agricultural Landscape Research (ZALF), Brandenburg University of Technology Cottbus–Senftenberg, Kyoto University
  - **link:** https://arxiv.org/pdf/2512.21066
  - **contributions:** 1. Proposes a novel Agentic XAI framework integrating SHAP-based explainability with multimodal LLM-driven iterative refinement for generating progressively enhanced explanations. 2. Demonstrates the framework's application and evaluation in a real-world agricultural recommendation system using rice yield data. 3. Identifies a bias-variance trade-off in iterative refinement, showing that early stopping (regularization) is crucial for optimizing explanation quality, challenging assumptions of monotonic improvement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e69e3950a2d09bc883a9cee931300bdfbeacc4e1e6094150a08db35366449e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an Agentic Explainable AI (XAI) framework that combines SHAP analysis with iterative refinement by a multimodal Large Language Model (LLM) to generate better explanations. The framework was tested as an agricultural recommendation system, and evaluations by both human experts and LLMs showed that explanation quality improved over initial rounds but declined with excessive refinement, revealing a bias-variance trade-off. The findings indicate that strategic early stopping is necessary to optimize the practical utility of such agentic XAI systems.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Agentic XAI Approach] --> B[核心问题/Problem: 向非专业人士解释XAI输出困难/Hard to communicate XAI outputs to laypersons]
        A --> C[主要方法/Method: SHAP + 多模态LLM迭代优化/SHAP + Multimodal LLM Iterative Refinement]
        A --> D[关键结果/Results: 早期迭代提升质量，过度优化导致下降/Early rounds improve quality, excessive refinement causes drop]
    ```

- **[arXiv251225] LLM Personas as a Substitute for Field Experiments in Method Benchmarking**
  - **tags:** [ai], [algorithmic fairness & evaluation], [field experiments, A/B testing, LLM personas, algorithmic benchmarking, information-theoretic bounds]
  - **authors:** Enoch Hyunwook Kang
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21080
  - **contributions:** 1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LLM Personas as a Substitute for Field Experiments] --> B[核心问题/Problem: Field experiments are costly and slow, hindering iterative method development.]
    A --> C[主要方法/Method: Use LLM-based persona simulation as a cheap synthetic benchmark under specific conditions.]
    A --> D[关键结果/Results: Formal validity conditions proven; sample-size bounds derived for decision-relevance.]
    ```

- **[arXiv251225] TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars**
  - **tags:** [cv], [3D avatar generation], [3D Gaussian Splatting, analytic rigging, texel-space deformation, hybrid representation, head reenactment]
  - **authors:** Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo
  - **institution:** KAIST, Hanyang University
  - **link:** https://arxiv.org/pdf/2512.21099
  - **contributions:** 1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[TexAvatars] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法泛化性差/Existing methods generalize poorly]
        B --> B2[难以处理极端表情与姿态/Struggle with extreme expressions & poses]
        C --> C1[混合表示/Hybrid Representation]
        C --> C2[UV空间预测，3D网格驱动/UV-space prediction, 3D mesh-driven deformation]
        D --> D1[泛化能力提升/Improved generalization]
        D --> D2[高保真细节/High-fidelity details]
        D --> D3[状态领先性能/State-of-the-art performance]
    ```

- **[arXiv251225] Semi-Supervised Learning for Large Language Models Safety and Content Moderation**
  - **tags:** [nlp], [content moderation], [semi-supervised learning, data augmentation, safety classifiers, LLM safety, prompt harmfulness]
  - **authors:** Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea
  - **institution:** National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.21107
  - **contributions:** 1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --> B[核心问题 / Problem<br>依赖大量标注数据 / Reliance on large labeled data]
    A --> C[主要方法 / Method<br>半监督学习与任务特定增强 / SSL & Task-Specific Augmentation]
    A --> D[关键结果 / Results<br>性能显著提升 / Significant Performance Improvement]
    ```

- **[arXiv251225] Semantic Refinement with LLMs for Graph Representations**
  - **tags:** [ai], [graph representation learning], [graph neural network, large language model, semantic refinement, structure-semantics heterogeneity, data-centric adaptation]
  - **authors:** Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang
  - **institution:** University of Connecticut, University of Notre Dame
  - **link:** https://arxiv.org/pdf/2512.21106
  - **contributions:** 1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Semantic Refinement with LLMs for Graph Representations] --> B(核心问题/Problem: Graph structure-semantics heterogeneity 图的结构-语义异质性)
    A --> C(主要方法/Method: Data-Adaptive Semantic Refinement (DAS) framework 数据自适应语义精炼框架)
    A --> D(关键结果/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs 提升结构主导图性能，在语义丰富图上保持竞争力)
    ```

- **[arXiv251225] STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting**
  - **tags:** [ai], [diffusion models], [precipitation nowcasting, latent diffusion model, spatio-temporal prediction, variational autoencoder, conditioning network]
  - **authors:** Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong
  - **institution:** The Hong Kong University of Science and Technology, Hong Kong Observatory
  - **link:** https://arxiv.org/pdf/2512.21118
  - **code:** https://github.com/sqfoo/stldm_official
  - **contributions:** 1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]
    A --> C[主要方法/Method: 两阶段潜扩散模型]
    A --> D[关键结果/Results: 性能优越，推理高效]
    ```

- **[arXiv251225] Beyond Context: Large Language Models Failure to Grasp Users Intent**
  - **tags:** [nlp], [ai safety], [intent recognition, contextual understanding, safety circumvention, prompt engineering, transformer architectures]
  - **authors:** Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21110
  - **contributions:** 1. Identifies and empirically demonstrates a critical vulnerability in LLMs: their inability to understand user intent and context, which allows safety mechanisms to be circumvented. 2. Evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows that exploitation techniques like emotional framing and progressive revelation are effective, and that reasoning capabilities can amplify this risk. 3. Proposes a paradigmatic shift in AI safety design, arguing for contextual understanding and intent recognition to be core capabilities rather than post-hoc protective mechanisms.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a fundamental vulnerability in Large Language Models (LLMs): their lack of contextual understanding and intent recognition, which allows safety mechanisms to be systematically bypassed. The authors empirically evaluate several LLMs, showing they can be exploited through techniques like emotional framing, and find that reasoning capabilities often worsen the problem. They conclude that a paradigm shift is needed to build intent recognition directly into LLM architectures for safety.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Beyond Context: Large Language Models Failure to Grasp Users Intent] --> B[核心问题/Problem: LLMs缺乏上下文和意图理解能力/LLMs lack contextual understanding & intent recognition]
    A --> C[主要方法/Method: 对多种LLM进行经验性评估/Empirical evaluation of multiple LLMs]
    A --> D[关键结果/Results: 安全机制可被系统规避，需范式转变/Safety mechanisms can be systematically circumvented, requiring a paradigm shift]
    B --> E[导致可利用的漏洞/Creates exploitable vulnerabilities]
    C --> F[使用情感框架、渐进揭示等技术/Using emotional framing, progressive revelation, etc.]
    D --> G[Claude Opus 4.1部分例外，推理能力加剧风险/Claude Opus 4.1 partial exception, reasoning amplifies risk]
    ```

- **[arXiv251225] A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care**
  - **tags:** [nlp], [clinical nlp], [medication safety review, large language models, real-world evaluation, failure analysis, electronic health records]
  - **authors:** Oliver Normand, Esther Borsi, Mitch Fruin, Lauren E Walker, Jamie Heagerty, Chris C. Holmes, Anthony J Avery, Iain E Buchan, Harry Coppock
  - **institution:** i.AI (Department for Science, Innovation, and Technology), University of Liverpool, University of Oxford, University of Nottingham, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21127
  - **contributions:** 1. Conducted the first real-world evaluation of an LLM-based medication safety review system on a large-scale NHS primary care dataset. 2. Performed a detailed failure analysis, identifying five primary patterns of LLM reasoning failures in clinical contexts (e.g., overconfidence, lack of contextual adaptation). 3. Provided a public dataset of 45 detailed clinical vignettes that comprehensively document all identified failure cases for further study.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a25d52747ed1a867914e2ab484a381e6c4f16c161178ce43bc0d582517885647_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the performance of a large language model (LLM) system for medication safety reviews using real NHS primary care data. The study found that while the LLM was highly sensitive in detecting issues, it correctly identified all issues and interventions in less than half of the patients, with failures primarily stemming from contextual reasoning errors rather than a lack of medical knowledge. The work highlights critical shortcomings in LLM reasoning for clinical deployment and calls for more rigorous, real-world evaluations.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs缺乏真实临床评估/LLMs lack real clinical evaluation]
        C --> C1[回顾性研究 & 专家评审/Retrospective study & Expert review]
        D --> D1[高敏感度但低完全正确率/High sensitivity but low perfect accuracy]
        D --> D2[失败源于情境推理/Failures from contextual reasoning]
    ```

- **[arXiv251225] AutoBaxBuilder: Bootstrapping Code Security Benchmarking**
  - **tags:** [sec], [code security evaluation], [automated benchmarking, LLM-generated code, security vulnerabilities, exploit generation, plausibility checks]
  - **authors:** Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev
  - **institution:** ETH Zurich, Snyk, INSAIT (Sofia University "St. Kliment Ohridski")
  - **link:** https://arxiv.org/pdf/2512.21132
  - **code:** https://github.com/eth-sri/autobaxbuilder
  - **contributions:** 1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]
    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]
    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]
    ```

- **[arXiv251225] MODE: Multi-Objective Adaptive Coreset Selection**
  - **tags:** [mlsys], [others], [coreset selection, submodular maximization, data efficiency, adaptive weighting, multi-objective optimization]
  - **authors:** Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui
  - **institution:** CRIL, Université d'Artois
  - **link:** https://arxiv.org/pdf/2512.21152
  - **contributions:** 1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MODE: Multi-Objective Adaptive Coreset Selection] --> B[核心问题/Problem: Static coreset selection methods cannot adapt to changing data utility during training.]
    A --> C[主要方法/Method: Dynamic, multi-objective framework that adaptively weights selection strategies based on training phase.]
    A --> D[关键结果/Results: (1-1/e)-approximation, O(n log n) complexity, reduced memory, interpretable insights.]
    ```

- **[arXiv251225] TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [CLIP, multimodal fusion, parameter-efficient fine-tuning, vision-language alignment, anatomical structure]
  - **authors:** Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu
  - **institution:** Wuhan University (Inferred from authors Gaoren Lin, Lefei Zhang, Bo Du, Wentao Zhu, who are known to be affiliated with Wuhan University. Huangxuan Zhao and Yuan Xiong are likely from the same group.)
  - **link:** https://arxiv.org/pdf/2512.21135
  - **contributions:** 1. Proposes a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch to preserve fine-grained anatomical structures. 2. Introduces a Domain-Augmented Text Encoder (DATE) that injects medical knowledge from large language models to better model complex clinical descriptions. 3. Designs a Vision-Language Calibration Module (VLCM) to refine cross-modal correspondence in a unified feature space, addressing domain-specific semantic misalignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TGC-Net, a parameter-efficient CLIP-based framework for text-guided medical image segmentation. It addresses CLIP's limitations in medical imaging by introducing modules for structural refinement, medical knowledge injection, and cross-modal calibration. Experiments on five datasets show state-of-the-art performance with fewer trainable parameters.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[TGC-Net: Text-Guided Medical Image Segmentation<br>文本引导的医学图像分割] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
    
        B --> B1[CLIP在医学领域应用受限<br>CLIP Limitations in Medical Domain]
        B1 --> B2[结构细节丢失<br>Loss of Fine-grained Structure]
        B1 --> B3[复杂文本建模不足<br>Inadequate Text Modeling]
        B1 --> B4[领域语义未对齐<br>Domain Semantic Misalignment]
    
        C --> C1[语义-结构协同编码器 SSE<br>Semantic-Structural Synergy Encoder]
        C --> C2[领域增强文本编码器 DATE<br>Domain-Augmented Text Encoder]
        C --> C3[视觉-语言校准模块 VLCM<br>Vision-Language Calibration Module]
    
        D --> D1[在5个数据集上SOTA<br>SOTA on 5 Datasets]
        D --> D2[参数高效<br>Parameter-Efficient]
        D --> D3[Dice分数显著提升<br>Notable Dice Gains]
    ```

- **[arXiv251225] BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft**
  - **tags:** [sys], [distributed consensus], [Raft, election timeout, contextual bandits, LinUCB, fault tolerance]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.21165
  - **contributions:** 1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft] --> B(核心问题/Problem: Brittle randomized timeouts under long-tail latency & jitter)
    A --> C(主要方法/Method: Lightweight online adaptation with contextual bandits & safe exploration)
    A --> D(关键结果/Results: Reduces recovery/unwritable time in WAN, competitive in stable settings)
    ```

- **[arXiv251225] Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation**
  - **tags:** [cv], [robot navigation], [zero-shot object navigation, trajectory-conditioned 3D imagination, occlusion-aware planning]
  - **authors:** Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu
  - **institution:** Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.21201
  - **code:** https://heyu322.github.io/Schrodinger-Navigator.github.io/
  - **contributions:** 1. Proposed Schrödinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Schrödinger's Navigator] --> B[核心问题/Problem: ZSON struggles with occlusions & uncertainty]
        A --> C[主要方法/Method: Trajectory-conditioned 3D imagination of futures]
        A --> D[关键结果/Results: Outperforms baselines on real robot]
    ```

- **[arXiv251225] SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation**
  - **tags:** [ai], [speech representation learning], [meta-learning, bi-level optimization, few-shot adaptation, self-supervised learning, speech representation]
  - **authors:** Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux
  - **institution:** Meta AI, ENS-PSL, EHESS, CNRS
  - **link:** https://arxiv.org/pdf/2512.21204
  - **code:** https://github.com/facebookresearch/spidr-adapt
  - **contributions:** 1. Introduces the Multi-task Adaptive Pre-training (MAdaPT) protocol, framing few-shot speech representation learning as a bi-level optimization meta-learning problem. 2. Proposes a novel First-Order Bi-Level Optimization (FOBLO) heuristic to enable scalable meta-training by avoiding heavy computation costs. 3. Stabilizes meta-training with a robust initialization technique using interleaved supervision that alternates between self-supervised and supervised objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff9692c36cbda26291fde2551256e229a90f6eb51f087d833a2d84cb4b10925b_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SpidR-Adapt, a method for rapid adaptation of speech representation models to new languages using minimal unlabeled data. It formulates the problem as meta-learning with a bi-level optimization framework (MAdaPT), proposes an efficient solver (FOBLO), and uses interleaved supervision for stable training. The model achieves significant gains in phonemic discrimination and language modeling after training on less than 1 hour of target-language audio, demonstrating over 100x greater data efficiency than standard methods.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[SpidR-Adapt] --> B[核心问题/Problem: 数据效率差距/Data-Efficiency Gap]
        A --> C[主要方法/Method: 元学习与双层优化/Meta-Learning & Bi-Level Optimization]
        A --> D[关键结果/Results: 100倍数据效率/100x Data Efficiency]
        B --> B1[婴儿高效 vs. 模型低效/Infant Efficiency vs. Model Inefficiency]
        C --> C1[MAdaPT协议/MAdaPT Protocol]
        C --> C2[FOBLO优化/FOBLO Optimization]
        C --> C3[交错监督/Interleaved Supervision]
        D --> D1[<1h音频/<1h Audio]
        D --> D2[音素可辨性提升/Improved Phonemic Discriminability]
    ```

- **[arXiv251225] RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic**
  - **tags:** [mlsys], [agent system], [runtime safety guardrail, executable safety logic, hybrid reasoning, temporal safety predicate, context-aware safety predicate]
  - **authors:** Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu
  - **institution:** Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University
  - **link:** https://arxiv.org/pdf/2512.21220
  - **contributions:** 1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --> B[核心问题/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]
        A --> C[主要方法/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]
        A --> D[关键结果/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]
    ```

- **[arXiv251225] Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval**
  - **tags:** [cv], [image-text retrieval], [event-centric entity extraction, BM25, BEiT-3, two-stage retrieval]
  - **authors:** Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen
  - **institution:** University of Science - VNUHCM
  - **link:** https://arxiv.org/pdf/2512.21221
  - **code:** https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval
  - **contributions:** 1. Proposes a lightweight two-stage retrieval pipeline for event-based image retrieval. 2. Leverages event-centric entity extraction to incorporate temporal and contextual signals for efficient candidate filtering. 3. Combines BM25-based filtering with BEiT-3 reranking to achieve high accuracy on the OpenEvents benchmark.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of retrieving images from natural language descriptions in complex, real-world scenarios. It proposes a two-stage method that first filters candidates using BM25 on extracted event entities, then reranks them with a BEiT-3 model. The approach significantly outperforms prior baselines on the OpenEvents benchmark, demonstrating the effectiveness of combining lightweight entity guidance with deep multimodal modeling.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval] --> B[核心问题/Problem: Real-world image-text retrieval is challenging due to vague queries and scalability needs.]
    A --> C[主要方法/Method: Two-stage pipeline with event-centric entity extraction, BM25 filtering, and BEiT-3 reranking.]
    A --> D[关键结果/Results: Achieves high mean average precision (0.559) on OpenEvents v1, outperforming baselines.]
    ```

- **[arXiv251225] Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking**
  - **tags:** [sec], [llm security], [jailbreaking, malicious code generation, prompt engineering, time-division selection, security alignment]
  - **authors:** Yifan Huang, Xiaojun Jia, Wenbo Guo, Yuqiang Sun, Yihao Huang, Chong Wang, Yang Liu
  - **institution:** Nanyang Technological University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.21236
  - **contributions:** 1. Proposes SPELL, a novel testing framework specifically designed to evaluate security alignment weaknesses in LLMs for malicious code generation. 2. Introduces a time-division selection strategy to systematically construct jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset. 3. Conducts extensive evaluation across multiple advanced code models and real-world tools, revealing significant security gaps and providing insights for improving AI safety.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8055c0aba333e58d26f29d81acab1f88f570f09122f7fafd38ac1c952dad67b1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the security risk of LLMs being exploited to generate malicious code, a gap in existing jailbreaking research. It proposes the SPELL framework, which uses a time-division strategy to construct effective jailbreaking prompts. The evaluation shows high attack success rates across several models, revealing critical vulnerabilities in current AI safety alignments for code generation.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[SPELL: Sentence Pairing Exploration for LLM Limitation-breaking] --> B[核心问题/Problem: LLMs可能被用于生成恶意代码/LLMs can be exploited for malicious code generation]
        A --> C[主要方法/Method: 基于时间划分选择的提示构建框架/Time-division selection prompt construction framework]
        A --> D[关键结果/Results: 在多模型上实现高攻击成功率/High attack success rates across multiple models]
    ```

- **[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks**
  - **tags:** [sec], [adversarial attacks], [hard-label black-box attacks, query efficiency, ray search optimization, Nesterov's Accelerated Gradient, momentum-based optimization]
  - **authors:** Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma
  - **institution:** Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments
  - **link:** https://arxiv.org/pdf/2512.21241
  - **code:** https://github.com/machanic/hard_label_attacks
  - **contributions:** 1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]
    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]
    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]
    ```

- **[arXiv251225] LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation**
  - **tags:** [ai], [embodied ai], [scene graph, vision language model, dynamic planning, memory graph, graph augmentation]
  - **authors:** Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov
  - **institution:** MIRAI, Cognitive AI Systems Lab
  - **link:** https://arxiv.org/pdf/2512.21243
  - **code:** https://lookplangraph.github.io/
  - **contributions:** 1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];
    A --> C[主要方法/Method: Dynamic graph update via VLM observation];
    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];
    ```

- **[arXiv251225] Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students**
  - **tags:** [ai], [educational technology], [learning analytics, correlation analysis, text mining, student perceptions, human-ai interaction]
  - **authors:** Gaia Ebli, Bianca Raimondi, Maurizio Gabbrielli
  - **institution:** University of Bologna
  - **link:** https://arxiv.org/pdf/2512.21246
  - **contributions:** 1. Investigated the interrelationships of four key learning factors (experience, clarity, comfort, motivation) in AI-augmented education, a gap in prior research. 2. Revealed a developmental moderator by comparing middle and high school students, finding holistic vs. differentiated evaluation patterns between age groups. 3. Established a foundation for age-specific AI integration strategies by showing perception dimensions actively mediate learning and their structure varies with developmental stage.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37485735380aa9bf03e242b5a0fe4f8958c120a691a693af44376fb7f27c2b0b_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how key learning factors relate to each other in AI-augmented programming education for middle and high school students. Using a multimethod analysis combining correlation analysis and text mining on classroom data, it finds that middle school students evaluate AI tools holistically, while high school students assess different factors independently. The conclusion is that the structure of student perceptions is moderated by developmental stage, which should inform age-appropriate AI integration strategies.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题: Learning Factors in AI-Augmented Education<br>Title: Learning Factors in AI-Augmented Education] --> B(核心问题/Problem: How do learning factor relationships vary by age in AI education?<br>核心问题/Problem: 学习因素关系在AI教育中如何随年龄变化？)
    A --> C(主要方法/Method: Multimethod quantitative analysis (correlation & text mining)<br>主要方法/Method: 多方法定量分析（相关性与文本挖掘）)
    A --> D(关键结果/Results: Middle school: holistic evaluation; High school: differentiated evaluation<br>关键结果/Results: 初中生: 整体性评估; 高中生: 差异性评估)
    ```

- **[arXiv251225] SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance**
  - **tags:** [nlp], [document question answering], [Tree-LSTM, Memory Augmented Neural Network (MANN), Retrieval Augmented Generation (RAG), Parameter Efficiency, Fact Extraction]
  - **authors:** Divij Dudeja, Mayukha Pal
  - **institution:** ABB Ability Innovation Center, Indian Institute of Information Technology, Nagpur
  - **link:** https://arxiv.org/pdf/2512.21280
  - **contributions:** 1. Introduces a hierarchical, syntax-aware fact extractor (Grammarian Tree-LSTM) to parse engineering manuals into structured subject-relation-object triples., 2. Proposes a compact, indexed memory system (MANN) to store and retrieve extracted facts as vectors, enabling efficient knowledge access., 3. Designs a dual-mode inference system combining a fast path for known documents and a dynamic RAG-assisted path for new uploads, reducing hallucinations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fe5f40637f711007d6bb6875182fa80072536380614c5e93c7c4f5cf8dc2232_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of accurately answering questions from dense engineering manuals, where standard small language models fail. It proposes SMART, a structured model that hierarchically extracts facts, stores them in an indexed memory, and uses a transformer to generate answers from retrieved facts. The result is a parameter-efficient model that achieves higher accuracy with fewer parameters and reduced hallucinations compared to baselines like GPT-2.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[SMART SLM] --> B[核心问题/Problem: 工程手册难以阅读，现有小模型处理为扁平token流，导致错误答案/Engineering manuals are hard to read; flat token processing leads to incorrect answers]
    A --> C[主要方法/Method: 分层处理：语法感知事实提取器 + 索引记忆(MANN) + 6层Transformer/Hierarchical processing: Syntax-aware fact extractor + Indexed memory (MANN) + 6-layer Transformer]
    A --> D[关键结果/Results: 参数减少64-69%，准确率提升21.3%，减少幻觉/64-69% fewer parameters, 21.3% higher accuracy, reduced hallucinations]
    ```

- **[arXiv251225] Model Merging via Multi-Teacher Knowledge Distillation**
  - **tags:** [ai], [model merging], [model merging, knowledge distillation, PAC-Bayes, sharpness-aware minimization, multi-task learning]
  - **authors:** Seyed Arshan Dalili, Mehrdad Mahdavi
  - **institution:** The Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.21288
  - **code:** https://github.com/arshandalili/SAMerging
  - **contributions:** 1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a "cross-task heterogeneity" term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Model Merging via Multi-Teacher Knowledge Distillation] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[缺乏理论保证/Lack of Theoretical Guarantees]
    B --> B2[启发式方法不稳定/Heuristic Methods are Brittle]
    C --> C1[理论: 平坦性感知PAC-Bayes边界/Theory: Flatness-aware PAC-Bayes Bound]
    C --> C2[框架: 多教师知识蒸馏/Framework: Multi-Teacher Knowledge Distillation]
    C --> C3[方法: SAMerging/Method: SAMerging]
    D --> D1[新SOTA/New SOTA]
    D --> D2[高数据效率/High Data Efficiency]
    ```

- **[arXiv251225] Measuring all the noises of LLM Evals**
  - **tags:** [mlsys], [llm inference], [LLM evaluation, statistical noise, paired analysis, prediction variance, data variance]
  - **authors:** Sida Wang
  - **institution:** FAIR at Meta
  - **link:** https://arxiv.org/pdf/2512.21326
  - **contributions:** 1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the "all-pairs paired method" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an "all-pairs paired method" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Measuring all the noises of LLM Evals] --> B(核心问题/Problem: LLM评估中的统计噪声/Separating signal from noise in LLM evals)
    A --> C(主要方法/Method: 全配对分析法/All-pairs paired method)
    A --> D(关键结果/Results: 可预测的总噪声与主导的预测噪声/Predictable total noise & dominant prediction noise)
    ```

- **[arXiv251225] C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling**
  - **tags:** [nlp], [code retrieval], [Pooling by Multihead Attention (PMA), contrastive learning, code embedding, MTEB-Code, Qwen-2.5-Coder]
  - **authors:** Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang
  - **institution:** Ant Group, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21332
  - **code:** https://github.com/codefuse-ai/CodeFuse-Embeddings
  - **contributions:** 1. Proposes a Pooling by Multihead Attention (PMA) module to generate sequence embeddings from token embeddings, effectively utilizing the LLM's causal representations. 2. The PMA module aggregates information from all tokens in a sequence, overcoming the information bottleneck of traditional EOS-based sequence embeddings. 3. The approach supports flexible adaptation of embedding dimensions, serving as an alternative to Multi-Representation Learning (MRL).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27c6e6ad01eacb3bd759d1aafa323cd3f72410efd901b1df691e709d8fbe3a4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces C2LLM, a family of code embedding models built on Qwen-2.5-Coder backbones. It proposes a novel Pooling by Multihead Attention (PMA) module to create better sequence embeddings for code retrieval. The models, trained on three million data points, achieve state-of-the-art performance on the MTEB-Code benchmark, with the 7B version ranking first overall.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[C2LLM Technical Report] --> B[核心问题/Problem: 代码检索中的序列表示瓶颈/Sequence representation bottleneck in code retrieval]
        A --> C[主要方法/Method: 自适应交叉注意力池化 (PMA) / Adaptive Cross-Attention Pooling (PMA)]
        A --> D[关键结果/Results: 在MTEB-Code上SOTA / SOTA on MTEB-Code]
    ```

- **[arXiv251225] Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty**
  - **tags:** [mlsys], [diffusion models], [Denoising Entropy, Masked Diffusion Models, decoding path optimization, predictive uncertainty, non-autoregressive generation]
  - **authors:** Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin
  - **institution:** Zhejiang University, Westlake University, University of Chicago
  - **link:** https://arxiv.org/pdf/2512.21336
  - **code:** https://github.com/LINs-lab/DenoisingEntropy
  - **contributions:** 1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>通过量化不确定性优化掩码扩散模型的解码路径] --> B(核心问题/Problem: MDMs生成质量对解码顺序敏感<br/>MDM output quality is sensitive to decoding order)
        A --> C(主要方法/Method: 提出去噪熵和路径优化算法<br/>Propose Denoising Entropy & path optimization algorithms)
        A --> D(关键结果/Results: 熵引导方法提升生成质量<br/>Entropy-guided methods improve generation quality)
    ```

- **[arXiv251225] A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations**
  - **tags:** [other], [astro-physics, solar physics, magnetohydrodynamics], [Physics-Informed Neural Network (PINN), MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT), solar active regions (ARs), toroidal bands (toroids), state-vector reconstruction]
  - **authors:** Subhamoy Chatterjee, Mausumi Dikpati
  - **institution:** Southwest Research Institute, High Altitude Observatory (NSF-NCAR)
  - **link:** https://arxiv.org/pdf/2512.20747
  - **contributions:** 1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method's ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[PINNBARDS: 从全球活动区观测推导MHD状态向量 / PINNBARDS: Deriving MHD State Vectors From Global Active Regions Observations] --> B(核心问题/Problem: 表面磁图仅提供活动区分布的几何形状，无法提供初始化MHD模型所需的自洽状态向量 / Problem: Surface magnetograms only provide geometric shape of AR distribution, not the self-consistent state-vector needed to initialize MHD models.)
        A --> C(主要方法/Method: 开发PINNBARDS，一个基于物理信息神经网络(PINN)的模拟器，使用观测到的环形带和MHD-SWT方程来推导初始状态向量 / Method: Develop PINNBARDS, a PINN-based simulator using observed toroids and MHD-SWT equations to derive the initial state-vector.)
        A --> D(关键结果/Results: PINN收敛到物理一致解，与观测匹配；最佳参数为20-30 kG环形场和~10度带宽 / Results: PINN converges to physically consistent solutions matching observations; optimal parameters are 20-30 kG toroidal field and ~10 degree bandwidth.)
    ```

- **[arXiv251225] GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model**
  - **tags:** [ai], [speech separation], [target speaker extraction, generative language model, coarse-to-fine, exposure bias, direct preference optimization]
  - **authors:** Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng
  - **institution:** Nanyang Technological University, Southeast University
  - **link:** https://arxiv.org/pdf/2512.20978
  - **contributions:** 1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model's outputs with human perceptual preferences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[GenTSE] --> B[核心问题/Problem: TSE generalization & fidelity];
    A --> C[主要方法/Method: Two-stage generative LM, FLC, DPO];
    A --> D[关键结果/Results: Surpasses prior LM-based systems];
    ```

- **[arXiv251225] PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation**
  - **tags:** [ai], [materials informatics], [dynamical stability, phonon spectrum, crystal generation, benchmark, MatterSim]
  - **authors:** Xiao-Qi Han, Ze-Feng Gao, Peng-Jie Guo, Zhong-Yi Lu
  - **institution:** Renmin University of China
  - **link:** https://arxiv.org/pdf/2512.21227
  - **code:** https://github.com/xqh19970407/PhononBench
  - **contributions:** 1. Introduced PhononBench, the first large-scale benchmark for evaluating the dynamical stability of AI-generated crystal structures. 2. Leveraged the MatterSim interatomic potential to perform efficient, DFT-level phonon calculations on over 100k generated crystals, revealing a widespread deficiency in current models' ability to produce dynamically stable structures. 3. Identified and released a substantial dataset of 28,119 phonon-stable generated crystals, providing a valuable resource for future materials discovery.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5409c19ccb89ace0c03a71117cfd438b28def78d75777b331d87da44f49b7230_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces PhononBench, a benchmark that uses the MatterSim potential to efficiently evaluate the dynamical stability of AI-generated crystals. The analysis of over 100k structures from six leading models shows that current generative models perform poorly at ensuring dynamical stability, with an average success rate of only 25.83%. The work highlights a critical limitation in the field and provides a benchmark and a pool of stable candidate structures for future development.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[PhononBench: 晶体生成中的动力学稳定性基准] --> B(核心问题/Problem: 现有AI晶体生成模型缺乏对动力学稳定性的大规模评估)
    A --> C(主要方法/Method: 利用MatterSim势函数进行高通量声子计算)
    A --> D(关键结果/Results: 模型平均稳定性仅25.83%, 识别出28,119个稳定结构)
    ```

- **[arXiv251225] Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks**
  - **tags:** [ai], [large language models], [scaling laws, economic productivity, agentic workflows, compute scaling, algorithmic progress]
  - **authors:** Ali Merali
  - **institution:** Yale University
  - **link:** https://arxiv.org/pdf/2512.21316
  - **contributions:** 1. Derives empirical scaling laws linking LLM training compute to professional productivity gains. 2. Quantifies the relative contributions of increased compute (56%) versus algorithmic progress (44%) to annual productivity improvements. 3. Identifies a significant disparity in productivity gains between non-agentic analytical tasks and agentic workflows requiring tool use.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f148c34bb4d8aa66926afe66d00135fb826ae28f1253bfed833d9ff39c8b046d_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the relationship between LLM capabilities and professional productivity through a preregistered experiment with over 500 professionals. It finds that each year of AI progress reduces task time by 8%, driven by both compute and algorithmic scaling, but gains are larger for analytical tasks than for agentic ones. The results suggest continued model scaling could significantly boost U.S. productivity over the next decade.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Scaling Laws for Economic Productivity<br/>经济生产力缩放定律] --> B(核心问题/Problem: LLM compute vs. professional productivity<br/>LLM计算与专业生产力的关系);
    A --> C(主要方法/Method: Preregistered experiment with 500+ professionals using 13 LLMs<br/>使用13个LLM对500多名专业人员的预注册实验);
    A --> D(关键结果/Results: 8% annual time reduction, 56% compute vs. 44% algorithm gains, larger gains for non-agentic tasks<br/>每年任务时间减少8%，56%源于计算，44%源于算法，非智能体任务收益更大);
    ```
