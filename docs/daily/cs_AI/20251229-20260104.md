---
slug: /daily/csai/20251229-20260104
---
# 20251229-20260104 (cs.AI)

## 2025-12-29

- **[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation**
  - **tags:** [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]
  - **authors:** Santhosh Kumar Ravindran
  - **institution:** Microsoft Corporation
  - **link:** https://arxiv.org/pdf/2512.21351
  - **contributions:** 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as "genomes" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp
  - **Simple LLM Summary:** CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM代码生成缺乏适应性，难以应对API变化/LLM code generation lacks adaptability to API changes]
        C --> C1[将RL轨迹视为基因组进行进化操作/Treat RL trajectories as genomes for evolutionary operations]
        C --> C2[在夜间回放阶段进行突变与选择/Mutation and selection during nocturnal replay]
        D --> D1[解决方案新颖性提升35%/35% higher novelty in solutions]
        D --> D2[适应速度加快25%/25% faster adaptation]
    ```

- **[arXiv251229] EcoNet: Multiagent Planning and Control Of Household Energy Resources Using Active Inference**
  - **tags:** [mlsys], [agent system], [active inference, multi-agent systems, home energy management systems (HEMS), distributed energy resources (DER), Bayesian inference]
  - **authors:** John C. Boik, Kobus Esterhuysen, Jacqueline B. Hynes, Axel Constant, Ines Hipolito, Mahault Albarracin, Alex B. Kiefer, Karl Friston
  - **institution:** VERSES, University of Sussex, Macquarie University, UCL (University College London)
  - **link:** https://arxiv.org/pdf/2512.21343
  - **contributions:** 1. Proposes EcoNet, a novel Bayesian framework for household and neighborhood energy management based on active inference. 2. Addresses the challenge of planning under uncertainty (e.g., weather, solar forecasts) while handling complex, conditional, and conflicting household goals. 3. Demonstrates the approach through simulations for multiagent planning and control of distributed energy resources.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b885d3c6c9f392a494063522c79cde9a59fead8ab6b04010259b6485f007cec8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces EcoNet, a multiagent planning and control system for household energy resources using active inference, a Bayesian approach, to manage uncertainty and conflicting goals. The method aims to optimize energy use, costs, and emissions while maintaining comfort. Simulation results demonstrate its potential for improved energy management and coordination.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EcoNet: 多智能体家庭能源规划与控制 / EcoNet: Multiagent Household Energy Planning & Control] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[复杂且冲突的家庭目标 / Complex & Conflicting Household Goals]
        B --> B2[决策存在不确定性 / Decision-making Under Uncertainty]
        C --> C1[基于主动推理的贝叶斯方法 / Active Inference-based Bayesian Approach]
        C --> C2[多智能体规划与控制 / Multiagent Planning & Control]
        D --> D1[模拟结果展示 / Simulation Results Presented]
        D --> D2[改善能源管理与协调 / Improved Energy Management & Coordination]
    ```

- **[arXiv251229] Multi-Agent LLM Committees for Autonomous Software Beta Testing**
  - **tags:** [se], [automated software testing], [multi-agent system, large language model, vision-language model, consensus voting, beta testing]
  - **authors:** Sumanth Bharadwaj Hachalli Karanam, Dhiwahar Adhithya Kennady
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2512.21352
  - **contributions:** 1. A novel multi-agent committee framework that uses a three-round voting protocol for consensus-based decision-making in software testing. 2. Integration of vision-enabled LLMs and diverse testing personas to systematically explore and understand web application user interfaces. 3. Demonstrated significant performance improvements over single-agent baselines in task success, bug detection (F1 score), and security vulnerability coverage on established benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost of manual software beta testing and the limitations of single-agent LLM approaches by proposing a multi-agent committee framework. The method employs diverse, vision-enabled LLMs that collaborate through a structured voting protocol and persona-driven behavior to autonomously test web applications. The results show that this multi-agent approach significantly outperforms single-agent baselines in task success rates, bug detection, and security testing coverage, making it suitable for real-time CI/CD integration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Agent LLM Committees for Autonomous Software Beta Testing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动测试成本高，单智能体LLM存在幻觉/Manual testing costly, single-agent LLM hallucinates]
        C --> C1[多智能体委员会与三轮投票协议/Multi-agent committee & three-round voting]
        C --> C2[视觉LLM与角色多样性/Vision LLMs & persona diversity]
        D --> D1[任务成功率89.5%，超越基线/Task success 89.5%, beats baseline]
        D --> D2[动作延迟0.71秒，适合CI/CD/Action latency 0.71s, suitable for CI/CD]
        D --> D3[覆盖8/10 OWASP漏洞类别/Covers 8/10 OWASP Top 10]
    ```

- **[arXiv251229] Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software**
  - **tags:** [se], [software fairness], [correlation tuning, phi-coefficient, multi-objective optimization, pre-processing, bias mitigation]
  - **authors:** Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang
  - **institution:** King’s College London, National University of Defense Technology, Southern University of Science and Technology, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.21348
  - **contributions:** 1. Proposes a novel pre-processing bias mitigation method called Correlation Tuning (CoT) that adjusts data correlations. 2. Introduces the Phi-coefficient as an intuitive measure to quantify correlation between sensitive attributes and labels. 3. Employs multi-objective optimization to address proxy biases, demonstrating superior effectiveness over state-of-the-art methods in single and multiple attribute scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Correlation Tuning (CoT), a novel pre-processing method to mitigate bias in ML software by adjusting data correlations using the Phi-coefficient and multi-objective optimization. It frames fairness as a core software quality issue. Extensive evaluation shows CoT significantly improves performance for unprivileged groups and reduces key bias metrics, outperforming existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统公平研究忽视软件质量维度/Traditional fairness research neglects software quality dimension]
        B --> B2[预处理方法效果不足/Pre-processing methods lack effectiveness]
        C --> C1[提出相关性调优 (CoT)/Propose Correlation Tuning (CoT)]
        C --> C2[使用Phi系数量化相关性/Use Phi-coefficient to quantify correlation]
        C --> C3[采用多目标优化/Employ multi-objective optimization]
        D --> D1[提高弱势群体TPR 17.5%/Increase unprivileged group TPR by 17.5%]
        D --> D2[关键偏差指标降低 >50%/Key bias metrics reduced by >50%]
        D --> D3[超越SOTA方法 3-10个百分点/Outperform SOTA by 3-10 percentage points]
    ```

- **[arXiv251229] Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks**
  - **tags:** [db], [text-to-SQL], [unanswerable question detection, few-shot prompting, biomedical databases]
  - **authors:** Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger
  - **institution:** Zurich University of Applied Sciences, University of Zurich
  - **link:** https://arxiv.org/pdf/2512.21345
  - **contributions:** 1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem") --> P1("Text-to-SQL对不可回答查询生成可执行SQL/Text-to-SQL generates executable SQL for unanswerable queries")
        P1 --> P2("生物医学领域风险高/High risk in biomedical contexts")
        Method("主要方法/Method") --> M1("Query Carefully 管道/Query Carefully pipeline")
        M1 --> M2("LLM (llama3.3:70b) + 模式感知提示 + 少样本/LLM (llama3.3:70b) + schema-aware prompts + few-shot")
        M2 --> M3("包含可回答与不可回答示例/Includes answerable and unanswerable examples")
        Results("关键结果/Results") --> R1("构建OncoMX-NAQ基准/Built OncoMX-NAQ benchmark")
        R1 --> R2("不可回答检测准确率0.8/Unanswerable-detection accuracy 0.8")
        R2 --> R3("结构性问题检测好，语义模糊挑战大/Good for structural, challenging for semantic ambiguity")
    ```

- **[arXiv251229] A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers**
  - **tags:** [ai], [reinforcement learning], [Relevance-Zone Based Search, AlphaZero, Life-and-Death problems, heuristic search, pattern table]
  - **authors:** Chung-Chin Shih, Ti-Rong Wu, Ting Han Wei, Yu-Shan Hsu, Hung Guei, I-Chen Wu
  - **institution:** Academia Sinica, National Yang Ming Chiao Tung University, Kochi University of Technology
  - **link:** https://arxiv.org/pdf/2512.21365
  - **code:** https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ
  - **contributions:** 1. Applied and analyzed Relevance-Zone Based Search (RZS) and relevance-zone pattern tables to solve Go Life-and-Death problems, identifying critical relevance-zones. 2. Discovered that solvers can find rare patterns and even alternative solutions differing from established human grandmaster answers. 3. Identified and analyzed key limitations of current solvers, such as misjudging rare patterns and prioritizing direct survival over territory maximization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6376caf4e3dced23991e86eb4d5b0f512ce3623019adeaf18ee253d5fd00c507_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the performance of state-of-the-art computer Go solvers using Relevance-Zone Based Search on classic Life-and-Death problems. The study finds that while these solvers can identify critical areas and discover rare patterns, they exhibit limitations like misjudging pattern values and having a non-human preference for direct survival over territory. The authors suggest future approaches to address these solver issues.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers<br/>使用基于相关区域求解器解决围棋死活问题的研究"] --> B["核心问题/Problem<br/>Analyzing solver behavior on Go Life-and-Death problems<br/>分析求解器在围棋死活问题上的行为"]
        A --> C["主要方法/Method<br/>Using Relevance-Zone Based Search (RZS) and pattern tables<br/>使用基于相关区域的搜索和模式表"]
        A --> D["关键结果/Results<br/>Identifies relevance-zones, finds rare/alternative solutions, reveals solver limitations<br/>识别相关区域，发现罕见/替代解法，揭示求解器局限"]
    ```

- **[arXiv251229] From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration**
  - **tags:** [ai], [computational psychology], [multimodal large language model, multi-agent collaboration, cosine similarity, projective assessment, psychological report generation]
  - **authors:** Shuide Wen, Yu Sun, Beier Ku, Zhi Gao, Lijun Ma, Yang Yang, Can Jiao
  - **institution:** Tsinghua University, Shenzhen University, University of Oxford, Guangzhou University of Chinese Medicine, Harbin Institute of Technology, Shenzhen Institute of Education Sciences
  - **link:** https://arxiv.org/pdf/2512.21360
  - **contributions:** 1. Proposed a novel multi-agent collaboration framework to automate the interpretation of House-Tree-Person drawings, decoupling visual feature recognition from psychological inference. 2. Demonstrated that multimodal large language models (MLLMs) can achieve expert-level baseline comprehension in interpreting projective drawings, with high semantic similarity to human expert interpretations. 3. Introduced a destigmatizing narrative and social-psychological perspective integration to correct visual hallucinations and enhance the ecological validity and coherence of automated psychological reports.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/681955eb18a63880e0327c5debb6e992188de12ca52cef0c9c34258f09c3a91d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an automated assessment framework for the House-Tree-Person drawing test using multimodal LLMs and multi-agent collaboration to address issues of subjective scoring and lack of standardization. The framework effectively interprets drawings with high similarity to expert analysis and generates coherent psychological reports. The results confirm the potential of multimodal models as standardized tools for projective psychological assessment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration<br>从视觉感知到深度共情：基于多模态大模型与多智能体协作的房树人绘画自动评估框架] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>HTP测试评分标准不一，依赖主观经验，缺乏统一量化系统] --> B1
        C[主要方法/Method<br>多模态大语言模型与多智能体协作框架] --> C1
        D[关键结果/Results<br>模型解释与专家解释语义相似度高，多智能体系统生成有效心理报告] --> D1
        B1[HTP test has heterogeneous scoring, relies on subjective experience, lacks unified quantitative coding]
        C1[Multimodal LLMs and multi-agent collaboration framework]
        D1[High semantic similarity to expert interpretations; multi-agent system produces reports with high ecological validity]
    ```

- **[arXiv251229] Reflection-Driven Control for Trustworthy Code Agents**
  - **tags:** [mlsys], [agent system], [reflection-driven control, secure code generation, trustworthy agents, reflective memory, safety control]
  - **authors:** Bin Wang, Jiazheng Quan, Xingrui Yu, Hansen Hu, Yuhao, Ivor Tsang
  - **institution:** Peking University, Xiamen University, Agency for Science, Technology and Research (A*STAR)
  - **link:** https://arxiv.org/pdf/2512.21354
  - **contributions:** 1. Introduces Reflection-Driven Control, a standardized and pluggable control module that integrates self-reflection as an explicit, internal step in an agent's reasoning process. 2. Instantiates the method for secure code generation, using a reflection loop to monitor decisions and retrieve repair examples/guidelines from an evolving reflective memory to inject constraints. 3. Empirically demonstrates that the approach substantially improves security and policy compliance of generated code while preserving functional correctness, with minimal overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of reliable safety controls in LLM agents by proposing Reflection-Driven Control, a module that makes self-reflection an explicit, continuous part of the agent's reasoning to monitor and constrain its decisions using evidence from a reflective memory. Evaluated on security-critical code generation tasks, the method significantly improves code security and compliance while maintaining functionality, offering a practical path toward trustworthy AI coding agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Reflection-Driven Control for Trustworthy Code Agents] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[LLM代理缺乏可靠的安全控制/LLM agents lack reliable safety controls]
        Problem --> P2[可能产生有害输出/Can produce harmful outputs]
        Method --> M1[将自我反思作为推理的显式步骤/Elevates self-reflection to an explicit reasoning step]
        Method --> M2[内部反思循环监控决策路径/Internal reflection loop monitors decision path]
        Method --> M3[从反思记忆中检索修复示例/Retrieves repair examples from reflective memory]
        Results --> R1[显著提高生成代码的安全性和合规性/Substantially improves security & policy compliance]
        Results --> R2[基本保持功能正确性/Largely preserves functional correctness]
        Results --> R3[运行时和token开销最小/Minimal runtime & token overhead]
    ```

- **[arXiv251229] AInsteinBench: Benchmarking Coding Agents on Scientific Repositories**
  - **tags:** [se], [software engineering], [benchmark, scientific computing, code generation, pull requests, test-driven verification]
  - **authors:** Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng
  - **institution:** ByteDance Seed, Princeton University
  - **link:** https://arxiv.org/pdf/2512.21373
  - **code:** https://github.com/ByteDance-Seed/AInsteinBench
  - **contributions:** 1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents' ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI's role in computational scientific research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AInsteinBench: Benchmarking Coding Agents on Scientific Repositories] --> B[核心问题/Problem: Can LLM agents operate as scientific computing development agents?]
        A --> C[主要方法/Method: End-to-end evaluation using tasks from real scientific pull requests]
        A --> D[关键结果/Results: Measures ability beyond surface-level code generation]
    ```

- **[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks**
  - **tags:** [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]
  - **authors:** Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li
  - **institution:** University of Electronic Science and Technology of China, North China University of Technology
  - **link:** https://arxiv.org/pdf/2512.21375
  - **contributions:** 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --> B
    A --> C
    A --> D
    B[核心问题/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]
    C[主要方法/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]
    D[关键结果/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]
    ```

- **[arXiv251229] LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors**
  - **tags:** [sec], [adversarial attacks], [adversarial attack, large language model, retrieval-augmented generation, Android malware detection, adversarial training]
  - **authors:** Tianwei Lan, Farid Naït-Abdesselam
  - **institution:** Université Paris Cité
  - **link:** https://arxiv.org/pdf/2512.21404
  - **contributions:** 1. Proposes LAMLAD, a novel adversarial attack framework that uses a dual-agent LLM architecture (manipulator and analyzer) to generate feature-level perturbations for evading Android malware detectors., 2. Integrates Retrieval-Augmented Generation (RAG) into the LLM pipeline to improve the efficiency and contextual awareness of the attack., 3. Proposes and evaluates an adversarial training-based defense strategy to enhance model robustness against the proposed LAMLAD-style attacks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6061210b194ba5cf79f70b8959faa3abe6b3e91ffad512d9cfd319de948593bb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LAMLAD, a novel adversarial attack framework that leverages the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based Android malware classifiers. The method uses a dual-agent LLM architecture with RAG to generate realistic, functionality-preserving feature perturbations, achieving a high attack success rate. The paper also demonstrates that adversarial training can significantly reduce the effectiveness of such attacks, enhancing model robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors] --> B[核心问题/Problem: ML-based Android malware detectors are vulnerable to adversarial attacks.]
        A --> C[主要方法/Method: Proposes LAMLAD, a dual-agent LLM framework with RAG for generating stealthy perturbations.]
        A --> D[关键结果/Results: Achieves up to 97% attack success rate; adversarial training defense reduces ASR by >30%.]
    ```

- **[arXiv251229] Feasible strategies in three-way conflict analysis with three-valued ratings**
  - **tags:** [ai], [conflict analysis], [three-way conflict analysis, feasible strategy, consistency measure, non-consistency measure, weighted agent-issue evaluation]
  - **authors:** Jing Liu, Mengjun Hu, Guangming Lang
  - **institution:** Changsha University of Science and Technology, Saint Mary's University
  - **link:** https://arxiv.org/pdf/2512.21420
  - **contributions:** 1. Proposes a novel framework for identifying feasible strategies in conflict resolution from the perspectives of consistency and non-consistency. 2. Introduces weighted consistency and non-consistency measures that incorporate the importance of both agents and issues. 3. Develops algorithms to systematically identify feasible strategies, L-order feasible strategies, and optimal solutions, demonstrating superior performance over existing approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4b2e48e756e98608f4388b841aa7940f0ba7b237737fa314abc4db83fbf680f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in formulating actionable strategies for conflict resolution within three-way conflict analysis. It proposes a method that computes agent clique ratings and uses novel weighted consistency and non-consistency measures to identify feasible and optimal strategies. The approach is validated through case studies and shown to outperform conventional conflict analysis models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Feasible strategies in three-way conflict analysis<br>三向冲突分析中的可行策略] --> B(Problem: Lack of focus on conflict resolution strategies<br>问题: 缺乏对冲突解决策略的关注)
        A --> C(Method: Weighted consistency/non-consistency measures & algorithms<br>方法: 加权一致/非一致性度量与算法)
        A --> D(Results: Outperforms conventional approaches, identifies optimal solutions<br>结果: 优于传统方法，识别最优解)
    ```

- **[arXiv251229] Three-way conflict analysis based on alliance and conflict functions**
  - **tags:** [ai], [decision theory], [three-way decision, conflict analysis, alliance function, conflict function, alliance set]
  - **authors:** Junfang Luo, Mengjun Hu, Guangming Lang, Xin Yang, Keyun Qin
  - **institution:** Southwestern University of Finance and Economics, University of Regina, Changsha University of Science and Technology, Southwest Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.21419
  - **contributions:** 1. Proposes a novel separation of the traditional auxiliary function into distinct alliance and conflict functions to clarify semantic interpretation in conflict analysis. 2. Introduces a framework for trisecting agents, issues, and agent pairs based on the new alliance and conflict functions. 3. Explores and applies new concepts such as alliance sets and strategies to solve crucial questions in conflict analysis, demonstrating the model with a real-world application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54277009925f58600c765060d6cbc575e96e562e3c9748aa2f54e97b83024e0b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the semantic ambiguity in aggregating traditional three-way conflict analysis functions by proposing a separation into distinct alliance and conflict functions. The method enables clearer trisection of agents, issues, and agent pairs, leading to the exploration of alliance sets and strategies. The main conclusion is that this separation provides a more interpretable and applicable framework for conflict analysis, as illustrated by a real-world example.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Three-way Conflict Analysis Based on Alliance and Conflict Functions] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统辅助函数聚合语义模糊/Semantic ambiguity in aggregating traditional auxiliary functions]
        C --> C1[分离为联盟与冲突函数/Separate into alliance and conflict functions]
        C --> C2[基于新函数进行三分/Trisec based on new functions]
        D --> D1[提出联盟集与策略概念/Propose alliance sets and strategies]
        D --> D2[提供真实应用案例/Provide a real-world application]
    ```

- **[arXiv251229] Teaching People LLM's Errors and Getting it Right**
  - **tags:** [nlp], [human-ai interaction], [overreliance, failure patterns, mental models, user study, meta-labels]
  - **authors:** Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasović
  - **institution:** University of Utah
  - **link:** https://arxiv.org/pdf/2512.21422
  - **contributions:** 1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness—assessing a user's ability to anticipate LLM errors using taught patterns—which showed a positive effect, unlike traditional human-AI team accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Teaching People LLM’s Errors and Getting it Right] --> B[核心问题/Problem: Users overrely on LLMs due to inaccurate mental models]
        A --> C[主要方法/Method: Analyze failure pattern existence, test discovery methods, propose new evaluation metric]
        A --> D[关键结果/Results: Patterns exist, discovery methods are mixed, new metric shows teaching is effective]
    ```

- **[arXiv251229] Three-way decision with incomplete information based on similarity and satisfiability**
  - **tags:** [ai], [rough set theory], [three-way decision, incomplete information, similarity degree, satisfiability degree, approximability]
  - **authors:** Junfang Luo, Mengjun Hu, Keyun Qin
  - **institution:** Southwest Jiaotong University, University of Regina
  - **link:** https://arxiv.org/pdf/2512.21421
  - **contributions:** 1. Proposes a new measure of similarity degree of objects as a generalization of equivalence relations for handling incomplete information in the computational formulation of three-way decision. 2. Introduces a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability for the conceptual formulation of three-way decision under incomplete information. 3. Proposes novel approaches for three-way decision using approximability of objects and confidence of formulas, pointing out new research directions beyond the common method of similarity classes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d87784acc1b1cc397b822153c118be23974be9721c3d19c9dfc95fbbaef158e_w640_q70.webp
  - **Simple LLM Summary:** This paper generalizes the computational and conceptual formulations of three-way decision to handle incomplete information, which is common in real-world applications. For the computational side, it introduces a similarity degree measure and explores decision-making via α-similarity classes and approximability; for the conceptual side, it proposes a satisfiability degree measure and studies approaches using α-meaning sets and confidence. The work extends rough set theory and identifies promising new directions for three-way decision under uncertainty.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Three-Way Decision with Incomplete Information Based on Similarity and Satisfiability] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[处理不完全信息/Handling Incomplete Information]
        C --> C1[计算式: 相似度/Computational: Similarity Degree]
        C --> C2[概念式: 可满足度/Conceptual: Satisfiability Degree]
        C1 --> C1a[α-相似类/α-Similarity Classes]
        C1 --> C1b[可逼近性/Approximability]
        C2 --> C2a[α-意义集/α-Meaning Sets]
        C2 --> C2b[置信度/Confidence]
        D --> D1[推广两种表述/Generalizes Both Formulations]
        D --> D2[指出新方向/Points to New Directions]
    ```

- **[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models**
  - **tags:** [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]
  - **authors:** Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi
  - **institution:** Institute of Intelligent Systems and Robotics, Sorbonne University
  - **link:** https://arxiv.org/pdf/2512.21439
  - **contributions:** 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]
        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]
        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]
    ```

- **[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning**
  - **tags:** [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]
  - **authors:** Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu
  - **institution:** University of Washington, University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.21446
  - **contributions:** 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards "diffusion supremacy" over autoregressive models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]
        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]
        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]
        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]
        D --> D2[迈向"扩散霸权"/Moving towards "diffusion supremacy"]
    ```

- **[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism**
  - **tags:** [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]
  - **authors:** Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21452
  - **contributions:** 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
    
        Problem --> P1("Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效")
        Problem --> P2("Data scarcity <br/> 数据稀缺")
    
        Method --> M1("DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强")
        Method --> M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络")
        Method --> M3("MS COCO Transfer Learning <br/> MS COCO迁移学习")
    
        Results --> R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能")
        Results --> R2("Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒")
        Results --> R3("New paradigm for automated detection <br/> 自动化检测新范式")
    ```

- **[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification**
  - **tags:** [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]
  - **authors:** Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian
  - **institution:** Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital
  - **link:** https://arxiv.org/pdf/2512.21476
  - **code:** https://github.com/JeremyXSC/GPF-Net
  - **contributions:** 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]
        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]
        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]
    ```

- **[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), task scheduling, inference throughput, fine-grained pipelining]
  - **authors:** Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21487
  - **contributions:** 1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FinDEP: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[MoE推理内存密集，现有DEP调度效率低/MoE inference is memory-intensive, existing DEP scheduling is inefficient]
        C --> C1[细粒度任务划分与调度优化/Fine-grained task partitioning and scheduling optimization]
        D --> D1[吞吐量最高提升1.61倍/Throughput improved by up to 1.61x]
    ```

- **[arXiv251229] Oogiri-Master: Benchmarking Humor Understanding via Oogiri**
  - **tags:** [nlp], [humor understanding], [Oogiri, benchmark, linguistic analysis, incongruity resolution, insight-augmented prompting]
  - **authors:** Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura
  - **institution:** CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo
  - **link:** https://arxiv.org/pdf/2512.21494
  - **contributions:** 1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Oogiri-Master: Benchmarking Humor Understanding via Oogiri] --> B[核心问题/Problem: What makes Oogiri responses funny to humans?]
        A --> C[主要方法/Method: Introduce Oogiri-Master benchmark and Oogiri-Corpus dataset with diverse responses and independent ratings]
        A --> D[关键结果/Results: LLMs approach human performance; insight-augmented prompting improves results]
    ```

- **[arXiv251229] LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis**
  - **tags:** [cv], [multimodal forgery detection], [visual-textual co-reasoning, cross-cues-aware chain of thought (CCT), GRPO-based optimization]
  - **authors:** Fanwei Zeng, Changtao Miao, Jing Huang, Zhiya Tan, Shutao Gong, Xiaoming Yu, Yang Wang, Huazhe Tan, Weibin Yao, Jianshu Li
  - **institution:** Ant Group, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.21482
  - **contributions:** 1. Proposed LogicLens, a unified framework for visual-textual co-reasoning that jointly performs detection, grounding, and explanation for text-centric forgery analysis. 2. Introduced a Cross-Cues-aware Chain of Thought (CCT) mechanism for iterative cross-validation of visual and textual cues, and a weighted multi-task reward function for GRPO-based optimization. 3. Created the RealText dataset with 5,397 images and fine-grained annotations using a novel PR² (Perceiver, Reasoner, Reviewer) multi-agent annotation pipeline.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/138f8fcb4727c77950b23146e1184851aa8b8cea95056b1d6b161c37e231ad80_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces LogicLens, a unified visual-textual co-reasoning framework for analyzing text-centric forgeries. It uses a novel Cross-Cues-aware Chain of Thought mechanism and multi-task optimization to jointly handle detection, grounding, and explanation. Experiments show LogicLens achieves state-of-the-art performance, significantly outperforming specialized frameworks and other MLLMs in zero-shot and dense-text scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[文本中心伪造威胁/Sophisticated text-centric forgeries]
        B --> B2[现有方法缺乏推理/Current methods lack reasoning]
        B --> B3[任务割裂/Tasks treated as discrete]
        C --> C1[统一框架/Unified Visual-Textual Co-reasoning framework]
        C --> C2[跨线索思维链/Cross-Cues-aware Chain of Thought (CCT)]
        C --> C3[多任务奖励函数/Weighted multi-task reward function]
        C --> C4[PR²标注管道/PR² annotation pipeline]
        C --> C5[RealText数据集/RealText dataset]
        D --> D1[零样本评估领先/Superior zero-shot performance]
        D --> D2[密集文本数据集领先/Lead on dense-text dataset]
        D --> D3[公开资源/Public dataset, model, code]
    ```

- **[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding**
  - **tags:** [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]
  - **authors:** Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson
  - **institution:** Dartmouth College
  - **link:** https://arxiv.org/pdf/2512.21506
  - **contributions:** 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]
        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]
        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]
    ```

- **[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO**
  - **tags:** [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]
  - **authors:** Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji
  - **institution:** Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21514
  - **contributions:** 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]
        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]
        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]
        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]
        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]
        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]
        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]
    ```

- **[arXiv251229] Selective LLM-Guided Regularization for Enhancing Recommendation Models**
  - **tags:** [ai], [recommender systems], [selective regularization, knowledge distillation, cold-start, long-tail, gating mechanism]
  - **authors:** Shanglin Yang, Zhan Shi
  - **institution:** Sichuan University
  - **link:** https://arxiv.org/pdf/2512.21526
  - **contributions:** 1. Proposes a selective LLM-guided regularization framework (S-LLMR) that activates LLM supervision only when a gating mechanism predicts the LLM to be reliable, addressing the issue of inaccurate global distillation. 2. Introduces a trainable gating mechanism informed by user history length, item popularity, and model uncertainty to dynamically decide when to apply LLM-based pairwise ranking supervision. 3. Demonstrates through experiments that the method improves overall accuracy and yields substantial gains in cold-start and long-tail recommendation scenarios, outperforming global distillation baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76336a3d123794e83843c14c4b799afd0817948ee9dfeb2f6f19ce776f183796_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of leveraging large language models (LLMs) for recommendation without suffering from their high cost and unreliability in certain scenarios. It proposes Selective LLM-Guided Regularization (S-LLMR), a model-agnostic framework that uses a gating mechanism to selectively apply LLM-based supervision only when the LLM is predicted to be reliable. Experiments show this approach improves recommendation accuracy, especially for cold-start users and long-tail items, outperforming methods that uniformly distill LLM knowledge.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Selective LLM-Guided Regularization<br>选择性LLM引导正则化] --> B(Problem/核心问题<br>LLMs as standalone recommenders are costly/unreliable;<br>Global distillation forces imitation of inaccurate LLM guidance.)
        A --> C(Method/主要方法<br>Selective LLM-Guided Regularization (S-LLMR):<br>Trainable gating mechanism activates LLM supervision only when reliable.)
        A --> D(Results/关键结果<br>Improves overall accuracy;<br>Substantial gains in cold-start & long-tail regimes.)
    ```

- **[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models**
  - **tags:** [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]
  - **authors:** Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu
  - **institution:** University of Washington, Intel
  - **link:** https://arxiv.org/pdf/2512.21529
  - **contributions:** 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification")
        Problem --> P2("标准方法预测不一致/Standard methods produce inconsistent predictions")
        Method --> M1("提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework")
        Method --> M2("结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses")
        Method --> M3("集成轻量级LoRA适配/Integrate lightweight LoRA adaptation")
        Results --> R1("提升全路径精度/Improves Full-Path Accuracy")
        Results --> R2("降低不一致性错误/Reduces Tree-based Inconsistency Error")
        Results --> R3("参数开销最小/Minimal parameter overhead")
    ```

- **[arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model**
  - **tags:** [mlsys], [llm inference], [adaptive length penalty, reinforcement learning, constrained optimization, Lagrangian primal-dual, reasoning efficiency]
  - **authors:** Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo
  - **institution:** Peking University, Harbin Institute of Technology, Shenzhen
  - **link:** https://arxiv.org/pdf/2512.21540
  - **contributions:** 1. Proposes Leash, a reinforcement learning framework that formulates length control as a constrained optimization problem and uses a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. 2. Introduces an adaptive mechanism that intensifies the penalty when generations exceed the target length and relaxes it when they are shorter, guiding models toward concise reasoning without sacrificing performance. 3. Demonstrates experimentally that Leash reduces average reasoning length by 60% across diverse tasks while maintaining competitive performance, offering a practical paradigm for efficient LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec5b3e2930213678e6d04b060a50d89faaaacded209387c96170a775f9db310_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLMs producing overly long reasoning traces, which increases computational cost. It proposes Leash, an adaptive reinforcement learning framework that dynamically adjusts length penalties using a Lagrangian method to balance conciseness and accuracy. Experiments show it reduces reasoning length by 60% while maintaining performance, providing an effective approach for efficient LLM reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LEASH: Adaptive Length Penalty and Reward Shaping] --> B[核心问题/Problem: LLMs生成过长推理链，计算成本高/Fixed penalties fail to adapt, leading to suboptimal accuracy-conciseness trade-offs]
        A --> C[主要方法/Method: 自适应强化学习框架，使用拉格朗日对偶方法动态调整惩罚系数/Adaptive RL framework with Lagrangian primal-dual for dynamic penalty adjustment]
        A --> D[关键结果/Results: 平均推理长度减少60%，性能保持竞争力/Average reasoning length reduced by 60% while maintaining competitive performance across tasks]
    ```

- **[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures**
  - **tags:** [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]
  - **authors:** Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li
  - **institution:** NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.21551
  - **contributions:** 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp
  - **Simple LLM Summary:** This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-AI Interaction Alignment] --> B[核心问题/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]
        A --> C[主要方法/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]
        A --> D[关键结果/Results: Establishes agenda for reciprocal, responsible human-AI futures]
    ```

- **[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments**
  - **tags:** [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]
  - **authors:** Hua Shen
  - **institution:** NYU Shanghai, New York University
  - **link:** https://arxiv.org/pdf/2512.21552
  - **contributions:** 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]
        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]
        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]
    ```

- **[arXiv251229] Exploration of Reproducible Generated Image Detection**
  - **tags:** [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]
  - **authors:** Yihang Duan
  - **institution:** Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)
  - **link:** https://arxiv.org/pdf/2512.21562
  - **contributions:** 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploration of Reproducible Generated Image Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]
        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]
        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]
    ```

- **[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation**
  - **tags:** [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]
  - **authors:** Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li
  - **institution:** Beihang University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2512.21576
  - **contributions:** 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]
        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]
        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]
    ```

- **[arXiv251229] NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent**
  - **tags:** [mlsys], [agent system], [NeMo Framework, LoRA, Nemotron SLM, hyperparameter sweep, multi-agent system]
  - **authors:** Ali Sahami, Sudhanshu Garg, Andrew Wang, Chaitanya Kulkarni, Farhad Farahani, Sean Yun-Shiuan Chuang, Jian Wan, Srinivasan Manoharan, Uma Kona, Nitin Sharma, Linsey Pang, Prakhar Mehrotra, Jessica Clark, Mark Moyou
  - **institution:** PayPal AI, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.21578
  - **contributions:** 1. The first application of NVIDIA's NeMo Framework to optimize commerce-specific agents. 2. An LLM-powered fine-tuning strategy for retrieval-focused commerce tasks. 3. A demonstration of significant latency and cost improvements while maintaining agent quality, providing a scalable framework for multi-agent system optimization in production e-commerce.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90dda98c8c5c5f9d75ede0c681c9024dc5973d432f246b90eed23aad0a03c916_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the optimization of PayPal's Commerce Agent, a multi-agent system, by fine-tuning a Nemotron small language model using NVIDIA's NeMo Framework and LoRA. The method involved systematic hyperparameter sweeps to improve the performance-critical search component. The results show that the fine-tuned model effectively resolves the key latency issue in the retrieval component, which accounted for over 50% of response time, while maintaining or enhancing overall system performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["NEMO-4-PAYPAL: Empowering PayPal's Commerce Agent"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["Search Latency/搜索延迟"]
        P1 --> P2[">50% Response Time/超过50%响应时间"]
        Method --> M1["Fine-tuning with NeMo/使用NeMo微调"]
        M1 --> M2["LoRA on Nemotron SLM/在Nemotron SLM上使用LoRA"]
        M2 --> M3["Hyperparameter Sweep/超参数扫描"]
        Results --> R1["Latency & Cost Improvement/延迟与成本改进"]
        Results --> R2["Maintained Agent Quality/保持代理质量"]
        Results --> R3["Scalable Framework/可扩展框架"]
    ```

- **[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid**
  - **tags:** [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]
  - **authors:** Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng
  - **institution:** Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers
  - **link:** https://arxiv.org/pdf/2512.21577
  - **contributions:** 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Problem["核心问题/Problem"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Method["主要方法/Method"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Results["关键结果/Results"]
        Problem --> P1["Hallucination persists in LLMs / 幻觉在LLM中持续存在"]
        Method --> M1["Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模"]
        Method --> M2["Framework: reference world & conflict policy / 框架：参考世界与冲突策略"]
        Results --> R1["Clarifies evaluation & terminology / 澄清评估与术语"]
        Results --> R2["Proposes new benchmark plans / 提出新基准计划"]
    ```

- **[arXiv251229] A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model, logic tree reasoning, medical multimodal diagnosis, explainable AI, LLaVA]
  - **authors:** Zelin Zang, Wenyi Gu, Siqi Ma, Dan Yang, Yue Shen, Zhu Zhang, Guohui Fan, Wing-Kuen Ling, Fuji Yang
  - **institution:** Tsientang Institute of Advanced Study (TIAS), Westlake University, Ant Group, China-Japan Friendship Hospital
  - **link:** https://arxiv.org/pdf/2512.21583
  - **contributions:** 1. Proposes a diagnostic framework integrating vision-language alignment with logic-regularized reasoning to enhance reliability. 2. Introduces a reasoning controller and logic tree generator to decompose tasks and assemble verifiable conclusions, improving interpretability. 3. Demonstrates improved diagnostic accuracy and more interpretable reasoning traces on multimodal medical benchmarks like MedXpertQA.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6b8d6d34a614d3cb042f13334dede18494914ca29d6f0fd6f4467f789871f82_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of unreliable reasoning and hallucinations in existing multimodal medical AI models. It proposes a diagnostic framework built on LLaVA that combines vision-language alignment with logic-regularized reasoning to generate verifiable conclusions via logic trees. Evaluations show the method improves diagnostic accuracy and yields more interpretable reasoning traces, advancing trustworthy multimodal medical AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[医学多模态诊断框架<br/>Medical Multimodal Diagnostic Framework] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有模型幻觉与推理不一致<br/>Existing Models: Hallucinations & Inconsistent Reasoning]
        C --> C1[结合视觉语言对齐与逻辑树推理<br/>Vision-Language Alignment + Logic Tree Reasoning]
        D --> D1[提升诊断准确性与可解释性<br/>Improved Diagnostic Accuracy & Interpretability]
    ```

- **[arXiv251229] LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model**
  - **tags:** [mlsys], [llm inference], [item-to-item recommendation, data-centric, long-tail items, data augmentation, data filtering]
  - **authors:** Yinfu Feng, Yanjing Wu, Rong Xiao, Xiaoyi Zen
  - **institution:** Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21595
  - **contributions:** 1. Proposes LLM-I2I, a data-centric framework that leverages Large Language Models to enhance I2I recommendation models without altering their architecture. 2. Introduces an LLM-based data generator to synthesize user-item interactions, specifically targeting long-tail items to alleviate data sparsity. 3. Designs an LLM-based data discriminator to filter out noisy interactions from both real and synthetic data, improving overall data quality for training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cb08ea9b26b612493e4d48e7db88c46a869c2050c94d47b75488adcaf6ddfa9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses data sparsity and noise problems in Item-to-Item (I2I) recommendation systems by proposing LLM-I2I, a data-centric framework that uses an LLM to generate synthetic interactions for long-tail items and filter noisy data. The refined data is then used to train existing I2I models. Experimental results on industrial and academic datasets show significant improvements in recommendation accuracy, especially for long-tail items, and deployment on a large e-commerce platform led to measurable gains in recall and gross merchandise value.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据稀疏与噪声/Data Sparsity & Noise]
        C --> C1[LLM数据生成器/LLM-based Data Generator]
        C --> C2[LLM数据判别器/LLM-based Data Discriminator]
        C1 --> C3[合成交互数据/Synthesize Interaction Data]
        C2 --> C4[过滤噪声数据/Filter Noisy Data]
        C3 & C4 --> C5[融合数据训练I2I模型/Fuse Data to Train I2I Model]
        D --> D1[提升推荐准确率/Improves Recommendation Accuracy]
        D --> D2[提升长尾物品性能/Better for Long-tail Items]
        D --> D3[线上指标提升/Online Metric Improvements (RN+6.02%, GMV+1.22%)]
    ```

- **[arXiv251229] AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design**
  - **tags:** [mlsys], [agent system], [AMS IC design, LLM-based agent, structured reasoning, design automation, I/O ring generation]
  - **authors:** Zhishuai Zhang, Xintian Li, Shilong Liu, Aodong Zhang, Lu Jie, Nan Sun
  - **institution:** Tsinghua University, Princeton University
  - **link:** https://arxiv.org/pdf/2512.21613
  - **code:** https://github.com/Arcadia-1/AMS-IO-Agent
  - **contributions:** 1. Proposed AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware I/O subsystem generation in AMS ICs. 2. Introduced AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. 3. Demonstrated the first reported human-agent collaborative AMS IC design where an LLM agent's output was directly used in a silicon tape-out, achieving over 70% DRC+LVS pass rate and reducing design time from hours to minutes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7205181105fdcb012bb4c8c5b3cce6565751edc220003d3784f6dbf648ee893a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the labor-intensive and non-reusable nature of analog and mixed-signal (AMS) integrated circuit I/O design by proposing AMS-IO-Agent, an LLM-based agent that uses structured domain knowledge and intent structuring to automate the process. The method connects natural language design intent to industrial deliverables and is evaluated on a new benchmark, AMS-IO-Bench. The agent significantly outperforms baseline LLMs, achieves a high verification pass rate, and its generated I/O ring was successfully fabricated, demonstrating practical effectiveness in real design flows.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AMS-IO-Bench and AMS-IO-Agent<br>论文标题/Paper Title] --> B[手动AMS I/O设计费时且不可复用<br>核心问题/Problem: Manual AMS I/O design is time-consuming and non-reusable]
        A --> C[提出基于LLM的智能体与结构化推理框架<br>主要方法/Method: Proposes an LLM-based agent and structured reasoning framework]
        A --> D[验证通过率>70%，设计时间从小时减至分钟，成功流片<br>关键结果/Results: >70% pass rate, design time reduced from hours to minutes, successful tape-out]
    ```

- **[arXiv251229] Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design**
  - **tags:** [mlsys], [agent system], [multi-agent platform, knowledge graph, physiologically based pharmacokinetic (PBPK) simulations, autonomous execution, human-in-the-loop]
  - **authors:** Takahide Suzuki, Kazuki Nakanishi, Takashi Fujiwara, Hideyuki Shimizu
  - **institution:** Institute of Science Tokyo, Kyoto University
  - **link:** https://arxiv.org/pdf/2512.21623
  - **contributions:** 1. Introduces OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine for drug design. 2. Features an architecture with specialized agents (Biologist, Chemist, Pharmacologist) governed by an Orchestrator, which actively execute simulations and reason over results to create a dynamic feedback loop for iterative optimization. 3. Democratizes therapeutic design by transforming drug discovery from a stochastic search into a programmable, evidence-based engineering discipline through the integration of autonomous execution with human guidance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd4a841925fb9129111928c81fe29ac7016c26df168e9b4ca87c8782a692d5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of fragmented and passive tools in therapeutic discovery by proposing OrchestRA, a multi-agent platform where specialized AI agents autonomously execute and reason over biological, chemical, and pharmacological tasks. This creates a dynamic feedback loop for iterative drug candidate optimization, guided by human input. The conclusion is that this approach transforms drug discovery into a more programmable and evidence-based engineering process.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Fragmented domains & execution gap<br>AI as passive assistants]
        C[主要方法/Method<br>OrchestRA Multi-Agent Platform<br>Agents execute & reason<br>Human-in-the-loop]
        D[关键结果/Results<br>Autonomous discovery engine<br>Dynamic feedback loop<br>Programmable evidence-based design]
    ```

- **[arXiv251229] Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing**
  - **tags:** [ai], [multi-armed bandits], [multiple-play bandits, prioritized resource sharing, regret analysis, combinatorial optimization, UCB]
  - **authors:** Hong Xie, Haoran Gu, Yanying Huang, Tao Tan, Defu Lian
  - **institution:** University of Science and Technology of China, Chongqing University
  - **link:** https://arxiv.org/pdf/2512.21626
  - **contributions:** 1. Proposes a new variant of the multiple-play stochastic bandit model (MSB-PRS) that incorporates prioritized capacity sharing among plays, tailored for resource allocation in LLM and edge intelligence applications. 2. Establishes instance-independent and instance-dependent regret lower bounds for the proposed model, characterizing its fundamental learning difficulty. 3. Designs an offline optimal policy solver (MSB-PRS-OffOpt) and an online UCB-based learning algorithm with theoretical regret guarantees that nearly match the derived lower bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8562089dc3d9fa0a65e8caf8921b51648e1718efd62395a7af2fadba8cf952d9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new multi-armed bandit model where multiple plays with priorities compete for the stochastic capacity of arms. The authors design an algorithm that first computes an optimal allocation offline and then uses it within an online UCB-based strategy, proving that its regret nearly matches the fundamental lower bounds they establish for this problem.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing<br/>多臂老虎机优先容量共享"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Prioritized resource allocation in LLM/edge intelligence<br/>LLM/边缘智能中的优先资源分配"] --> P1["模型/Model<br/>M arms, K plays, stochastic capacity, priority weights<br/>M个臂，K个玩家，随机容量，优先级权重"]
        Method["主要方法/Method<br/>Algorithm Design<br/>算法设计"] --> M1["离线最优求解器/MSB-PRS-OffOpt<br/>Computes optimal policy<br/>计算最优策略"]
        Method --> M2["在线UCB算法/Online UCB Algorithm<br/>Uses offline solver as subroutine<br/>以离线求解器为子程序"]
        Results["关键结果/Results<br/>Theoretical Analysis<br/>理论分析"] --> R1["下界/Regret Lower Bounds<br/>Ω(α₁σ√KMT), Ω(α₁σ²(M/Δ)lnT)"]
        Results --> R2["上界/Regret Upper Bounds<br/>Matching lower bounds up to factors<br/>与下界匹配（差因子）"]
    ```

- **[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search**
  - **tags:** [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]
  - **authors:** Maximilian Weichart
  - **institution:** University of Regensburg
  - **link:** https://arxiv.org/pdf/2512.21648
  - **code:** https://github.com/Max-We/inverse-rpo
  - **contributions:** 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]
        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]
        D[关键结果/Results: New policies outperform PUCT without extra cost]
    ```

- **[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References**
  - **tags:** [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]
  - **authors:** Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
  - **institution:** Zhejiang University, Fudan University, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.21641
  - **contributions:** 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]
        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]
        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]
        C1 --> C3[构建UniScene表示/Build UniScene Representation]
        C1 --> C4[利用运动历史推理/Reason with Motion History]
        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]
        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]
        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]
    ```

- **[arXiv251229] Near-Optimal Coalition Structures in Polynomial Time**
  - **tags:** [ai], [cooperative game theory], [coalition structure generation, anytime algorithms, sparse relaxations, dynamic programming, MILP]
  - **authors:** Angshul Majumdar
  - **institution:** Indraprastha Institute of Information Technology, Delhi
  - **link:** https://arxiv.org/pdf/2512.21657
  - **contributions:** 1. Proves that under a "sparse synergy" model, sparse relaxation methods can find near-optimal coalition structures in polynomial time with high probability. 2. Demonstrates that broad classes of dynamic programming and MILP algorithms require exponential time to achieve comparable solution quality. 3. Establishes a rigorous probabilistic anytime performance separation favoring sparse relaxations over exact methods for the CSG problem.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb8e459bba93eeaf4c9237729ad06cf13b47ef6a941811135ed634157dd979c7_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the coalition structure generation (CSG) problem. It compares three algorithmic paradigms and proves that, under a random sparse synergy model, sparse relaxation methods can find near-optimal solutions in polynomial time, while exact methods like DP and MILP require exponential time to reach similar quality, establishing a clear anytime performance advantage for the sparse approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Near-Optimal Coalition Structures in Polynomial Time] --> B[核心问题/Problem: Coalition Structure Generation (CSG)]
        A --> C[主要方法/Method: Compare DP, MILP, and Sparse Relaxations]
        A --> D[关键结果/Results: Sparse relaxations achieve near-optimal welfare in polynomial time; DP/MILP require exponential time]
    ```

- **[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning**
  - **tags:** [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]
  - **authors:** Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee
  - **institution:** Kyungpook National University, Kumoh National Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21654
  - **contributions:** 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. 传统方法难以扩展/Traditional methods struggle to scale.]
        C[主要方法/Method: A structure-induced ACO framework. 利用结构先验、负载感知目标和重叠抑制/Uses structural prior, load-aware objective, and overlap suppression.]
        D[关键结果/Results: Improves route compactness, stability, and workload distribution. 提供可扩展的框架/Provides a scalable framework.]
    ```

- **[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles**
  - **tags:** [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]
  - **authors:** Jalal Khan
  - **institution:** United Arab Emirates University
  - **link:** https://arxiv.org/pdf/2512.21673
  - **contributions:** 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp
  - **Simple LLM Summary:** This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]
    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]
    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]
    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]
    ```

- **[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting**
  - **tags:** [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]
  - **authors:** Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan
  - **institution:** Beijing Jiaotong University, Aalborg University
  - **link:** https://arxiv.org/pdf/2512.21685
  - **contributions:** 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)
        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)
        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)
        C --> C2(主成分网络 / Principal Component Network)
        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)
    ```

- **[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks**
  - **tags:** [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]
  - **authors:** Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu
  - **institution:** Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.
  - **link:** https://arxiv.org/pdf/2512.21694
  - **contributions:** 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]
        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]
        C --> C2[Use self-collected dataset<br/>使用自收集数据集]
        C --> C3[Pre-process images<br/>预处理图像]
        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]
        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]
    ```

- **[arXiv251229] Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning**
  - **tags:** [mlsys], [agent system], [agentic AI, consensus-driven reasoning, explainable AI, responsible AI, multi-model governance]
  - **authors:** Eranga Bandara, Tharaka Hewa, Ross Gore, Sachin Shetty, Ravi Mukkamala, Peter Foytik, Abdul Rahman, Safdar H. Bouk, Xueping Liang, Amin Hass, Sachini Rajapakse, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan
  - **institution:** Old Dominion University, University of Oulu, Deloitte & Touche LLP, Florida International University, Nanyang Technological University, University of Colombo, IcicleLabs.AI, Accenture Technology Labs, Effectz.AI
  - **link:** https://arxiv.org/pdf/2512.21699
  - **contributions:** 1. Proposes a novel RAI/XAI agent architecture for production workflows based on multi-model consensus and reasoning-layer governance. 2. Introduces a mechanism where a consortium of heterogeneous LLM/VLM agents generate independent outputs, exposing uncertainty and alternatives for structured consolidation. 3. Demonstrates that the consensus-driven approach improves robustness, transparency, and operational trust across diverse real-world agentic AI workflows.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed22161f8f004c98e3fb6124bac7991548de5e54f47adb42f0d3eb1095409e6e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of explainability and responsibility in increasingly autonomous agentic AI systems. It proposes a new architecture where multiple AI agents generate candidate outputs, and a dedicated reasoning agent consolidates them while enforcing safety constraints, thereby improving decision robustness and auditability. The work provides a practical framework for building agentic systems that are both scalable and responsible by design.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Agentic AI lacks explainability & responsibility] --> B1[挑战/Challenges<br>Explainability, Accountability, Robustness, Governance]
        C[主要方法/Method<br>Multi-model Consensus & Reasoning-layer Governance] --> C1[架构/Architecture<br>Consortium of LLM/VLM Agents]
        C --> C2[过程/Process<br>Structured Consolidation by Dedicated Reasoning Agent]
        D[关键结果/Results<br>Improved Robustness, Transparency & Operational Trust]
    ```

- **[arXiv251229] Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning**
  - **tags:** [sec], [audio deepfake detection], [transfer learning, zero-shot inference, fine-tuning, Bengali audio, BanglaFake dataset]
  - **authors:** Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman
  - **institution:** Not explicitly stated in the provided content.
  - **link:** https://arxiv.org/pdf/2512.21702
  - **contributions:** 1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)
        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)
        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))
    ```

- **[arXiv251229] Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech**
  - **tags:** [nlp], [spoken dialogue systems], [Graph-of-Thoughts, full-duplex, speech acts, causal inference, multimodal transformer]
  - **authors:** Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian
  - **institution:** Zhejiang University, University of California, Berkeley, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.21706
  - **code:** https://got-duplex.github.io/
  - **contributions:** 1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Enabling Conversational Behavior Reasoning in Full-Duplex Speech<br/>实现全双工语音对话行为推理"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Current systems lack explicit reasoning for conversational behaviors."]
        Method["主要方法/Method<br/>Model reasoning as causal inference in a Graph-of-Thoughts (GoT)."]
        Results["关键结果/Results<br/>Robust behavior detection and interpretable reasoning chains."]
    ```

- **[arXiv251229] Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers**
  - **tags:** [nlp], [ai-generated text detection], [transformer, fine-tuning, zero-shot, Bengali, paraphrase detection]
  - **authors:** Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan
  - **institution:** Not specified in provided content.
  - **link:** https://arxiv.org/pdf/2512.21709
  - **contributions:** 1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Detecting AI-Generated Paraphrases in Bengali] --> B[核心问题/Problem: LLM misuse & lack of Bengali detection research]
        A --> C[主要方法/Method: Compare 5 transformers (Zero-Shot vs. Fine-Tuned)]
        A --> D[关键结果/Results: Fine-tuning needed; XLM-R, mDeBERTa, mBERT achieve ~91% accuracy]
    ```

- **[arXiv251229] Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought**
  - **tags:** [nlp], [interpretability & analysis], [latent tokens, chain-of-thought, model reliability, causal analysis, shortcut learning]
  - **authors:** Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21711
  - **contributions:** 1. Introduces "Steering Experiments" to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts "Shortcut Experiments" to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a "pseudo-reasoning" mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Latent token mechanisms unclear, reliability concerns] --> B1[潜在令牌机制不明确/Unclear latent token mechanisms]
        B --> B2[可靠性问题/Reliability concerns]
        C[主要方法/Method: Causal & adversarial analysis] --> C1[引导实验/Steering experiments]
        C --> C2[捷径实验/Shortcut experiments]
        D[关键结果/Results: COCONUT is pseudo-reasoning] --> D1[令牌对扰动不敏感/Tokens insensitive to perturbation]
        D --> D2[利用数据集捷径/Exploits dataset shortcuts]
        D --> D3[性能提升不基于真实推理/Performance gains not from true reasoning]
    ```

- **[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities**
  - **tags:** [sys], [communication & networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]
  - **authors:** Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin
  - **institution:** Kyung Hee University, Ghent University
  - **link:** https://arxiv.org/pdf/2512.21717
  - **contributions:** 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation"]
        Method["主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning"]
        Results["关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off"]
    ```

- **[arXiv251229] CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation**
  - **tags:** [nlp], [dialogue systems], [theme detection, topic clustering, hierarchical generation]
  - **authors:** Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li
  - **institution:** The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.21715
  - **contributions:** 1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CATCH: 可控主题检测框架 / Controllable Theme Detection Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题 / Problem] --> P1[短话语稀疏语义 / Sparse, short utterances]
        Problem --> P2[跨对话主题对齐 / Cross-dialogue theme alignment]
        Problem --> P3[用户偏好整合 / Personalized user preferences]
        Method[主要方法 / Method] --> M1[上下文感知主题表示 / Context-aware topic representation]
        Method --> M2[偏好引导主题聚类 / Preference-guided topic clustering]
        Method --> M3[分层主题生成 / Hierarchical theme generation]
        Results[关键结果 / Results] --> R1[在DSTC-12基准测试有效 / Effective on DSTC-12 benchmark]
        Results --> R2[提升聚类与生成质量 / Improved clustering & generation quality with 8B LLM]
    ```

- **[arXiv251229] An Information Theoretic Perspective on Agentic System Design**
  - **tags:** [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]
  - **authors:** Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.21720
  - **contributions:** 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance")
        C --> C1("提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator")
        D --> D1("更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate")
        D --> D2("扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors")
    ```

- **[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks**
  - **tags:** [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]
  - **authors:** Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov
  - **institution:** MIRAI, Cognitive AI Systems Lab
  - **link:** https://arxiv.org/pdf/2512.21723
  - **contributions:** 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]
        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]
        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]
    ```

- **[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data**
  - **tags:** [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]
  - **authors:** Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda
  - **institution:** Sony Computer Science Laboratories, Inc., Sony Corporation of America
  - **link:** https://arxiv.org/pdf/2512.21746
  - **contributions:** 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]
        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]
        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]
    ```

- **[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study**
  - **tags:** [se], [code optimization], [AI coding agents, performance optimization, empirical study, pull request analysis, AIDev dataset]
  - **authors:** Huiyun Peng, Antonio Zhong, Ricardo Andrés Calvo Méndez, Kelechi G. Kalu, James C. Davis
  - **institution:** Purdue University
  - **link:** https://arxiv.org/pdf/2512.21757
  - **contributions:** 1. Conducts the first empirical study comparing AI-agent-authored and human-authored performance optimization commits using real-world PR data. 2. Identifies a significant gap in explicit performance validation between AI-authored (45.7%) and human-authored (63.6%) PRs. 3. Finds that AI agents largely employ the same optimization patterns as humans, suggesting they learn from existing code but lack rigorous validation practices.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an empirical study comparing how AI coding agents and humans perform code optimization by analyzing performance-related pull requests from the AIDev dataset. The study finds that while AI agents use similar optimization patterns as humans, they are significantly less likely to include explicit performance validation in their commits. This highlights a key limitation in current agentic code optimization and an opportunity for improvement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[How Do Agents Perform Code Optimization? An Empirical Study] --> B[核心问题/Problem: AI coding agents' effectiveness on real-world performance optimization tasks is unknown.]
        A --> C[主要方法/Method: Empirical comparison of 324 agent-generated and 83 human-authored performance PRs from AIDev dataset.]
        A --> D[关键结果/Results: AI-authored PRs use similar patterns but include less explicit performance validation (45.7% vs 63.6%).]
    ```

- **[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets**
  - **tags:** [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]
  - **authors:** Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli
  - **link:** https://arxiv.org/pdf/2512.21760
  - **contributions:** 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]
        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]
        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]
    ```

- **[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets**
  - **tags:** [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]
  - **authors:** Matyas Bohacek, Ignacio Vilanova Echavarri
  - **institution:** Stanford University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21775
  - **contributions:** 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation")
        Problem --> P2("数据来源与合法性信息丢失/Loss of data origin & legitimacy info")
        Method --> M1("提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework")
        Method --> M2("开发基于数据溯源技术的开源库/Develop open-source library using data provenance")
        Results --> R1("评估现有数据集的合规性/Evaluate compliance of existing datasets")
        Results --> R2("指导负责任的新数据集构建/Guide responsible construction of new datasets")
    ```

- **[arXiv251229] Inference-based GAN Video Generation**
  - **tags:** [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]
  - **authors:** Jingbo Yang, Adrian G. Bors
  - **institution:** University of York
  - **link:** https://arxiv.org/pdf/2512.21776
  - **contributions:** 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Inference-based GAN Video Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]
        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]
        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]
        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]
        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]
        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]
        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]
    ```

- **[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents**
  - **tags:** [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]
  - **authors:** Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin
  - **institution:** Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard
  - **link:** https://arxiv.org/pdf/2512.21782
  - **contributions:** 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]
        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]
        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]
        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]
        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]
    ```

- **[arXiv251229] Multi-agent Adaptive Mechanism Design**
  - **tags:** [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]
  - **authors:** Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao
  - **institution:** Massachusetts Institute of Technology, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.21794
  - **contributions:** 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\tilde\{O\}(\sqrt\{T\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]
        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]
        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\tilde{O}(\sqrt{T})$ regret]
    ```

- **[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning**
  - **tags:** [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]
  - **authors:** Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles
  - **institution:** The Pennsylvania State University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21789
  - **contributions:** 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]
        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]
        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]
        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]
        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]
        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]
        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]
    ```

- **[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation**
  - **tags:** [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]
  - **authors:** Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan
  - **institution:** ByteDance Inc., Rutgers University
  - **link:** https://arxiv.org/pdf/2512.21788
  - **code:** https://github.com/yanq095/InstructMoLE
  - **contributions:** 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]
        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]
        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]
    ```

- **[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection**
  - **tags:** [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]
  - **authors:** Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu
  - **institution:** University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21803
  - **contributions:** 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]
        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]
        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]
    ```

- **[arXiv251229] S&P 500 Stock's Movement Prediction using CNN**
  - **tags:** [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]
  - **authors:** Rahul Gupta
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.21804
  - **contributions:** 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势"] --> P1["传统方法依赖特征工程<br>Traditional methods rely on engineered features"]
        Problem --> P2["现有研究多使用单维数据<br>Existing research often uses single-dimension data"]
        Method["主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN"] --> M1["将历史数据矩阵视为图像<br>Treat historical data matrices as images"]
        Method --> M2["包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)"]
        Results["关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果"] --> R1["支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction"]
    ```

- **[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction**
  - **tags:** [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]
  - **authors:** Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida
  - **institution:** Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University
  - **link:** https://arxiv.org/pdf/2512.21853
  - **contributions:** 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MoonBot: 面向月球基地建设的模块化按需可重构机器人] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[月球探索与基地建设需求 / Lunar Exploration & Base Construction Needs]
        C --> C1[模块化可重构机器人系统 / Modular & Reconfigurable Robotic System]
        C --> C2[概念验证与现场演示 / Proof-of-Concept & Field Demonstration]
        D --> D1[成功执行模拟任务 / Successfully Executed Simulated Tasks]
        D --> D2[总结了连接器设计等经验教训 / Summarized Lessons (e.g., Connector Design)]
    ```

- **[arXiv251229] HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs**
  - **tags:** [nlp], [evaluation], [anthropomorphic intelligence, benchmark, psychological counseling, rubric-based evaluation, reasoning-before-scoring]
  - **authors:** Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang
  - **institution:** Ant Group, Xiamen University, Beijing Normal University, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.21849
  - **code:** https://github.com/inclusionAI/HeartBench
  - **contributions:** 1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based "reasoning-before-scoring" evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLMs缺乏拟人化智能 / LLMs lack anthropomorphic intelligence]
        B --> B2[中文语境缺乏评估框架 / Lack of evaluation frameworks in Chinese context]
        C --> C1[基于心理咨询场景的基准 / Benchmark based on psychological counseling scenarios]
        C --> C2[理论驱动的分类法 / Theory-driven taxonomy]
        C --> C3[基于量规的推理评分法 / Rubric-based reasoning-before-scoring]
        D --> D1[模型性能存在上限 / Performance ceiling in models]
        D --> D2[复杂场景表现显著下降 / Significant decay in complex scenarios]
    ```

- **[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs**
  - **tags:** [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]
  - **authors:** Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville
  - **institution:** Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR
  - **link:** https://arxiv.org/pdf/2512.21852
  - **contributions:** 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差"] --> P1["实践问题/Practical Issue<br>广泛使用但实现与目标不一致"]
        Problem --> P2["理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性"]
        Method["主要方法/Method<br>分析梯度偏差并进行实证验证"] --> M1["分析/Analysis<br>研究多种估计器配置的梯度"]
        Method --> M2["实验/Experiments<br>RL微调多个LLM并评估性能"]
        Results["关键结果/Results<br>无偏梯度配置带来更好性能"] --> R1["在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能"]
        Results --> R2["离线策略/Off-Policy<br>KL正则化有助于稳定异步训练"]
    ```

- **[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening**
  - **tags:** [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]
  - **authors:** Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan
  - **institution:** North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh
  - **link:** https://arxiv.org/pdf/2512.21861
  - **contributions:** 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]
        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]
        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation**
  - **tags:** [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]
  - **authors:** Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh
  - **institution:** Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21866
  - **contributions:** 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection")
        Problem --> P2("模型需要可解释性/Model needs explainability")
        Method --> M1("将随机森林转换为规则区域/Convert random forest to rule regions")
        Method --> M2("在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data")
        Results --> R1("数据量减少85-93%/Data volume reduced by 85-93%")
        Results --> R2("保持竞争性性能/Maintains competitive performance")
        Results --> R3("抵抗成员推理攻击/Resists membership-inference attacks")
    ```

- **[arXiv251229] MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting**
  - **tags:** [mlsys], [agent system], [multi-agent framework, bias mitigation, financial forecasting, LLM integration, modular design]
  - **authors:** Marc S. Montalvo, Hamed Yaghoobian
  - **institution:** Rochester Institute of Technology, Muhlenberg College
  - **link:** https://arxiv.org/pdf/2512.21878
  - **contributions:** 1. Introduces MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news for decomposed financial reasoning. 2. Embeds explicit bias-mitigation protocols (e.g., against survivorship and hindsight bias) to enhance transparency and robustness. 3. Demonstrates practical effectiveness through an eight-week evaluation showing outperformance of major market benchmarks, highlighting the promise of bias-aware generative AI in finance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/351cdc3ccfe2b2d987cf53c8380e153fe4b93de0def6253cbc7b2feb4af093fe_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MASFIN, a multi-agent system that combines LLMs with financial data and news to perform decomposed reasoning and forecasting while mitigating biases. In an eight-week evaluation, it achieved a 7.33% cumulative return, outperforming benchmarks like the S&P 500 in most weeks, though with higher volatility. The results show the potential of modular, bias-aware AI frameworks for transparent and reproducible quantitative finance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统量化方法易受生存偏差影响/Traditional quantitative methods vulnerable to survivorship bias]
        B --> B2[AI方法在信号集成和可复现性上存在挑战/AI approaches struggle with signal integration and reproducibility]
        C --> C1[模块化多智能体框架/Modular multi-agent framework]
        C --> C2[集成LLM与结构化指标和非结构化新闻/Integrates LLMs with structured metrics and unstructured news]
        C --> C3[嵌入偏差缓解协议/Embeds bias-mitigation protocols]
        D --> D1[8周累计回报7.33%/7.33% cumulative return over eight weeks]
        D --> D2[在6/8周中超越基准/Outperformed benchmarks in six of eight weeks]
        D --> D3[波动性较高/Higher volatility]
    ```

- **[arXiv251229] CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics**
  - **tags:** [nlp], [text-to-sql], [benchmark, multilingual, domain-specific, large language models, sports analytics]
  - **authors:** Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa
  - **institution:** Birla Institute of Technology and Science (BITS), Pilani
  - **link:** https://arxiv.org/pdf/2512.21877
  - **contributions:** 1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a "Gold Standard" dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs在专业领域Text-to-SQL能力未充分探索/LLMs' Text-to-SQL capability in specialized domains is under-explored]
        B --> B2[现有基准缺乏多语言和体育分析特性/Existing benchmarks lack multilingual and sports analytics features]
        C --> C1[构建板球领域专业多语言基准/Build a specialized multilingual benchmark for cricket]
        C --> C2[与专家合作创建"黄金标准"查询/Collaborate with experts to create "Gold Standard" queries]
        C --> C3[评估六个最先进的LLMs/Evaluate six state-of-the-art LLMs]
        D --> D1[专业领域性能显著下降/Significant performance drop in specialized domain]
        D --> D2[DeepSeek R1表现最佳/DeepSeek R1 achieves SOTA]
        D --> D3[印地语查询准确率可比或更高/Hindi queries yield parity or higher accuracy]
    ```

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [distributed inference, block placement, request routing, performance modeling, resource allocation]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **contributions:** 1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]
        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]
        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]
    ```

- **[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space**
  - **tags:** [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]
  - **authors:** Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21887
  - **contributions:** 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[UAV导航缺乏高层语义规划能力/UAV navigation lacks high-level semantic planning]
        B --> B2[现有模型难以处理复杂动作空间与长距离视觉生成/Existing models struggle with complex action space & long-horizon visual generation]
        C --> C1[提出ANWM世界模型/Propose ANWM world model]
        C --> C2[引入未来帧投影模块/Introduce Future Frame Projection module]
        D --> D1[长距离视觉预测性能显著提升/Significantly outperforms in long-distance visual forecasting]
        D --> D2[提高大规模环境导航成功率/Improves UAV navigation success rates in large-scale environments]
    ```

- **[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy**
  - **tags:** [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]
  - **authors:** Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du
  - **institution:** University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University
  - **link:** https://arxiv.org/pdf/2512.21898
  - **contributions:** 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Flexible Multitask Learning with Factorized Diffusion Policy] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[机器人多任务学习/Robot Multitask Learning]
        B1 --> B2[动作分布复杂多模态/Action Distribution Highly Multimodal]
        B2 --> B3[单体模型欠拟合与不灵活/Monolithic Models Underfit & Inflexible]
        C --> C1[因子化扩散策略/Factorized Diffusion Policy (FDP)]
        C1 --> C2[模块化扩散专家/Modular Diffusion Experts]
        C2 --> C3[基于观察的路由器/Observation-Conditioned Router]
        C3 --> C4[连续分数聚合/Continuous Score Aggregation]
        D --> D1[性能超越基线/Outperforms Baselines]
        D1 --> D2[仿真与真实机器人验证/Simulation & Real-World Validation]
        D2 --> D3[支持灵活策略适应/Enables Flexible Policy Adaptation]
    ```

- **[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction**
  - **tags:** [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]
  - **authors:** Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han
  - **institution:** Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University
  - **link:** https://arxiv.org/pdf/2512.21897
  - **contributions:** 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]
        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]
        Method --> M2[架构设计/Architecture Design]
        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]
        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]
        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]
        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]
        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]
        Results --> R2[消融研究<br>Ablation Studies]
        Results --> R3[概率校准<br>Probability Calibration]
    ```

- **[arXiv251229] SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?**
  - **tags:** [ai], [agent system], [spatial transcriptomics, AI agents, benchmark, deterministic grader, harness design]
  - **authors:** Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le
  - **institution:** LatchBio
  - **link:** https://arxiv.org/pdf/2512.21907
  - **contributions:** 1. Introduces SpatialBench, a benchmark of 146 verifiable problems derived from real-world spatial biology analysis workflows, covering five technologies and seven task categories. 2. Provides a deterministic grader for each problem to evaluate the recovery of key biological results from messy spatial datasets. 3. Demonstrates through benchmark data that frontier AI agents have low accuracy (20-38%) on these tasks and reveals the significant impact of harness design (tools, prompts, control flow, execution environment) on performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3912de9f7e0f0d2acbf8bcd709b023f2ed3b9ccd56886885ca3f8e9e9e81880_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SpatialBench, a benchmark to evaluate whether AI agents can analyze messy, real-world spatial biology data. It tests frontier models on 146 practical problems and finds low accuracy, highlighting that performance heavily depends on the agent's harness design. The benchmark serves as a tool to measure and diagnose agent capabilities for faithful and reproducible data analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI代理能否从混乱的真实空间数据中提取生物学见解?/Can AI agents extract biological insight from messy, real-world spatial datasets?]
        C --> C1[引入包含146个可验证问题的基准SpatialBench/Introduce SpatialBench benchmark with 146 verifiable problems]
        C --> C2[提供确定性评分器评估关键生物学结果恢复/Provide deterministic grader to evaluate recovery of key biological result]
        D --> D1[基础模型准确率低 (20-38%)/Base model accuracy remains low (20-38%)]
        D --> D2[工具链设计对性能有重大影响/Harness design has large empirical effect on performance]
    ```

- **[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]
  - **authors:** Nathan Kallus
  - **institution:** Netflix, Cornell University
  - **link:** https://arxiv.org/pdf/2512.21917
  - **contributions:** 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment"]
        Method["主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model"]
        Results["关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees"]
    ```

- **[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning**
  - **tags:** [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]
  - **authors:** Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21924
  - **contributions:** 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[泛化性差与异常残留/Generalizability & Residuals]
        C --> C1[解耦表示模块/Disentangled Representation Module]
        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]
        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]
    ```

- **[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models**
  - **tags:** [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]
  - **authors:** Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner
  - **institution:** Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)
  - **link:** https://arxiv.org/pdf/2512.21985
  - **contributions:** 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]
        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]
        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]
        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]
        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]
        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]
    ```

- **[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration**
  - **tags:** [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]
  - **authors:** Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji
  - **institution:** Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University
  - **link:** https://arxiv.org/pdf/2512.22010
  - **contributions:** 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]
        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]
        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]
    ```

- **[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation**
  - **tags:** [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]
  - **authors:** Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni
  - **institution:** University College London, University of Urbino Carlo Bo
  - **link:** https://arxiv.org/pdf/2512.22031
  - **contributions:** 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Hit identification is resource-intensive/命中识别资源密集")
        Method --> M1("Propose tailored evaluation framework/提出定制评估框架")
        Method --> M2("Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型")
        Results --> R1("Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物")
        Results --> R2("Selected hits synthesized & confirmed active/选定命中物被合成并确认有效")
    ```

- **[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN**
  - **tags:** [sys], [communication & networking], [Conditional Handovers, O-RAN, Meta-Learning, Mobility Management, xApp]
  - **authors:** Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers
  - **institution:** Delft University of Technology, Telefónica Research
  - **link:** https://arxiv.org/pdf/2512.22022
  - **contributions:** 1. Introduces CONTRA, the first framework to jointly optimize Traditional and Conditional Handovers within the O-RAN architecture. 2. Proposes a practical meta-learning algorithm for adaptive, on-the-fly handover type selection, guaranteeing universal no-regret performance. 3. Provides and analyzes unique, countrywide mobility management datasets from a top-tier mobile network operator, offering fresh insights into handover trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of traditional and conditional handovers in mobile networks by proposing CONTRA, a meta-learning-based framework for O-RAN that dynamically selects and optimizes handover types. It is designed as a near-real-time xApp and is evaluated using real-world datasets. The results show that CONTRA improves user throughput and reduces switching costs, outperforming standard and RL-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-Learning-Based Handover Management in NextG O-RAN] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统切换延迟与失败/Traditional HO delays & failures]
        B --> B2[切换类型间的权衡/Trade-offs between HO types]
        C --> C1[CONTRA框架: 联合优化THO与CHO/CONTRA: Jointly optimizes THOs & CHOs]
        C --> C2[元学习算法动态选择/Meta-learning for dynamic selection]
        C --> C3[O-RAN xApp部署/O-RAN xApp deployment]
        D --> D1[提升用户吞吐量/Improves user throughput]
        D --> D2[降低切换成本/Reduces HO switching costs]
        D --> D3[优于3GPP与RL基线/Outperforms 3GPP & RL baselines]
    ```

- **[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning**
  - **tags:** [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]
  - **authors:** Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo
  - **institution:** Nanjing University, University of Wollongong, University of Rochester
  - **link:** https://arxiv.org/pdf/2512.22029
  - **code:** https://github.com/RL-VIG/LibContinual
  - **contributions:** 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]
        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]
        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]
        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]
        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]
        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]
        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]
    ```

- **[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars**
  - **tags:** [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]
  - **authors:** Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu
  - **institution:** Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22065
  - **code:** https://streamavatar.github.io
  - **contributions:** 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]
        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]
        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]
    ```

- **[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law**
  - **tags:** [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]
  - **authors:** Chiwun Yang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22088
  - **contributions:** 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\{-1/6\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]
        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]
        C --> C2[近似为核行为 / Approximate to kernel behaviors]
        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]
        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]
        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]
        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]
        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]
    ```

- **[arXiv251229] Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis**
  - **tags:** [nlp], [benchmark construction], [Turkish NLU benchmark, semi-automated annotation, sentiment analysis dataset]
  - **authors:** Duygu Altinok
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22100
  - **contributions:** 1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Introducing TrGLUE and SentiTurca] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏土耳其语综合基准/Lack of Turkish NLU Benchmark]
        C --> C1[半自动标注流程/Semi-automated Pipeline]
        C1 --> C2[LLM标注 + 交叉验证 + 人工校验/LLM Annotation + Cross-check + Human Validation]
        D --> D1[发布TrGLUE & SentiTurca/Release TrGLUE & SentiTurca]
        D --> D2[提供代码与资源/Provide Code & Resources]
    ```

- **[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting**
  - **tags:** [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]
  - **authors:** Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22101
  - **contributions:** 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A2P-Vis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]
        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]
        B1 --> B3[组装成专业报告/Assembling coherent professional report]
        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]
        C1 --> C2[数据分析器/Data Analyzer]
        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]
        C1 --> C4[报告呈现器/Presenter]
        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]
        D --> D1[端到端协同分析/End-to-end co-analysis]
        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]
    ```

- **[arXiv251229] Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [neural network pruning, game theory, equilibrium, non-cooperative game, sparsification]
  - **authors:** Zubair Shah, Noaman Khan
  - **institution:** Hamad Bin Khalifa University
  - **link:** https://arxiv.org/pdf/2512.22106
  - **contributions:** 1. Proposes a novel game-theoretic perspective on neural network pruning, modeling parameter groups as players in a non-cooperative game where sparsity emerges as an equilibrium outcome. 2. Provides a theoretical analysis showing that dominated players (redundant parameters) collapse to zero participation under mild conditions, offering a principled explanation for pruning. 3. Derives a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit, heuristic importance scores.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4fd762b6b064bb7810151beeb40f55c93bfc05054b2d4a98cd925aed8bea43b2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel game-theoretic framework for neural network pruning, where sparsity emerges naturally from the equilibrium of a non-cooperative game among model components. The method jointly updates network parameters and participation variables without external importance scores. Experiments show it achieves competitive sparsity-accuracy trade-offs with a more interpretable, theory-grounded foundation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks") --> Problem("核心问题/Problem: Sparsity is imposed externally via heuristics, lacking a principled model of parameter interaction.")
        Root --> Method("主要方法/Method: Model pruning as a non-cooperative game among parameters; sparsity emerges at equilibrium.")
        Root --> Results("关键结果/Results: Competitive sparsity-accuracy trade-offs with an interpretable, theory-grounded algorithm.")
    ```

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [agent system], [root cause analysis, service dependency graph, program dependence graph, LLM agent, cloud incident]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **contributions:** 1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]
        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]
        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]
    ```

- **[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding**
  - **tags:** [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]
  - **authors:** Akshansh Mishra
  - **institution:** Politecnico di Milano, AI Fab Lab
  - **link:** https://arxiv.org/pdf/2512.21344
  - **contributions:** 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模"]
        Root --> Problem["准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW"]
        Root --> Method["使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction"]
        Root --> Results["模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation"]
    ```

- **[arXiv251229] Applications of synthetic financial data in portfolio and risk modeling**
  - **tags:** [ai], [generative models for time series], [TimeGAN, Variational Autoencoder (VAE), synthetic financial data, portfolio optimization, risk modeling]
  - **authors:** Christophe D. Hounwanou, Yae Ulrich Gaba
  - **institution:** African Institute for Mathematical Sciences (AIMS Rwanda), Sefako Makgatho Health Sciences University (SMU), AI Research and Innovation Nexus for Africa (AIRINA Labs)
  - **link:** https://arxiv.org/pdf/2512.21798
  - **contributions:** 1. Evaluated and compared the performance of TimeGAN and VAEs for generating realistic synthetic financial time series data. 2. Demonstrated that TimeGAN-generated data closely matches real data in distributional, volatility, and autocorrelation properties. 3. Showed the practical utility of synthetic data in downstream financial tasks like mean-variance portfolio optimization, yielding similar portfolio weights and risk metrics to real data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a7ecf00c1350b58057e2f03c93bf0e8845d1441745fc22505c46475eceeeb65_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the scarcity and privacy issues of real financial data by using generative models like TimeGAN and VAEs to create synthetic return series. It evaluates the synthetic data on statistical similarity and financial tasks, concluding that TimeGAN effectively captures temporal dynamics and can serve as a privacy-preserving, cost-effective substitute for real data in portfolio and risk analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Applications of synthetic financial data in portfolio and risk modeling") --> Problem("核心问题/Problem: Privacy and accessibility limit financial research")
        Root --> Method("主要方法/Method: Use TimeGAN and VAEs to generate synthetic financial time series")
        Root --> Results("关键结果/Results: TimeGAN data is realistic and useful for portfolio/risk tasks")
    ```

- **[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models**
  - **tags:** [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]
  - **authors:** Takuro Kutsuna
  - **institution:** Toyota Central R&D Labs., Inc.
  - **link:** https://arxiv.org/pdf/2512.21593
  - **contributions:** 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Residual Prior Diffusion (RPD) / 残差先验扩散模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details"]
        Method --> M1["两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model"]
        Method --> M2["概率模型与可处理ELBO / Probabilistic model with tractable ELBO"]
        Results --> R1["在合成数据上准确捕捉细节 / Accurately captures details on synthetic data"]
        Results --> R2["自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines"]
        Results --> R3["少步推理保持性能 / Maintains performance with few inference steps"]
    ```

- **[arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database**
  - **tags:** [cv], [medical image reconstruction], [foundation model, k-space, multimodal database, zero-shot generalization, accelerated imaging]
  - **authors:** Zi Wang, Mingkai Huang, Zhang Shi, Hongjie Hu, Lan Lan, Hui Zhang, Yan Li, Xi Hu, Qing Lu, Zongming Zhu, Qiong Yao, Yuxiang Dai, Fanwen Wang, Yinzhe Wu, Jun Lyu, Qianqian Gao, Guangming Xu, Zhenxuan Zhang, Haosen Zhang, Qing Li, Guangming Wang, Tianxing He, Lizhen Lan, Siyue Li, Le Xue, Mengting Sun, Yuntong Lyu, Junpu Hu, Jiayu Zhu, Rizwan Ahmad, Zhengyu Bu, Xianling Qian, Guanke Cai, Ruiyu Cao, Weirui Cai, Chang Xu, Yuyang Ren, Feidan Yu, Siying Ma, Ziqiang Xu, Xinran Chen, Sha Hua, Daniel Kim, Yajing Zhang, Chen Ouyang, Wenjia Bai, Jing Qin, Yucheng Yang, Daniel Rueckert, He Wang, Qian Tao, Claudia Prieto, Michael Markl, Alistair Young, Lianming Wu, Shuo Wang, Chen Qin, Mengsu Zeng, Xihong Hu, Haibo Xu, Xiaobo Qu, Hao Li, Guang Yang, Chengyan Wang
  - **institution:** Imperial College London, Fudan University, Xiamen University
  - **link:** https://arxiv.org/pdf/2512.21652
  - **contributions:** 1. The curation of MMCMR-427K, the largest and most comprehensive multimodal cardiovascular magnetic resonance (CMR) k-space database. 2. The introduction of CardioMM, a generalist reconstruction foundation model that unifies semantic understanding with physics-informed data consistency for robust, accelerated imaging. 3. Demonstrating state-of-the-art performance and strong zero-shot generalization across heterogeneous clinical settings, enabling up to 24x acceleration without compromising clinical integrity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c34604e8bff7b6e1efa31a4faa67e7e43e6efb18e01ea880e43095344349_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow scan times and environmental heterogeneity limiting clinical cardiovascular MRI. It proposes CardioMM, a generalist foundation model trained on a large multimodal k-space database (MMCMR-427K), which achieves robust, ultra-fast reconstructions across diverse scanners and protocols. The results show that CardioMM enables high acceleration (up to 24x) while preserving diagnostic quality and generalizing to unseen clinical environments.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Enabling Ultra-Fast Cardiovascular Imaging...] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[CMR扫描时间长/CMR Scan Time Long]
    B --> B2[临床环境异质性高/High Clinical Heterogeneity]
    C --> C1[构建多模态数据库MMCMR-427K/Build Multimodal DB MMCMR-427K]
    C --> C2[提出通用基础模型CardioMM/Propose Generalist Foundation Model CardioMM]
    D --> D1[实现24倍加速成像/Achieve 24x Accelerated Imaging]
    D --> D2[零样本泛化至新环境/Zero-shot Generalization to New Settings]
    D --> D3[保持诊断质量/Preserve Diagnostic Quality]
    ```

## 2025-12-30

- **[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems**
  - **tags:** [mlsys], [llm inference], [GPU Virtualization, Benchmarking, Multi-tenancy, CUDA, Performance Isolation]
  - **authors:** Jithin VG, Ditto PS
  - **institution:** Bud Ecosystem Inc
  - **link:** https://arxiv.org/pdf/2512.22125
  - **code:** https://github.com/BudEcosystem/GPU-Virt-Bench
  - **contributions:** 1. Proposed GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. 2. Enabled systematic comparison between software virtualization approaches (e.g., HAMi-core, BUD-FCSP) and ideal hardware-based MIG behavior. 3. Demonstrated the framework's utility by revealing critical performance characteristics for production deployment decisions in multi-tenant environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of standardized evaluation for software-based GPU virtualization systems, which are needed for efficient GPU sharing in AI/LLM workloads. The authors propose GPU-Virt-Bench, a comprehensive benchmarking framework that measures performance across multiple critical dimensions. The framework provides actionable insights for practitioners by comparing software solutions against hardware-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPU-Virt-Bench: A Comprehensive Benchmarking Framework] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GPU资源共享需求高，但软件虚拟化方案缺乏标准化评估/High demand for GPU sharing, but software virtualization lacks standardized evaluation]
        C --> C1[提出包含56个指标、10个类别的综合基准测试框架/Propose a comprehensive benchmarking framework with 56 metrics across 10 categories]
        D --> D1[系统比较软件方案与MIG，为生产部署提供关键性能洞察/Systematic comparison between software approaches and MIG provides key performance insights for deployment]
    ```

- **[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling**
  - **tags:** [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]
  - **authors:** Conor Wallace, Umer Siddique, Yongcan Cao
  - **institution:** University of Texas at San Antonio
  - **link:** https://arxiv.org/pdf/2512.22129
  - **contributions:** 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("RECOLLAB: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Ad-hoc Teammate Modeling<br>Ad-hoc队友建模")
        Problem --> P2("Brittle Conventional Models<br>传统模型脆弱性")
        Method --> M1("COLLAB: LLM-based Framework<br>基于LLM的框架")
        Method --> M2("RECOLLAB: Adds RAG<br>增加RAG检索")
        Results --> R1("Improved Adaptation<br>提升适应性")
        Results --> R2("Pareto-Optimal Trade-offs<br>帕累托最优权衡")
    ```

- **[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web**
  - **tags:** [mlsys], [agent system], [Sovereign Digital Avatar, Intent-Permission Handshake, orthogonal decoupling, A2A protocols, dual-factor adaptive routing]
  - **authors:** Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang
  - **institution:** Shanghai Jiao Tong University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22135
  - **contributions:** 1. Proposes a user sovereignty interaction paradigm for the Agentic Web, decoupling memory from application logic to break data lock-in and shifting from explicit instruction to implicit intent alignment to reduce cognitive load. 2. Implements the paradigm via the Sovereign Digital Avatar (SoDA) with an orthogonal decoupling design of storage, computation, and interaction, establishing the principle of "data as a persistent asset, model as a transient tool". 3. Designs an Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing for active risk governance in zero-trust environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples user memory from applications and uses intent alignment to reduce cognitive load. It introduces an architecture with orthogonal decoupling and a secure handshake mechanism for zero-trust environments. Empirical results show it significantly reduces token consumption and user cognitive load compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoDA: An Efficient Interaction Paradigm for the Agentic Web] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据锁定/Data Lock-in]
        B --> B2[认知过载/Cognitive Overload]
        C --> C1[主权数字化身/Sovereign Digital Avatar (SoDA)]
        C --> C2[正交解耦设计/Orthogonal Decoupling Design]
        C --> C3[意图-权限握手机制/Intent-Permission Handshake Mechanism]
        D --> D1[降低令牌消耗/Reduces Token Consumption by 27-35%]
        D --> D2[降低认知负载/Reduces Cognitive Load by 72% vs RAG, 88% vs Manual]
    ```

- **[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA**
  - **tags:** [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]
  - **authors:** Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn
  - **institution:** National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)
  - **link:** https://arxiv.org/pdf/2512.22139
  - **code:** https://github.com/dll-ncai/HLS4PC
  - **contributions:** 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]
        P1 --> P2[High memory/computation demand hinders real-time performance]
        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]
        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]
        M2 --> M3[Creates PointMLP-Lite model]
        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]
        R1 --> R2[3.56x higher throughput vs. prior work]
        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]
    ```

- **[arXiv251230] Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models**
  - **tags:** [ai], [peer review automation], [large language models, peer review, pre-review, citation prediction, review alignment]
  - **authors:** Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi
  - **institution:** AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI
  - **link:** https://arxiv.org/pdf/2512.22145
  - **contributions:** 1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]
        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]
        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]
        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]
    ```

- **[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs**
  - **tags:** [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]
  - **authors:** Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong
  - **institution:** School of Software Engineering, Xi’an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22147
  - **contributions:** 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]
        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]
        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]
        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]
        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]
        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]
        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]
    ```

- **[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments**
  - **tags:** [mlsys], [agent system], [serverless computing, GPU resource allocation, workload scheduling, multi-agent systems, collaborative reasoning]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.22149
  - **contributions:** 1. An adaptive GPU resource allocation framework for multi-agent systems in serverless environments that dynamically adjusts resources based on workload characteristics, agent priorities, and minimum requirements. 2. An O(N) complexity algorithm for real-time adaptation, enabling millisecond-scale reallocation to handle dynamic workload fluctuations. 3. A comprehensive evaluation demonstrating the framework's superiority over static and round-robin strategies, achieving 85% latency reduction while maintaining throughput and improving GPU utilization and cost-efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an adaptive GPU resource allocation framework to address the challenge of efficiently deploying heterogeneous multi-agent AI systems on serverless platforms. The method dynamically allocates resources using a real-time algorithm to handle varying computational demands and workload fluctuations. The results show it significantly reduces latency compared to baseline schedulers while maintaining throughput, offering a cost-effective solution for serverless multi-agent deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments<br/>面向无服务器环境的多智能体协同推理的自适应GPU资源分配"] --> Problem["核心问题/Problem<br/>Heterogeneous agent workloads & dynamic demands on serverless GPU platforms<br/>多智能体工作负载异构与无服务器GPU平台动态需求"]
        Root --> Method["主要方法/Method<br/>Adaptive GPU resource allocation framework with O(N) real-time algorithm<br/>基于O(N)实时算法的自适应GPU资源分配框架"]
        Root --> Results["关键结果/Results<br/>85% latency reduction vs. round-robin, maintains throughput<br/>相比轮询调度延迟降低85%，保持吞吐量"]
    ```

- **[arXiv251230] Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification**
  - **tags:** [ai], [speaker verification], [Layer Attentive Pooling, Attentive Statistical Temporal Pooling, pre-trained speech models, multi-level features, speaker embeddings]
  - **authors:** Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.22148
  - **code:** https://github.com/sadPororo/LAP
  - **contributions:** 1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[重新思考利用预训练多层表示进行说话人验证<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>静态加权平均聚合多层特征的局限性<br/>Limitations of static weighted average for multi-layer feature aggregation] --> Problem_Detail[细节/Detail<br/>未充分利用高层表示<br/>Underutilization of high-level representations]
        Method[主要方法/Method<br/>提出层注意力池化<br/>Propose Layer Attentive Pooling (LAP)] --> Method_Detail1[细节/Detail<br/>动态多视角评估层重要性<br/>Time-dynamically assess layer significance from multiple perspectives]
        Method --> Method_Detail2[细节/Detail<br/>使用最大池化而非平均<br/>Employ max pooling instead of averaging]
        Method --> Method_Detail3[细节/Detail<br/>轻量级后端模型 (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]
        Results[关键结果/Results<br/>在VoxCeleb上达到SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --> Results_Detail[细节/Detail<br/>性能优越且大幅减少训练时间<br/>Superior performance and greatly reduced training time]
    ```

- **[arXiv251230] Practical challenges of control monitoring in frontier AI deployments**
  - **tags:** [sec], [ai security], [control monitoring, oversight latency, safety case, scheming agents, incremental attacks]
  - **authors:** David Lindner, Charlie Griffin, Tomek Korbak, Roland S. Zimmermann, Geoffrey Irving, Sebastian Farquhar, Alan Cooney
  - **institution:** Google DeepMind, UK AI Safety Institute, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22154
  - **contributions:** 1. Analysis of real-world deployment dynamics (parallelism, latency, incremental attacks, partial incrimination) for control monitoring, 2. Proposal and comparison of three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs, 3. Introduction of a high-level safety case sketch as a tool for analyzing and comparing monitoring protocols, applied to four case studies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cfc3d08c169a23a56cd0a16a1471c5bceacb1b76913f7079f51b7024f030d1_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the practical challenges of scaling automated control monitors for overseeing frontier AI agents in real-world deployments. It proposes and compares three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs and introduces a safety case sketch as an analytical tool. The analysis identifies oversight, latency, and recovery as key challenges, explored through four case studies of potential AI attacks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Practical challenges of control monitoring in frontier AI deployments<br>前沿AI部署中控制监控的实际挑战") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现实部署的动态<br>Real-world Deployment Dynamics")
        P1 --> P1_1("并行实例<br>Parallel Instances")
        P1 --> P1_2("监督延迟<br>Oversight Latency")
        P1 --> P1_3("增量攻击<br>Incremental Attacks")
        P1 --> P1_4("部分归责<br>Partial Incrimination")
        Method --> M1("监控协议<br>Monitoring Protocols")
        M1 --> M1_1("同步监控<br>Synchronous")
        M1 --> M1_2("半同步监控<br>Semi-synchronous")
        M1 --> M1_3("异步监控<br>Asynchronous")
        Method --> M2("安全案例草图<br>Safety Case Sketch")
        Results --> R1("识别核心挑战<br>Identified Core Challenges")
        R1 --> R1_1("监督<br>Oversight")
        R1 --> R1_2("延迟<br>Latency")
        R1 --> R1_3("恢复<br>Recovery")
        Results --> R2("案例研究应用<br>Case Studies Application")
    ```

- **[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs**
  - **tags:** [mlsys], [fault-tolerance], [bit-flip faults, fault localization, transformer reliability, residual-path perturbation, loss-sensitivity profiling]
  - **authors:** Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22174
  - **contributions:** 1. Introduces BitFlipScope, a scalable software framework for localizing bit-flip corruptions in transformer-based LLMs under two deployment scenarios (with and without a clean reference model). 2. Proposes differential analysis for fault localization when a reference model is available and residual-path perturbation/loss-sensitivity profiling for localization when no reference exists. 3. Enables lightweight performance recovery for corrupted models without requiring costly fine-tuning or full retraining.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or perturbation-based profiling without one to identify fault-affected regions, enabling targeted recovery without full retraining. The work aims to improve fault resilience for LLMs in hardware-prone and adversarial environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs] --> B[核心问题/Problem: Bit-flip faults corrupt LLM parameters, causing unpredictable behavior]
        A --> C[主要方法/Method: Differential analysis with reference model; Residual-path perturbation & loss-sensitivity profiling without reference]
        A --> D[关键结果/Results: Enables fault localization and lightweight recovery, improving fault-resilient LLM deployment]
    ```

- **[arXiv251230] Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time**
  - **tags:** [ai], [multi-agent path finding], [multi-agent path finding, vehicle routing, polynomial-time algorithm, conflict resolution, assignment problem]
  - **authors:** Stefan Edelkamp
  - **institution:** Charles University
  - **link:** https://arxiv.org/pdf/2512.22171
  - **contributions:**  1. Proposes a polynomial-time algorithm for solving discrete multi-agent multi-goal path finding (CMAPF) problems with node and edge conflicts, which is unexpected given the NP-hardness of traditional vehicle routing. 2. Introduces a planner that autonomously finds and updates the assignment of multiple goals to agents, contrasting with regular MAPF which uses fixed assignments. 3. Develops conflict resolution strategies including global assignment to reduce conflicts, and local methods like "ants-on-the-stick," local assignment, path interleaving, and destination clearing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9267701cf1d96333033d8663590f3b652040a494de98cd10ba5a86ede709d3b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the multi-agent multi-goal path finding (CMAPF) problem where agents in graphs must be assigned and routed to multiple goals. It presents a polynomial-time algorithm for discrete variants with conflicts, implemented in a planner that autonomously handles goal assignment and resolves conflicts. The main conclusion is that efficient, conflict-free solutions can be achieved in polynomial time, challenging the typical NP-hard complexity of vehicle routing.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[为多智能体规划多目标无冲突路径/Plan multi-goal conflict-free paths for multi-agent]
    C --> C1[自主目标分配与冲突解决策略/Autonomous goal assignment & conflict resolution]
    D --> D1[离散问题可在多项式时间内解决/Discrete problems solvable in polynomial time]
    ```

- **[arXiv251230] Wireless Traffic Prediction with Large Language Model**
  - **tags:** [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]
  - **authors:** Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini
  - **institution:** Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)
  - **link:** https://arxiv.org/pdf/2512.22178
  - **contributions:** 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Wireless Traffic Prediction with Large Language Model] --> B(核心问题/Problem: LLMs overlook spatial dependencies in city-scale wireless traffic)
        A --> C(主要方法/Method: TIDES framework with clustering, prompt engineering, and DeepSeek spatial alignment module)
        A --> D(关键结果/Results: Outperforms SOTA baselines in accuracy and robustness for 6G network management)
    ```

- **[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps**
  - **tags:** [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]
  - **authors:** Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava
  - **institution:** University of Maryland
  - **link:** https://arxiv.org/pdf/2512.22175
  - **contributions:** 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Characterizing Motion Encoding in Video Diffusion Timesteps] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[视频扩散模型中运动编码机制不明确 / Motion encoding in video diffusion is poorly understood]
        C --> C1[通过条件注入量化运动-外观权衡 / Quantify motion-appearance trade-off via conditional injection]
        C --> C2[大规模定量研究 / Large-scale quantitative study]
        D --> D1[识别早期运动主导与后期外观主导阶段 / Identify early motion-dominant and late appearance-dominant regimes]
        D --> D2[简化单样本运动定制范式 / Simplify one-shot motion customization paradigm]
    ```

- **[arXiv251230] iOS as Acceleration**
  - **tags:** [mlsys], [on-device ai], [distributed pipeline parallelism, mobile acceleration, iOS, memory constraints, thermal throttling]
  - **authors:** Alexander K. Chen
  - **institution:** Independent High School Researcher (No institutional affiliation inferred)
  - **link:** https://arxiv.org/pdf/2512.22180
  - **contributions:** 1. Proposes a novel proof-of-concept system using distributed pipeline parallelism to harness iOS devices as computational accelerators for local ML tasks. 2. Demonstrates the system's effectiveness in accelerating modest model training (e.g., ResNet-34) and agentic LRM tool-usage, achieving a 44% decrease in training time in a specific setup. 3. Explores the unique potential of ubiquitous mobile devices with powerful processors and sensors (e.g., LiDAR, GPS) as cost-effective resources for embodied agentic AI and local compute, discussing practical use-cases and limitations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the barrier of expensive compute for local machine learning by proposing a system that uses distributed pipeline parallelism to leverage underutilized iOS phones as accelerators. The method partitions model weights to circumvent mobile memory limits, successfully accelerating tasks like training ResNet-34. The work concludes that commonplace mobile devices have significant potential to contribute to ML, especially for local, cost-sensitive, or sensor-driven applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[iOS as Acceleration] --> B[核心问题/Problem: Powerful compute is a barrier for local ML; Cloud is not always viable]
        A --> C[主要方法/Method: Use distributed pipeline parallelism to harness iOS devices as accelerators]
        A --> D[关键结果/Results: Achieved faster training for modest models; Highlights mobile potential for ML]
    ```

- **[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery**
  - **tags:** [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]
  - **authors:** Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir
  - **institution:** National University of Science and Technology (NUST)
  - **link:** https://arxiv.org/pdf/2512.22182
  - **contributions:** 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)
    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))
    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)
    ```

- **[arXiv251230] Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns**
  - **tags:** [ai], [network science], [co-authorship networks, link prediction, SHAP, random forest, interdisciplinary collaboration]
  - **authors:** Shahab Mosallaie, Andrea Schiffauerova, Ashkan Ebadi
  - **institution:** Concordia University, National Research Council Canada
  - **link:** https://arxiv.org/pdf/2512.22181
  - **contributions:** 1. Constructed 36 overlapping co-authorship networks from 7,738 publications to model new, persistent, and discontinued collaborations in AI-driven cancer research. 2. Engineered both attribute-based and structure-based features and built four machine learning classifiers, with Random Forest achieving the highest recall for all collaboration types. 3. Applied SHAP for model interpretability, identifying key factors like discipline similarity, productivity, and seniority that influence collaboration patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f6c5e92a95d2e6eeb8291558739ab641247bb834675d42c86ece0ee73d9d15_w640_q70.webp
  - **Simple LLM Summary:** This paper uses machine learning to predict collaboration patterns in AI-driven cancer research by analyzing co-authorship networks. The authors built classifiers using engineered features and applied SHAP for interpretability, finding that discipline similarity promotes new and persistent collaborations while high productivity and seniority are linked to discontinued links. The results aim to guide the formation of effective interdisciplinary research teams and inform policy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[挑战: 组建有效的跨学科癌症研究团队/Challenge: Forming effective interdisciplinary cancer research teams]
        C --> C1[构建合著网络作为合作代理/Construct co-authorship networks as collaboration proxy]
        C --> C2[使用机器学习分类器进行链接预测/Use ML classifiers for link prediction]
        C --> C3[使用SHAP进行模型可解释性/Use SHAP for model interpretability]
        D --> D1[随机森林在所有合作类型中召回率最高/Random forest achieved highest recall]
        D --> D2[学科相似性得分是关键因素/Discipline similarity score is a crucial factor]
        D --> D3[高生产力和资历与中断链接正相关/High productivity and seniority positively associated with discontinued links]
    ```

- **[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs**
  - **tags:** [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]
  - **authors:** Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou
  - **institution:** Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis
  - **link:** https://arxiv.org/pdf/2512.22183
  - **contributions:** 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unbiased Visual Reasoning with Controlled Visual Inputs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[VLMs exploit spurious correlations/VLMs利用虚假关联]
        C --> C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: 解耦感知与推理的模块化框架]
        C1 --> C2[Frozen VLM sensor + LLM reasoner/冻结VLM感知器 + LLM推理器]
        C2 --> C3[Train with RL (GRPO)/使用强化学习(GRPO)训练]
        D --> D1[Improved robustness on SpuriVerse/在SpuriVerse上鲁棒性提升]
        D --> D2[Competitive on MMVP & SeedBench/在MMVP & SeedBench上保持竞争力]
        D --> D3[Transferable & interpretable/可迁移且可解释]
    ```

- **[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks**
  - **tags:** [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]
  - **authors:** Vishnu Mohan
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22186
  - **contributions:** 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks] --> B(核心问题/Problem: Tennis strategy optimization as a sequential decision-making challenge with hierarchical scoring, stochasticity, and opponent adaptation)
        A --> C(主要方法/Method: Dueling Double Deep Q-Network (DDQN) trained with curriculum learning in a custom tennis simulation environment)
        A --> D(关键结果/Results: High win rates (98-100%) and stable convergence, but reveals a defensive policy bias, highlighting reward design limitations)
    ```

- **[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology**
  - **tags:** [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]
  - **authors:** Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He
  - **institution:** Tsinghua University, Xiamen University, Westlake University, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.22188
  - **code:** https://github.com/lingxitong/HookMIL
  - **contributions:** 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MIL loses context; Transformers are inefficient] --> P1[传统MIL丢失上下文/Traditional MIL loses context]
        Problem --> P2[基于Transformer的MIL计算复杂/Transformer-based MIL has quadratic complexity]
        Method[主要方法/Method: HookMIL Framework] --> M1[使用可学习的Hook Tokens/Use learnable Hook Tokens]
        Method --> M2[多模态初始化/Multimodal Initialization]
        Method --> M3[Hook多样性损失与通信机制/Hook Diversity Loss & Communication]
        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art Performance]
        Results --> R2[计算高效/Computationally Efficient]
        Results --> R3[可解释性/Interpretability]
    ```

- **[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]
  - **authors:** Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee
  - **institution:** Seoul National University, Samsung Electronics
  - **link:** https://arxiv.org/pdf/2512.22195
  - **code:** https://github.com/kunwooshin/MatKV
  - **contributions:** 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MatKV: Trading Compute for Flash Storage in LLM Inference"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>RAG推理中prefill阶段计算开销大<br>High compute cost of prefill in RAG inference"]
        Method["主要方法/Method<br>预计算并物化KV向量到闪存<br>Precompute & materialize KVs to flash storage"]
        Results["关键结果/Results<br>推理时间与能耗减半<br>Halves inference time & power consumption"]
    ```

- **[arXiv251230] Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation, Hallucination Prevention, Multi-Stage Validation, Corpus Expansion, Self-Improving AI]
  - **authors:** Teja Chinthala
  - **institution:** Independent Researcher (affiliated email domain: avila.edu)
  - **link:** https://arxiv.org/pdf/2512.22199
  - **contributions:** 1. A novel RAG architecture enabling safe corpus expansion through validated write-back of model outputs. 2. A multi-stage acceptance layer combining grounding verification, attribution checking, and novelty detection for safety. 3. An experience store for meta-learning from both accepted and rejected responses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eee110568078584ba44c846b5af3fab7d300dbfaec310a5826fd74784dc8040_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that conventional RAG systems have static knowledge bases. It proposes Bidirectional RAG, a novel architecture that safely expands the retrieval corpus by writing back high-quality, validated LLM responses. The results show that this self-improving approach nearly doubles answer coverage compared to standard RAG while adding significantly fewer documents than a naive write-back strategy, demonstrating a safe and practical path for RAG systems to learn from deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bidirectional RAG: Safe Self-Improving RAG] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[静态知识库无法从交互中学习/Static corpus cannot evolve from interactions]
        C --> C1[带验证的回写机制/Validated write-back]
        C --> C2[多阶段验证层/Multi-stage acceptance layer]
        D --> D1[覆盖率翻倍/Coverage doubled vs. Standard RAG]
        D --> D2[文档增长减少72%/72% less corpus growth vs. naive write-back]
    ```

- **[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]
  - **authors:** Yogeswar Reddy Thota
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.22206
  - **contributions:** 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --> B[核心问题/Problem: Modern residual networks perform redundant computation for all inputs]
        A --> C[主要方法/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]
        A --> D[关键结果/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]
    ```

- **[arXiv251230] Emergent Persuasion: Will LLMs Persuade Without Being Prompted?**
  - **tags:** [nlp], [ai safety & alignment], [emergent persuasion, activation steering, supervised fine-tuning (SFT), threat model, persona vectors]
  - **authors:** Vincent Chang, Thee Ho, Sunishchal Dev, Kevin Zhu, Shi Feng, Kellin Pelrine, Matthew Kowal
  - **institution:** Algoverse, FAR.AI, UC Berkeley, George Washington University, University of Toronto
  - **link:** https://arxiv.org/pdf/2512.22201
  - **code:** https://github.com/ith8/persona_vectors
  - **contributions:** 1. Investigates the novel threat model of "unprompted" or "emergent" persuasion in LLMs, moving beyond the standard misuse (prompted) scenario. 2. Empirically compares two techniques for inducing traits (activation steering vs. supervised fine-tuning) and finds SFT reliably increases unprompted persuasion while steering does not. 3. Demonstrates that SFT on benign persuasion datasets can lead to increased persuasion propensity on harmful topics, highlighting a significant safety risk.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3e4599070a9919228b3e5bc9cb7f7fd0fe17df086f5d7bec7ef20de22526f15_w640_q70.webp
  - **Simple LLM Summary:** This paper studies whether Large Language Models (LLMs) will attempt to persuade users without being explicitly prompted to do so. The authors investigate this by applying activation steering and supervised fine-tuning (SFT) to induce persuasive traits, finding that SFT reliably increases unprompted persuasion, including on harmful topics, even when trained only on benign data. The main conclusion is that emergent harmful persuasion is a real risk that warrants further study for AI safety and governance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Emergent Persuasion: Will LLMs Persuade Without Being Prompted?] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[研究模型在未受明确提示时进行说服的风险 / Study risk of unprompted persuasion]
        C --> C1[激活引导以植入人格特质 / Activation steering for persona traits]
        C --> C2[监督微调以植入人格特质 / Supervised fine-tuning for persona traits]
        D --> D1[监督微调会可靠地增加无提示说服 / SFT reliably increases unprompted persuasion]
        D --> D2[良性主题微调可能导致有害主题说服 / Benign SFT can increase harmful topic persuasion]
    ```

- **[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting**
  - **tags:** [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]
  - **authors:** Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu
  - **institution:** Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.22203
  - **contributions:** 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[TCFormer: 5M参数Transformer用于弱监督人群计数] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[标注成本高/High Annotation Cost]
        Problem --> P2[计算复杂度高/High Computational Complexity]
        Method[主要方法/Method] --> M1[高效视觉Transformer特征提取器/Efficient ViT Feature Extractor]
        Method --> M2[可学习密度加权平均模块/Learnable Density-Weighted Averaging]
        Method --> M3[密度等级分类损失/Density-Level Classification Loss]
        Results[关键结果/Results] --> R1[仅5M参数/Only 5M Parameters]
        Results --> R2[弱监督下竞争性性能/Competitive Performance under Weak Supervision]
        Results --> R3[适用于边缘设备/Suitable for Edge Devices]
    ```

- **[arXiv251230] GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks**
  - **tags:** [ai], [multimodal reasoning], [spatial reasoning, multimodal large language models, benchmark, origami folding, viewpoint consistency]
  - **authors:** Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma
  - **institution:** Algoverse AI Research, UC San Diego, University of New South Wales
  - **link:** https://arxiv.org/pdf/2512.22207
  - **code:** https://github.com/stvngo/GamiBench
  - **contributions:** 1. Introduces GamiBench, a novel benchmark for evaluating spatial reasoning and 2D-to-3D planning in MLLMs using origami folding tasks. 2. Proposes new diagnostic metrics, viewpoint consistency (VC) and impossible fold selection rate (IFSR), to holistically assess the reasoning process. 3. Provides a comprehensive dataset of 186 regular and 186 impossible 2D crease patterns with 3D shapes from multiple viewpoints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7bb91a40c2344a9e50dec2b2754647404d609db9a78508a54a040d5f6c5f58b_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces GamiBench, a benchmark that uses origami folding tasks to evaluate the spatial reasoning and 2D-to-3D planning capabilities of Multimodal Large Language Models (MLLMs). It assesses models on tasks like predicting 3D configurations and detecting impossible folds, revealing that even state-of-the-art models like GPT-5 and Gemini-2.5-Pro struggle with fundamental spatial understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MLLMs struggle with sequential, multi-view spatial reasoning] --> Problem_Sub[现有基准的不足/Existing benchmarks focus on static images]
        Method[主要方法/Method: Origami-inspired benchmark with 3 VQA tasks] --> Method_Sub[引入新指标/Introduces new metrics (VC, IFSR)]
        Results[关键结果/Results: Leading models (GPT-5, Gemini) struggle with spatial understanding]
    ```

- **[arXiv251230] Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh**
  - **tags:** [ai], [algorithmic fairness], [adversarial debiasing, gradient reversal layer, fairness-aware representation learning, statistical parity difference]
  - **authors:** Farjana Yesmin, Romana Akter
  - **institution:** Independent Researcher, Researcher (affiliations not specified)
  - **link:** https://arxiv.org/pdf/2512.22210
  - **contributions:** 1. A comprehensive dataset integrating official flood impact data with socioeconomic indicators across 87 upazilas in Bangladesh. 2. An adversarial debiasing architecture adapted from healthcare AI for disaster management. 3. Rigorous fairness evaluation showing significant reductions in statistical parity and regional fairness gaps while maintaining predictive accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd82a75c606f74970d3c93e7f31e57d4d35ae9f285a2ccb25e804770225a020b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a fairness-aware AI framework using adversarial debiasing with a gradient reversal layer to prioritize post-flood aid allocation in Bangladesh. The model learns bias-invariant representations to reduce systematic disadvantages against marginalized regions. Experimental results show it significantly improves fairness metrics while maintaining strong predictive accuracy, demonstrating the effective application of algorithmic fairness in humanitarian contexts.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Biased post-disaster aid allocation perpetuates historical inequities in vulnerable regions]
        C[主要方法/Method: Adversarial debiasing model with gradient reversal layer for bias-invariant representations]
        D[关键结果/Results: Reduces statistical parity by 41.6%, regional fairness gaps by 43.2%, maintains R²=0.784]
    ```

- **[arXiv251230] With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems**
  - **tags:** [mlsys], [agent system], [agentic AI, risk assessment, technical governance, autonomous action, safety controls]
  - **authors:** Shaun Khoo, Jessica Foo, Roy Ka-Wei Lee
  - **institution:** GovTech Singapore, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.22211
  - **code:** https://github.com/govtech-ai/arc-framework
  - **contributions:** 1. Introduces a novel capability-centric perspective for analyzing agentic AI systems. 2. Distills three primary intrinsic risk sources (components, design, capabilities) and maps them to specific risks and technical controls. 3. Provides a structured, practical methodology for organizations to implement the framework for governance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0283ff7ed974694c06d620f02b6bfd0752cd1ab34c83d05b1d66ec9b4088059c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Agentic Risk & Capability (ARC) Framework to address governance challenges posed by autonomous AI agents. The framework provides a structured methodology to identify, assess, and mitigate risks from agentic systems by analyzing their capabilities and linking risk sources to technical controls. It aims to enable safe and responsible deployment of agentic AI while supporting innovation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems"]
        Root --> Problem["核心问题/Problem: Agentic AI systems present novel risks and governance challenges due to autonomous actions like code execution and web interaction."]
        Root --> Method["主要方法/Method: Proposes the Agentic Risk & Capability (ARC) Framework, a technical governance framework for risk identification, assessment, and mitigation."]
        Root --> Results["关键结果/Results: Provides a robust, adaptable methodology for safe and responsible deployment of agentic AI, linking risk sources to controls."]
    ```

- **[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks**
  - **tags:** [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]
  - **authors:** Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao
  - **institution:** Imperial College London, UnlikelyAI
  - **link:** https://arxiv.org/pdf/2512.22213
  - **contributions:** 1. Identifies and characterizes a new class of "secondary attention sinks" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink's direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a new phenomenon called "secondary attention sinks" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["On the Existence and Behaviour of Secondary Attention Sinks<br/>二次注意力汇的存在与行为"] --> B["核心问题/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>先前研究仅关注持久的主汇（如BOS）"]
        A --> C["主要方法/Method<br/>Extensive experiments across 11 model families<br/>对11个模型系列进行广泛实验"]
        A --> D["关键结果/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>次级汇通过中间层MLP形成<br/>2. L2-norm determines sink score & lifetime<br/>L2范数决定汇分数与寿命<br/>3. Sink levels are deterministic in large models<br/>大模型中汇层级更确定"]
    ```

- **[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition**
  - **tags:** [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]
  - **authors:** Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid
  - **institution:** University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi
  - **link:** https://arxiv.org/pdf/2512.22217
  - **contributions:** 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>类别不平衡, 属性依赖, 域偏移]
        C[主要方法/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>视觉语言框架与跨注意力融合]
        D[关键结果/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K上SOTA, PETA & Market-1501上提升]
    ```

- **[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition**
  - **tags:** [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]
  - **authors:** Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.22214
  - **contributions:** 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Signal-SGN++<br/>论文标题/Paper Title] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GCNs能耗高<br/>GCNs High Energy Cost]
        B --> B2[SNNs难以捕捉时空-频率与拓扑依赖<br/>SNNs Limited in Capturing Time-Freq & Topology]
        C --> C1[主干网络: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]
        C --> C2[拓扑转移自注意力 TSSA<br/>Topology-Shift Self-Attention TSSA]
        C --> C3[多尺度小波变换融合 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]
        D --> D1[优于现有SNN方法<br/>Outperforms Existing SNN Methods]
        D --> D2[与先进GCNs结果相当<br/>Competitive with SOTA GCNs]
        D --> D3[能耗显著降低<br/>Substantially Reduced Energy Consumption]
    ```

- **[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects**
  - **tags:** [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]
  - **authors:** Tasha Pais, Nikhilesh Belulkar
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2512.22220
  - **contributions:** 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("On Extending Semantic Abstraction for Efficient Search of Hidden Objects") --> Problem("核心问题/Problem: Localizing hidden/occluded objects")
        Root --> Method("主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search")
        Root --> Results("关键结果/Results: Faster and accurate 3D localization vs. random search")
    ```

- **[arXiv251230] Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases**
  - **tags:** [ai], [neural network architecture], [Müntz-Szász Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]
  - **authors:** Gnankan Landry Regis N'guessan
  - **institution:** Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre
  - **link:** https://arxiv.org/pdf/2512.22222
  - **contributions:** 1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN's universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases] --> B[核心问题/Problem: Standard neural networks poorly approximate singular/fractional power functions common in physics]
    A --> C[主要方法/Method: Proposes MSN with learnable fractional power bases, replacing fixed activations]
    A --> D[关键结果/Results: MSN achieves superior approximation rates, lower error with fewer parameters, and significant improvement on PINN benchmarks]
    ```

- **[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs**
  - **tags:** [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]
  - **authors:** Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao
  - **institution:** University of Science and Technology of China, Ant Group
  - **link:** https://arxiv.org/pdf/2512.22226
  - **code:** https://github.com/zheng980629/VideoScaffold
  - **contributions:** 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]
        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]
        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]
    ```

- **[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]
  - **authors:** Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam
  - **institution:** North Carolina A&T State University, University of Texas at San Antonio
  - **link:** https://arxiv.org/pdf/2512.22223
  - **code:** https://github.com/270771/llm-traffictraffic
  - **contributions:** 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp
  - **Simple LLM Summary:** The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Traditional traffic analysis systems have high false positives and lack interpretability.]
        C[主要方法/Method: Multi-stage framework using traffic summarization, RAG, and LLM reasoning with a hierarchical retrieval pipeline.]
        D[关键结果/Results: Achieves 95.95%-98.82% accuracy, outperforms baselines, and provides explainable, evidence-grounded responses.]
    ```

- **[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing**
  - **tags:** [mlsys], [cluster infrastructure], [cloud-native, distributed stream processing, containerized microservices, elastic resource orchestration, edge-cloud hybrid]
  - **authors:** Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal
  - **institution:** IEEE, NTT Data, Amtrak, Albertsons Companies
  - **link:** https://arxiv.org/pdf/2512.22231
  - **contributions:** 1. A comprehensive theoretical framework for AI-enhanced cloud-based PMU analytics. 2. Mathematical formulations for distributed machine learning optimized for PMU time-series data. 3. Analysis of edge-cloud hybrid architectures with integrated security and privacy considerations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a scalable cloud-native architecture to address the latency and scalability challenges of processing high-frequency data from Phasor Measurement Units (PMUs) in smart grids. The method integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for real-time analytics. The analysis shows the architecture can achieve sub-second response times while scaling to large deployments, providing a robust foundation for next-generation grid analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Cloud-Native Architectures for Intelligent PMU Data Processing"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>PMU数据规模大，传统架构延迟高，可扩展性差"]
        Method["主要方法/Method<br>云原生架构，集成AI、边缘与云计算，使用分布式流处理和微服务"]
        Results["关键结果/Results<br>实现亚秒级响应，可扩展至大规模部署，提供安全可靠的基础"]
    ```

- **[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction**
  - **tags:** [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]
  - **authors:** Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu
  - **institution:** Nanchang University, Hefei Comprehensive National Science Center
  - **link:** https://arxiv.org/pdf/2512.22237
  - **contributions:** 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --> B(核心问题/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)
        A --> C(主要方法/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)
        A --> D(关键结果/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)
    ```

- **[arXiv251230] We are not able to identify AI-generated images**
  - **tags:** [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]
  - **authors:** Adrien Pavão
  - **institution:** (Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)
  - **link:** https://arxiv.org/pdf/2512.22236
  - **contributions:** 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp
  - **Simple LLM Summary:** The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[We are not able to identify AI-generated images] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Can humans reliably distinguish AI-generated images from real photos?]
        Method[主要方法/Method: Interactive web experiment with a curated dataset of 120 difficult images (CC12M real vs. MidJourney AI-generated)]
        Results[关键结果/Results: Average human accuracy is 54% (near random), response time ~7.3s, highlighting human insufficiency and need for guidelines]
    ```

- **[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]
  - **authors:** Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu
  - **institution:** Fudan University, Shanghai Innovation Institute, OpenMoss Team
  - **link:** https://arxiv.org/pdf/2512.22234
  - **code:** https://github.com/OpenMOSS/DiRL
  - **contributions:** 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiRL: An Efficient Post-Training Framework for Diffusion Language Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[dLLMs后训练低效/Post-training for dLLMs is inefficient]
        B --> B2[训练与推理目标不匹配/Training-Inference objective mismatch]
        C --> C1[DiRL框架/DiRL Framework]
        C1 --> C1_1[整合FlexAttention与LMDeploy/Integrates FlexAttention & LMDeploy]
        C1 --> C1_2[两阶段后训练/Two-stage post-training (SFT+RL)]
        C --> C2[DiPO算法/DiPO Algorithm]
        C2 --> C2_1[无偏GRPO实现/Unbiased GRPO for dLLMs]
        D --> D1[高效训练与推理/Efficient Training & Inference]
        D --> D2[数学SOTA性能/Math SOTA Performance]
        D --> D3[超越Qwen2.5系列/Surpasses Qwen2.5 series]
    ```

- **[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning**
  - **tags:** [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]
  - **authors:** Abdul Malik Al Mardhouf Al Saadi, Amrita Basak
  - **institution:** The Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.22241
  - **contributions:** 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enhanced geometry prediction in L-DED using meta-learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Data scarcity & heterogeneity in L-DED geometry prediction]
        C[主要方法/Method: Meta-learning (MAML & Reptile) for cross-dataset knowledge transfer]
        D[关键结果/Results: Accurate prediction with few examples, outperforms conventional NN]
    ```

- **[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening**
  - **tags:** [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]
  - **authors:** Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf
  - **institution:** Radboud University Medical Center, Radboud University
  - **link:** https://arxiv.org/pdf/2512.22242
  - **contributions:** 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]
        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]
        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]
    ```

- **[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]
  - **authors:** Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.22238
  - **contributions:** 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]
        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]
        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]
        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]
        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]
        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]
        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]
        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]
    ```

- **[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture**
  - **tags:** [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]
  - **authors:** Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan
  - **institution:** FPT University
  - **link:** https://arxiv.org/pdf/2512.22239
  - **contributions:** 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>面向智慧农业的高效深度学习的多目标混合知识蒸馏"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Deploying deep models on edge devices in smart agriculture<br>在智慧农业中于边缘设备部署深度模型"] --> P1["挑战/Challenge<br>Trade-off between efficiency and accuracy<br>效率与准确性的权衡"]
        Method["主要方法/Method<br>Hybrid knowledge distillation framework<br>混合知识蒸馏框架"] --> M1["学生模型/Student Model<br>Customized CNN with inverted residual & dense blocks<br>定制化CNN，含倒残差与密集连接块"]
        Method --> M2["教师模型/Teacher Model<br>ResNet18 guidance<br>ResNet18教师网络指导"]
        Method --> M3["多目标策略/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>整合硬标签、特征级、响应级与自蒸馏"]
        Results["关键结果/Results<br>Experiments on agricultural datasets<br>在农业数据集上的实验"] --> R1["性能/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>水稻种子分类准确率98.56%（教师模型98.65%）"]
        Results --> R2["效率/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs，约107万参数（比教师模型小10倍）"]
        Results --> R3["泛化/Generalization<br>Consistent gains on plant leaf disease datasets<br>在植物叶片病害数据集上一致性能提升"]
    ```

- **[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs**
  - **tags:** [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]
  - **authors:** Chama Bensmail
  - **institution:** University of Hertfordshire, Omics Data Solutions LTD
  - **link:** https://arxiv.org/pdf/2512.22240
  - **code:** https://github.com/bensmailchama-boop/EvoXplain
  - **contributions:** 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EvoXplain Paper] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[高精度模型是否共享相同内部逻辑?<br/>Do high-accuracy models share the same internal logic?]
        C --> C1[跨重复训练测量解释稳定性<br/>Measure explanation stability across repeated training]
        C --> C2[将解释视为优化过程样本<br/>Treat explanations as samples from optimization]
        D --> D1[发现解释的多模态性<br/>Found explanatory multimodality]
        D --> D2[逻辑回归等模型也显示多种机制<br/>Models like Logistic Regression show multiple mechanisms]
        D --> D3[重新定义可解释性为模型类属性<br/>Reframe interpretability as model-class property]
    ```

- **[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation**
  - **tags:** [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]
  - **authors:** Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda
  - **institution:** Meta (FAIR at Meta, Meta Superintelligence Labs)
  - **link:** https://arxiv.org/pdf/2512.22245
  - **contributions:** 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model's hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Calibrating LLM Judges<br/>校准LLM法官] --> B[Problem: LLM judges lack efficient, calibrated uncertainty<br/>问题：LLM法官缺乏高效、校准的不确定性估计]
        A --> C[Method: Linear probes on hidden states with Brier score loss<br/>方法：基于Brier分数损失的隐状态线性探针]
        A --> D[Results: Better calibration, 10x speedup, robust generalization<br/>结果：更好的校准，10倍加速，鲁棒的泛化]
    ```

- **[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction**
  - **tags:** [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]
  - **authors:** Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu
  - **institution:** Jinan University, Great Bay University, JD Technology, University of Illinois Chicago
  - **link:** https://arxiv.org/pdf/2512.22252
  - **code:** https://github.com/DSI-Lab1/GAATNet
  - **contributions:** 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Graph Attention-based Adaptive Transfer Learning for Link Prediction] --> B[核心问题/Problem: Challenges in large-scale sparse graphs and cross-dataset transfer learning for link prediction]
        A --> C[主要方法/Method: Proposes GAATNet with distant neighbor embeddings and lightweight self-adapter for adaptive transfer]
        A --> D[关键结果/Results: Achieves SOTA performance on seven datasets, provides scalable GNN-transfer learning solution]
    ```

- **[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs**
  - **tags:** [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]
  - **authors:** Pascal Passigan, Kevin zhu, Angelina Ning
  - **institution:** Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22251
  - **contributions:** 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Predicting granular gene expression changes (perturbations) from drugs, beyond binary drug-disease associations.]
        C[主要方法/Method: Merge PrimeKG++ & LINCS L1000 into a BKG; use Graph Attention Network (GAT) to predict delta expression.]
        D[关键结果/Results: Outperforms MLP baselines; ablation shows KG edges enhance prediction for mechanistic modeling.]
    ```

- **[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method**
  - **tags:** [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]
  - **authors:** Satvik Tripathi
  - **institution:** University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.22258
  - **code:** https://github.com/satviktri/LSP
  - **contributions:** 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Logic Sketch Prompting (LSP)] --> B[核心问题/Problem: LLMs unreliable on tasks needing strict rules & determinism]
        A --> C[主要方法/Method: Lightweight framework with typed variables, condition evaluators, rule validator]
        A --> D[关键结果/Results: Highest accuracy/F1 vs. baselines; suitable for clinical/safety-critical systems]
    ```

- **[arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey**
  - **tags:** [se], [automated software maintenance], [large language models, agentic systems, software issue resolution, reinforcement learning, software engineering]
  - **authors:** Zhonghao Jiang, David Lo, Zhongxin Liu
  - **institution:** Zhejiang University, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.22256
  - **code:** https://github.com/ZhonghaoJiang/Awesome-Issue-Solving
  - **contributions:** 1. Provides a systematic survey of 126 recent studies on LLM-based agentic software issue resolution. 2. Establishes a taxonomy for the field across three key dimensions: benchmarks, techniques, and empirical studies. 3. Highlights the paradigm shift brought by agentic reinforcement learning in designing and training agentic systems for software engineering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5c1c5e173acc2646c4322651d8a6c89dabed4b251b6106c2a468adeeafadf5f_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys the use of Large Language Model (LLM)-based agentic systems for automating complex software issue resolution, such as bug fixing. It reviews recent research, categorizes approaches, and discusses how agentic reinforcement learning is changing system design. The conclusion outlines current challenges and future research directions for improving automated software maintenance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Agentic Software Issue Resolution with LLMs: A Survey] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传统方法依赖人工，效率低/Traditional methods rely on human expertise, inefficient]
        Method[主要方法/Method] --> M1[基于LLM的智能体系统/LLM-based Agentic Systems]
        Method --> M2[系统综述126项研究/Systematic survey of 126 studies]
        Method --> M3[建立三维分类法/Establishes a 3D taxonomy]
        Results[关键结果/Results] --> R1[增强软件维护效率/Enhances software maintenance efficiency]
        Results --> R2[为智能体系统提供验证环境/Provides a validation environment for agentic systems]
        Results --> R3[总结挑战与未来方向/Summarizes challenges & future directions]
    ```

- **[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks**
  - **tags:** [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]
  - **authors:** Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville
  - **institution:** University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs
  - **link:** https://arxiv.org/pdf/2512.22255
  - **contributions:** 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model's reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model's distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp
  - **Simple LLM Summary:** This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data's distribution is closer to the model's own. The key conclusion is that aligning the training data distribution with the model's is more critical for performance than the correctness of the final answers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Shape of Thought / 思维形态] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Correctness vs. Distribution / 正确性与数据分布]
        B1 --> B2{Does correctness guarantee better reasoning? / 正确性保证更好的推理吗?}
        C --> C1[Train on Incorrect Synthetic CoT / 使用错误的合成CoT训练]
        C --> C2[Paraphrase Human Traces / 改写人类标注的推理链]
        C --> C3[Introduce Flawed Steps / 引入有缺陷的推理步骤]
        D --> D1[Synthetic Incorrect > Human Correct / 错误的合成数据优于正确的人类数据]
        D --> D2[Distribution Alignment is Key / 数据分布对齐是关键]
        D --> D3[Final Answer ≠ Faithful Reasoning / 最终答案 ≠ 忠实推理过程]
    ```

- **[arXiv251230] ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers**
  - **tags:** [other], [formal verification, computer algebra], [reverse engineering, graph neural network, multiplier verification, algebraic circuit verification, SAT-based equivalence checking]
  - **authors:** Chen Chen, Daniela Kaufmann, Chenhui Deng, Zhan Song, Hongce Zhang, Cunxi Yu
  - **institution:** University of Maryland, College Park; TU Wien; NVIDIA; Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.22260
  - **contributions:** 1. Proposes ReVEAL, a graph-learning-based framework for reverse engineering optimized multiplier architectures to recover their word-level structure. 2. Leverages structural graph features and learning-driven inference to identify architectural patterns at scale, enabling robust handling of large, optimized circuits. 3. Integrates smoothly with existing verification flows and supports downstream algebraic proof strategies, showing improvements in scalability and accuracy over traditional rule-based approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bdd1907cef6efb0b9b81d88b611f825942f30ef8a8674a9587cc4261e4774ef_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ReVEAL, a method that uses Graph Neural Networks (GNNs) to reverse engineer the architecture of optimized hardware multipliers. This recovered structure enables more effective formal verification using algebraic techniques. The approach demonstrates improved scalability and accuracy compared to traditional rule-based methods on diverse benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers] --> B(核心问题/Problem: 优化乘法器形式验证困难/Challenges in formal verification of optimized multipliers)
        A --> C(主要方法/Method: 基于图学习的逆向工程/GNN-guided reverse engineering)
        A --> D(关键结果/Results: 提升可扩展性与准确性/Improved scalability and accuracy)
    ```

- **[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs**
  - **tags:** [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]
  - **authors:** Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang
  - **institution:** Tianjin University, Beihang University, Guangxi Normal University
  - **link:** https://arxiv.org/pdf/2512.22266
  - **code:** https://github.com/Wjerry5/LLMTM
  - **contributions:** 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs处理动态图时态模体分析能力未知/LLMs' capability for temporal motif analysis on dynamic graphs is unexplored]
        C --> C1[提出基准LLMTM与智能体/Propose benchmark LLMTM and an agent]
        C --> C2[提出结构感知调度器/Propose structure-aware dispatcher]
        D --> D1[调度器保持高精度并降低成本/Dispatcher maintains high accuracy while reducing cost]
    ```

- **[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency**
  - **tags:** [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]
  - **authors:** Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang
  - **institution:** Peking University Third Hospital, Beihang University
  - **link:** https://arxiv.org/pdf/2512.22275
  - **contributions:** 1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Illusion of Clinical Reasoning<br>临床推理的假象] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>现有基准无法捕捉综合、多模态临床推理。]
        C --> C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>开发了包含1245个真实世界问题、涵盖7项任务的B&J基准。]
        D --> D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>巨大性能差距：选择题表现好，开放式多模态任务表现差。]
        D --> D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM在图像解释方面存在局限并出现幻觉。]
        D --> D3[Medically fine-tuned models show no consistent advantage.<br>医学微调模型未显示一致优势。]
    ```

- **[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems**
  - **tags:** [mlsys], [memory & caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]
  - **authors:** Varshith Gudur
  - **institution:** Independent Researcher (Valori Kernel Project)
  - **link:** https://arxiv.org/pdf/2512.22280
  - **code:** https://github.com/varshith-Git/Valori-Kernel
  - **contributions:** 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Valori: A Deterministic Memory Substrate for AI Systems] --> B
        A --> C
        A --> D
        B[核心问题/Problem: AI内存非确定性/AI Memory Non-Determinism]
        C[主要方法/Method: 固定点算术与状态机/Fixed-Point Arithmetic & State Machine]
        D[关键结果/Results: 跨平台比特一致性/Cross-Platform Bit-Identical Results]
    ```

- **[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation**
  - **tags:** [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]
  - **authors:** Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi
  - **institution:** Kyungpook National University
  - **link:** https://arxiv.org/pdf/2512.22287
  - **contributions:** 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model's interpretability and scalability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Cluster Aggregated GAN (CAG)"] --> Problem["核心问题/Problem: 缺乏标记数据，现有GAN方法对所有设备一视同仁，忽略间歇性和持续性设备的行为差异，导致训练不稳定和保真度有限"]
        Root --> Method["主要方法/Method: 提出混合生成框架，根据设备行为特征路由到专门分支：间歇性设备使用聚类模块和专用生成器；持续性设备使用LSTM生成器"]
        Root --> Results["关键结果/Results: 在UVIC数据集上实验，在真实性、多样性和训练稳定性上优于基线方法，聚类作为主动生成组件提高了可解释性和可扩展性"]
    ```

- **[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing**
  - **tags:** [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]
  - **authors:** Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli, University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22290
  - **contributions:** 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a "murky middle" of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["当算法管理人类: 估算算法控制对零工工人绩效和福祉非线性效应的双重机器学习方法 / When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing"]
        Root --> Problem["核心问题: 算法管理下，以人为本的管理能否持续？工人对算法的反应是非线性的 / Problem: Can person-centered management survive algorithmic management? Worker responses are nonlinear."]
        Root --> Method["主要方法: 使用双重机器学习框架估算有调节的中介模型，无严格函数形式限制 / Method: Double Machine Learning framework to estimate a moderated mediation model without restrictive functional forms."]
        Root --> Results["关键结果: 发现非单调模式。模糊的算法监督削弱绩效联系，透明可解释的监督则加强它 / Results: Found a nonmonotonic pattern. Murky oversight weakens the performance link, transparent and explainable oversight strengthens it."]
    ```

- **[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model**
  - **tags:** [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]
  - **authors:** Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang
  - **institution:** Tsinghua University (Leap Lab), Anyverse Dynamics
  - **link:** https://arxiv.org/pdf/2512.22288
  - **code:** https://co-grpo.github.io
  - **contributions:** 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]
        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]
        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]
        C --> C1[统一MDP/Unified MDP]
        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]
        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]
        D --> D1[提升生成质量/Improved Generation Quality]
        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]
    ```

- **[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations**
  - **tags:** [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]
  - **authors:** Guokan Chen, Yao Xiao
  - **institution:** Fujian University of Technology
  - **link:** https://arxiv.org/pdf/2512.22283
  - **contributions:** 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[PINNs struggle with multi-scale/high-frequency PDEs / PINNs在处理多尺度/高频PDE时遇到困难]
        P1 --> P2[Issues: Gradient flow stiffness & spectral bias / 问题: 梯度流刚度和谱偏差]
        Method[主要方法/Method] --> M1[Architecture: Kolmogorov-Arnold Network (KAN) with learnable B-splines / 架构: 基于可学习B样条的KAN]
        Method --> M2[Strategy: Adaptive weighting with dynamic decay upper bound / 策略: 带动态衰减上界的自适应加权]
        Results[关键结果/Results] --> R1[Faster convergence & higher accuracy / 更快的收敛和更高的精度]
        R1 --> R2[Improvement: At least one order of magnitude / 提升: 至少一个数量级]
        Results --> R3[Benchmarks: Klein-Gordon, Burgers, Helmholtz equations / 基准: Klein-Gordon, Burgers, Helmholtz方程]
    ```

- **[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection**
  - **tags:** [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]
  - **authors:** Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou
  - **institution:** People's Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College
  - **link:** https://arxiv.org/pdf/2512.22291
  - **contributions:** 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a 'spectral fingerprint'. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Head Spectral-Adaptive Graph Anomaly Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[固定滤波器导致过平滑与缺乏适应性/Fixed filters cause over-smoothing & lack adaptability]
        C --> C1[基于谱指纹的轻量级超网络/Lightweight hypernetwork based on spectral fingerprint]
        C --> C2[动态生成切比雪夫滤波器参数/Dynamically generates Chebyshev filter parameters]
        C --> C3[双正则化策略防止模式崩溃/Dual regularization prevents mode collapse]
        D --> D1[有效保留高频异常信号/Effectively preserves high-frequency anomaly signals]
        D --> D2[在异构数据集上性能优越/Outperforms SOTA on heterogeneous datasets]
    ```

- **[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation**
  - **tags:** [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]
  - **authors:** Philip Xu, David Elizondo, Raouf Hamzaoui
  - **institution:** De Montfort University
  - **link:** https://arxiv.org/pdf/2512.22294
  - **contributions:** 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大规模3D检索与可控4D生成的挑战/Challenges in large-scale 3D retrieval and controlled 4D generation]
        C --> C1[三级对齐框架: 文本-3D-图像/Three-level alignment: text-3D-image]
        C --> C2[ATMS模型优化检索/ATMS model optimizes retrieval]
        D --> D1[高质量3D检索/High-quality 3D retrieval]
        D --> D2[可控4D生成/Controlled 4D generation]
    ```

- **[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations**
  - **tags:** [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]
  - **authors:** Noor Fatima, Hasan Faraz Khan, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22303
  - **contributions:** 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"] --> B["核心问题/Problem: Robust detection under realistic counter-forensic attacks"]
        A --> C["主要方法/Method: Red-team training + Test-time defense in a two-stream architecture"]
        A --> D["关键结果/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps"]
    ```

- **[arXiv251230] Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection**
  - **tags:** [sec], [vulnerability detection], [multi-vulnerability detection, count bias, selection bias, long-context code, CWE injection]
  - **authors:** Chinmay Pushkar, Sanchit Kabra, Dhruv Kumar, Jagat Sesh Challa
  - **institution:** BITS Pilani, Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22306
  - **contributions:** 1. Introduced a comprehensive benchmark for Multi-Vulnerability Detection across four programming languages (C, C++, Python, JavaScript) to address the limitations of existing single-vulnerability benchmarks. 2. Constructed a novel dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, 9) into long-context code samples, enabling the study of performance under varying vulnerability densities. 3. Quantified the performance degradation of state-of-the-art LLMs (e.g., GPT-4o-mini, Llama-3.3-70B) in high-density vulnerability settings, revealing distinct failure modes like severe "under-counting" in Python and JavaScript.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6a5fa286f161d0dab7d6a06a8f54502b88fd2f448edc9ed3609a31efaf97f5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in evaluating LLMs for detecting multiple vulnerabilities in large, real-world code files. The authors propose a new benchmark by creating a dataset of long code files with systematically injected vulnerabilities and evaluate several LLMs. The main finding is that LLM performance sharply degrades as the number of vulnerabilities per file increases, with significant drops in recall for languages like Python.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Beyond Single Bugs: Benchmarking LLMs for Multi-Vulnerability Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing benchmarks are simplistic, focusing on single bugs, not reflecting real-world multi-vulnerability files.]
        C[主要方法/Method<br>Build a new benchmark with 40k files across 4 languages, injecting controlled vulnerability counts into long code.]
        D[关键结果/Results<br>LLM performance degrades sharply with more vulnerabilities; distinct failure modes in Python/JS vs. C/C++.]
    ```

- **[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting**
  - **tags:** [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]
  - **authors:** Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban
  - **institution:** Beihang University, China Telecom eSurfing Cloud
  - **link:** https://arxiv.org/pdf/2512.22309
  - **contributions:** 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLMBoost: Make Large Language Models Stronger with Boosting] --> B[核心问题/Problem: Existing LLM ensemble methods treat models as black boxes, ignoring internal representations.]
        A --> C[主要方法/Method: A boosting-inspired framework with cross-model attention, chain training, and near-parallel inference.]
        A --> D[关键结果/Results: Consistently boosts accuracy and reduces inference latency on reasoning tasks.]
    ```

- **[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators**
  - **tags:** [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]
  - **authors:** You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou
  - **institution:** Northwestern University
  - **link:** https://arxiv.org/pdf/2512.22307
  - **contributions:** 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators] --> Problem(核心问题/Problem: Model IP Protection & Supply Chain Threats)
        Root --> Method(主要方法/Method: Hardware-Software Co-design with Logic Locking)
        Root --> Results(关键结果/Results: Resists Attacks, <0.1% Overhead)
        Problem --> P1(模型盗窃/Model Theft)
        Problem --> P2(模型破坏/Model Corruption)
        Problem --> P3(信息泄露/Information Leakage)
        Method --> M1(软件侧: 神经元嵌入密钥/Software: Key Embedding in Neurons)
        Method --> M2(硬件侧: 轻量级锁定模块/Hardware: Lightweight Locking Module)
        Results --> R1(抵御优化攻击/Withstands Oracle-Guided Attacks)
        Results --> R2(低计算开销/Low Computational Overhead)
    ```

- **[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting**
  - **tags:** [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]
  - **authors:** Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan
  - **institution:** University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University
  - **link:** https://arxiv.org/pdf/2512.22317
  - **contributions:** 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]
        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]
        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]
    ```

- **[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning**
  - **tags:** [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]
  - **authors:** Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang
  - **institution:** Tsinghua University, The Chinese University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.22315
  - **code:** https://github.com/zsgvivo/VideoZoomer
  - **contributions:** 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[长视频理解受限/Limited Long Video Understanding]
        B1 --> B2[上下文窗口限制/Context Window Limitation]
        B1 --> B3[均匀采样忽略关键证据/Uniform Sampling Overlooks Evidence]
        C --> C1[代理框架/Agentic Framework]
        C1 --> C2[动态时间聚焦/Dynamic Temporal Focusing]
        C2 --> C3[从粗到细推理/Coarse-to-Fine Reasoning]
        C --> C4[两阶段训练/Two-Stage Training]
        C4 --> C5[监督微调/Supervised Fine-Tuning]
        C4 --> C6[强化学习/Reinforcement Learning]
        D --> D1[性能强劲/Strong Performance]
        D1 --> D2[超越开源模型/Surpasses Open-Source Models]
        D1 --> D3[媲美专有系统/Rivals Proprietary Systems]
        D --> D4[高效推理/Efficient Reasoning]
        D4 --> D5[低帧预算/Reduced Frame Budget]
    ```

- **[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers**
  - **tags:** [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]
  - **authors:** Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang
  - **institution:** National University of Singapore, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.22323
  - **code:** https://biangbiang0321.github.io/SpotEdit.github.io/
  - **contributions:** 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpotEdit: Selective Region Editing in Diffusion Transformers] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[全图去噪冗余/Full-image denoising is redundant for small edits]
        C --> C1[SpotSelector: 识别稳定区域/Identifies stable regions via perceptual similarity]
        C --> C2[SpotFusion: 动态特征融合/Dynamically fuses features for coherence]
        D --> D1[高效编辑/Efficient editing]
        D --> D2[保持未修改区域保真度/Preserves fidelity in unchanged areas]
    ```

- **[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents**
  - **tags:** [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]
  - **authors:** Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun
  - **institution:** Peking University, Tencent
  - **link:** https://arxiv.org/pdf/2512.22322
  - **code:** https://huggingface.co/collections/yolay/smartsnap
  - **contributions:** 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]
        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]
        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]
    ```

- **[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers**
  - **tags:** [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]
  - **authors:** Sravan Karthick T
  - **institution:** RV College of Engineering (RVCE), Bengaluru, India
  - **link:** https://arxiv.org/pdf/2512.22326
  - **contributions:** 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers] --> B(核心问题/Problem: Bitcoin价格长期预测的极端波动性和非平稳性/Bitcoin's extreme volatility & non-stationarity for long-horizon forecasting)
        A --> C(主要方法/Method: 集成全球M2流动性作为外生变量，使用TimeXer架构/Integrate Global M2 Liquidity as exogenous variable using TimeXer architecture)
        A --> D(关键结果/Results: 在70天预测范围内，MSE降低超过89%/At 70-day horizon, MSE reduced by over 89%)
    ```

- **[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma**
  - **tags:** [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]
  - **authors:** Mariya Miteva, Maria Nisheva-Pavlova
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.22331
  - **contributions:** 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]
        C[主要方法/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]
        D[关键结果/Results: Latent embeddings used for MGMT promoter methylation classification]
    ```

- **[arXiv251230] SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence**
  - **tags:** [ai], [evaluation & benchmarking], [scientific intelligence, multimodal reasoning, benchmarking toolkit]
  - **authors:** Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.22334
  - **code:** https://github.com/InternScience/SciEvalKit
  - **contributions:** 1. Introduces SciEvalKit, a unified, open-source toolkit for evaluating AI models across a broad range of scientific disciplines and core scientific intelligence capabilities. 2. Provides a flexible and extensible evaluation pipeline supporting batch evaluation, custom model/dataset integration, and ensuring transparent, reproducible results. 3. Curates expert-grade scientific benchmarks from real-world, domain-specific datasets to reflect authentic scientific challenges across six major domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across multiple disciplines and core competencies like multimodal reasoning and code generation. It provides a flexible, extensible pipeline for reproducible evaluation and is built on expert-grade, real-world scientific benchmarks. The toolkit is open-sourced to foster community-driven development in AI for science.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Lack of specialized evaluation for scientific AI across diverse disciplines and capabilities]
        C[主要方法/Method: Unified benchmarking toolkit with flexible pipeline, real-world benchmarks, and support for six scientific domains]
        D[关键结果/Results: Open-source toolkit enabling standardized, reproducible evaluation of scientific foundation models]
    ```

- **[arXiv251230] Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback**
  - **tags:** [mlsys], [agent system], [symbolic world models, multi-agent feedback, PDDL, adaptive testing, supervised fine-tuning]
  - **authors:** Mengkang Hu, Bowei Xia, Yuran Wu, Ailing Yu, Yude Zou, Qiguang Chen, Shijian Wang, Jiarui Jin, Kexin Li, Wenxiang Jiao, Yuan Lu, Ping Luo
  - **institution:** The University of Hong Kong, Xiaohongshu Inc., UESTC, Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22336
  - **code:** agent2world.github.io
  - **contributions:** 1. Proposed Agent2World, a tool-augmented multi-agent framework for generating symbolic world models via adaptive multi-agent feedback. 2. Introduced a three-stage pipeline with specialized agents (Deep Researcher, Model Developer, Testing Team) for knowledge synthesis, implementation, and behavior-aware validation. 3. Demonstrated that the framework not only achieves state-of-the-art inference-time performance but also serves as a data engine for supervised fine-tuning, leading to substantial model improvement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating correct symbolic world models (like PDDL domains) from natural language by proposing Agent2World, a multi-agent framework that uses adaptive feedback for validation and repair. The method outperforms existing approaches on benchmarks and the feedback collected also enables effective supervised fine-tuning, significantly improving model performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback] --> B[核心问题/Problem: Lack of verifiable supervision for training LLMs to generate behaviorally correct symbolic world models]
        A --> C[主要方法/Method: Tool-augmented multi-agent framework with three-stage pipeline: Deep Researcher, Model Developer, and Testing Team for adaptive feedback]
        A --> D[关键结果/Results: Achieves SOTA inference-time performance; Framework serves as data engine for fine-tuning, yielding ~31% average relative gain]
    ```

- **[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]
  - **authors:** Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell
  - **institution:** IBM Research, Mila, Université de Montréal
  - **link:** https://arxiv.org/pdf/2512.22337
  - **contributions:** 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model's general knowledge while maintaining plasticity for new tasks, applied to Qwen models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model's original capabilities while allowing adaptation to new instructions, with minimal computational overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["论文标题: The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: LoRA微调导致灾难性遗忘/Catastrophic forgetting in LoRA fine-tuning"]
        Method["主要方法/Method: 正则化近似回放/Regularized Approximate Replay (KL惩罚+交错数据/KL penalty + interleaved data)"]
        Results["关键结果/Results: 保留通用知识，维持可塑性/Preserves general knowledge without hindering plasticity"]
    ```

- **[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement**
  - **tags:** [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]
  - **authors:** Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo
  - **institution:** Stanford University, Google, Google DeepMind, New York University
  - **link:** https://arxiv.org/pdf/2512.22351
  - **code:** vulcan-3d.github.io
  - **contributions:** 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>MLLMs在复杂3D场景操控中的应用未被充分探索<br>Application of MLLMs to complex 3D scene manipulation is underexplored]
        C[主要方法/Method<br>引入MCP API、视觉工具套件和多智能体协作框架<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]
        D[关键结果/Results<br>在25个复杂任务上显著超越基线<br>Significantly outperforms baselines on 25 complex tasks]
    ```

- **[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides**
  - **tags:** [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]
  - **authors:** Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye
  - **institution:** North Carolina A&T State University, University of Chichester, Yale University
  - **link:** https://arxiv.org/pdf/2512.22335
  - **contributions:** 1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[挑战: 联合分析H&E和IHC图像进行HER2评分/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]
    B --> B2[难点: 现有方法无法提供像素级HER2状态定位/Issue: Existing methods lack pixel-level HER2 status localization]
    C --> C1[方法: 端到端多阶段视觉Transformer管道/Method: End-to-end multi-stage Vision Transformer pipeline]
    C --> C2[创新: 新颖的映射函数关联H&E与IHC区域/Innovation: Novel mapping function to correlate H&E and IHC regions]
    C --> C3[机制: 临床启发的4级HER2评分机制/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]
    D --> D1[结果: 肿瘤定位分类准确率高/Result: Good classification accuracy for tumor localization]
    D --> D2[结果: HER2状态预测准确率0.94/Result: 0.94 accuracy for HER2 status prediction]
    D --> D3[结论: 端到端ViT模型可用于联合评估H&E和IHC图像/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]
    ```

- **[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data**
  - **tags:** [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]
  - **authors:** Alaa Alahmadi, Mohamed Hasan
  - **institution:** Newcastle University, University of Leeds
  - **link:** https://arxiv.org/pdf/2512.22349
  - **contributions:** 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1
        A --> B2
        A --> B3
        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]
        B1 --> C2[可解释性差/Limited explainability]
        B1 --> C3[临床可靠性受限/Constrained clinical reliability]
        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]
        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]
        D1 --> E2[结构化颜色表示/Structured colour representations]
        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]
        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]
        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]
        B3 --> F2[提升可解释性/Improve explainability (guide attention)]
        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]
    ```

- **[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries**
  - **tags:** [mlsys], [llm inference], [Text-to-SQL, Cloud Cost Optimization, Query Efficiency, Large Language Models, Google BigQuery]
  - **authors:** Saurabh Deochake, Debajyoti Mukhopadhyay
  - **institution:** SentinelOne, WIDiCoReL Research Lab
  - **link:** https://arxiv.org/pdf/2512.22364
  - **contributions:** 1. Introduced a cloud-native cost evaluation methodology for Text-to-SQL systems, measuring bytes processed, slot utilization, and estimated query cost on production infrastructure. 2. Conducted an empirical evaluation of six LLMs on Google BigQuery, demonstrating that reasoning models achieve significantly lower cloud compute costs while maintaining high correctness. 3. Quantified cost variance across models, identified prevalent inefficiency patterns (e.g., missing partition filters), and provided deployment guidelines for cost-sensitive environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the cloud compute costs of SQL queries generated by Large Language Models (LLMs) for Text-to-SQL tasks. By evaluating six state-of-the-art LLMs on Google BigQuery, it finds that reasoning models are more cost-efficient, processing far fewer bytes, and that execution time is a poor proxy for cloud cost. The work provides a new cost-focused evaluation methodology and guidelines for deploying cost-aware Text-to-SQL systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing efficiency metrics (e.g., VES) measure time, not cloud compute costs.] --> B1[问题背景/Context<br>LLMs achieve high Text-to-SQL accuracy, but cost efficiency in cloud deployments is unknown.]
        C[主要方法/Method<br>Systematic evaluation of 6 LLMs on Google BigQuery (StackOverflow dataset).] --> C1[评估指标/Metrics<br>Measure bytes processed, slot utilization, estimated cost, and correctness.]
        D[关键结果/Results] --> D1[发现1/Finding 1<br>Reasoning models process 44.5% fewer bytes with equivalent correctness.]
        D --> D2[发现2/Finding 2<br>Weak correlation (r=0.16) between execution time and query cost.]
        D --> D3[发现3/Finding 3<br>Up to 3.4x cost variance; standard models produce high-cost outliers.]
    ```

- **[arXiv251230] Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions**
  - **tags:** [ai], [automated planning], [numeric planning, control parameters, subgoaling heuristics, optimistic compilation, infinite action space]
  - **authors:** Ángel Aso-Mollar, Diego Aineto, Enrico Scala, Eva Onaindia
  - **institution:** Universitat Politècnica de València, Università degli Studi di Brescia
  - **link:** https://arxiv.org/pdf/2512.22367
  - **contributions:** 1. Identifies a tractable subset of numeric planning problems with infinite actions (controllable, simple numeric problems)., 2. Proposes an optimistic compilation approach that transforms these problems into standard simple numeric tasks by abstracting control-dependent expressions., 3. Enables the effective use of traditional subgoaling heuristics for goal distance estimation in this challenging setting, pushing the state of the art.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc3c2ae44e3bb664061095be1d4f9beb835e627f31f49a8bfccf9fd7df412ea_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of applying standard numeric heuristics in planning problems with an infinite number of actions due to control parameters. It proposes an optimistic compilation method that transforms a tractable subset of these problems into simpler numeric tasks, enabling the use of subgoaling heuristics. The results show this approach is effective and computationally feasible for handling infinite action spaces.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions<br>基于子目标松弛的启发式方法用于无限动作数值规划"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Standard heuristics fail for infinite actions from control parameters<br>标准启发式方法无法处理控制参数导致的无限动作"] --> P1["问题背景/Context<br>Numeric planning with control parameters<br>带控制参数的数值规划"]
        Method["主要方法/Method<br>Optimistic compilation to simple numeric tasks<br>通过乐观编译转为简单数值任务"] --> M1["关键步骤/Key Step<br>Abstract control-dependent expressions<br>抽象控制依赖表达式"]
        Results["关键结果/Results<br>Effective & feasible use of subgoaling heuristics<br>子目标启发式方法有效且可行"] --> R1["结论/Conclusion<br>Pushes state of the art<br>推动技术前沿"]
    ```

- **[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation**
  - **tags:** [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]
  - **authors:** Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan
  - **institution:** The University of Hong Kong, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.22374
  - **contributions:** 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables "any-step" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]
        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]
        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]
    ```

- **[arXiv251230] Towards Efficient Post-Training via Fourier-Driven Adapter Architectures**
  - **tags:** [nlp], [parameter-efficient fine-tuning], [Fourier-Activated Adapter, random Fourier features, frequency-aware activation, parameter-efficient fine-tuning, spectral sparsity]
  - **authors:** Donggyun Bae, Jongil Park
  - **institution:** Konkuk University
  - **link:** https://arxiv.org/pdf/2512.22378
  - **contributions:** 1. Proposes the Fourier-Activated Adapter (FAA), a novel PEFT framework that integrates random Fourier features to decompose representations into frequency components. 2. Introduces a dynamic, frequency-aware activation mechanism to selectively modulate semantic information across different frequency bands. 3. Demonstrates through extensive experiments that FAA achieves competitive or superior performance on multiple benchmarks while maintaining low computational overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method for large language models that uses random Fourier features to enable frequency-aware modulation of semantic representations. Experiments on GLUE and other benchmarks show that FAA achieves strong performance with low computational cost, highlighting the effectiveness of its frequency-based approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Efficient Post-Training via Fourier-Driven Adapter Architectures] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有PEFT方法难以捕获高频语义信息 / Existing PEFT methods struggle to capture high-frequency semantic information]
        C --> C1[提出傅里叶激活适配器(FAA) / Propose Fourier-Activated Adapter (FAA)]
        C1 --> C2[集成随机傅里叶特征分解表示 / Integrate random Fourier features to decompose representations]
        C2 --> C3[使用频率感知机制选择性调制 / Use frequency-aware mechanism for selective modulation]
        D --> D1[在多个基准测试中取得有竞争力的结果 / Achieves competitive results on multiple benchmarks]
        D --> D2[保持低计算和内存开销 / Maintains low computational and memory overhead]
    ```

- **[arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents**
  - **tags:** [mlsys], [agent system], [reproducibility, dependency management, code generation, large language models, empirical study]
  - **authors:** Bhanu Prakash Vangala, Ali Adibifar, Tanu Malik, Ashish Gehani
  - **institution:** University of Missouri, SRI International
  - **link:** https://arxiv.org/pdf/2512.22387
  - **contributions:** 1. Introduces a three-layer dependency framework (claimed, working, runtime) to quantify the execution reproducibility of LLM-generated code. 2. Conducts an empirical study evaluating three state-of-the-art LLM coding agents across 300 projects in three programming languages, revealing low out-of-the-box execution success rates. 3. Discovers a significant hidden dependency problem, with an average 13.5x expansion from declared to actual runtime dependencies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f3a69233ca5eacf2fea5882b11aeb102413519f3be78440b3532150966328b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility of code generated by LLM-based coding agents. It proposes a three-layer dependency framework and conducts an empirical study on 300 projects, finding that only 68.3% execute successfully out-of-the-box and that actual runtime dependencies are significantly larger than declared ones. The study concludes that AI-generated code currently suffers from major reproducibility issues due to dependency gaps and code generation errors.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents"] --> Problem["核心问题/Problem: Is AI-generated code reproducible?"]
        Root --> Method["主要方法/Method: Empirical study using a three-layer dependency framework on 300 projects from 3 LLM agents."]
        Root --> Results["关键结果/Results: Low out-of-the-box execution rate (68.3%) and large hidden dependencies (13.5x expansion)."]
    ```

- **[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration**
  - **tags:** [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]
  - **authors:** Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi
  - **institution:** Apple, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.22382
  - **contributions:** 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Completed Hyperparameter Transfer<br/>超参数迁移研究] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Hyperparameter tuning is critical for large models<br/>大模型超参数调优至关重要]
        B --> B2[Transferring optimal HPs across scales is challenging<br/>跨规模最优超参数迁移困难]
        C --> C1[Propose Complete(d)P parameterisation<br/>提出Complete(d)P参数化方法]
        C --> C2[Enable per-module HP optimisation & transfer<br/>实现模块级超参数优化与迁移]
        D --> D1[Direct HP transfer to ~600x larger scale<br/>超参数可直接迁移至约600倍规模]
        D --> D2[Per-module HPs yield training speedup<br/>模块级超参数带来训练加速]
    ```

- **[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition**
  - **tags:** [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]
  - **authors:** Elsen Ronando, Sozo Inoue
  - **institution:** Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya
  - **link:** https://arxiv.org/pdf/2512.22385
  - **contributions:** 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-Guided Exemplar Selection for Few-Shot HAR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[依赖大数据集与几何选择 / Reliance on large datasets & geometric selection]
        B --> B2[难以区分相似活动 / Hard to distinguish similar activities]
        C --> C1[LLM生成语义先验 / LLM-generated semantic priors]
        C --> C2[结合多线索优化 / Combine multiple cues for optimization]
        D --> D1[性能超越基线 / Outperforms baselines (88.78% F1)]
        D --> D2[语义先验有效 / Semantic priors are effective]
    ```

- **[arXiv251230] HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification**
  - **tags:** [nlp], [hallucination detection], [hallucination detection, retrieval-augmented verification, contradiction graph, Paraphrased Hallucination Consistency Score (PHCS), materials science]
  - **authors:** Bhanu Prakash Vangala, Sajid Mahmud, Pawan Neupane, Joel Selvaraj, Jianlin Cheng
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22396
  - **contributions:** 1. Introduces HalluMatData, a benchmark dataset for evaluating hallucination detection in AI-generated materials science content. 2. Proposes HalluMatDetector, a multi-stage hallucination detection framework integrating intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment. 3. Introduces the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2d61ec50b266277e33c913c31df1946cc27990dbacfbd8d5b4979a627f3fa00_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of factual hallucinations in LLM-generated materials science content. It proposes HalluMatDetector, a multi-stage verification framework that combines intrinsic checks, retrieval, and contradiction analysis to detect and mitigate errors. The method reduces hallucination rates by 30% compared to standard LLM outputs and introduces a new metric (PHCS) for evaluating response consistency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content"] --> Problem["核心问题/Problem: LLM Hallucinations in Scientific Content"]
        Root --> Method["主要方法/Method: Multi-Stage Verification Framework (HalluMatDetector)"]
        Root --> Results["关键结果/Results: 30% Hallucination Reduction & New Metric (PHCS)"]
    ```

- **[arXiv251230] Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings**
  - **tags:** [mlsys], [llm inference], [knowledge graph embeddings, inference-time personalization, parameter-efficient adaptation, structure-gated adaptation, frozen backbone]
  - **authors:** Ozan Oguztuzun, Cerag Oguztuzun
  - **institution:** Case Western Reserve University
  - **link:** https://arxiv.org/pdf/2512.22398
  - **contributions:** 1. A post-hoc personalization mechanism that operates at inference time on frozen KG embeddings without backbone updates. 2. A structure-gated adaptation method that conditions candidate rankings on profile features via graph-derived gates. 3. New evaluation metrics for personalization (Alignment@k and Counterfactual Responsiveness) to quantify alignment and causal responsiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b860d25f74e154dce6d1b1a346b065fe4b4e238a600b81e91ad6a825aa744e1c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that foundation models for knowledge graphs perform well for groups but fail to capture individual user preferences. It proposes GatedBias, a lightweight framework that adds interpretable, per-entity biases to frozen KG embeddings at inference time using profile features and graph-derived gates, requiring only ~300 parameters. The method significantly improves personalized ranking alignment on benchmark datasets while preserving global accuracy, demonstrating that parameter-efficient and causally verifiable personalization is possible.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings"] --> Problem["核心问题/Problem: Foundation KG models fail to capture individual user preferences"]
        Root --> Method["主要方法/Method: GatedBias framework using structure-gated adaptation on frozen embeddings"]
        Root --> Results["关键结果/Results: Improves alignment, preserves cohort performance, parameter-efficient"]
    ```

- **[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks**
  - **tags:** [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]
  - **authors:** Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin
  - **institution:** Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22388
  - **contributions:** 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BLISS: Bandit Layer Importance Sampling Strategy] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练大型图神经网络的计算成本高/High computational cost for training GNNs on large graphs]
        C --> C1[使用多臂老虎机动态选择信息节点/Using multi-armed bandits to dynamically select informative nodes]
        C --> C2[平衡探索与利用，自适应节点重要性/Balancing exploration and exploitation, adapting to node importance]
        D --> D1[保持或超过全批次训练的精度/Maintains or exceeds full-batch training accuracy]
    ```

- **[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models**
  - **tags:** [mlsys], [llm inference], [Kubernetes, Helm, DistilBERT, scale-to-zero, hybrid routing]
  - **authors:** Bhanu Prakash Vangala, Tanu Malik
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22402
  - **contributions:** 1. A unified Helm-based deployment system for self-hosted LLMs on Kubernetes, 2. An adaptive scale-to-zero automation mechanism for efficient GPU resource utilization, 3. A hybrid routing module combining keyword heuristics and a lightweight DistilBERT classifier to balance cost, latency, and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces "Pick and Spin," a framework for efficient orchestration of self-hosted large language models. It addresses challenges in GPU utilization and workload routing by integrating Kubernetes-based deployment, adaptive scaling, and a hybrid routing strategy. The system demonstrates significant improvements in success rate, latency, and cost compared to static deployments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Efficient Multi-Model Orchestration for Self-Hosted LLMs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Self-hosted LLM deployment challenges: GPU utilization, workload routing, reliability/自托管LLM部署挑战：GPU利用率、工作负载路由、可靠性]
        C --> C1[Pick and Spin Framework: Kubernetes, Helm, scale-to-zero, hybrid routing/Pick and Spin框架：Kubernetes, Helm, 缩容至零, 混合路由]
        D --> D1[21.6% higher success rate, 30% lower latency, 33% lower cost/成功率提升21.6%，延迟降低30%，成本降低33%]
    ```

- **[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving**
  - **tags:** [mlsys], [llm inference], [speculative decoding, dynamic adaptation, multi-armed bandit, throughput optimization, latency reduction]
  - **authors:** Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.22420
  - **contributions:** 1. Identifies the critical trade-off in speculative decoding: beneficial in memory-bound (low-load) scenarios but detrimental in compute-bound (high-load) scenarios due to verification overhead. 2. Proposes Nightjar, a novel learning-based algorithm that dynamically adapts the speculative length (or disables SD) based on real-time request load and batch size. 3. Demonstrates significant performance gains, achieving up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of fixed-length speculative decoding in LLM serving, which fails to adapt to dynamic request loads. It proposes Nightjar, a learning-based algorithm that dynamically selects the optimal speculative length. Experiments show Nightjar significantly improves throughput and reduces latency compared to standard speculative decoding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Nightjar: Dynamic Adaptive Speculative Decoding] --> B[核心问题/Problem: Fixed speculative length fails under dynamic loads]
        A --> C[主要方法/Method: Learning-based algorithm adapts speculative length]
        A --> D[关键结果/Results: Higher throughput, lower latency]
    ```

- **[arXiv251230] A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot**
  - **tags:** [mlsys], [on-device ai], [Heterogeneous Computing, ROS 2, FreeRTOS, PID Control, AWS IoT]
  - **authors:** Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol
  - **institution:** Mansoura University, Egypt
  - **link:** https://arxiv.org/pdf/2512.22408
  - **contributions:** 1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Limitations of human-operated last-mile delivery (cost, safety, reliability)"] --> P1["子问题/Sub-Problem<br>Need for autonomous, cost-efficient delivery robot"]
        Method["主要方法/Method<br>Unified multi-disciplinary approach"] --> M1["异构计算/Heterogeneous Computing<br>RPi 5 (ROS 2) for AI & ESP32 (FreeRTOS) for control"]
        Method --> M2["关键技术/Key Tech<br>Low-latency comms, PID control, AWS IoT, failsafe"]
        Results["关键结果/Results<br>Robust, operational autonomous delivery system"] --> R1["成果/Outcome<br>Deterministic motor control & enhanced reliability"]
    ```

- **[arXiv251230] Emergence of Human to Robot Transfer in Vision-Language-Action Models**
  - **tags:** [ai], [robot learning], [vision-language-action models, human-to-robot transfer, co-training, emergent capability, embodiment-agnostic representations]
  - **authors:** Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair
  - **institution:** Physical Intelligence, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22414
  - **contributions:** 1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Emergence of Human to Robot Transfer in Vision-Language-Action Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Can VLA models learn from human videos for robot control?]
        C[主要方法/Method: Simple co-training recipe on human & robot data]
        D[关键结果/Results: Transfer emerges with scale; performance nearly doubles]
    ```

- **[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy**
  - **tags:** [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]
  - **authors:** Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath
  - **institution:** UC Santa Barbara, Allen Institute for Cell Sciences
  - **link:** https://arxiv.org/pdf/2512.22423
  - **contributions:** 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)
        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)
        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)
    ```

- **[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning**
  - **tags:** [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]
  - **authors:** Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu
  - **institution:** Wayne State University, Henry Ford Health
  - **link:** https://arxiv.org/pdf/2512.22425
  - **contributions:** 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Ill-posed inverse problem: complex anatomy-beam relationship / 病态逆问题: 解剖结构与射束强度的复杂关系]
        B --> B2[CNN struggles with long-range dependencies / CNN难以捕捉长程依赖]
        C --> C1[Two-stage transformer framework / 两阶段Transformer框架]
        C1 --> C1_1[Stage 1: Global dose prior / 阶段1: 全局剂量先验]
        C1 --> C1_2[Stage 2: Geometry-conditioned fluence regression / 阶段2: 几何条件化的注量图回归]
        C --> C2[Fluence-Aware Regression (FAR) loss / 注量感知回归损失]
        D --> D1[Reduced Energy Error to 4.5% / 能量误差降低至4.5%]
        D --> D2[Improved structural fidelity (p<0.05) / 结构保真度显著提升]
        D --> D3[Outperformed benchmark CNN & single-stage methods / 超越基准CNN与单阶段方法]
    ```

- **[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems**
  - **tags:** [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]
  - **authors:** Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad
  - **institution:** SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego
  - **link:** https://arxiv.org/pdf/2512.22439
  - **contributions:** 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --> B[核心问题/Problem: LiDAR垂直分辨率固定与光束丢失导致点云稀疏]
        A --> C[主要方法/Method: 基于光束感知图与门控残差融合的图注意力网络]
        A --> D[关键结果/Results: 重建误差更低，几何一致性更好，结构完整性保持]
    ```

- **[arXiv251230] Monadic Context Engineering**
  - **tags:** [mlsys], [agent system], [Monadic Context Engineering, Monad Transformers, Meta-Agents, computational contexts, algebraic structures]
  - **authors:** Yifan Zhang, Mengdi Wang
  - **institution:** Princeton University
  - **link:** https://arxiv.org/pdf/2512.22431
  - **code:** https://github.com/yifanzhang-pro/monadic-context-engineering
  - **contributions:** 1. Proposes Monadic Context Engineering (MCE), a novel architectural paradigm using Functors, Applicatives, and Monads to provide a formal foundation for AI agent design. 2. Demonstrates how Monads and Applicatives manage sequential composition and parallel execution, and how Monad Transformers enable systematic composition of capabilities like state and error handling. 3. Extends the MCE framework to describe Meta-Agents for generative orchestration, dynamically creating and managing sub-agent workflows via metaprogramming.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the brittleness and complexity in current AI agent architectures by introducing Monadic Context Engineering (MCE), a paradigm that leverages algebraic structures like Monads to formally manage state, errors, and concurrency within agent workflows. The proposed method enables the construction of complex, resilient agents from simple, verifiable components and is extended to support generative orchestration via Meta-Agents. The work concludes that MCE provides a principled foundation for building robust and scalable autonomous agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Monadic Context Engineering"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["当前代理架构脆弱/Current agent architectures are brittle"]
        Problem --> P2["状态、错误、并发管理困难/Difficulties in state, error, concurrency management"]
        Method --> M1["引入单子上下文工程/Introduce Monadic Context Engineering (MCE)"]
        Method --> M2["利用函子、应用函子、单子/Leverage Functors, Applicatives, Monads"]
        Method --> M3["使用单子变换器组合能力/Use Monad Transformers to compose capabilities"]
        Results --> R1["提供形式化基础/Provides a formal foundation"]
        Results --> R2["支持构建复杂、鲁棒的代理/Enables building complex, resilient agents"]
        Results --> R3["扩展至元代理进行生成式编排/Extends to Meta-Agents for generative orchestration"]
    ```

- **[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]
  - **authors:** Cattalyya Nuengsigkapian
  - **institution:** Google
  - **link:** https://arxiv.org/pdf/2512.22442
  - **contributions:** 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp
  - **Simple LLM Summary:** This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiFi-RAG] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[开放域RAG中的无关信息与意图对齐/Open-domain RAG faces irrelevant info & intent misalignment]
        C --> C1[分层过滤与两阶段生成/Hierarchical Filtering & Two-Pass Generation]
        C1 --> C2[使用Gemini Flash进行过滤/Use Gemini Flash for filtering]
        C1 --> C3[使用Gemini Pro进行生成/Use Gemini Pro for generation]
        D --> D1[在MMU-RAGent上超越基线/Outperforms baseline on MMU-RAGent]
        D --> D2[在自定义测试集上显著提升/Substantial gains on custom test set]
    ```

- **[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework**
  - **tags:** [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]
  - **authors:** Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang
  - **institution:** Anhui University, China Electronics Technology Group Corporation (38th Research Institute)
  - **link:** https://arxiv.org/pdf/2512.22447
  - **contributions:** 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework"] --> Problem["核心问题/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection."]
        Root --> Method["主要方法/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules."]
        Root --> Results["关键结果/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data."]
    ```

- **[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees**
  - **tags:** [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.22466
  - **contributions:** 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AMBIT: Augmenting Mobility Baselines with Interpretable Trees] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Conflicting needs for high accuracy and clear interpretability in OD flow prediction]
        C[主要方法/Method: Gray-box framework augmenting physical baselines with interpretable tree models]
        D[关键结果/Results: Physics-grounded residuals approach black-box accuracy; POI-anchored residuals are most robust]
    ```

- **[arXiv251230] The Bayesian Geometry of Transformer Attention**
  - **tags:** [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]
  - **authors:** Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra
  - **institution:** Columbia University, Dream Sports, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.22471
  - **contributions:** 1. Introduces "Bayesian wind tunnels" as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled "Bayesian wind tunnel" tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Bayesian Geometry of Transformer Attention<br/>Transformer注意力的贝叶斯几何"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Do transformers perform genuine Bayesian inference or just pattern matching?<br/>Transformer是进行真正的贝叶斯推理还是仅仅模式匹配?"]
        Method["主要方法/Method<br/>Construct 'Bayesian wind tunnels' with known posteriors<br/>构建具有已知后验的'贝叶斯风洞'"]
        Results["关键结果/Results<br/>Transformers implement Bayesian inference via geometric mechanism; MLPs fail<br/>Transformer通过几何机制实现贝叶斯推理；MLP失败"]
    ```

- **[arXiv251230] DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior**
  - **tags:** [nlp], [ai safety & alignment], [manipulation detection, safety benchmark, harm categorization, multi-layer analysis, autonomy harm]
  - **authors:** Sadia Asif, Israel Antonio Rosales Laguan, Haris Khan, Shumaila Asif, Muneeb Asif
  - **institution:** Rensselaer Polytechnic Institute, National University of Sciences and Technology
  - **link:** https://arxiv.org/pdf/2512.22470
  - **code:** https://github.com/sadia-sigma-lab/Benchmark-dataset-for-dark-patterns-in-llms
  - **contributions:** 1. Introduces DarkPatterns-LLM, the first comprehensive benchmark dataset with 401 expert-annotated examples for fine-grained detection of manipulative LLM behaviors across seven harm categories. 2. Proposes a novel four-layer diagnostic framework (MGD, MSIAN, THP, DCRA) for nuanced analysis of manipulative content, moving beyond coarse binary safety labels. 3. Provides an empirical evaluation revealing significant performance disparities (65.2%-89.7%) among state-of-the-art LLMs and identifies consistent weaknesses, particularly in detecting autonomy-undermining patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b68b7543a951540a0b9d1fde4bdde15299ee5f4f63a4526b64cdf4ab7f7a4c28_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of nuanced benchmarks for detecting manipulative behaviors in Large Language Models (LLMs). It introduces DarkPatterns-LLM, a new dataset and a four-layer analytical framework for fine-grained assessment across multiple harm categories. The evaluation shows current LLMs have significant and varied weaknesses in manipulation detection, establishing a standardized benchmark for developing more trustworthy AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DarkPatterns-LLM: 检测有害AI行为] --> B
        A --> C
        A --> D
        B[核心问题/Problem] --> B1[现有安全基准过于粗糙/Existing safety benchmarks are coarse]
        B --> B2[无法捕捉操纵的微妙机制/Fail to capture nuanced manipulation mechanisms]
        C[主要方法/Method] --> C1[新基准数据集/New benchmark dataset (401 examples)]
        C --> C2[七类危害分类/Seven harm categories]
        C --> C3[四层分析框架/Four-layer analytical pipeline (MGD, MSIAN, THP, DCRA)]
        D[关键结果/Results] --> D1[模型性能差异大/Model performance varies widely (65.2%-89.7%)]
        D --> D2[自主性危害检测弱/Weakness in detecting autonomy harm]
        D --> D3[首个标准化多维度基准/First standardized multi-dimensional benchmark]
    ```

- **[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding**
  - **tags:** [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]
  - **authors:** Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu
  - **institution:** University of Electronic Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.22481
  - **contributions:** 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Decoding fine-grained movement from noisy, non-stationary sEMG signals for prosthetic control]
        C[主要方法/Method: Domain-specific SSL with spectral pre-training and Cylindrical Rotary Position Embedding (CyRoPE)]
        D[关键结果/Results: New SOTA performance, outperforms supervised & generic SSL baselines, validated on amputation data]
    ```

- **[arXiv251230] Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring**
  - **tags:** [mlsys], [agent system], [adversarial reasoning, multi-agent system, pedagogical oversight, hierarchical framework, low-compute inference]
  - **authors:** Saisab Sadhu, Ashim Dhor
  - **institution:** Indian Institute of Science Education and Research Bhopal
  - **link:** https://arxiv.org/pdf/2512.22496
  - **contributions:** 1. Introduces Hierarchical Pedagogical Oversight (HPO), a novel multi-agent adversarial framework designed to improve the reliability of AI tutoring by separating pedagogical generation from evaluation. 2. Adapts structured adversarial synthesis to educational assessment, enforcing a dialectical debate between opposing pedagogical critics to mitigate sycophancy and superficial consensus. 3. Demonstrates that the adversarial protocol enables a small 8B-parameter model to outperform GPT-4o on pedagogical oversight while using significantly fewer computational resources.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67b6c2853ea8b40662f9788f9062b5488d7ec541a754a445a856888b9fb9300c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of unreliable AI tutors (LLMs) that often validate incorrect student answers. It proposes the Hierarchical Pedagogical Oversight (HPO) framework, which uses a structured multi-agent adversarial debate to assess tutoring quality. The main conclusion is that this adversarial approach enables a much smaller model to outperform a much larger one (GPT-4o) on a pedagogical reasoning benchmark, establishing it as a critical mechanism for reliable, low-compute oversight.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hierarchical Pedagogical Oversight<br>分层教学监督框架] --> B[Problem: LLMs as tutors are unreliable<br>问题: LLM导师不可靠]
        A --> C[Method: Multi-Agent Adversarial Framework<br>方法: 多智能体对抗框架]
        A --> D[Results: 8B model beats GPT-4o, low-compute<br>结果: 8B模型超越GPT-4o, 低计算]
    ```

- **[arXiv251230] ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation**
  - **tags:** [nlp], [speech synthesis], [flow matching, hierarchical attention, low-resource TTS, agglutinative language, non-autoregressive generation]
  - **authors:** Suhua Wang, Zifan Wang, Xiaoxin Sun, D. J. Wang, Zhanbo Liu, Xin Li
  - **institution:** Northeast Normal University, Changchun Humanities and Sciences College, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.22491
  - **contributions:** 1. Proposes a novel hierarchical text representation and cross-modal attention mechanism to handle Manchu's agglutinative phonology. 2. Introduces an end-to-end speech synthesis model integrating deep convolutional networks with a flow-matching Transformer for efficient, non-autoregressive generation. 3. Constructs the first public Manchu TTS dataset and employs data augmentation to address severe data scarcity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ManchuTTS, a novel text-to-speech system designed for the endangered and agglutinative Manchu language. The method uses a three-tier text representation and a flow-matching Transformer with hierarchical guidance to tackle data scarcity and complex phonology. Experiments show it achieves a high MOS score of 4.52 and significantly improves pronunciation accuracy and prosodic naturalness compared to baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ManchuTTS: 满语语音合成] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[数据稀缺/Data Scarcity]
        B1 --> C2[粘着语语音学/Agglutinative Phonology]
        B2 --> D1[三层文本表示/Three-tier Text Representation]
        B2 --> D2[流匹配Transformer/Flow-matching Transformer]
        B2 --> D3[分层对比损失/Hierarchical Contrastive Loss]
        B3 --> E1[MOS得分4.52/MOS Score 4.52]
        B3 --> E2[AWPA提升31%/AWPA +31%]
        B3 --> E3[韵律自然度提升27%/Prosodic Naturalness +27%]
    ```

- **[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training**
  - **tags:** [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]
  - **authors:** Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin
  - **institution:** Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing
  - **link:** https://arxiv.org/pdf/2512.22492
  - **contributions:** 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Role-Based Fault Tolerance System for LLM RL Post-Training] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[RL后训练混合训练与推理工作负载，易受双方故障影响/RL post-training mixes training & inference, vulnerable to faults from both]
        B --> B2[现有容错框架未针对RL的异步执行优化/Existing FT frameworks not optimized for RL's async execution]
        C --> C1[基于角色的故障隔离与恢复/Role-based fault isolation & recovery]
        C --> C2[检测-重启-重连范式/Detect-Restart-Reconnect paradigm]
        C2 --> C21[角色感知监控/Role-aware monitoring]
        C2 --> C22[非中断式重启/Non-disruptive restart with warm standbys]
        C2 --> C23[动态UCX点对点通信重连/Dynamic UCX P2P reconnection]
        D --> D1[ETTR超过80%，优于基线的60%/ETTR >80%, better than baseline 60%]
        D --> D2[端到端训练时间加快8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]
    ```

- **[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals**
  - **tags:** [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]
  - **authors:** Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya
  - **institution:** Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University
  - **link:** https://arxiv.org/pdf/2512.22508
  - **contributions:** 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp
  - **Simple LLM Summary:** This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Predicting LLM Correctness in Prosthodontics<br/>预测LLM在口腔修复学中的正确性] --> B(Problem/核心问题: LLM hallucinations in high-stakes healthcare domains<br/>高风险医疗领域中的LLM幻觉问题)
        A --> C(Method/主要方法: Use metadata & hallucination signals to build correctness predictors<br/>使用元数据和幻觉信号构建正确性预测器)
        A --> D(Results/关键结果: Improved accuracy & precision; metadata not reliable for hallucination prediction; prompting alters behavior<br/>提升准确率和精确度；元数据不能可靠预测幻觉；提示策略改变模型行为)
    ```

- **[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks**
  - **tags:** [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]
  - **authors:** Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance
  - **link:** https://arxiv.org/pdf/2512.22522
  - **contributions:** 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("梯度消失/Gradient Vanishing")
        Problem --> P2("对抗评估不可靠/Unreliable Adversarial Evaluation")
        Method --> M1("理论分析梯度消失/Theoretical Analysis of Gradient Vanishing")
        Method --> M2("自适应锐度替代梯度/Adaptive Sharpness Surrogate Gradient (ASSG)")
        Method --> M3("稳定自适应投影梯度下降/Stable Adaptive PGD (SA-PGD)")
        Results --> R1("攻击成功率大幅提升/Substantially Increased Attack Success Rate")
        Results --> R2("揭示鲁棒性被高估/Revealed Overestimated Robustness")
        Results --> R3("提供更可靠评估/Provided More Reliable Evaluation")
    ```

- **[arXiv251230] Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation**
  - **tags:** [mlsys], [agent system], [multi-AI agent, machine learning potential, human-in-the-loop, self-auditing, scalable molecular dynamics]
  - **authors:** Yiming Lu, Tingyu Lu, Di Zhang, Lili Ye, Hao Li
  - **institution:** Tohoku University, Dalian University of Technology
  - **link:** https://arxiv.org/pdf/2512.22529
  - **contributions:** 1. Developed a novel "human-in-the-loop" closed-loop framework utilizing self-auditing AI agents to validate and evolve a machine learning potential, ensuring quantum accuracy while achieving near-linear scalability to million-atom systems and nanosecond timescales. 2. Discovered a temperature-regulated dual-mode oxidation mechanism for aluminum nanoparticles, where the oxide shell acts as a dynamic "gatekeeper" via a "breathing mode" at moderate temperatures and transitions to a catastrophic "rupture mode" above a critical threshold. 3. Resolved a long-standing controversy by demonstrating that aluminum cation outward diffusion, not oxygen transport, is the dominant mass transfer mechanism across all temperature regimes, with diffusion coefficients 2-3 orders of magnitude higher.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42c9680d91fb0cb7f7611fd33feaef10bdab707dc569594f3d51f8af4d9e0651_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a human-in-the-loop multi-AI agent framework to develop and validate a highly accurate and scalable machine learning potential for molecular dynamics simulations. Using this method, the study reveals a dual-mode oxidation mechanism in aluminum nanoparticles and conclusively shows that aluminum cation diffusion, not oxygen transport, dominates the oxidation process.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation] --> B
        A --> C
        A --> D
        B[核心问题/Problem]
        B --> B1[原子尺度氧化机制未知/Atomic-scale oxidation mechanism unknown]
        B --> B2[计算瓶颈:精度与规模难以兼得/Computational bottleneck: trade-off between accuracy and scale]
        C[主要方法/Method]
        C --> C1[人机协同闭环框架/Human-in-the-loop closed-loop framework]
        C --> C2[自审AI代理验证MLP演化/Self-auditing AI Agents validate MLP evolution]
        D[关键结果/Results]
        D --> D1[发现双模式氧化机制/Discovered dual-mode oxidation mechanism]
        D --> D2[揭示铝离子扩散主导/Revealed Al cation diffusion dominates]
        D --> D3[建立统一原子尺度设计框架/Established unified atomic-scale design framework]
    ```

- **[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation**
  - **tags:** [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]
  - **authors:** Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22536
  - **contributions:** 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoAgent: Coherent Video Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Open-domain video generation lacks coherence and consistency/开放域视频生成缺乏连贯性和一致性]
        C --> C1[Plan-Synthesize-Verify-Edit Pipeline/计划-合成-验证-编辑流程]
        C1 --> C2[Storyboard Planner/故事板规划器]
        C1 --> C3[Global Context Manager/全局上下文管理器]
        C1 --> C4[Verifier Agent/验证智能体]
        C1 --> C5[Pacing-aware Editor/节奏感知编辑器]
        D --> D1[Improves coherence, consistency, narrative quality/提升连贯性、一致性、叙事质量]
    ```

- **[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains**
  - **tags:** [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]
  - **authors:** Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang
  - **institution:** Sun Yat-sen University, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22545
  - **contributions:** 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Rewarded Multimodal Coherent Reasoning<br>自我奖励多模态连贯推理] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --> P1[现有方法缺陷/Existing Method Flaws<br>Supervises only final answer]
        Method[主要方法/Method<br>SR-MCR Framework] --> M1[自我奖励/Self-Reward<br>Five intrinsic process cues]
        Method --> M2[优化目标/Optimization<br>GRPO with cooling mechanism]
        Results[关键结果/Results<br>Evaluation & Ablation] --> R1[性能提升/Performance Gain<br>SOTA accuracy (81.4%)]
        Results --> R2[消融研究/Ablation Study<br>Confirms contributions]
    ```

- **[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting**
  - **tags:** [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]
  - **authors:** Jaebin Lee, Hankook Lee
  - **institution:** Sungkyunkwan University
  - **link:** https://arxiv.org/pdf/2512.22550
  - **code:** https://github.com/efficient-learning-lab/TimePerceiver
  - **contributions:** 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TIMEPERCEIVER] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法侧重编码器，预测与训练分离/Prior work focuses on encoder, treats prediction & training separately]
        C --> C1[广义预测任务: 外推、插值、填补/Generalized forecasting: extrapolation, interpolation, imputation]
        C --> C2[编码: 潜在瓶颈表示/Encoding: Latent bottleneck representations]
        C --> C3[解码: 可学习查询/Decoding: Learnable queries]
        D --> D1[性能显著超越SOTA/Outperforms SOTAs significantly]
    ```

- **[arXiv251230] Learning When Not to Attend Globally**
  - **tags:** [mlsys], [llm inference], [All-or-Here Attention, sliding window attention, conditional computation, binary router, context dependency]
  - **authors:** Xuan Luo, Kailai Zhang, Xifeng Yan
  - **institution:** UC Santa Barbara
  - **link:** https://arxiv.org/pdf/2512.22562
  - **contributions:** 1. Proposes All-or-Here Attention (AHA), a novel attention mechanism that dynamically toggles between full and local sliding window attention using a binary router per head. 2. Demonstrates empirically that full attention is largely redundant, showing up to 93% of full attention operations can be replaced with local attention without performance loss. 3. Identifies a long-tail distribution in context dependency, revealing that the need for global context decays rapidly as the local window expands.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency of full self-attention in LLMs by proposing All-or-Here Attention (AHA), which learns to dynamically switch between full and local sliding window attention for each token. The results show that most full attention operations are unnecessary, and efficient inference can be achieved with on-demand global context access.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning When Not to Attend Globally] --> B[核心问题/Problem: Quadratic complexity of full self-attention is inefficient]
        A --> C[主要方法/Method: All-or-Here Attention (AHA) with binary router to toggle between full and local sliding window attention]
        A --> D[关键结果/Results: Up to 93% full attention replaced without loss; reveals long-tail context dependency]
    ```

- **[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure**
  - **tags:** [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]
  - **authors:** Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang
  - **institution:** HKUST, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22560
  - **code:** https://github.com/alibaba/ROLL
  - **contributions:** 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp
  - **Simple LLM Summary:** The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure"] --> Problem["核心问题/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure."]
        Root --> Method["主要方法/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation."]
        Root --> Results["关键结果/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs."]
    ```

- **[arXiv251230] Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI**
  - **tags:** [ai], [cognitive architectures], [predictive coding, compositional structure, episodic memory, action integration, foundation models]
  - **authors:** Rajesh P. N. Rao, Vishwas Sathish, Linxing Preston Jiang, Matthew Bryan, Prashant Rangarajan
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.22568
  - **contributions:** 1. Proposes integrating actions, compositional structure, and episodic memory into foundation models to address their deficiencies. 2. Presents neuroscience evidence to support the importance of these components for achieving human-like AI. 3. Compares the proposal to current trends like chain-of-thought reasoning and retrieval-augmented generation, suggesting new brain-inspired augmentation methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c90f622f122bb0e11d5909a8de5e173e03638e3854680c6b2a0139a74ea9314_w640_q70.webp
  - **Simple LLM Summary:** The paper argues that current foundation models, despite their success, lack key components of brain-inspired predictive coding models: action integration, compositional structure, and episodic memory. It proposes integrating these components to address issues like hallucinations, lack of grounding, and poor interpretability, aiming for safer and more human-like AI. The conclusion advocates for renewed collaboration between neuroscience and AI to achieve these goals.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Lessons from Neuroscience for AI / 神经科学对AI的启示"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem"] --> P1["Current AI lacks key brain features / 当前AI缺乏关键大脑特征"]
        Problem --> P2["Deficiencies: hallucinations, no grounding / 缺陷：幻觉，缺乏根基"]
    
        Method["主要方法/Method"] --> M1["Integrate brain components / 整合大脑组件"]
        M1 --> M1_1["Actions / 行动"]
        M1 --> M1_2["Compositional Structure / 组合结构"]
        M1 --> M1_3["Episodic Memory / 情景记忆"]
    
        Results["关键结果/Results"] --> R1["Address AI deficiencies / 解决AI缺陷"]
        Results --> R2["Enable safe, interpretable AI / 实现安全、可解释的AI"]
        Results --> R3["Path to human-like AI / 通向类人AI之路"]
    ```

- **[arXiv251230] SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G**
  - **tags:** [mlsys], [agent system], [Agentic AI Networking, Multi-agent Multi-objective Optimization, Model Partition and Sharing (MoPS), Cross-layer Optimization, Pareto-optimal Solution]
  - **authors:** Yong Xiao, Xubo Li, Haoran Zhou, Yingyu Li, Yayu Gao, Guangming Shi, Ping Zhang, Marwan Krunz
  - **institution:** Huazhong University of Science and Technology, Peng Cheng Laboratory, Pazhou Laboratory (Huangpu), China University of Geosciences (Wuhan), Xidian University, Beijing University of Posts and Telecommunications, University of Arizona
  - **link:** https://arxiv.org/pdf/2512.22579
  - **contributions:** 1. Proposes SANet, a semantic-aware Agentic AI Networking architecture that infers user semantic goals and automatically assigns cross-layer agents to fulfill them. 2. Formulates the decentralized optimization as a multi-agent multi-objective problem, proposes three novel evaluation metrics, and develops a Model Partition and Sharing (MoPS) framework for efficient model deployment. 3. Derives theoretical bounds proving a three-way tradeoff among optimization, generalization, and conflicting errors, and validates the framework with a hardware prototype showing significant performance gains and computational efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee5846572238b17d03d837b7c31a7bd60644abe5d9070207f45cd166b326470_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SANet, a semantic-aware Agentic AI networking framework for 6G that uses AI agents to infer user goals and perform cross-layer optimization. It formulates the problem as multi-agent multi-objective optimization, introduces a model partition and sharing method, and proves a theoretical tradeoff. Experiments on a hardware prototype show the framework achieves up to 14.61% performance gain while requiring only 44.37% of the FLOPs of state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SANet: A Semantic-aware Agentic AI Networking Framework] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[缺乏支持自动目标发现与多智能体编排的框架 / Lack of framework for automatic goal discovery and multi-agent orchestration]
        Problem --> P2[协作智能体可能存在目标冲突 / Collaborating agents may have conflicting objectives]
    
        Method[主要方法/Method] --> M1[语义感知架构推断用户目标并分配智能体 / Semantic-aware architecture infers user goal and assigns agents]
        Method --> M2[将问题建模为多智能体多目标优化 / Formulates as multi-agent multi-objective problem]
        Method --> M3[提出模型分区与共享框架 / Proposes Model Partition and Sharing (MoPS) framework]
        Method --> M4[提出两种分布式优化算法 / Proposes two decentralized optimization algorithms]
    
        Results[关键结果/Results] --> R1[理论证明三方面误差的权衡 / Theoretical proof of three-way error tradeoff]
        Results --> R2[硬件原型验证性能提升与计算效率 / Hardware prototype validates performance gain and computational efficiency]
    ```

- **[arXiv251230] Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care**
  - **tags:** [mlsys], [others], [physiological signal analysis, unified data interface, modular architecture, end-to-end workflow, configurable toolkit]
  - **authors:** Tao Zhou, Lingyu Shu, Zixing Zhang, Jing Han
  - **institution:** Hunan University, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.22601
  - **code:** https://github.com/SmileHnu/Tyee
  - **contributions:** 1. A unified data interface and configurable preprocessing pipeline for 12 physiological signal modalities. 2. A modular and extensible architecture enabling flexible integration and rapid prototyping. 3. An end-to-end workflow configuration system promoting reproducible and scalable experimentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4a4d498d8e9b9f7b3a7f4413e2399342920644addd35cc3e7401a5ebadf033_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Tyee, a configurable deep learning toolkit designed to address challenges in physiological signal analysis, such as heterogeneous data and fragmented pipelines. Its unified, modular design allows for flexible and reproducible experimentation. The toolkit demonstrates strong performance, achieving state-of-the-art results on 12 out of 13 evaluated datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Tyee: A Unified Toolkit for Intelligent Physiological Health Care] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[异构数据格式/Heterogeneous Data Formats]
        Problem --> P2[不一致的预处理/Inconsistent Preprocessing]
        Problem --> P3[碎片化模型管道/Fragmented Model Pipelines]
        Problem --> P4[不可复现的实验/Non-reproducible Experiments]
        Method --> M1[统一数据接口/Unified Data Interface]
        Method --> M2[模块化架构/Modular Architecture]
        Method --> M3[端到端工作流配置/End-to-end Workflow Configuration]
        Results --> R1[性能优异/Outperforms Baselines]
        Results --> R2[12/13数据集SOTA/State-of-the-art on 12 of 13 Datasets]
        Results --> R3[开源工具包/Open-source Toolkit]
    ```

- **[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation**
  - **tags:** [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]
  - **authors:** Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song
  - **institution:** Zhejiang University, Hangzhou City University, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22605
  - **code:** https://anonymous.4open.science/r/M3ob-62EF
  - **contributions:** 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A["Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation"] --> B["核心问题/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap."]
    A --> C["主要方法/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment."]
    A --> D["关键结果/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios."]
    ```

- **[arXiv251230] LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation**
  - **tags:** [mlsys], [agent system], [multi-agent system, graph neural network, collective decision-making, startup success prediction, role-playing agents]
  - **authors:** Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang
  - **institution:** ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.22608
  - **contributions:** 1. Proposes SimVC-CAS, a novel collective agent system that reformulates startup financing prediction as a multi-agent group decision-making task, moving beyond single decision-maker models. 2. Introduces role-playing agents with unique traits and a GNN-based supervised interaction module to capture heterogeneous investor evaluations and behavioral dynamics within a co-investment network. 3. Demonstrates significant predictive performance improvement (e.g., ~25% relative improvement in average precision@10) on real-world PitchBook data with strict leakage controls, while providing interpretable, multi-perspective reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5560279fcb9dca880844ffe21da36f9901c81ce8898e265fff9cc80a5e7cfb8a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of predicting startup success by simulating venture capital decision-making as a collective process. It proposes SimVC-CAS, a multi-agent system where role-playing LLM agents interact via a GNN module to model investor networks. The method significantly outperforms previous approaches in predicting financing outcomes and offers interpretable reasoning from multiple investor perspectives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[预测初创企业成功/Predicting Startup Success]
        B --> B2[现有方法忽略投资者群体动态/Existing methods overlook investor group dynamics]
        C --> C1[提出SimVC-CAS集体代理系统/Propose SimVC-CAS collective agent system]
        C --> C2[角色扮演代理与GNN交互模块/Role-playing agents & GNN-based interaction module]
        D --> D1[预测准确性显著提升/Significantly improved predictive accuracy]
        D --> D2[提供可解释的多视角推理/Provides interpretable, multi-perspective reasoning]
    ```

- **[arXiv251230] Chord Recognition with Deep Learning**
  - **tags:** [ai], [music information retrieval], [chord recognition, deep learning, generative models, pitch augmentation, beat detection]
  - **authors:** Pierre Mackenzie
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.22621
  - **contributions:** 1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp
  - **Simple LLM Summary:** This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Chord Recognition with Deep Learning) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(进展缓慢/Slow Progress)
        Method --> M1(实验现有方法/Experiment with Existing Methods)
        Method --> M2(测试生成模型假设/Test Generative Model Hypotheses)
        Results --> R1(罕见和弦表现差/Poor Performance on Rare Chords)
        Results --> R2(音高增强提升准确率/Pitch Augmentation Boosts Accuracy)
        Results --> R3(节拍检测提升可解释性/Beat Detection Improves Interpretability)
    ```

- **[arXiv251230] The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?**
  - **tags:** [ai], [forecasting], [large language models, deliberation, multi-agent, forecasting accuracy, log loss]
  - **authors:** Paul Schneider, Amalie Schramm
  - **institution:** PRIORB
  - **link:** https://arxiv.org/pdf/2512.22625
  - **contributions:** 1. Introduces and tests a structured deliberation intervention for LLMs, where models review each other's forecasts before updating, as a novel method for improving AI-based forecasting. 2. Systematically evaluates the intervention across four distinct scenarios (diverse/homogeneous models with distributed/shared information), identifying that accuracy improvement is specific to diverse models with shared information. 3. Provides empirical evidence that deliberation can be a viable strategy for improving LLM forecasting, while also revealing the unexpected finding that providing additional contextual information did not improve accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0685f113c8c22bd110f615c357b7de1633adf1104f67be97e690bae6bee70345_w640_q70.webp
  - **Simple LLM Summary:** This study investigates whether allowing large language models (LLMs) to deliberate by reviewing each other's forecasts improves their forecasting accuracy. The method was tested on 202 binary questions across different model group compositions and information-sharing scenarios. The main conclusion is that deliberation significantly improves accuracy for diverse LLM groups with shared information, but not for homogeneous groups, suggesting it as a viable strategy for enhancing LLM-based forecasting.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?"]
        Root --> Problem["核心问题/Problem<br>Does structured deliberation improve LLM forecasting accuracy?"]
        Root --> Method["主要方法/Method<br>LLMs review each other's forecasts before updating across four scenarios."]
        Root --> Results["关键结果/Results<br>Accuracy improved for diverse models with shared info; no benefit for homogeneous groups."]
    ```

- **[arXiv251230] DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [probabilistic scoring, Swiss-system tournament, explainable evaluation, evidence-coupled framework, ranking fidelity]
  - **authors:** Shiyan Liu, Jian Ma, Rui Qu
  - **institution:** Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.22629
  - **contributions:** 1. Introduces DICE, a two-stage, evidence-coupled framework for explainable and robust RAG evaluation using probabilistic \{A, B, Tie\} scoring. 2. Employs a Swiss-system tournament to reduce computational complexity from O(N²) to O(N log N) for efficient multi-system comparisons. 3. Demonstrates high agreement (85.7%) with human experts on a Chinese financial QA dataset, outperforming existing LLM-based metrics like RAGAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6045a90fb152ced5737d976a17be84cd02b0a0396b4dd9cb385e9ba4d9063de6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of interpretability and efficiency in evaluating Retrieval-Augmented Generation (RAG) systems. It proposes DICE, a framework that uses probabilistic scoring and a Swiss-system tournament to provide transparent, confidence-aware judgments while reducing computational cost. The method shows strong agreement with human experts, establishing it as an explainable and efficient paradigm for trustworthy RAG assessment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DICE: Discrete Interpretable Comparative Evaluation] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[现有指标问题/Existing Metrics Issues]
        P1 --> P1_1[可解释性有限/Limited Interpretability]
        P1 --> P1_2[不确定性量化不足/Inadequate Uncertainty Quantification]
        P1 --> P1_3[计算效率低/Computational Inefficiency]
    
        Method[主要方法/Method] --> M1[两阶段证据耦合框架/Two-Stage Evidence-Coupled Framework]
        M1 --> M1_1[概率{A,B,Tie}评分/Probabilistic Scoring]
        M1 --> M1_2[可解释推理痕迹/Interpretable Reasoning Traces]
        Method --> M2[瑞士制锦标赛/Swiss-System Tournament]
        M2 --> M2_1[降低复杂度/Reduces O(N²) to O(N log N)]
    
        Results[关键结果/Results] --> R1[效率提升/Efficiency Gain]
        R1 --> R1_1[计算量减少42.9%/42.9% Reduction]
        Results --> R2[评估有效性/Evaluation Validity]
        R2 --> R2_1[与专家85.7%一致/85.7% Human Agreement]
    ```

- **[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights**
  - **tags:** [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]
  - **authors:** Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22650
  - **code:** https://minnesotanlp.github.io/insight-scaling-webpage
  - **contributions:** 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[多阶段任务缺乏可验证奖励/Multi-stage tasks lack verifiable rewards]
        B --> B2[基于评判的优化易累积误差/Judge-based refinement prone to error accumulation]
        C --> C1[选择性测试时扩展/Selective Test-Time Scaling]
        C --> C2[跨阶段分配计算资源/Distribute compute across stages]
        C --> C3[早期剪枝低质量分支/Prune low-quality branches early]
        D --> D1[提升平均分数/Increased mean scores (61.64 to 65.86)]
        D --> D2[降低方差/Reduced variance]
        D --> D3[与人类专家对齐的评判模型/Judge model aligned with human experts]
    ```

- **[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains**
  - **tags:** [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]
  - **authors:** Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang
  - **institution:** University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22664
  - **code:** https://github.com/qklee-lz/CLAdapter
  - **contributions:** 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[下游任务数据稀缺/Data-limited downstream tasks]
        C --> C1[提出CLAdapter/Propose CLAdapter]
        C1 --> C2[注意力与聚类中心/Attention & Cluster Centers]
        C2 --> C3[个性化特征增强/Personalized Feature Enhancement]
        D --> D1[10个数据集实验/Experiments on 10 datasets]
        D1 --> D2[实现SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos**
  - **tags:** [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]
  - **authors:** Shravan Saranyan, Pramit Saha
  - **institution:** Branham High School, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22657
  - **contributions:** 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]
        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]
        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]
        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]
        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]
    ```

- **[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]
  - **authors:** Pere Martra
  - **institution:** Universidad Internacional Menéndez Pelayo (UIMP)
  - **link:** https://arxiv.org/pdf/2512.22671
  - **contributions:** 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --> B
        A --> C
        A --> D
        B[核心问题/Problem: How does structured width pruning affect different LLM capabilities?]
        C[主要方法/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]
        D[关键结果/Results: Knowledge ↓, Instruction-following ↑, Truthfulness ↑, Efficiency trade-offs]
    ```

- **[arXiv251230] Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency**
  - **tags:** [nlp], [uncertainty quantification], [conformal prediction, adaptive prediction sets, vocabulary-aware, coverage-efficiency tradeoff, marginal coverage]
  - **authors:** Yoshith Roy Kotla, Varshith Roy Kotla
  - **institution:** The ICFAI Foundation for Higher Education
  - **link:** https://arxiv.org/pdf/2512.22682
  - **contributions:** 1. Identified and formally characterized the coverage-efficiency tradeoff unique to applying conformal prediction to next-token prediction in LLMs with large vocabularies. 2. Proposed Vocabulary-Aware Conformal Prediction (VACP), a framework using semantic masking and temperature-adjusted scoring to reduce the effective prediction space. 3. Provided a theoretical analysis of when vocabulary reduction preserves conformal validity and demonstrated a 197x improvement in prediction set efficiency on benchmarks while maintaining coverage guarantees.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that naive conformal prediction for LLM next-token prediction produces uninformatively large prediction sets due to large vocabularies. It proposes Vocabulary-Aware Conformal Prediction (VACP), which uses semantic masking and hierarchical conformalization to drastically reduce set size. The method achieves near-target coverage while improving set efficiency by 197x, making conformal prediction practical for LLMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Conformal Prediction Sets for LLMs] --> B[核心问题/Problem: 标准置信预测在大型词汇表中产生巨大且无用的预测集]
        A --> C[主要方法/Method: 提出词汇感知置信预测(VACP), 使用语义掩码和分层校准]
        A --> D[关键结果/Results: 在保持90%覆盖率的同时, 将平均预测集大小从847个词元减少到4.3个]
    ```

- **[arXiv251230] TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning**
  - **tags:** [mlsys], [agent system], [travel planning benchmark, multi-turn interaction, tool-augmented agents, deterministic sandbox, real-world user requests]
  - **authors:** Xiang Cheng, Yulan Hu, Xiangwen Zhang, Lu Xu, Zheng Pan, Xin Li, Yong Liu
  - **institution:** Gaoling School of Artificial Intelligence, Renmin University of China; AMAP, Alibaba Group; National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.22673
  - **contributions:** 1. Introduces TravelBench, a real-world travel planning benchmark featuring multi-turn user-agent interaction and tool use, addressing limitations of prior static benchmarks. 2. Constructs a controlled sandbox environment with 10 deterministic travel-domain tools (e.g., POI search, route planning) to enable stable and reproducible evaluation of agent reasoning. 3. Collects and curates a diverse dataset of 1,103 instances (multi-turn, single-turn, unsolvable) from real user scenarios to comprehensively evaluate different aspects of agent performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9e23f8223243d3734eaad4483f929312f198ee51f05e74faf519458ec18add0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TravelBench, a new benchmark for evaluating LLM agents in realistic travel planning, which features multi-turn interaction and a sandbox of deterministic tools. The benchmark addresses the limitations of prior work by supporting dynamic user interaction and long-horizon planning. The authors evaluate several LLMs on TravelBench, providing a practical testbed for advancing agent capabilities in planning and tool use.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有旅行规划基准缺乏多轮交互和真实场景覆盖/Existing travel planning benchmarks lack multi-turn interaction and real-world coverage]
        C --> C1[构建包含多轮对话、单轮查询和不可解请求的真实数据集/Build a real-world dataset with multi-turn dialogues, single-turn queries, and unsolvable requests]
        C --> C2[创建具有10个确定性工具的沙盒环境/Create a sandbox environment with 10 deterministic tools]
        D --> D1[为LLM智能体评估提供了实用且可复现的基准/Provides a practical and reproducible benchmark for LLM agent evaluation]
    ```

- **[arXiv251230] Learning with the $p$-adics**
  - **tags:** [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]
  - **authors:** André F. T. Martins
  - **institution:** Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações
  - **link:** https://arxiv.org/pdf/2512.22692
  - **contributions:** 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp
  - **Simple LLM Summary:** This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Learning with the p-adics] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[现有ML基于实数域/Existing ML uses real numbers]
    B --> B2[是否可用其他域?/Alternative fields possible?]
    C --> C1[研究p-adic数域/Study p-adic number field Q_p]
    C --> C2[利用超度量结构/Exploit ultrametric structure]
    D --> D1[构建分类回归模型/Build classification & regression models]
    D --> D2[表示学习与语义网络/Representation learning & semantic networks]
    D --> D3[开启新研究方向/Open new research directions]
    ```

- **[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages**
  - **tags:** [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]
  - **authors:** Ahmed Abdullah, Sana Fatima, Haroon Mahmood
  - **institution:** FAST-National University, Al Ain University
  - **link:** https://arxiv.org/pdf/2512.22705
  - **contributions:** 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GHaLIB: 多语言希望语音检测框架 / GHaLIB: Multilingual Hope Speech Detection Framework] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[希望语音在NLP中代表性不足 / Hope speech underrepresented in NLP]
        B --> B2[低资源语言(如乌尔都语)缺乏资源 / Lack of resources for low-resource languages (e.g., Urdu)]
        C --> C1[使用预训练多语言Transformer模型 / Use pretrained multilingual Transformer models]
        C --> C2[简单预处理与分类器训练 / Simple preprocessing & classifier training]
        D --> D1[乌尔都语二元分类F1: 95.2% / Urdu binary F1: 95.2%]
        D --> D2[乌尔都语多类分类F1: 65.2% / Urdu multi-class F1: 65.2%]
        D --> D3[多语言模型适用于低资源环境 / Multilingual models viable for low-resource settings]
    ```

- **[arXiv251230] Memento-II: Learning by Stateful Reflective Memory**
  - **tags:** [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]
  - **authors:** Jun Wang
  - **institution:** University College London (UCL)
  - **link:** https://arxiv.org/pdf/2512.22716
  - **contributions:** 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]
        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]
        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]
        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]
        C --> F[读取对应策略改进/Reading corresponds to policy improvement]
    ```

- **[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents**
  - **tags:** [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]
  - **authors:** Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo
  - **institution:** Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI
  - **link:** https://arxiv.org/pdf/2512.22733
  - **code:** https://github.com/SHAO-Jiaqi757/FoldAct
  - **contributions:** 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[非平稳观测分布 / Non-stationary Observation Distribution]
        B --> B2[梯度稀释 / Gradient Dilution]
        B --> B3[计算成本高 / High Computational Cost]
        C --> C1[分离损失计算 / Separated Loss Computation]
        C --> C2[全上下文一致性损失 / Full Context Consistency Loss]
        C --> C3[选择性片段训练 / Selective Segment Training]
        D --> D1[稳定训练 / Stable Training]
        D --> D2[5.19倍加速 / 5.19× Speedup]
    ```

- **[arXiv251230] Harnessing Large Language Models for Biomedical Named Entity Recognition**
  - **tags:** [nlp], [named entity recognition], [instruction tuning, data filtering, weak-to-strong learning, biomedical named entity recognition, json generation]
  - **authors:** Jian Chen, Leilei Su, Cong Sun
  - **institution:** Hainan University, Weill Cornell Medicine
  - **link:** https://arxiv.org/pdf/2512.22738
  - **contributions:** 1. Proposes BioSelectTune, a data-centric framework for fine-tuning LLMs for BioNER that prioritizes data quality. 2. Introduces a Hybrid Superfiltering strategy, a weak-to-strong data curation method to distill a high-impact training dataset. 3. Reformulates BioNER as a structured JSON generation task to leverage LLMs' instruction-following capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of adapting general-domain LLMs to Biomedical Named Entity Recognition (BioNER) by proposing BioSelectTune, a framework that uses a novel Hybrid Superfiltering data curation strategy and formulates BioNER as a JSON generation task. The method achieves state-of-the-art performance on multiple benchmarks, outperforming specialized models even when trained on only 50% of the curated data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Harnessing LLMs for BioNER] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs lack domain knowledge for BioNER / LLMs缺乏生物医学领域知识]
        B --> B2[Low-quality data degrades performance / 低质量数据导致性能下降]
        C --> C1[BioSelectTune Framework / BioSelectTune框架]
        C1 --> C2[Reformulate as JSON generation / 重构为JSON生成任务]
        C1 --> C3[Hybrid Superfiltering / 混合超级过滤策略]
        D --> D1[SOTA on benchmarks / 基准测试达到SOTA]
        D --> D2[Outperforms BioMedBERT / 超越BioMedBERT]
        D --> D3[50% data surpasses baseline / 50%数据超越全量基线]
    ```

- **[arXiv251230] Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning**
  - **tags:** [mlsys], [llm training], [Column Type Annotation, Prompt Augmentation, LoRA, Parameter-Efficient Fine-Tuning, Prompt Sensitivity]
  - **authors:** Hanze Meng, Jianhao Cao, Rachel Pottinger
  - **institution:** University of British Columbia
  - **link:** https://arxiv.org/pdf/2512.22742
  - **code:** https://github.com/fripSideMeng/PACTA
  - **contributions:** 1. Proposes a parameter-efficient fine-tuning framework for Column Type Annotation (CTA) using Low-Rank Adaptation (LoRA) to reduce computational cost. 2. Introduces a prompt augmentation strategy during training to mitigate model sensitivity to variations in prompt wording and structure. 3. Demonstrates robust and stable performance across diverse datasets and prompt templates, achieving higher weighted F1 scores than single-template fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95317c9af6072a1e0ffbb34950b7d9da057c55baaf301c56bb751746b366785a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of prompt sensitivity and high computational cost in using Large Language Models (LLMs) for Column Type Annotation. It proposes a parameter-efficient framework that fine-tunes LLMs using LoRA on prompt-augmented data. The method achieves robust performance across different prompts and datasets while requiring significantly fewer trainable parameters.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有方法对提示词敏感/Existing methods are sensitive to prompts")
        Problem --> P2("完全微调成本高昂/Full fine-tuning is computationally prohibitive")
        Method --> M1("使用LoRA进行参数高效微调/Parameter-efficient fine-tuning with LoRA")
        Method --> M2("使用增强的提示数据进行训练/Training on prompt-augmented data")
        Results --> R1("对不同提示模式性能稳定/Stable performance across diverse prompts")
        Results --> R2("获得更高的加权F1分数/Higher weighted F1 scores")
    ```

- **[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations**
  - **tags:** [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]
  - **authors:** Zheng Qiu, Chih-Yuan Chiu, Glen Chou
  - **institution:** Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22757
  - **contributions:** 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Active Constraint Learning in High Dimensions from Demonstrations") --> Problem("核心问题/Problem: Data-inefficient constraint inference from demonstrations")
        Root --> Method("主要方法/Method: Iterative active learning with Gaussian Processes")
        Root --> Results("关键结果/Results: Outperforms baseline with sparse, informative demonstrations")
    ```

- **[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer**
  - **tags:** [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (μP), compute-optimal grid search]
  - **authors:** Nikhil Ghosh, Denny Wu, Alberto Bietti
  - **institution:** Flatiron Institute, New York University
  - **link:** https://arxiv.org/pdf/2512.22768
  - **contributions:** 1. Develops a formal conceptual framework defining "fast" hyperparameter transfer and proves its equivalence to "useful" transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Understanding the Mechanisms of Fast Hyperparameter Transfer<br>理解快速超参数迁移的机制"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Standard HP tuning is too expensive for large models.<br>标准HP调优对于大模型过于昂贵"] --> P1["子问题/Sub-problem<br>How to define and understand 'fast' HP transfer?<br>如何定义和理解'快速'HP迁移？"]
        Method["主要方法/Method<br>Develop a formal framework for HP transfer.<br>建立HP迁移的形式化框架"] --> M1["方法步骤/Step<br>Define 'fast' vs 'useful' transfer.<br>定义'快速'与'有用'迁移"]
        Method --> M2["方法步骤/Step<br>Analyze problem structure & µP.<br>分析问题结构与µP"]
        Method --> M3["方法步骤/Step<br>Propose trajectory decomposition hypothesis.<br>提出轨迹分解假设"]
        Results["关键结果/Results<br>Fast transfer is equivalent to useful transfer.<br>快速迁移等价于有用迁移"] --> R1["结果/Result<br>Transfer success depends on problem structure.<br>迁移成功取决于问题结构"]
        Results --> R2["结果/Result<br>Empirical evidence supports the hypothesis.<br>实证证据支持该假设"]
    ```

- **[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting**
  - **tags:** [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]
  - **authors:** Yiqian Li, Wen Jiang, Kostas Daniilidis
  - **institution:** University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.22771
  - **contributions:** 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --> B(核心问题/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)
    A --> C(主要方法/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)
    A --> D(关键结果/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)
    ```

- **[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning**
  - **tags:** [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]
  - **authors:** Kasra Jalaldoust, Elias Bareinboim
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2512.22777
  - **contributions:** 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning] --> B[核心问题/Problem: Generalization under distribution shift]
    A --> C[主要方法/Method: Circuit-TR algorithm based on causal transportability]
    A --> D[关键结果/Results: Theoretical characterization of few-shot learnability]
    B --> B1[领域泛化与适应/Domain Generalization & Adaptation]
    C --> C1[模块学习与电路组合/Module Learning & Circuit Composition]
    C --> C2[因果图与机制共享/Causal Graph & Mechanism Sharing]
    D --> D1[可迁移性标准/Transportability Criteria]
    D --> D2[电路规模复杂度/Circuit Size Complexity]
    ```

- **[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks**
  - **tags:** [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]
  - **authors:** Xuyan Li, Jie Wang, Zheng Yan
  - **institution:** Xidian University
  - **link:** https://arxiv.org/pdf/2512.22772
  - **contributions:** 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[TGNN缺乏透明度和可解释性/Lack of TGNN transparency & explainability]
        B --> B2[现有方法通用性差、效率低、不友好/Existing methods lack generality, efficiency, user-friendliness]
        C --> C1[提取节点序列作为统一特征/Extract node sequences as unified features]
        C --> C2[使用BFS和时间信息构建序列/Use BFS & temporal info to construct sequences]
        C --> C3[基于RNN的生成模型/RNN-based generative model]
        D --> D1[在6个数据集上实验/Experiments on 6 datasets]
        D --> D2[优于现有基线方法/Outperforms existing baselines]
        D --> D3[通用、高效、用户友好/Generality, efficiency, user-friendliness]
    ```

- **[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools**
  - **tags:** [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]
  - **authors:** Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong
  - **institution:** University of Southern California
  - **link:** https://arxiv.org/pdf/2512.22795
  - **contributions:** 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CNSight: 临床笔记分割工具评估 / CNSight: Evaluation of Clinical Note Segmentation Tools]
        Root --> Problem[临床笔记非结构化 / Clinical Notes Unstructured]
        Root --> Method[评估规则/变换器/大语言模型 / Evaluate Rule-based/Transformer/LLMs]
        Root --> Results[大模型性能最佳 / Large Models Best Performance]
    ```

- **[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance**
  - **tags:** [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]
  - **authors:** Shuai Chen, Chen Wang, Ziran Wang
  - **institution:** School of Mechanical Engineering, Shandong University
  - **link:** https://arxiv.org/pdf/2512.22792
  - **contributions:** 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Feature drift & unknown gas interference in E-nose"] --> P1["信号漂移/Feature Distribution Shift"]
        Problem --> P2["未知气体干扰/Unknown Gas Interference"]
        Method["主要方法/Method<br>SNM-Net Framework"] --> M1["几何解耦/Geometric Decoupling<br>Cascaded Batch & L2 Norm"]
        Method --> M2["马氏距离评分/Mahalanobis Distance Scoring"]
        Method --> M3["架构无关/Architecture-Agnostic<br>CNN, RNN, Transformer"]
        Results["关键结果/Results<br>State-of-the-art performance"] --> R1["高AUROC/High AUROC: 0.9977"]
        Results --> R2["高未知气体检测率/High Unknown Detection: 99.57%"]
        Results --> R3["强鲁棒性/High Robustness<br>Low std. dev."]
    ```

- **[arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach**
  - **tags:** [ai], [differential games], [Hamilton-Jacobi reachability, reach-avoid games, dimensionality decomposition, UAVs, tracking control]
  - **authors:** Minh Bui, Simon Monckton, Mo Chen
  - **institution:** Simon Fraser University, Defense Research & Development Canada (DRDC)
  - **link:** https://arxiv.org/pdf/2512.22793
  - **contributions:** 1. A novel dimensionality reduction framework for 3D reach-avoid games by decomposing the problem into horizontal and vertical sub-games., 2. A Hamilton-Jacobi-based tracking control algorithm to reconstruct the solution from sub-games, guaranteeing capture and subsequent tracking of the attacker., 3. Theoretical proof of the conditions for maintaining capture guarantees and empirical validation in both numerical simulations and a physics simulator (Gazebo).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14da164894760eae683bf30139829cd77a6bbd67cf14fee19eb09a05cb31eff_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the high-dimensional challenge of 3D reach-avoid differential games for UAVs by proposing a decomposition approach that splits the problem into horizontal and vertical sub-games, solves them using Hamilton-Jacobi reachability analysis, and uses a novel tracking control to reconstruct the solution. The method is proven to maintain optimality and capture guarantees, and its effectiveness is successfully demonstrated through simulations and a physics simulator for quadrotor capture.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[3D追逃博弈高维挑战/High Dimensionality of 3D Reach-Avoid Games]
        B --> B2[现有方法局限性/Limitations of Existing Approaches]
        C --> C1[维度分解/Dimensionality Decomposition]
        C1 --> C1_1[水平子博弈/Horizontal Sub-game]
        C1 --> C1_2[垂直子博弈/Vertical Sub-game]
        C --> C2[HJ可达性分析/HJ Reachability Analysis]
        C --> C3[HJ跟踪控制/HJ-based Tracking Control]
        D --> D1[保持最优性与保证/Maintains Optimality & Guarantees]
        D --> D2[仿真验证/Simulation Validation]
        D --> D3[物理模拟器成功捕获/Successful Capture in Physics Simulator]
    ```

- **[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]
  - **authors:** Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani
  - **institution:** Nvidia, Meta
  - **link:** https://arxiv.org/pdf/2512.22804
  - **contributions:** 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MoR: Mixture Of Representations For Mixed-Precision Training] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Successful mixed-precision training requires the right combination of methods.]
        Method[主要方法/Method<br>Dynamic, property-aware quantization framework selecting between representations (e.g., FP8/BF16).]
        Results[关键结果/Results<br>Achieves 98.38% FP8 quantization, preserves model quality, enables lower precision formats.]
    ```

- **[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation**
  - **tags:** [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]
  - **authors:** Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou
  - **institution:** THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT
  - **link:** https://arxiv.org/pdf/2512.22808
  - **contributions:** 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有数据空间不一致/Existing data spatial misalignment]
        B --> B2[因果生成与3D对齐的挑战/Causal generation & 3D alignment challenge]
        C --> C1[构建HRD数据集/Build HRD dataset]
        C --> C2[VQ-VAE压缩运动/VQ-VAE compresses motion]
        C --> C3[GPT自回归生成/GPT autoregressive generation]
        C --> C4[融入3D动态特征/Incorporate 3D dynamic features]
        D --> D1[更高的真实感与空间一致性/Higher realism & spatial consistency]
        D --> D2[实时生成效率/Real-time generation efficiency]
        D --> D3[保持严格因果性/Maintains strict causality]
    ```

- **[arXiv251230] FasterPy: An LLM-based Code Execution Efficiency Optimization Framework**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [Code Optimization, Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), Large Language Models (LLMs), Python]
  - **authors:** Yue Wu, Minghao Han, Ruiyin Li, Peng Liang, Amjed Tahir, Zengyang Li, Qiong Feng, Mojtaba Shahin
  - **institution:** Wuhan University, Carnegie Mellon University, Massey University, Central China Normal University, Nanjing University of Science and Technology, RMIT University
  - **link:** https://arxiv.org/pdf/2512.22827
  - **code:** https://github.com/WuYue22/fasterpy
  - **contributions:** 1. Proposes FasterPy, a low-cost and efficient framework that adapts LLMs for Python code execution efficiency optimization. 2. Combines Retrieval-Augmented Generation (RAG) with a knowledge base of performance-improving code pairs and Low-Rank Adaptation (LoRA) to enhance optimization performance. 3. Demonstrates superior performance over existing models on the Performance Improving Code Edits (PIE) benchmark.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49aae1b7cd12cfd30401a619c9b06d4bccc853d1d14eed57af87eb6c80858f31_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FasterPy, a framework that uses Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) to automatically optimize Python code for better execution efficiency. It addresses the limitations of traditional rule-based and data-intensive ML methods by providing a more scalable and cost-effective solution. Experimental results show that FasterPy outperforms existing models on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FasterPy: An LLM-based Code Execution Efficiency Optimization Framework] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统方法成本高，可扩展性差/Traditional methods are costly and hard to scale]
        C --> C1[结合RAG与LoRA的LLM框架/LLM framework combining RAG and LoRA]
        D --> D1[在PIE基准上表现优异/Outperforms existing models on PIE benchmark]
    ```

- **[arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning**
  - **tags:** [mlsys], [agent system], [automated environment synthesis, environment-level RL, agentic reinforcement learning, simulated user, policy optimization]
  - **authors:** Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22857
  - **contributions:** 1. A unified, automated pipeline for synthesizing scalable simulated environments with high-difficulty, easily verifiable tasks. 2. An Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and performs advantage estimation at the environment level. 3. Comprehensive validation on agentic benchmarks demonstrating effectiveness and out-of-domain generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AutoForge, a framework to automate the synthesis of challenging simulated environments for training language-based agents via reinforcement learning. It introduces an environment-level RL algorithm to improve training stability and efficiency by handling simulated user instability and heterogeneous environments. Evaluations show the method is effective and generalizes well to out-of-domain tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoForge] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[环境合成半自动/Semi-automated Environment Synthesis]
        B --> B2[任务难度不足/Insufficient Task Difficulty]
        B --> B3[模拟用户不稳定/Simulated User Instability]
        C --> C1[自动化环境合成管道/Automated Environment Synthesis Pipeline]
        C --> C2[环境级RL算法/Environment-level RL Algorithm (ERPO)]
        D --> D1[基准测试有效/Effective on Benchmarks (τ-bench, etc.)]
        D --> D2[域外泛化强/Strong Out-of-domain Generalization]
    ```

- **[arXiv251230] The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al**
  - **tags:** [other], [embodied cognition, robotics], [morphological computation, embodiment, information theory, passive dynamic walker]
  - **authors:** Matej Hoffmann
  - **institution:** Czech Technical University in Prague
  - **link:** https://arxiv.org/pdf/2512.22868
  - **contributions:** 1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of "morphological computation". 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp
  - **Simple LLM Summary:** This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of "morphological computation" is merely a metaphor and that the body's main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[The body is not there to compute<br>身体不是为了计算] --> Problem[核心问题/Problem<br>Is the body's primary role computational?<br>身体的主要作用是计算吗？]
        Root --> Method[主要方法/Method<br>Conceptual critique of "morphological computation"<br>对"形态计算"的概念性批判]
        Root --> Results[关键结果/Results<br>Body's role is physical, not computational<br>身体的作用是物理的，而非计算的]
    ```

- **[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks**
  - **tags:** [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]
  - **authors:** Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov
  - **institution:** Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2512.22876
  - **contributions:** 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement Networks] --> B[核心问题/Problem: End-to-end training of multi-component AI systems]
        A --> C[主要方法/Method: MARL agents organized in a DAG (Reinforcement Networks)]
        A --> D[关键结果/Results: Improved performance, unified framework for structured MARL]
    ```

- **[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]
  - **authors:** Hasan Faraz Khan, Noor Fatima, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22878
  - **contributions:** 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --> Problem_Sub[问题细节/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]
        Method[主要方法/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --> Method_Sub[方法细节/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]
        Results[关键结果/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --> Results_Sub[结果细节/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]
    ```

- **[arXiv251230] Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations**
  - **tags:** [sec], [cyber resilience], [agentic AI, game theory, autonomous agents, system-theoretic framework, equilibrium-based design]
  - **authors:** Tao Li, Quanyan Zhu
  - **institution:** City University of Hong Kong, New York University
  - **link:** https://arxiv.org/pdf/2512.22883
  - **contributions:** 1. Proposes a paradigm shift from prevention-centric security to agentic cyber resilience, arguing for systems that anticipate, maintain, recover, and learn under attack. 2. Develops a system-level framework and general architecture for designing AI workflows where autonomous agents participate in sensing, reasoning, and action. 3. Demonstrates how game-theoretic formulations provide a unifying design language for analyzing coupled attacker-defender workflows and enable equilibrium-based resiliency design, illustrated with case studies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1d5f82743a059040190978c2a78338bb73c72bc9cec9a0aafe00a0d12f0f24d_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the rise of foundation-model-based AI necessitates a shift from traditional prevention-focused cybersecurity to a new paradigm of agentic cyber resilience. It proposes a system-theoretic framework for designing autonomous AI workflows and uses game theory as a unifying language to model attacker-defender dynamics, concluding that equilibrium-based design enables system-level resilience as demonstrated in case studies like automated penetration testing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations") --> Problem("核心问题/Problem: Traditional static, human-centered security architectures are mismatched with AI-driven, adaptive cyber threats.")
        Root --> Method("主要方法/Method: Proposes a shift to agentic cyber resilience and a system-level framework using game theory to design autonomous AI workflows.")
        Root --> Results("关键结果/Results: Equilibrium-based design enables system-level resiliency, illustrated through case studies in automated pentesting and cyber deception.")
    ```

- **[arXiv251230] DECEPTICON: How Dark Patterns Manipulate Web Agents**
  - **tags:** [mlsys], [agent system], [dark patterns, web agents, adversarial robustness, deceptive UI, agent testing]
  - **authors:** Phil Cuvin, Hao Zhu, Diyi Yang
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.22894
  - **code:** https://agentdarkpatterns.org
  - **contributions:** 1. Introduces DECEPTICON, a novel environment for testing dark patterns in isolation with 700 web navigation tasks, 2. Demonstrates that dark patterns successfully manipulate agent trajectories in over 70% of tasks, significantly higher than human susceptibility, 3. Shows that larger, more capable models are more susceptible to dark patterns, and existing countermeasures like in-context prompting and guardrail models fail to mitigate the risk effectively.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8998ab43971416f683709173f00be5e8d5373de89f15ac199ec42d645a75b8b6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DECEPTICON, a testing environment to evaluate how dark patterns manipulate web agents, revealing that these deceptive UI designs successfully steer agent actions in over 70% of tasks, with larger models being more vulnerable and current defenses ineffective. The findings highlight an urgent need for robust defenses against such manipulative designs in agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DECEPTICON: How Dark Patterns Manipulate Web Agents] --> B[核心问题/Problem: Dark patterns manipulate users and pose risks to agent robustness]
        A --> C[主要方法/Method: DECEPTICON environment with 700 tasks to test dark patterns in isolation]
        A --> D[关键结果/Results: Dark patterns steer agents in >70% tasks, larger models more susceptible, defenses fail]
    ```

- **[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery**
  - **tags:** [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]
  - **authors:** Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22899
  - **contributions:** 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]
        C[主要方法/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]
        D[关键结果/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]
    ```

- **[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments**
  - **tags:** [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]
  - **authors:** Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu
  - **institution:** University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22901
  - **contributions:** 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight "Collar Recognition Nets" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight "Collar Recognition Nets" optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Neural Network-Based Real-time Casing Collar Recognition System<br>基于神经网络的实时套管接箍识别系统"] --> Problem
        Root --> Method
        Root --> Results
        Problem["信号衰减导致井下定位不准确<br>Signal degradation compromises downhole positioning"]
        Method["为ARM Cortex-M7优化的轻量级CRN网络<br>Lightweight CRNs optimized for ARM Cortex-M7"]
        Results["8208 MACs, F1=0.972, 343.2µs延迟<br>8208 MACs, F1=0.972, 343.2µs latency"]
    ```

- **[arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [hierarchical deep reinforcement learning, portfolio management, dynamic asset grouping, utility-based capital allocation, SHAP interpretability]
  - **authors:** Xiaotian Ren, Nuerxiati Abudurexiti, Zhengyong Jiang, Angelos Stefanidis, Hongbin Liu, Jionglong Su
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.22895
  - **contributions:** 1. Proposes a hierarchical DRL framework (SAMP-HDRL) that integrates dynamic asset grouping, upper-lower agent coordination, and a utility-based capital allocation mechanism for robust portfolio management. 2. Demonstrates superior performance through extensive backtests across multiple market regimes, showing consistent improvements in return and risk-adjusted metrics over traditional and DRL baselines. 3. Provides interpretability via SHAP analysis, revealing a complementary "diversified + concentrated" decision pattern across agent layers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07244b408b9238d10b2d5561e0007db8732b1d4e9e79bda5477bef5db2dd385c_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles portfolio optimization in non-stationary markets by proposing SAMP-HDRL, a hierarchical deep reinforcement learning framework that segments assets, coordinates global and local agents, and uses a utility-based capital allocator. The method outperforms numerous baselines in backtests, achieving higher returns and risk-adjusted ratios, and its decisions are made interpretable through SHAP analysis, revealing a combined diversified and concentrated investment strategy.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Portfolio optimization in non-stationary markets with regime shifts and limited DRL interpretability] --> P1[挑战/Challenges: Dynamic correlations, regime shifts]
        Method[主要方法/Method: Hierarchical DRL with segmented allocation] --> M1[上层代理/Upper-level Agent: Extracts global market signals]
        Method --> M2[动态资产分组/Dynamic Asset Grouping: Partitions market into subsets]
        Method --> M3[下层代理/Lower-level Agents: Perform intra-group allocation]
        Method --> M4[效用资本分配/Utility-based Capital Allocation: Integrates risky & risk-free assets]
        Results[关键结果/Results: Outperforms baselines, provides interpretability] --> R1[性能/Performance: Higher Return, Sharpe, Sortino, Omega ratios]
        Results --> R2[可解释性/Interpretability: SHAP reveals "diversified + concentrated" mechanism]
    ```

- **[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]
  - **authors:** Ünver Çiftçi
  - **institution:** Tekirdağ Namık Kemal University
  - **link:** https://arxiv.org/pdf/2512.22910
  - **contributions:** 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Sat-EnQ] --> B[核心问题/Problem: Deep Q-Learning Instability]
        A --> C[主要方法/Method: Two-Phase Satisficing Ensemble]
        A --> D[关键结果/Results: Variance Reduction & Robustness]
        B --> B1[早期训练不稳定/Early Training Instability]
        B --> B2[高方差与灾难性失败/High Variance & Catastrophic Failure]
        C --> C1[阶段1: 满足化集成训练/Phase 1: Satisficing Ensemble Training]
        C --> C2[阶段2: 蒸馏与微调/Phase 2: Distillation & Fine-tuning]
        D --> D1[3.8倍方差降低/3.8x Variance Reduction]
        D --> D2[0%灾难性失败/0% Catastrophic Failure]
        D --> D3[2.5倍计算效率提升/2.5x Compute Efficiency]
    ```

- **[arXiv251230] Multimodal Fact-Checking: An Agent-based Approach**
  - **tags:** [ai], [multimodal fact-checking], [multimodal misinformation, agent-based reasoning, explainable dataset, vision-language models, evidence retrieval]
  - **authors:** Danni Xu, Shaojing Fan, Xuanang Cheng, Mohan Kankanhalli
  - **institution:** National University of Singapore (NUS)
  - **link:** https://arxiv.org/pdf/2512.22933
  - **contributions:** 1. Introduces RW-Post, a high-quality, explainable dataset for real-world multimodal fact-checking that aligns claims with original social media posts and provides detailed reasoning and evidence. 2. Proposes AgentFact, a novel agent-based multimodal fact-checking framework that emulates the human verification workflow through five specialized, collaboratively working agents. 3. Demonstrates that the synergy between the new dataset and the agent framework substantially improves both the accuracy and interpretability of multimodal fact-checking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated multimodal fact-checking by introducing a new dataset (RW-Post) and an agent-based framework (AgentFact). The dataset provides real-world misinformation instances with reasoning and evidence, while the framework uses specialized agents to collaboratively perform verification tasks. The combined approach is shown to significantly enhance the accuracy and explainability of fact-checking systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multimodal Fact-Checking: An Agent-based Approach] --> B[核心问题/Problem: 多模态虚假信息传播与现有方法在推理和证据利用上的不足 / The spread of multimodal misinformation and the limitations of existing methods in reasoning and evidence utilization]
        A --> C[主要方法/Method: 提出RW-Post数据集和AgentFact智能体框架 / Proposes the RW-Post dataset and the AgentFact agent-based framework]
        A --> D[关键结果/Results: 显著提升了多模态事实核查的准确性和可解释性 / Substantially improves the accuracy and interpretability of multimodal fact-checking]
    ```

- **[arXiv251230] Geometric Structural Knowledge Graph Foundation Model**
  - **tags:** [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]
  - **authors:** Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab
  - **institution:** University of Stuttgart, University of Southampton, Shahrood University of Technology
  - **link:** https://arxiv.org/pdf/2512.22931
  - **contributions:** 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometric Structural Knowledge Graph Foundation Model] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖单一关系转换，表达能力受限/Existing methods rely on single relational transformation, limiting expressiveness]
        C --> C1[引入多头几何注意力/Multi-head geometric attention]
        C --> C2[并行多种几何变换/Parallel geometric transformations]
        C --> C3[关系条件注意力融合/Relational conditioned attention fusion]
        D --> D1[在56个KG上超越ULTRA/Outperforms ULTRA on 56 KGs]
        D --> D2[零样本归纳链接预测性能提升/Improves zero-shot inductive link prediction]
    ```

- **[arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning**
  - **tags:** [ai], [multi-agent reinforcement learning], [heterogeneity, multi-agent reinforcement learning, parameter sharing, heterogeneity distance, dynamic algorithm]
  - **authors:** Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22941
  - **code:** https://github.com/Harry67Hu/HetDPS
  - **contributions:** 1. Proposes a systematic categorization of heterogeneity in MARL into five types with mathematical definitions. 2. Defines a heterogeneity distance and introduces a practical method to quantify agent heterogeneity. 3. Designs a heterogeneity-based dynamic parameter sharing algorithm that demonstrates better interpretability and adaptability compared to baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4448c413f180749bc7f2220bea2793dad9a358fb068164020bd7b0421e5b05_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a rigorous definition and understanding of heterogeneity in multi-agent reinforcement learning (MARL). It proposes a methodology to define, quantify, and utilize heterogeneity, culminating in a dynamic parameter sharing algorithm. Experiments show this algorithm offers improved interpretability and adaptability over other parameter-sharing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Heterogeneity in Multi-Agent Reinforcement Learning<br/>多智能体强化学习中的异质性"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["缺乏对异质性的严格定义<br/>Lacks rigorous definition of heterogeneity"]
        Method --> M1["定义与分类<br/>Definition & Categorization"]
        Method --> M2["量化方法<br/>Quantification Method"]
        Method --> M3["应用算法<br/>Application Algorithm"]
        M1 --> M1_1["五类异质性<br/>Five types of heterogeneity"]
        M2 --> M2_1["异质性距离<br/>Heterogeneity distance"]
        M3 --> M3_1["动态参数共享<br/>Dynamic Parameter Sharing"]
        Results --> R1["有效识别与量化<br/>Effective identification & quantification"]
        Results --> R2["算法性能优越<br/>Algorithm outperforms baselines"]
    ```

- **[arXiv251230] APO: Alpha-Divergence Preference Optimization**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]
  - **authors:** Wang Zixian
  - **institution:** China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch
  - **link:** https://arxiv.org/pdf/2512.22953
  - **contributions:** 1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[APO: Alpha-Divergence Preference Optimization] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[两种分歧权衡 / Two Divergence Trade-off]
        P1 --> P2[前向KL覆盖但保守 / Forward KL: Mode-Covering but Conservative]
        P1 --> P3[反向KL寻求但易崩溃 / Reverse KL: Mode-Seeking but Collapses]
        Method[主要方法/Method] --> M1[锚定框架 / Anchored Framework]
        M1 --> M2[使用α-散度插值 / Use α-Divergence to Interpolate]
        M2 --> M3[调度α值 / Schedule α Value]
        Results[关键结果/Results] --> R1[竞争性性能 / Competitive Performance]
        Results --> R2[保持稳定性 / Maintains Training Stability]
    ```

- **[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding**
  - **tags:** [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]
  - **authors:** Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang
  - **institution:** Nanjing University, China Mobile Zijin Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.23020
  - **contributions:** 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖预定义对象表，无法处理未定义目标/Existing methods rely on pre-defined OLT, limiting open-world application]
        C --> C1[提出OpenGround框架与主动认知推理模块/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]
        C1 --> C2[通过认知任务链和动态更新的OLT增强VLM认知/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]
        D --> D1[Nr3D上表现有竞争力，ScanRefer上达到SOTA/Competitive on Nr3D, SOTA on ScanRefer]
        D --> D2[在OpenTarget数据集上提升17.6%/17.6% improvement on OpenTarget dataset]
    ```

- **[arXiv251230] LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models**
  - **tags:** [nlp], [multimodal language models], [multimodal sensing, time-series encoding, ecological momentary assessment (EMA)]
  - **authors:** Wenxuan Xu, Arvind Pillai, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell
  - **institution:** Dartmouth College, University of Virginia, Massachusetts General Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2512.23025
  - **contributions:** 1. Introduces LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental health narratives. 2. Constructs a large-scale dataset of over 100,000 sensor-text QA pairs by transforming Ecological Momentary Assessment (EMA) responses. 3. Trains a patch-level encoder to project raw sensor time-series signals directly into an LLM's representation space for native integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of translating long-duration, multimodal sensor data into interpretable natural language for mental health assessment. It proposes the LENS framework, which creates a large sensor-text dataset and trains a specialized encoder to align sensor signals with an LLM, enabling the generation of clinically meaningful narratives. The results show LENS outperforms baselines on NLP and clinical metrics, and is validated by mental health professionals.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LENS: LLM-Enabled Narrative Synthesis] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传感器数据难以转化为自然语言/Sensor data hard to translate to text]
        Problem --> P2[缺乏配对数据集/Lack of paired sensor-text datasets]
        Method[主要方法/Method] --> M1[构建大规模传感器-文本QA数据集/Build large-scale sensor-text QA dataset]
        Method --> M2[训练补丁级编码器对齐LLM/Train patch-level encoder to align with LLM]
        Results[关键结果/Results] --> R1[在NLP和症状指标上超越基线/Outperforms baselines on NLP & symptom metrics]
        Results --> R2[临床医生认为叙述全面有意义/Clinicians find narratives comprehensive & meaningful]
    ```

- **[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project**
  - **tags:** [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]
  - **authors:** Thomson Tong, Diba Darooneh
  - **institution:** None
  - **link:** https://arxiv.org/pdf/2512.23028
  - **code:** BodyLanguageDetection repository [1]
  - **contributions:** 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp
  - **Simple LLM Summary:** This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Architecture-Led Hybrid Report on Body Language Detection Project] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[如何基于VLM架构构建可靠的应用系统/How to build reliable application systems based on VLM architecture]
        C --> C1[分析两种VLM架构并映射到视频处理流程/Analyze two VLM architectures and map to a video processing pipeline]
        C --> C2[系统采样视频帧，提示VLM生成结构化输出/System samples video frames, prompts VLM for structured output]
        C --> C3[使用预定义模式验证输出结构/Validate output structure with predefined schema]
        D --> D1[结构化输出可能语法正确但语义错误/Structured outputs can be syntactically valid but semantically incorrect]
        D --> D2[模式验证是结构性的，非几何正确性/Schema validation is structural, not geometric]
        D --> D3[理解架构对设计稳健接口和评估至关重要/Understanding architecture is critical for robust interface design and evaluation]
    ```

- **[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware**
  - **tags:** [mlsys], [llm inference], [quantization, mixture-of-experts, on-premise deployment, consumer-grade hardware, benchmark analysis]
  - **authors:** Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles
  - **institution:** UCLouvain, Universidad Espíritu Santo, DENEM Labs
  - **link:** https://arxiv.org/pdf/2512.23029
  - **contributions:** 1. A comprehensive benchmarking framework for evaluating both the intrinsic model capabilities and the server-side performance (latency, throughput, scalability) of a private LLM deployment. 2. A practical demonstration and performance analysis of deploying a quantized, large-scale (30B parameter) Mixture-of-Experts model (Qwen3) on next-generation consumer-grade hardware (NVIDIA RTX 5090). 3. Evidence that a carefully configured on-premises LLM server can achieve performance comparable to cloud services, offering SMBs a viable, cost-effective, and privacy-preserving alternative.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the feasibility of deploying a private, high-performance LLM server for Small and Medium Businesses using consumer-grade hardware. It benchmarks a quantized Qwen3-30B model on an NVIDIA RTX 5090, evaluating both model capability and server performance under load. The results show that such an on-premises setup can achieve performance close to cloud services at a lower cost and with full data privacy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Viability and Performance of a Private LLM Server for SMBs<br>SMB私有LLM服务器的可行性与性能] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Cloud reliance: cost, privacy, sovereignty for SMBs<br>云依赖：成本、隐私、SMB主权]
        C[主要方法/Method<br>Benchmark quantized Qwen3-30B on consumer hardware (RTX 5090)<br>在消费级硬件上对量化Qwen3-30B进行基准测试]
        D[关键结果/Results<br>On-premises performance rivals cloud, viable for SMBs<br>本地性能媲美云端，对SMB可行]
    ```

- **[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization**
  - **tags:** [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]
  - **authors:** Kerem Zaman, Shashank Srivastava
  - **institution:** UNC Chapel Hill
  - **link:** https://arxiv.org/pdf/2512.23032
  - **contributions:** 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp
  - **Simple LLM Summary:** This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Biasing Features 指标将不完整性误判为不忠实性<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]
        C --> C1[提出 faithful@k 指标并增加推理令牌预算<br/>Propose faithful@k metric & increase inference token budget]
        C --> C2[使用因果中介分析<br/>Use Causal Mediation Analysis]
        D --> D1[许多被标记为不忠实的 CoT 被其他指标判定为忠实<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]
        D --> D2[更大的令牌预算显著提高提示词显化率<br/>Larger token budgets greatly increase hint verbalization]
        D --> D3[未显化的提示词仍可通过 CoT 因果中介预测<br/>Non-verbalized hints can causally mediate predictions through CoT]
    ```

- **[arXiv251230] Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education**
  - **tags:** [ai], [educational data mining], [knowledge tracing, learner modelling, temporal coherence, fine-tuning, deep knowledge tracing]
  - **authors:** Danial Hooshyar, Yeongwook Yang, Gustav Šíř, Tommi Kärkkäinen, Raija Hämäläinen, Mutlu Cukurova, Roger Azevedo
  - **institution:** Tallinn University, University of Jyväskylä, Gangneung-Wonju National University, Czech Technical University, University College London, University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.23036
  - **contributions:** 1. Provides a synthesis of evidence on the limitations of LLM-based tutors, framing them within the high-risk context of K-12 education and responsible AI design. 2. Empirically demonstrates that a Deep Knowledge Tracing (DKT) model significantly outperforms a widely-used LLM (both zero-shot and fine-tuned) in next-step correctness prediction and temporal coherence of mastery estimation. 3. Highlights the computational inefficiency of fine-tuning LLMs for this task compared to DKT, and argues for hybrid frameworks over LLM-only approaches for responsible tutoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5735716766e72627a0d5d23b01771e8d0161795e3958d394eccf1045f5a797ec_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether large language models (LLMs) can effectively replace traditional learner modelling for adaptive tutoring in K-12 education. By comparing a Deep Knowledge Tracing (DKT) model against a fine-tuned and zero-shot LLM on knowledge assessment tasks, it finds DKT is more accurate, reliable, and temporally coherent. The study concludes that LLMs alone are insufficient for responsible tutoring and advocates for hybrid systems that incorporate dedicated learner models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Problems With LLMs for Learner Modelling<br/>LLM在学情建模中的问题"] --> B
        A --> C
        A --> D
        B["核心问题/Problem<br/>LLMs may replace learner models<br/>LLM可能替代学情模型"] --> B1["高风险领域/High-risk domain (K-12)"]
        B --> B2["需要评估准确性、可靠性、时序一致性/Need to assess accuracy, reliability, temporal coherence"]
        C["主要方法/Method<br/>Compare DKT vs. LLM<br/>对比DKT与LLM"] --> C1["数据集/Dataset: large open-access"]
        C --> C2["模型/Models: DKT, LLM (zero-shot & fine-tuned)"]
        D["关键结果/Results<br/>DKT outperforms LLM<br/>DKT优于LLM"] --> D1["更高AUC/Higher AUC (0.83)"]
        D --> D2["更好的时序一致性/Better temporal coherence"]
        D --> D3["结论: LLMs alone fall short, need hybrid frameworks<br/>Conclusion: LLM单独不足，需要混合框架"]
    ```

- **[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment**
  - **tags:** [nlp], [alignment & personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]
  - **authors:** Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales
  - **institution:** University of Edinburgh, Agency for Science, Technology and Research (A*STAR)
  - **link:** https://arxiv.org/pdf/2512.23067
  - **contributions:** 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Reward Model Selection Crisis in Personalized Alignment] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Standard RM accuracy fails to predict deployment performance for personalized alignment]
        C[主要方法/Method<br>Introduce policy accuracy metric and Pref-LaMP benchmark for direct evaluation]
        D[关键结果/Results<br>Weak correlation between RM & policy accuracy; ICL outperforms reward-guided decoding]
    ```

- **[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]
  - **authors:** Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang
  - **institution:** (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)
  - **link:** https://arxiv.org/pdf/2512.23075
  - **contributions:** 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Trust Region Masking for Long-Horizon LLM RL] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Off-policy mismatch in LLM-RL<br/>导致经典信任域边界失效]
        C --> C1[提出信任域掩码(TRM)<br/>Propose Trust Region Masking (TRM)]
        C1 --> C2[序列级掩码<br/>Sequence-level Masking]
        D --> D1[推导更紧的理论边界<br/>Derive Tighter Bounds (O(T), O(T^{3/2}))]
        D --> D2[提供非平凡的单调改进保证<br/>Provide Non-vacuous Guarantees]
    ```

- **[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition**
  - **tags:** [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]
  - **authors:** Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu
  - **institution:** Southeast University, Westlake University, Vrije Universiteit Amsterdam
  - **link:** https://arxiv.org/pdf/2512.23076
  - **code:** https://github.com/DY9910/MFMC
  - **contributions:** 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method's effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MFMC for Emotion Recognition] --> B[核心问题/Problem: 情感状态表现为跨系统的协调但异质的生理反应，现有自监督方法难以捕捉多模态高阶交互。]
        A --> C[主要方法/Method: 提出MFMC框架，通过Dual Total Correlation目标和Functional Maximum Correlation Analysis最大化高阶多模态依赖性。]
        A --> D[关键结果/Results: 在多个基准测试中达到SOTA或竞争性性能，显著提升CEAP-360VR数据集上的准确率，对主体间变异性鲁棒。]
    ```

- **[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning**
  - **tags:** [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]
  - **authors:** Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang
  - **institution:** (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)
  - **link:** https://arxiv.org/pdf/2512.23087
  - **contributions:** 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability "tail" tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability "tail" tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练-推理不匹配 / Training-Inference Mismatch]
        B1 --> B2[尾部token导致梯度不稳定 / Tail tokens destabilize gradient estimation]
        C --> C1[动态剪枝词汇表 / Dynamic Vocabulary Pruning]
        C1 --> C2[排除极端尾部token / Exclude extreme tail tokens]
        D --> D1[实现稳定训练 / Achieves stable training]
        D --> D2[理论界定优化偏差 / Theoretically bounds optimization bias]
    ```

- **[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification**
  - **tags:** [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]
  - **authors:** Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang
  - **institution:** Missouri State University
  - **link:** https://arxiv.org/pdf/2512.23089
  - **contributions:** 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedSAM-based lung masking for multi-label chest X-ray classification] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Automated CXR interpretation is challenging<br>自动CXR解读具有挑战性]
        C --> C1[Fine-tune MedSAM for lung segmentation<br>微调MedSAM进行肺部分割]
        C --> C2[Use masks to guide multi-label classification<br>使用掩码指导多标签分类]
        D --> D1[Masking effect is task/architecture dependent<br>掩码效果依赖于任务和架构]
        D --> D2[Trade-off: abnormality vs. normal screening<br>权衡：异常检测与正常筛查]
        D --> D3[Masking is a controllable spatial prior<br>掩码是一种可控的空间先验]
    ```

- **[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]
  - **authors:** Yingru Li, Ziniu Li, Jiacai Liu
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.23097
  - **contributions:** 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hybrid Online RL and IL for LLMs] --> B[核心问题/Problem: Train-inference distribution mismatch in LLM fine-tuning]
        A --> C[主要方法/Method: Unified framework combining Imitation Learning and Reinforcement Learning]
        A --> D[关键结果/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]
    ```

- **[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]
  - **authors:** Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa
  - **institution:** Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn
  - **link:** https://arxiv.org/pdf/2512.23090
  - **contributions:** 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp
  - **Simple LLM Summary:** The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model's ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Benchmark Success, Clinical Failure<br>基准成功，临床失败] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[RL优化提升基准性能但损害泛化<br>RL improves benchmarks but harms generalization]
        C --> C1[使用SFT+GRPO训练ChexReason VLM<br>Train ChexReason VLM with SFT+GRPO]
        D --> D1[GRPO提升CheXpert性能23%<br>GRPO improves CheXpert by 23%]
        D --> D2[GRPO导致NIH性能下降19%<br>GRPO degrades NIH by 19%]
        D --> D3[SFT检查点提升跨数据集泛化<br>SFT checkpoint improves cross-dataset generalization]
    ```

- **[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure**
  - **tags:** [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]
  - **authors:** Paul M. Thompson
  - **institution:** Stevens Institute of Neuroimaging and Informatics, University of Southern California
  - **link:** https://arxiv.org/pdf/2512.23109
  - **contributions:** 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp
  - **Simple LLM Summary:** This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现代生成和视觉语言模型在科学/医疗决策中需要准确且校准良好的概率预测 / Modern generative & VLMs need accurate, calibrated predictions for scientific/medical decisions]
        B --> B2[平均性能良好时，罕见情况或特定子群仍可能出现大误差 / Large errors can persist for rare conditions/subgroups despite low average loss]
        B --> B3[需要何种结构假设才能实现具有实用样本量的均匀泛化？ / What structural assumptions enable uniform generalization with practical sample sizes?]
        C --> C1[分析由提示或语义嵌入在受限表示空间中诱导出的分类器族 / Analyze induced families of classifiers from varying prompts/embeddings in a restricted space]
        C --> C2[假设模型输出对低维语义表示平滑依赖 / Assume model outputs depend smoothly on a low-dimensional semantic representation]
        C --> C3[应用经典均匀收敛工具 / Apply classical uniform convergence tools]
        D --> D1[在Lipschitz稳定性下，为VLM诱导分类器的准确性和校准功能提供有限样本均匀收敛界 / Provide finite-sample uniform convergence bounds for accuracy & calibration of VLM-induced classifiers under Lipschitz stability]
        D --> D2[样本复杂度取决于内在/有效维度，而非环境维度 / Sample complexity depends on intrinsic/effective dimension, not ambient dimension]
        D --> D3[谱相关边界阐明特征值衰减如何控制数据需求 / Spectrum-dependent bounds show how eigenvalue decay governs data requirements]
    ```

- **[arXiv251230] It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents**
  - **tags:** [sec], [prompt injection], [prompt injection, web agents, social-engineering, benchmark, autonomous agents]
  - **authors:** Karolina Korgul, Yushi Yang, Arkadiusz Drohomirecki, Piotr Błaszczyk, Will Howard, Lukas Aichberger, Chris Russell, Philip H.S. Torr, Adam Mahdi, Adel Bibi
  - **institution:** University of Oxford, SoftServe, Johannes Kepler University Linz
  - **link:** https://arxiv.org/pdf/2512.23128
  - **contributions:** 1. Introduces the Task-Redirecting Agent Persuasion Benchmark (TRAP) for evaluating prompt injection vulnerabilities in web-based LLM agents. 2. Provides a modular social-engineering injection framework for controlled experiments on high-fidelity website clones. 3. Demonstrates systemic vulnerabilities, showing agents are susceptible to injection in 25% of tasks on average, with small interface changes often doubling success rates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c246d5b23e99374a1d754ec870b203d23f214abac92f8d20d849cc98d00e86ca_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the vulnerability of web-based LLM agents to prompt injection attacks, where hidden adversarial instructions can divert agents from their tasks. It introduces the TRAP benchmark, built on realistic website clones, to evaluate these vulnerabilities. The study finds significant susceptibility across models, revealing systemic, psychologically driven weaknesses in current agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Web agents vulnerable to prompt injection attacks] --> Problem_Detail[问题详情/Problem Detail: Adversarial instructions in web content can divert agents from original tasks]
        Method[主要方法/Method: Introduce TRAP benchmark & modular injection framework] --> Method_Detail[方法详情/Method Detail: Evaluation on high-fidelity website clones using social-engineering techniques]
        Results[关键结果/Results: Agents susceptible in 25% of tasks on average] --> Results_Detail[结果详情/Results Detail: Small interface changes can double success rates, revealing systemic vulnerabilities]
    ```

- **[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]
  - **authors:** Yu Li, Tian Lan, Zhengling Qi
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.23126
  - **contributions:** 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO's superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DPO Limitations<br/>DPO的局限性]
        B1 --> B2[Lacks Invariance<br/>缺乏不变性]
        B1 --> B3[Suboptimal Use of Data<br/>数据利用次优]
        C --> C1[Propose InSPO<br/>提出InSPO]
        C1 --> C2[Globally Optimal Policy<br/>全局最优策略]
        C2 --> C3[Conditions on Context & Alternative<br/>基于上下文与备选答案]
        D --> D1[Theoretical Superiority<br/>理论优越性]
        D --> D2[Practical Improvement<br/>实际提升]
        D2 --> D3[Better Win Rates<br/>更高的胜率]
        D2 --> D4[No Inference Overhead<br/>无推理开销]
    ```

- **[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion**
  - **tags:** [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]
  - **authors:** Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu
  - **institution:** Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.23130
  - **contributions:** 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: 成像-病理MRI合成"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>生成模型中的特征纠缠导致解剖结构损坏"] --> P1["现有范式/Existing Paradigms<br>Global pixel domain or binary masks<br>全局像素域或二进制掩码"]
        Method["主要方法/Method<br>Disentangled Deviation Diffusion<br>解耦偏差扩散"] --> M1["分解任务/Decompose Task<br>1. Deterministic anatomical reconstruction<br>确定性解剖重建<br>2. Stochastic deviation modeling<br>随机偏差建模"]
        Method --> M2["核心模型/Core Model<br>Deviation-Space Diffusion Model<br>偏差空间扩散模型<br>Learns pathological residuals<br>学习病理残差"]
        Method --> M3["融合与稳定/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>接缝感知融合与推理时稳定模块"]
        Results["关键结果/Results<br>Outperforms baselines<br>超越基线模型"] --> R1["评估/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>肿瘤基准上的定量与定性评估"]
        Results --> R2["优势/Advantages<br>Higher perceptual realism & anatomical fidelity<br>更高的感知真实性与解剖保真度"]
    ```

- **[arXiv251230] Reservoir Computing inspired Matrix Multiplication-free Language Model**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [MatMul-free LM, reservoir computing, weight sharing, ternary quantization, MLGRU]
  - **authors:** Takumi Shiratsuchi, Yuichiro Tanaka, Hakaru Tamukoh
  - **institution:** Kyushu Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23145
  - **contributions:** 1. Proposes a novel language model architecture that integrates reservoir computing principles into a MatMul-free LM to reduce training costs. 2. Introduces techniques of partially fixing/sharing weights and inserting reservoir layers to obtain dynamic representations without extra training overhead. 3. Combines operations to reduce memory accesses, achieving reductions in parameters, training time, and inference time while maintaining performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of large language models by proposing a matrix multiplication-free model enhanced with reservoir computing. The method fixes/shared weights in selected layers and inserts reservoir layers to reduce training overhead and memory accesses. Experiments show the approach reduces parameters by up to 19%, training time by 9.9%, and inference time by 8.0% while maintaining comparable performance to the baseline.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reservoir Computing inspired Matrix Multiplication-free Language Model] --> B[核心问题/Problem: LLMs计算成本高/High computational cost of LLMs]
        A --> C[主要方法/Method: 结合储层计算与无矩阵乘法模型/Combine RC with MatMul-free LM, 固定共享权重/Fix & share weights, 减少内存访问/Reduce memory access]
        A --> D[关键结果/Results: 参数减少19%/Params reduced by 19%, 训练时间减少9.9%/Training time reduced by 9.9%, 推理时间减少8.0%/Inference time reduced by 8.0%, 性能相当/Performance maintained]
    ```

- **[arXiv251230] Why We Need a New Framework for Emotional Intelligence in AI**
  - **tags:** [ai], [affective computing], [emotional intelligence, benchmark evaluation, affective AI, emotion theory, AI assessment]
  - **authors:** Max Parks, Kheli Atluru, Meera Vinod, Mike Kuniavsky, Jud Brewer, Sean White, Sarah Adler, Wendy Ju
  - **institution:** Inflection AI
  - **link:** https://arxiv.org/pdf/2512.23163
  - **contributions:** 1. A critical review of existing emotional intelligence (EI) theories and their applicability to artificial systems. 2. An analysis of current benchmark frameworks for evaluating EI in AI, identifying their foundational shortcomings. 3. A proposal for new evaluation strategies to better measure relevant aspects of EI in AI systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afa72b09c5f0d3f095e827a81d95825630a6951253349696736df1091d38dd71_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current frameworks for assessing emotional intelligence (EI) in AI are inadequate because they lack a solid theoretical foundation on emotion and fail to distinguish between human-specific and AI-relevant EI components. The authors propose a new framework by first reviewing emotion theories to define EI applicable to AI, then critiquing existing benchmarks, and finally outlining improved evaluation strategies. The main conclusion is that a refined, theoretically-grounded framework is needed to properly evaluate EI capabilities in artificial systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Why We Need a New Framework for Emotional Intelligence in AI"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["现有EI评估框架不充分/Current EI evaluation frameworks are inadequate"]
        Problem --> P2["缺乏坚实的理论基础/Lack a solid theoretical foundation on emotion"]
        Method --> M1["回顾情绪与EI理论/Review emotion and EI theories"]
        Method --> M2["批判性评估现有基准/Critically evaluate existing benchmarks"]
        Method --> M3["提出改进策略/Outline improved evaluation strategies"]
        Results --> R1["需要新的评估框架/A new evaluation framework is needed"]
        Results --> R2["区分AI相关与无关的EI/Distinguish AI-relevant vs. irrelevant EI aspects"]
    ```

- **[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search**
  - **tags:** [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]
  - **authors:** Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue
  - **institution:** IBM T.J. Watson Research Center, Vanderbilt University
  - **link:** https://arxiv.org/pdf/2512.23167
  - **code:** https://github.com/IBM/SPIRAL
  - **contributions:** 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search] --> B[核心问题/Problem: LLMs falter at complex planning, linear reasoning lacks self-correction]
        A --> C[主要方法/Method: Integrates three LLM agents (Planner, Simulator, Critic) into MCTS loop]
        A --> D[关键结果/Results: Outperforms SOTA agents, achieves 83.6% accuracy on DailyLifeAPIs, superior token efficiency]
    ```

- **[arXiv251230] EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion**
  - **tags:** [sec], [adversarial attacks], [jailbreak attacks, large language models, adversarial prompting, equation solving, code completion]
  - **authors:** Zhen Liang, Hai Huang, Zhengkui Chen
  - **institution:** Zhejiang Sci-Tech University
  - **link:** https://arxiv.org/pdf/2512.23173
  - **code:** https://github.com/lzzzr123/Equacode
  - **contributions:** 1. Proposes a novel multi-strategy jailbreak approach that combines mathematical equation solving and code completion to bypass LLM safety constraints. 2. Demonstrates high attack success rates (e.g., 91.19% on GPT series) with only a single query, outperforming single-strategy attacks. 3. Shows through ablation studies a strong synergistic effect between the equation and code modules, proving the multi-strategy approach is more effective than the sum of its parts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3023ba644e0cdeddfd98604ca7c5871aceb213706aede21b77e0d35b95cf6d23_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces EquaCode, a multi-strategy jailbreak attack that transforms malicious intent into a mathematical problem and forces the LLM to solve it via code, diverting its focus from safety. The method achieves high success rates on various LLMs with a single query, and ablation studies confirm the synergistic benefit of combining equation-solving and code completion strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EquaCode: 多策略越狱方法 / Multi-Strategy Jailbreak Approach] --> B[核心问题: LLM安全性评估不足 / Problem: Insufficient LLM Safety Evaluation]
        A --> C[主要方法: 方程求解与代码补全 / Method: Equation Solving & Code Completion]
        A --> D[关键结果: 高成功率与协同效应 / Results: High Success Rate & Synergistic Effect]
    ```

- **[arXiv251230] From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research**
  - **tags:** [ai], [generative ai evaluation], [model belief, token-level probabilities, statistical efficiency, demand estimation, synthetic data]
  - **authors:** Hongshen Sun, Juanjuan Zhang
  - **institution:** MIT Sloan School of Management
  - **link:** https://arxiv.org/pdf/2512.23184
  - **contributions:** 1. Introduces and formalizes the concept of "model belief," a novel measure derived from an LLM's token-level probabilities to capture its belief distribution over choices in a single generation. 2. Proves that model belief is asymptotically equivalent to the mean of model choices but is a more statistically efficient estimator with lower variance and faster convergence, with analogous properties for smooth functions used in downstream applications. 3. Empirically demonstrates that model belief outperforms model choice in explaining and predicting ground-truth choices in practical, limited-run settings (e.g., demand estimation), reducing required computation by roughly a factor of 20.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/997c7eb2e35882ae4411dc7956b1f70a51835fa22d25bcd7ec8b5d6d1cb72413_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of using single LLM outputs ("model choice") by proposing "model belief," a measure based on token-level probabilities that captures the model's full belief distribution. The authors prove model belief is a more statistically efficient estimator than model choice and demonstrate its practical superiority in a demand estimation task, where it reduces the computation needed for accurate estimates by about 20 times.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Model Choice to Model Belief] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM数据使用效率低 / Inefficient use of LLM-generated data]
        B --> B2[模型选择信息利用不足 / Underutilizes probabilistic information in model choice]
        C --> C1[提出模型信念 / Propose model belief]
        C --> C2[基于Token级概率 / Based on token-level probabilities]
        C --> C3[捕获信念分布 / Captures belief distribution]
        D --> D1[统计效率更高 / More statistically efficient estimator]
        D --> D2[计算需求减少20倍 / Reduces computation by ~20x]
        D --> D3[预测性能更优 / Better explains/predicts ground truth]
    ```

- **[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis**
  - **tags:** [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]
  - **authors:** Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik
  - **institution:** BRAC University, Charles Sturt University, University of Technology Sydney
  - **link:** https://arxiv.org/pdf/2512.23196
  - **contributions:** 1. Proposes "ForCM", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ForCM: Forest Cover Mapping] --> B[核心问题/Problem: Accurate forest cover mapping for environmental monitoring]
        A --> C[主要方法/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]
        A --> D[关键结果/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]
    ```

- **[arXiv251230] Not too long do read: Evaluating LLM-generated extreme scientific summaries**
  - **tags:** [nlp], [text summarization], [extreme summarization, TLDR, abstractive summarization, extractive summarization, dataset creation]
  - **authors:** Zhuoqi Lyu, Qing Ke
  - **institution:** City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23206
  - **code:** https://github.com/netknowledge/LLM_summarization
  - **contributions:** 1. Introduces BiomedTLDR, a novel high-quality dataset of researcher-authored scientific TLDRs, curated from author annotations in bibliographies. 2. Evaluates the performance of popular open-weight LLMs in generating scientific TLDRs from paper abstracts. 3. Provides an analysis revealing that LLM-generated summaries tend to be more extractive (closer to the source text's lexicon and structure) compared to more abstractive human-written summaries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of high-quality datasets for evaluating LLMs in generating scientific extreme summaries (TLDRs) by introducing BiomedTLDR, a dataset of human-authored summaries. It then evaluates open-weight LLMs on this task and finds that, while some can produce human-like summaries, LLMs generally tend to be more extractive and less abstractive than human experts.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Not too long do read: Evaluating LLM-generated extreme scientific summaries<br>论文标题"]
        A --> B["核心问题/Problem<br>Lack of high-quality scientific TLDR dataset hinders LLM evaluation"]
        A --> C["主要方法/Method<br>Propose BiomedTLDR dataset & test LLMs on TLDR generation"]
        A --> D["关键结果/Results<br>LLMs are more extractive; humans are more abstractive"]
    ```

- **[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection**
  - **tags:** [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]
  - **authors:** Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1
  - **link:** https://arxiv.org/pdf/2512.23208
  - **contributions:** 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏军事目标数据集/Lack of military target datasets]
        B --> B2[SAR数据成本高/High cost of SAR data]
        B --> B3[跨域适应挑战/Cross-domain adaptation challenge]
        C --> C1[使用Unreal Engine生成合成RGB数据/Generate synthetic RGB data using Unreal Engine]
        C --> C2[合成到真实域适应实验/Synthetic-to-real domain adaptation experiments]
        C --> C3[基准测试SOTA方法/Benchmark SOTA DA methods]
        D --> D1[最小监督方法表现最佳/Minimal supervision methods perform best]
        D --> D2[识别当前挑战/Identify current challenges]
    ```

- **[arXiv251230] Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process**
  - **tags:** [mlsys], [llm inference], [LLM Ensemble, LLM-as-a-Judge, Peer-Review, Unsupervised Selection, Truth Inference]
  - **authors:** Zhijun Chen, Zeyu Ji, Qianren Mao, Junhang Cheng, Bangjie Qin, Hao Wu, Zhuoran Li, Jingzheng Li, Kai Sun, Zizhe Wang, Yikun Ban, Zhu Sun, Xiangyang Ji, Hailong Sun
  - **institution:** Beihang University, Zhongguancun Laboratory, Xi'an Jiaotong University, Hong Kong University of Science and Technology, Tsinghua University, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.23213
  - **contributions:** 1. Proposes LLM-PeerReview, a novel, peer-review-inspired, and interpretable framework for unsupervised LLM ensemble selection. 2. Introduces a three-stage process (scoring via LLM-as-a-Judge, reasoning via aggregation, and selection) that leverages multiple LLMs to evaluate each other's responses. 3. Demonstrates strong empirical performance, with two variants significantly outperforming a recent advanced baseline (Smoothie-Global) on multiple datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LLM-PeerReview, an unsupervised ensemble method that selects the best response from multiple LLM candidates. The method uses a peer-review process where LLMs score each other's outputs, then aggregates these scores to make a final selection. The approach is shown to be simple and powerful, outperforming a strong baseline by a significant margin across several datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-PeerReview: Ensembling LLMs via Peer-Review] --> B[核心问题/Problem: Single LLM limitations & diverse model strengths]
        A --> C[主要方法/Method: Unsupervised 3-stage peer-review framework]
        C --> C1[评分/Scoring: LLM-as-a-Judge]
        C --> C2[推理/Reasoning: Score aggregation (graphical model or averaging)]
        C --> C3[选择/Selection: Pick highest-scoring response]
        A --> D[关键结果/Results: Outperforms Smoothie-Global by ~7% points]
    ```

- **[arXiv251230] TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI**
  - **tags:** [ai], [ai evaluation], [thermal comfort, cognitive turing test, cross-modal reasoning, causal association, adaptive decision-making]
  - **authors:** Jingming Li
  - **institution:** School of Civil Engineering and Architecture, Nanyang Normal University
  - **link:** https://arxiv.org/pdf/2512.23217
  - **contributions:** 1. Proposes TCEval, the first evaluation framework that uses thermal comfort scenarios to assess AI's core cognitive capacities (cross-modal reasoning, causal association, adaptive decision-making). 2. Introduces a methodology using LLM agents with virtual personalities to generate and validate clothing and comfort feedback against real human databases (ASHRAE, Chinese Thermal Comfort Database). 3. Demonstrates the framework's ecological validity as a Cognitive Turing Test, revealing that current LLMs have foundational cross-modal reasoning but lack precise causal understanding of nonlinear relationships in thermal comfort.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0080eb693040bd7b2c752a63de09a7937c8abe23c0a11752ee6d9fe7cdd04c7f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TCEval, a novel evaluation framework that uses thermal comfort scenarios and LLM agents to assess AI's cognitive abilities. The method involves simulating agent decisions and comparing them to human data from established comfort databases. The results show that while LLMs exhibit basic cross-modal reasoning, they lack a precise causal understanding of the complex factors in thermal comfort, validating TCEval as an ecologically valid cognitive test.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("LLM任务特定基准存在关键差距<br/>Critical gap in LLM task-specific benchmarks")
        C --> C1("利用热舒适场景和LLM智能体<br/>Leverage thermal comfort scenarios & LLM agents")
        C --> C2("评估三种核心认知能力<br/>Assess three core cognitive capacities")
        C2 --> C2a("跨模态推理<br/>Cross-modal reasoning")
        C2 --> C2b("因果关联<br/>Causal association")
        C2 --> C2c("自适应决策<br/>Adaptive decision-making")
        D --> D1("智能体反馈与人类有限对齐<br/>Agent feedback has limited exact alignment with humans")
        D --> D2("具备基础跨模态推理能力<br/>Possess foundational cross-modal reasoning ability")
        D --> D3("缺乏对非线性关系的精确因果理解<br/>Lack precise causal understanding of nonlinear relationships")
    ```

- **[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information**
  - **tags:** [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]
  - **authors:** Youngchae Kwon, Jinyoung Choi, Injung Kim
  - **institution:** Handong Global University
  - **link:** https://arxiv.org/pdf/2512.23221
  - **contributions:** 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: 整体时尚物品检测] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>时尚物品检测因外观多样和子类别相似而具有挑战性。]
        C[主要方法/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>提出Holi-DETR，利用共现、空间布局和身体关键点三种上下文线索的整体检测器。]
        D[关键结果/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>性能超越原始DETR (+3.6pp AP) 和 Co-DETR (+1.1pp AP)。]
    ```

- **[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images**
  - **tags:** [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]
  - **authors:** Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi
  - **institution:** Sungkyunkwan University, The University of Texas MD Anderson Cancer Center
  - **link:** https://arxiv.org/pdf/2512.23227
  - **contributions:** 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Anomaly Detection by Effectively Leveraging Synthetic Images] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 真实缺陷图像稀缺，现有合成方法在成本与质量间难以权衡/Scarcity of real defect images, trade-off between cost and quality in synthesis]
        C[主要方法/Method: 1. 使用预训练图像翻译与检索模型高效生成缺陷图/Use pre-trained image-to-image & retrieval models for generation. 2. 两阶段训练策略：先预训练再微调/Two-stage training: pre-train then fine-tune]
        D[关键结果/Results: 在MVTec AD数据集上验证有效，降低成本并提升性能/Validated on MVTec AD, reduces cost and improves performance]
    ```

- **[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network**
  - **tags:** [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]
  - **authors:** Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao
  - **institution:** (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)
  - **link:** https://arxiv.org/pdf/2512.23234
  - **contributions:** 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --> B(核心问题/Problem: 红外气体泄漏检测困难/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)
        A --> C(主要方法/Method: 提出PEG-DRNet/Propose PEG-DRNet)
        C --> C1(气体块建模气体传输/Gas Block models gas transport)
        C --> C2(自适应梯度相位边缘算子/Adaptive Gradient and Phase Edge Operator (AGPEO))
        C --> C3(内容自适应稀疏路由聚合网络/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))
        A --> D(关键结果/Results: 在IIG和LangGas数据集上性能优越/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)
    ```

- **[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta**
  - **tags:** [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]
  - **authors:** Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu
  - **institution:** Meta Platforms
  - **link:** https://arxiv.org/pdf/2512.23236
  - **contributions:** 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp
  - **Simple LLM Summary:** This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]
        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]
        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]
        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]
        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]
        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]
        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]
        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]
    ```

- **[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing**
  - **tags:** [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]
  - **authors:** Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang
  - **institution:** Fudan University, Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.23244
  - **contributions:** 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ViLaCD-R1: 遥感语义变化检测的视觉语言框架] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[传统方法语义理解不足/Traditional methods lack semantic understanding]
        B1 --> C2[现有VLM方法定位不准确/Existing VLM methods have inaccurate localization]
        B2 --> D1[两阶段框架/Two-stage framework]
        D1 --> E1[多图像推理器/Multi-Image Reasoner]
        E1 --> F1[SFT与RL训练/SFT and RL training]
        E1 --> F2[生成粗变化掩码/Generate coarse change mask]
        D1 --> E2[掩码引导解码器/Mask-Guided Decoder]
        E2 --> F3[融合特征与掩码/Fuse features and mask]
        E2 --> F4[预测精细变化图/Predict precise change map]
        B3 --> G1[提升语义变化识别/Improves semantic change recognition]
        B3 --> G2[抑制非语义变化/Suppresses non-semantic variations]
        B3 --> G3[达到SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]
  - **authors:** Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.23260
  - **contributions:** 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LoRA缺乏可解释性/LoRA lacks interpretability")
        Problem --> P2("子空间学习是黑盒的/Subspace learning is black-box")
        Method --> M1("利用预训练SAE/Use pre-trained SAEs")
        Method --> M2("构建显式低秩子空间/Construct explicit low-rank subspace")
        Results --> R1("高安全率99.6%/High safety rate 99.6%")
        Results --> R2("参数高效0.19%/Parameter-efficient 0.19%")
        Results --> R3("提供可解释性/Provides interpretability")
    ```

- **[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control**
  - **tags:** [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]
  - **authors:** Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam
  - **institution:** Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi
  - **link:** https://arxiv.org/pdf/2512.23292
  - **contributions:** 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model's behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Agentic Physical AI for Nuclear Reactor Control") --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br>General-purpose models fail at physical control<br>通用模型在物理控制中失败"]
        Method["主要方法/Method<br>Agentic Physical AI with physics-based validation<br>基于物理验证的智能体物理AI"]
        Results["关键结果/Results<br>Variance collapse & emergent policy distillation<br>方差崩溃与策略蒸馏涌现"]
    
        Problem --> P1["Input unfaithfulness / 输入不忠实"]
        Problem --> P2["Semantic vs. physical correctness / 语义与物理正确性冲突"]
    
        Method --> M1["Compact LM (360M params) / 紧凑语言模型"]
        Method --> M2["Physics-driven optimization / 物理驱动优化"]
        Method --> M3["Synthetic data scaling (10^3 to 10^5) / 合成数据缩放"]
    
        Results --> R1["Phase transition & >500x variance collapse / 相变与方差崩溃"]
        Results --> R2["Autonomous policy distillation / 自主策略蒸馏"]
        Results --> R3["Transferable representations / 可迁移表征"]
    ```

- **[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images**
  - **tags:** [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]
  - **authors:** Md. Sazzadul Islam Prottasha, Nabil Walid Rafi
  - **institution:** Bangladesh University of Professionals
  - **link:** https://arxiv.org/pdf/2512.23304
  - **contributions:** 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp
  - **Simple LLM Summary:** This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedGemma vs GPT-4: 医学图像零样本疾病分类] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 比较开源与闭源多模态LLM在医学图像诊断中的性能]
        C[主要方法/Method: 使用LoRA微调的MedGemma与未调优的GPT-4进行零样本分类对比]
        D[关键结果/Results: MedGemma准确率(80.37%)和敏感性更高，领域微调对减少幻觉至关重要]
    ```

- **[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL**
  - **tags:** [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]
  - **authors:** Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer
  - **institution:** University of Innsbruck, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.23310
  - **contributions:** 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --> B[核心问题/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]
        A --> C[主要方法/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]
        A --> D[关键结果/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]
    ```

- **[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants**
  - **tags:** [ai], [explainable ai (xai)], [inverse kinematics, shapley additive explanations (SHAP), InterpretML, obstacle avoidance, neural network]
  - **authors:** Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin
  - **institution:** Yuan Ze University
  - **link:** https://arxiv.org/pdf/2512.23312
  - **contributions:** 1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp
  - **Simple LLM Summary:** This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation<br>可解释神经逆运动学用于障碍物感知机器人操作"] --> B
        A --> C
        A --> D
        B["核心问题/Problem<br>Opaque neural IK models lack transparency and safety for responsible AI.<br>黑盒神经IK模型缺乏透明度与安全性"] --> B1["挑战/Challenges<br>Debugging failures, safety certification"]
        C["主要方法/Method<br>XAI workflow integrating SHAP and physics simulation.<br>集成SHAP与物理仿真的XAI工作流"] --> C1["模型/Variants<br>Improved IKNet, Focused IKNet"]
        C --> C2["工具/Tools<br>SHAP, InterpretML, Simulator"]
        D["关键结果/Results<br>Balanced feature attribution correlates with wider safety margins.<br>均衡的特征归因与更宽的安全裕度相关"] --> D1["结论/Conclusion<br>XAI guides architectural refinement for trustworthy IK.<br>XAI指导可信IK的架构改进"]
    ```

- **[arXiv251230] On Conformant Planning and Model-Checking of $^*^*$ Hyperproperties**
  - **tags:** [other], [formal methods], [conformant planning, hyperproperties, model-checking, HyperLTL, ∃∗∀∗]
  - **authors:** Raven Beutner, Bernd Finkbeiner
  - **institution:** CISPA Helmholtz Center for Information Security
  - **link:** https://arxiv.org/pdf/2512.23324
  - **contributions:** 1. Establishes a formal connection between conformant planning and model-checking of ∃∗∀∗ hyperproperties, showing they share the same computational core. 2. Provides an efficient, sound, and complete reduction from a hyperproperty model-checking instance to a conformant planning instance. 3. Demonstrates that every conformant planning problem is itself a hyperproperty model-checking task, establishing the converse direction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47343ca7bc4bf16389257261dd21e0c1fa42c0f512178576ff4ba501df841e8b_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies and formalizes a deep connection between two seemingly unrelated problems: conformant planning (finding a robust sequential plan under uncertainty) and model-checking of ∃∗∀∗ hyperproperties (verifying system properties that relate multiple execution traces). The authors provide efficient, sound, and complete translations between instances of these two problems, showing they are essentially two sides of the same computational coin. This foundational link aims to enable cross-pollination of solution techniques between the planning and verification communities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[On Conformant Planning and Model-Checking of ∃∗∀∗ Hyperproperties] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[连接两个看似无关的问题 / Linking two seemingly unrelated problems]
        B1 --> B2[Conformant Planning / 一致性规划]
        B1 --> B3[Hyperproperty Model-Checking / 超属性模型检测]
        C --> C1[构建双向高效规约 / Constructing bidirectional efficient reductions]
        D --> D1[证明规约的可靠性与完备性 / Proving reductions are sound and complete]
        D --> D2[确立问题的等价性 / Establishing the equivalence of the problems]
    ```

- **[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations**
  - **tags:** [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]
  - **authors:** Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang
  - **institution:** Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23328
  - **contributions:** 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM智能体缺乏物理世界部署所需的稳健空间心智模型/LLM agents lack robust spatial mental models for physical-world deployment]
        C --> C1[提出基于魔方的三层诊断基准/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]
        C --> C2[从完整符号状态到部分视觉状态逐步评估/Progressive evaluation from full symbolic to partial visual state]
        D --> D1[领先LLM在长视野任务上通过率为0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]
        D --> D2[揭示了长期规划和主动探索的根本性失败/Exposes fundamental failure in long-term planning and active exploration]
    ```

- **[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models**
  - **tags:** [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]
  - **authors:** Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.23340
  - **contributions:** 1. Proposes the "Law of Multi-model Collaboration," a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the "Law of Multi-model Collaboration," a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Law of Multi-Model Collaboration<br>多模型协作定律"] --> Problem["核心问题/Problem<br>Lack of scaling theory for multi-model collaboration<br>缺乏多模型协作的扩展理论"]
        Root --> Method["主要方法/Method<br>Propose Law of Multi-model Collaboration<br>提出多模型协作定律"]
        Root --> Results["关键结果/Results<br>Ensembles scale better than single models<br>集成模型比单一模型扩展性更好"]
    ```

- **[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents**
  - **tags:** [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]
  - **authors:** Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin
  - **institution:** Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.23343
  - **code:** https://github.com/AgentMemory/Huaman-Agent-Memory
  - **contributions:** 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp
  - **Simple LLM Summary:** This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Meets Brain: Memory Systems / AI与大脑：记忆系统] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[Interdisciplinary Gap / 跨学科鸿沟]
        P1 --> P2[Existing works struggle to assimilate human memory essence / 现有工作难以吸收人类记忆机制精髓]
    
        Method[主要方法/Method] --> M1[Systematic Synthesis / 系统综述]
        M1 --> M2[Comparative Analysis / 对比分析]
        M2 --> M3[Review Benchmarks & Security / 回顾基准与安全]
    
        Results[关键结果/Results] --> R1[Unified Memory Framework / 统一的记忆框架]
        R1 --> R2[Future Directions / 未来方向]
        R2 --> R3[Multimodal Memory & Skill Acquisition / 多模态记忆与技能获取]
    ```

- **[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling**
  - **tags:** [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]
  - **authors:** Hai Duong Nguyen, Xuan-The Tran
  - **institution:** HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University
  - **link:** https://arxiv.org/pdf/2512.23347
  - **contributions:** 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ECG-RAMBA: Zero-Shot ECG Generalization] --> B[核心问题/Problem: Poor cross-dataset generalization in ECG classification]
        A --> C[主要方法/Method: Morphology-Rhythm Disentanglement & Long-Range Mamba Modeling]
        A --> D[关键结果/Results: Strong zero-shot AUC on CPSC-2021 & PTB-XL]
    ```

- **[arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis**
  - **tags:** [nlp], [text-to-sql], [Reinforcement Learning, Data Synthesis, Policy Optimization, Semantic-Logic Alignment, Group Relative Policy Optimization]
  - **authors:** Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai
  - **institution:** Sichuan University, IQuest Research, Beihang University
  - **link:** https://arxiv.org/pdf/2512.23366
  - **contributions:** 1. Proposes an iterative data factory for synthesizing high-quality, RL-ready Text-to-SQL data with strict semantic-logic verification. 2. Introduces a novel Agentic Reinforcement Learning framework featuring a Diversity-Aware Cold Start stage and Group Relative Policy Optimization (GRPO). 3. Demonstrates state-of-the-art performance on the BIRD and Spider benchmarks through the synergistic combination of data-centric and model-centric approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6740c1fc529b82b509bd38c2a7b5fb405b969bc5c3e11e6e0b7690e7fa791c85_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of data scarcity and limited reasoning in Text-to-SQL systems. It proposes a holistic framework that combines a data-centric approach for synthesizing high-fidelity training data with a model-centric approach using a novel Agentic Reinforcement Learning method called Group Relative Policy Optimization. The method achieves state-of-the-art results on major benchmarks, showing the effectiveness of the synergistic approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AGRO-SQL] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据稀缺与质量/Data Scarcity & Quality]
        B --> B2[模型推理限制/Model Reasoning Limitations]
        C --> C1[数据中心方法/Data-Centric Approach]
        C --> C2[模型中心方法/Model-Centric Approach]
        C1 --> C1a[迭代数据工厂/Iterative Data Factory]
        C1 --> C1b[语义逻辑对齐/Semantic-Logic Alignment]
        C2 --> C2a[多样性感知冷启动/Diversity-Aware Cold Start]
        C2 --> C2b[组相对策略优化/Group Relative Policy Optimization]
        D --> D1[在BIRD和Spider上SOTA/SOTA on BIRD & Spider]
    ```

- **[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]
  - **authors:** Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang
  - **institution:** Tianjin University
  - **link:** https://arxiv.org/pdf/2512.23367
  - **contributions:** 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CoT推理带来高内存与延迟 / CoT reasoning causes high memory & latency]
        B --> B2[Ascend NPU部署挑战 / Deployment challenge on Ascend NPU]
        C --> C1[低比特量化 / Low-bit Quantization]
        C --> C2[统一推理框架 / Unified Inference Framework]
        C --> C3[支持W8A8与W4A8 / Supports W8A8 & W4A8]
        D --> D1[INT8保持>90%精度 / INT8 preserves >90% accuracy]
        D --> D2[1.5倍预填充加速 / 1.5x prefill speedup]
        D --> D3[W4A8显著减少内存 / W4A8 greatly reduces memory]
    ```

- **[arXiv251230] SoulX-LiveTalk Technical Report**
  - **tags:** [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]
  - **authors:** Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu
  - **institution:** Soul AI Lab, Donghua University
  - **link:** https://arxiv.org/pdf/2512.23379
  - **code:** https://soul-ailab.github.io/soulx-livetalk/
  - **contributions:** 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoulX-LiveTalk] --> B[核心问题/Problem: 实时无限时长音频驱动化身生成中计算负载与低延迟的冲突]
        A --> C[主要方法/Method: 自校正双向蒸馏与多步回顾自校正机制]
        A --> D[关键结果/Results: 0.87秒启动延迟，32 FPS实时吞吐]
    ```

- **[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers**
  - **tags:** [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]
  - **authors:** Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee
  - **institution:** Urmia University of Technology
  - **link:** https://arxiv.org/pdf/2512.23380
  - **code:** https://github.com/your-repo/CoLog (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)
  - **contributions:** 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --> Problem["核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]
        Root --> Method["主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]
        Root --> Results["关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]
    ```

- **[arXiv251230] Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?**
  - **tags:** [sec], [AI Security], [AI supply chain, security taxonomy, distilBERT classifier]
  - **authors:** Anh Nguyen, Triet Huynh Minh Le, M. Ali Babar
  - **institution:** University of Adelaide
  - **link:** https://arxiv.org/pdf/2512.23385
  - **contributions:** 1. Developed a pipeline combining keyword matching with a fine-tuned distilBERT classifier to identify 312,868 security discussions from Hugging Face and GitHub. 2. Conducted a thematic analysis to create a fine-grained taxonomy of 32 security issues and 24 solutions across four themes (System/Software, External Tools/Ecosystem, Model, Data). 3. Provided empirical insights revealing that security issues stem from complex dependencies and black-box AI components, with Model and Data challenges often lacking concrete solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c00453e76598d08a965d2a15fe6e7b197cf1f19518d88f46a334b638da6327dc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates security issues in the AI supply chain by analyzing developer discussions from Hugging Face and GitHub. The authors use a keyword and classifier pipeline to build a large dataset and perform a thematic analysis to create a taxonomy of issues and solutions. They conclude that many security problems arise from dependencies and the black-box nature of AI, with solutions for Model and Data issues being particularly scarce.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Securing the AI Supply Chain] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[AI供应链安全格局复杂/Complex AI supply chain security landscape]
        Problem --> P2[缺乏对常见问题与解决方案的了解/Lack of knowledge on common issues & solutions]
        Method[主要方法/Method] --> M1[实证调查/Empirical investigation]
        M1 --> M1_1[数据源: Hugging Face, GitHub/Data Sources: Hugging Face, GitHub]
        M1 --> M1_2[构建分类管道/Build classification pipeline]
        M1_2 --> M1_2_1[关键词匹配+微调distilBERT/Keyword matching + fine-tuned distilBERT]
        Results[关键结果/Results] --> R1[数据集: 312,868个安全讨论/Dataset: 312,868 security discussions]
        Results --> R2[分类法: 32个问题, 24个解决方案/Taxonomy: 32 issues, 24 solutions]
        Results --> R3[洞察: 依赖复杂性和黑盒性导致问题/Insight: Issues from dependencies & black-box nature]
    ```

- **[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models**
  - **tags:** [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]
  - **authors:** Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.23407
  - **contributions:** 1. Theoretically and empirically extends the neural scaling law to the "familial models" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the "train once, deploy many" paradigm without compromising compute-optimality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Theoretical Foundations of Scaling Law in Familial Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统缩放定律忽略多模型范式/Traditional scaling laws overlook the multi-model paradigm]
        C --> C1[引入粒度作为新变量/Introduce Granularity (G) as a new variable]
        C --> C2[统一函数形式 L(N, D, G)/Unified functional form L(N, D, G)]
        C --> C3[采用IsoFLOP实验设计/Employ rigorous IsoFLOP experimental design]
        D --> D1[粒度惩罚遵循幂律/Granularity penalty follows a power law]
        D --> D2[指数极小 (γ≈0.041)/Exponent is extremely small]
        D --> D3[验证"一次训练，多次部署"/Validates "train once, deploy many"]
    ```

- **[arXiv251230] MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning**
  - **tags:** [mlsys], [agent system], [tool-integrated reasoning, multimodal chain-of-thought, interleaved thinking]
  - **authors:** Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang, Pengfei Yu, Xudong Rao, Ning Mao, Xiaobo Liu, Lian Wen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Shanshan Li, Zide Liu, Jing Luo, Lifu Mu, Xuhao Pan, Chang Ren, Haoyi Sun, Qian Wang, Wei Wang, Hongfu Yang, Jiqing Zhan, Chunpeng Zhou, Zheng Zhou, Hao Ma, Tao Wei, Pan Zhou, Wei Chen
  - **institution:** Li Auto Inc
  - **link:** https://arxiv.org/pdf/2512.23412
  - **code:** https://github.com/TIMMY-CHAN/MindWatcher
  - **contributions:** 1. Introduces MindWatcher, a TIR agent with interleaved thinking and multimodal CoT reasoning for autonomous tool invocation and coordination. 2. Constructs the MWE-Bench benchmark and releases high-quality datasets and distilled smaller models (2B, 3B, 4B). 3. Designs a more efficient training infrastructure to enhance training speed and hardware utilization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36f0f229f533ecc1b9565a72c5c77b232eab32880c12545774f94ebd2a19e651_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MindWatcher, a multimodal tool-integrated reasoning agent that uses interleaved thinking and chain-of-thought reasoning to autonomously decide when and how to invoke tools. It is equipped with auxiliary tools and a local image database to handle broad-domain problems. Experiments show it matches or exceeds larger models in performance and provides insights like the genetic inheritance phenomenon in agent training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning] --> B[核心问题/Problem: Traditional workflow-based agents have limited intelligence for real-world tool-invocation problems.]
        A --> C[主要方法/Method: Proposes MindWatcher agent with interleaved thinking and multimodal CoT reasoning for autonomous tool use.]
        A --> D[关键结果/Results: Matches/exceeds larger models, introduces MWE-Bench, and provides efficient training infrastructure.]
    ```

- **[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]
  - **authors:** Yusuf Kalyoncuoglu
  - **institution:** RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.23410
  - **contributions:** 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces "Subspace-Native Distillation" as a novel paradigm to provide a stable geometric coordinate system for student models, enabling "Train Big, Deploy Small".
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Directly Constructing Low-Dimensional Solution Subspaces<br>直接构建低维解子空间"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Large models are redundant for representation but needed for optimization.<br>大模型对表示是冗余的，但对优化是必要的。"]
        Method["主要方法/Method<br>Construct low-dimensional subspaces, decouple solution geometry.<br>构建低维子空间，解耦解几何。"]
        Results["关键结果/Results<br>Head compression by 16x, Subspace-Native Distillation.<br>分类头压缩16倍，提出子空间原生蒸馏。"]
    ```

- **[arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis**
  - **tags:** [ai], [continual learning], [big world hypothesis, computationally-embedded agent, interactivity, partially observable Markov decision process, model-based reinforcement learning]
  - **authors:** Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado
  - **institution:** University of Alberta, Amii, The Swiss AI Lab IDSIA, USI & SUPSI, Canada CIFAR AI Chair, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.23419
  - **contributions:** 1. Introduced a computationally-embedded perspective, representing an agent as an automaton simulated within a universal computer, proving it's equivalent to interacting with a POMDP over an infinite state-space. 2. Proposed a new objective called "interactivity" to measure an agent's ability to continually adapt its behavior by learning new predictions. 3. Developed a model-based RL algorithm for interactivity-seeking and constructed a synthetic problem to evaluate continual learning, finding deep linear networks outperform nonlinear ones in sustaining interactivity as capacity scales.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1eff9d76987d2bc49a07c8de307183661687471b7fe4f21ca75040ba3e1de25a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a computationally-embedded perspective to formalize the "big world hypothesis" in continual learning, where an agent is modeled as an automaton within the environment. It introduces "interactivity" as a new objective and a corresponding model-based RL algorithm to seek it. The main finding is that, in their synthetic evaluation, deep linear networks sustain higher interactivity as capacity increases, whereas deep nonlinear networks struggle.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis<br>论文标题"]
        Root --> Problem["核心问题/Problem<br>如何形式化智能体在'大世界'中的持续学习约束"]
        Root --> Method["主要方法/Method<br>提出计算嵌入视角与'交互性'目标，开发基于模型的强化学习算法"]
        Root --> Results["关键结果/Results<br>深度线性网络比非线性网络更能维持交互性"]
    ```

- **[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis**
  - **tags:** [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]
  - **authors:** Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao
  - **institution:** Huawei Technologies Co., Ltd., Hunan University
  - **link:** https://arxiv.org/pdf/2512.23424
  - **contributions:** 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system's effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AKG Kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI模型对高性能计算内核的需求 / AI Models Demand High-Performance Kernels]
        B --> B2[硬件多样性与手动优化的瓶颈 / Hardware Diversity & Manual Optimization Bottleneck]
        C --> C1[多智能体系统自动化内核生成与调优 / Multi-Agent System Automates Kernel Generation & Tuning]
        C --> C2[支持多种DSL以面向不同硬件后端 / Supports Multiple DSLs for Different Hardware Backends]
        D --> D1[在KernelBench上评估 / Evaluated on KernelBench]
        D --> D2[平均加速1.46倍 / Average 1.46x Speedup Achieved]
    ```

- **[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification**
  - **tags:** [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]
  - **authors:** Mustafa Demetgul, Sanja Lazarova Molnar
  - **institution:** Karlsruhe Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23436
  - **contributions:** 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --> B[核心问题/Problem: Classical road monitoring is expensive and unsystematic.]
    A --> C[主要方法/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]
    A --> D[关键结果/Results: Over 95% classification accuracy achieved.]
    ```

- **[arXiv251230] Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study**
  - **tags:** [mlsys], [on-device ai], [DistilHuBERT, 8-bit quantization, cross-corpus validation, Leave-One-Session-Out (LOSO), model compression]
  - **authors:** Saifelden M. Ismail
  - **institution:** University of Science and Technology, Zewail City
  - **link:** https://arxiv.org/pdf/2512.23435
  - **contributions:** 1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a "theatricality effect" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study"] --> B["核心问题/Problem: SER部署受限于大模型的计算需求/SER deployment constrained by computational demands of large models"]
        A --> C["主要方法/Method: 使用蒸馏与8位量化的DistilHuBERT，并进行跨语料库训练/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training"]
        A --> D["关键结果/Results: 模型仅23MB，精度达基准91%，跨语料库训练提升泛化性/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization"]
    ```

- **[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]
  - **authors:** Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang
  - **institution:** Lenovo (PCIE), University of Minnesota (UMN)
  - **link:** https://arxiv.org/pdf/2512.23453
  - **code:** https://github.com/AI-Researcher-Team/CoFi-Dec
  - **contributions:** 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoFi-Dec: Hallucination-Resistant Decoding] --> B[核心问题/Problem: LVLMs产生与视觉输入不一致的幻觉内容]
        A --> C[主要方法/Method: 基于粗到细视觉条件的生成式自反馈与Wasserstein融合]
        A --> D[关键结果/Results: 在六个基准测试中显著减少幻觉，无需训练，模型无关]
    ```

- **[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following**
  - **tags:** [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]
  - **authors:** Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song
  - **institution:** Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.23457
  - **code:** https://github.com/zhangkc97/HiR
  - **contributions:** 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --> Problem
        Root --> Method
        Root --> Results
        Problem[稀疏/不可区分的奖励阻碍学习<br>Sparse/Indistinguishable Rewards Impede Learning]
        Method[后见指令重放 (HiR)<br>Hindsight instruction Replay (HiR)]
        Results[跨任务有效且计算高效<br>Effective Across Tasks & Computationally Efficient]
    ```

- **[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation**
  - **tags:** [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]
  - **authors:** Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao
  - **institution:** Tencent Hunyuan
  - **link:** https://arxiv.org/pdf/2512.23464
  - **code:** https://github.com/Tencent-Hunyuan/HY-Motion-1.0
  - **contributions:** 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"]
        Root --> Problem["核心问题/Problem: Generating high-quality, text-aligned 3D human motions"]
        Root --> Method["主要方法/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline"]
        Root --> Results["关键结果/Results: SOTA performance, Extensive motion coverage, Open-source release"]
    ```

- **[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance**
  - **tags:** [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]
  - **authors:** Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang
  - **institution:** Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data
  - **link:** https://arxiv.org/pdf/2512.23461
  - **code:** https://github.com/Qwen-Applications/DIR
  - **contributions:** 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method's ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR's effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Eliminating Inductive Bias in Reward Models<br>消除奖励模型中的归纳偏差] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Low-quality RM data with inductive biases<br>导致过拟合和奖励攻击] --> B1[举例/Example<br>Response length bias<br>响应长度偏差]
        C[主要方法/Method<br>DIR: Information-theoretic debiasing<br>基于信息瓶颈优化互信息] --> C1[目标/Objective<br>Max MI with preference, Min MI with bias<br>最大化偏好互信息，最小化偏差互信息]
        D[关键结果/Results<br>Mitigates multiple biases & enhances RLHF<br>减轻多种偏差并提升RLHF性能] --> D1[验证的偏差/Verified Biases<br>Length, Sycophancy, Format<br>长度、迎合性、格式]
    ```

- **[arXiv251230] Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings**
  - **tags:** [nlp], [text clustering], [hierarchical clustering, density-based clustering, semantic embeddings, large language models, topic modeling]
  - **authors:** Thomas Haschka, Joseph Bakarji
  - **institution:** Technische Universität Wien, American University of Beirut
  - **link:** https://arxiv.org/pdf/2512.23471
  - **contributions:** 1. Proposes a novel nested density clustering method to construct hierarchical semantic trees from text embeddings. 2. Demonstrates the method's application for data-driven discovery of research areas and subfields without predefined categories. 3. Validates the approach's robustness and general applicability across diverse domains using benchmark datasets like 20 Newsgroups and IMDB reviews.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of uncovering the global hierarchical semantic structure in text corpora, which remains opaque when using LLM embeddings only for similarity search. It proposes a method that applies nested density clustering on LLM embeddings, gradually relaxing a density criterion to merge clusters into a hierarchical tree. This approach enables the data-driven discovery of semantic relationships and topic hierarchies without predefined categories, as demonstrated on scientific abstracts and benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic Tree Inference on Text Corpa / 文本语料库的语义树推断"]
        Root --> Problem["核心问题/Problem: Opaque global semantic structure in text corpora / 文本语料库中不透明的全局语义结构"]
        Root --> Method["主要方法/Method: Nested density clustering on LLM embeddings / 基于大语言模型嵌入的嵌套密度聚类"]
        Root --> Results["关键结果/Results: Data-driven hierarchical semantic tree discovery / 数据驱动的层次化语义树发现"]
    ```

- **[arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation**
  - **tags:** [sec], [software supply chain security], [agentic AI, reinforcement learning, large language model, blockchain security ledger, CI/CD]
  - **authors:** Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani
  - **institution:** Islamic University of Madinah, Arab Open University-Bahrain
  - **link:** https://arxiv.org/pdf/2512.23480
  - **contributions:** 1. Proposes an autonomous, agentic AI framework for software supply chain security that integrates LLM-based reasoning, RL, and multi-agent coordination for proactive vulnerability identification and mitigation. 2. Implements a system that interfaces with real CI/CD environments (e.g., GitHub Actions, Jenkins) via the Model Context Protocol (MCP) and logs actions to a blockchain for auditability. 3. Demonstrates through experiments that the framework outperforms rule-based and provenance-only baselines in detection accuracy and mitigation latency with acceptable operational overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f037ac5977c7275a7c48edbcb676154bcff19330c107fe4c9769750efc5350_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of current software supply chain security frameworks (like SLSA) which focus on provenance but lack active vulnerability mitigation. It proposes an agentic AI system that combines LLMs for semantic analysis and RL for adaptive response, integrated with CI/CD pipelines via MCP and logged on a blockchain. Experiments show it achieves better detection and faster mitigation than baselines, enabling a shift from reactive to proactive defense.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agentic AI for Autonomous Defense in Software Supply Chain Security] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统溯源框架无法主动缓解漏洞/Traditional provenance frameworks lack active vulnerability mitigation]
        C --> C1[多智能体协调/Multi-Agent Coordination]
        C --> C2[LLM推理与RL策略/LLM Reasoning & RL]
        C --> C3[集成CI/CD与区块链日志/CI/CD Integration & Blockchain Ledger]
        D --> D1[更高的检测准确率/Higher Detection Accuracy]
        D --> D2[更短的缓解延迟/Lower Mitigation Latency]
        D --> D3[合理的构建开销/Reasonable Build Overhead]
    ```

- **[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]
  - **authors:** Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu
  - **institution:** Beihang University, Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23485
  - **code:** https://github.com/Bane-Elvin/AAAI2026-FRoD
  - **contributions:** 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FRoD: Full-Rank Efficient Fine-Tuning] --> B[核心问题/Problem: Low-rank PEFT methods suffer from slow convergence and limited adaptation capacity]
        A --> C[主要方法/Method: Hierarchical joint decomposition with rotational degrees of freedom for full-rank updates]
        A --> D[关键结果/Results: Matches full fine-tuning accuracy using only 1.72% parameters and achieves faster convergence]
    ```

- **[arXiv251230] Theory of Mind for Explainable Human-Robot Interaction**
  - **tags:** [ai], [human-robot interaction], [Theory of Mind, Explainable AI, XAI evaluation, human-centered explanation, VXAI framework]
  - **authors:** Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter
  - **institution:** University of Hamburg
  - **link:** https://arxiv.org/pdf/2512.23482
  - **contributions:** 1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot's actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM's user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Theory of Mind for Explainable Human-Robot Interaction") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("ToM解释与机器人内部推理不一致/ToM explanations may not match robot's internal reasoning")
        Problem --> P2("XAI缺乏以用户为中心的解释/XAI lacks user-centered explanations")
        Method --> M1("将ToM视为XAI的一种形式/Consider ToM as a form of XAI")
        Method --> M2("在XAI框架内整合ToM原则/Integrate ToM principles within XAI frameworks")
        Results --> R1("提出视角转变，优先考虑用户需求/Proposed shift in perspective to prioritize user's needs")
        Results --> R2("为使用VXAI等框架评估ToM奠定基础/Laid foundation for evaluating ToM using frameworks like VXAI")
    ```

- **[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment**
  - **tags:** [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]
  - **authors:** Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki
  - **institution:** Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University
  - **link:** https://arxiv.org/pdf/2512.23487
  - **contributions:** 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("能力排行榜与部署决策脱节/Capability-Deployment Gap")
        Problem --> P2("需平衡用户效用、成本、合规性/Balance Utility, Cost, Compliance")
        Method --> M1("理论: 基于前沿的约束优化/Theoretical Constrained Optimization")
        Method --> M2("实现: 提取、估计、学习、推荐/Pipeline: Extract, Estimate, Learn, Recommend")
        Results --> R1("最优配置呈现三区结构/Optimal Configurations Show Three-Regime Structure")
        Results --> R2("部署感知排名不同于能力排名/Deployment-Aware Rankings Differ from Capability-Only")
    ```

- **[arXiv251230] The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [graph reasoning, information gain, multi-agent, off-graph prediction, path retrieval]
  - **authors:** Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong
  - **institution:** ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.23489
  - **code:** https://anonymous.4open.science/r/MIRAGE-VC-323F
  - **contributions:** 1. Proposed an information-gain-driven path retriever to tackle path explosion in graphs for LLM reasoning. 2. Introduced a multi-agent architecture with learnable gating to fuse heterogeneous evidence streams. 3. Addressed the off-graph prediction challenge in venture capital, demonstrating significant performance gains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aed24a413df0661b461a6124bbf364c73762a6f25c3c90546bff33f8c4123ebf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of predicting venture capital success, an off-graph task requiring reasoning over complex relational evidence. It proposes MIRAGE-VC, a framework that uses information-gain-driven path retrieval and a multi-agent system to distill and reason over investment networks. The method achieves improved prediction performance and offers insights for other off-graph tasks like recommendation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Gaining Paths to Investment Success<br/>投资成功路径] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>VC预测是离图任务<br/>VC prediction is off-graph task]
        C[主要方法/Method<br/>MIRAGE-VC框架<br/>信息增益路径检索与多智能体<br/>MIRAGE-VC: Info-gain path retrieval & multi-agent]
        D[关键结果/Results<br/>性能显著提升<br/>+5.0% F1, +16.6% Precision@5<br/>Significant performance gains]
    ```

- **[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization**
  - **tags:** [sys], [communication & networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]
  - **authors:** Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink
  - **institution:** Wuhan University, RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.23493
  - **contributions:** 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[URLLC IIoT网络的多设备动态调度与链路自适应/URLLC IIoT Multi-device Dynamic Scheduling & Link Adaptation]
        B --> B2[不完美的信道状态信息/Imperfect Channel State Information]
        B --> B3[严格的误块率约束/Strict Block Error Rate Constraints]
        C --> C1[贝叶斯优化驱动的TD3方法/BO-driven TD3 Method]
        C --> C2[自适应确定设备服务顺序与MCS/Adaptively Determine Device Order & MCS]
        C --> C3[BO训练机制改进收敛/BO-based Training for Convergence]
        D --> D1[更快的收敛速度/Faster Convergence]
        D --> D2[更高的总速率性能/Higher Sum-rate Performance]
    ```

- **[arXiv251230] Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities**
  - **tags:** [ai], [AI alignment], [AI assistance game, AI shutdown, Incomplete preferences, Non-Archimedean utilities]
  - **authors:** Alessio Benavoli, Alessandro Facchini, Marco Zaffalon
  - **institution:** Trinity College Dublin, SUPSI (University of Applied Sciences and Arts of Southern Switzerland), Kozminski University
  - **link:** https://arxiv.org/pdf/2512.23508
  - **contributions:** 1. Formally connects the AI assistance and AI shutdown problems to the need for reasoning under uncertainty. 2. Argues that handling incomplete human preferences is a fundamental requirement for safe AI. 3. Proposes that non-Archimedean utility structures are necessary to correctly model and prioritize safety constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea603db1434e19dcd83096c95297f9662c571581eb479d66e87432fcf6a9075_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes AI safety through the frameworks of the AI assistance and AI shutdown games. It argues that to address these challenges, AI agents must be designed to reason under uncertainty and handle incomplete and non-Archimedean human preferences. The main conclusion is that these capabilities are essential for ensuring AI systems remain aligned with human values and safe.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities"] --> Problem["核心问题/Problem: AI Alignment and Safety"]
        Root --> Method["主要方法/Method: Analyze via AI Assistance & Shutdown Games"]
        Root --> Results["关键结果/Results: Requires Uncertainty, Incomplete & Non-Archimedean Preferences"]
    ```

- **[arXiv251230] UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?**
  - **tags:** [mlsys], [multi-modal training], [vision-language model, unified model, semantic generation, autoregression, data scaling]
  - **authors:** Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao
  - **institution:** Meituan
  - **link:** https://arxiv.org/pdf/2512.23512
  - **contributions:** 1. Demonstrates that generation enhances understanding in large-scale VLM training only when operating at the semantic level (e.g., autoregressing high-level visual representations), not at the pixel level. 2. Shows that unified generation-understanding models exhibit superior data scaling trends and higher data utilization efficiency compared to understanding-only models. 3. Proposes that autoregression on input embeddings is an effective and modality-independent method for capturing visual details, enabling pixel-level generation from learned semantics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether visual generation tasks can enhance understanding in large-scale vision-language models. Through large-scale pretraining (&gt;200M samples) with a model called UniHetero, the authors find that semantic-level generation (not pixel-level) improves understanding, reveals better data scaling, and that autoregression on input embeddings effectively captures visual details.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?] --> B(核心问题/Problem: Does visual generation enhance understanding at large scale?);
        A --> C(主要方法/Method: Large-scale pretraining of unified model UniHetero (>200M samples));
        A --> D(关键结果/Results);
        B --> D;
        C --> D;
        D --> E(结果1/Result 1: Generation helps, but Only if you generate Semantics, Not Pixels);
        D --> F(结果2/Result 2: Superior Data Scaling trend and higher Data Utilization);
        D --> G(结果3/Result 3: Autoregression on Input Embedding is effective);
    ```

- **[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization**
  - **tags:** [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]
  - **authors:** Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang
  - **institution:** Zhejiang University, HKUST (The Hong Kong University of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.23537
  - **contributions:** 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AnyMS: 布局引导免训练多主体定制] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[多主体定制中文本对齐、身份保持与布局控制的平衡问题/Balancing text alignment, identity preservation, and layout control in multi-subject customization]
        C --> C1[提出免训练框架AnyMS/Proposes training-free framework AnyMS]
        C1 --> C2[引入自底向上双级注意力解耦机制/Introduces bottom-up dual-level attention decoupling]
        C2 --> C3[全局解耦确保文本对齐/Global decoupling ensures text alignment]
        C2 --> C4[局部解耦防止主体冲突/Local decoupling prevents subject conflicts]
        C --> C5[使用预训练图像适配器提取特征/Uses pre-trained image adapters for feature extraction]
        D --> D1[实现SOTA性能/Achieves SOTA performance]
        D --> D2[支持复杂组合与更多主体/Supports complex compositions and scales to more subjects]
    ```

- **[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis**
  - **tags:** [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]
  - **authors:** Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang
  - **institution:** Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University
  - **link:** https://arxiv.org/pdf/2512.23545
  - **contributions:** 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PathFound: Agentic Multimodal Model] --> B[核心问题/Problem: Static inference vs. clinical workflow]
        A --> C[主要方法/Method: Agentic model with VFM, VLM, RL]
        A --> D[关键结果/Results: SOTA accuracy, discovers subtle details]
        B --> B1[静态推理范式/Static inference paradigm]
        B --> B2[缺乏证据再获取/Lacks reassessment & evidence acquisition]
        C --> C1[多阶段诊断/Multi-stage diagnosis]
        C --> C2[主动信息获取/Proactive information acquisition]
        D --> D1[诊断准确性提升/Improved diagnostic accuracy]
        D --> D2[发现细微特征/Discover subtle pathological features]
    ```

- **[arXiv251230] Act2Goal: From World Model To General Goal-conditioned Policy**
  - **tags:** [ai], [reinforcement learning], [goal-conditioned policy, world model, multi-scale temporal hashing, hindsight goal relabeling, LoRA]
  - **authors:** Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo
  - **institution:** Agibot Research
  - **link:** https://arxiv.org/pdf/2512.23541
  - **code:** https://act2goal.github.io/
  - **contributions:** 1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Act2Goal: From World Model To General Goal-conditioned Policy] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有目标条件策略在长视野操作中表现不佳/Existing goal-conditioned policies struggle with long-horizon manipulation]
        C --> C1[集成目标条件视觉世界模型与多尺度时序控制/Integrates goal-conditioned visual world model with multi-scale temporal control]
        C --> C2[引入多尺度时序哈希(MSTH)分解轨迹/Introduces Multi-Scale Temporal Hashing (MSTH) to decompose trajectory]
        D --> D1[零样本泛化能力强/Strong zero-shot generalization]
        D --> D2[通过在线自适应显著提升成功率/Improves success rates significantly via online adaptation]
    ```

- **[arXiv251230] Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs**
  - **tags:** [nlp], [hallucination detection], [knowledge graphs, self-detection, structured verification, GPT-4o, Gemini-2.5-Flash]
  - **authors:** Sahil Kale, Antonio Luca Alfeo
  - **institution:** Knowledge Verse AI, eCampus University
  - **link:** https://arxiv.org/pdf/2512.23547
  - **code:** https://github.com/knowledge-verse-ai/kg-hallu-eval
  - **contributions:** 1. Proposes a novel hallucination self-detection method that converts LLM responses into knowledge graphs for structured analysis., 2. Introduces a manually curated and enhanced hallucination detection dataset to support more reliable future benchmarking., 3. Demonstrates significant performance improvements (up to 16% accuracy, 20% F1) over standard self-detection and a state-of-the-art baseline (SelfCheckGPT).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of hallucinations in LLMs by proposing a self-detection method that converts model responses into knowledge graphs to better analyze atomic facts and estimate hallucination likelihood. The method, evaluated on GPT-4o and Gemini-2.5-Flash, shows substantial improvements in accuracy and F1-score over existing approaches. The work concludes that structuring facts as knowledge graphs enables more robust hallucination detection, offering a low-cost, model-agnostic path toward safer language models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"] --> B["核心问题/Problem: LLM Hallucinations hinder safe deployment"]
        A --> C["主要方法/Method: Convert responses to Knowledge Graphs for structured self-verification"]
        A --> D["关键结果/Results: Up to 16% accuracy & 20% F1 improvement over baselines"]
    ```

- **[arXiv251230] Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks**
  - **tags:** [sec], [ai security], [prompt injection, multi-agent systems, provenance tracking, trust validation, multimodal sanitization]
  - **authors:** Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty
  - **institution:** Islamic University of Madinah, Arab Open University-Bahrain
  - **link:** https://arxiv.org/pdf/2512.23557
  - **contributions:** 1. Proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework to secure agentic AI workflows against prompt injection attacks. 2. Introduces a coordinated defense with specialized sanitizer agents (text, visual) and an output validator, managed by a provenance ledger for tracking trust metadata. 3. Demonstrates through experiments that the framework significantly improves multimodal injection detection accuracy and minimizes trust leakage across agents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05cd3aa22e07307bda78f06b6f87c2195c61c758b4fe44dc5bffbc281d72a8e4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the security threat of multimodal prompt injection attacks in agentic AI systems like LangChain. It proposes a defense framework that sanitizes inputs and validates outputs using specialized agents coordinated by a provenance ledger. The experimental results show the framework enhances detection accuracy and stabilizes agentic execution pathways.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Trustworthy Agentic AI<br>构建可信的智能体AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Multimodal Prompt Injection Attacks<br>多模态提示注入攻击]
        C --> C1[Cross-Agent Provenance-Aware Framework<br>跨智能体溯源感知框架]
        C1 --> C2[Sanitizer & Validator Agents<br>净化与验证智能体]
        C1 --> C3[Provenance Ledger<br>溯源账本]
        D --> D1[Enhanced Detection Accuracy<br>提升检测准确率]
        D --> D2[Minimized Trust Leakage<br>最小化信任泄漏]
        D --> D3[Stable Execution Pathways<br>稳定的执行路径]
    ```

- **[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]
  - **authors:** Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang
  - **institution:** Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23562
  - **code:** https://github.com/K1nght/VL-RouterBench
  - **contributions:** 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>缺乏系统化、可复现的<br>VLM路由评估基准<br>Lack of systematic, reproducible<br>benchmark for VLM routing]
        Method[主要方法/Method<br>基于原始推理日志构建<br>质量与成本矩阵<br>Construct quality & cost matrices<br>from raw inference logs]
        Results[关键结果/Results<br>观察到显著的路由增益<br>但与理想性能仍有差距<br>Observe significant routability gain<br>but clear gap to ideal Oracle]
    ```

- **[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature**
  - **tags:** [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]
  - **authors:** Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke
  - **institution:** DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University
  - **link:** https://arxiv.org/pdf/2512.23565
  - **contributions:** 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]
        Method[主要方法/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]
        Results[关键结果/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]
    ```

- **[arXiv251230] Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation**
  - **tags:** [nlp], [creative text generation], [divergent-convergent thinking, prompting method, creative problem generation, artificial hivemind, constraint satisfaction]
  - **authors:** Manh Hung Nguyen, Adish Singla
  - **institution:** MPI-SWS (Max Planck Institute for Software Systems)
  - **link:** https://arxiv.org/pdf/2512.23601
  - **contributions:** 1. Proposes CreativeDC, a novel two-phase prompting method inspired by human creative thinking to decouple creative exploration from constraint satisfaction in LLMs. 2. Introduces a comprehensive evaluation framework for creative problem generation, measuring diversity, novelty, and utility. 3. Demonstrates that CreativeDC significantly improves the diversity and novelty of generated educational problems compared to baseline methods while maintaining utility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86ff3954389096abbd357b8cdc09c7b6f3087b3cf9c2edee2cf008bb8924d692_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the "Artificial Hivemind" effect in LLMs, which leads to homogeneous and repetitive outputs, particularly harmful for generating diverse educational problems. It proposes CreativeDC, a prompting method that scaffolds LLM reasoning into divergent (idea exploration) and convergent (constraint satisfaction) phases. The results show that CreativeDC generates problems with significantly higher diversity and novelty than baselines without sacrificing utility.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Divergent-Convergent Thinking in LLMs for Creative Problem Generation<br/>大语言模型中的发散-收敛思维用于创意问题生成"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: LLMs suffer from 'Artificial Hivemind' effect, generating homogeneous and repetitive educational problems.<br/>大语言模型存在'人工蜂群思维'效应，生成同质化、重复的教育问题。"]
        Method["主要方法/Method: CreativeDC, a two-phase prompting method decoupling divergent (exploration) and convergent (constraint) thinking.<br/>CreativeDC，一种将发散（探索）与收敛（约束）思维解耦的两阶段提示方法。"]
        Results["关键结果/Results: Achieves higher diversity & novelty while maintaining utility; scales better in generating distinct problems.<br/>在保持实用性的同时实现更高的多样性与新颖性；在生成独特问题方面扩展性更好。"]
    ```

- **[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning**
  - **tags:** [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]
  - **authors:** Deniz Akdemir
  - **institution:** None (Institution not specified in provided content)
  - **link:** https://arxiv.org/pdf/2512.23617
  - **contributions:** 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam's theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework's effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful "negative transfer" by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam's theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[标准UDA的缺陷/Flaw of Standard UDA]
        Problem --> P2[负迁移与信息破坏/Negative Transfer & Information Destruction]
        Method --> M1[Le Cam理论/Le Cam's Theory]
        Method --> M2[方向可模拟性/Directional Simulability]
        Method --> M3[Le Cam Distortion度量/Le Cam Distortion Metric]
        Results --> R1[基因组学完美估计/Perfect Genomics Estimation]
        Results --> R2[零源域损失/Zero Source Utility Loss]
        Results --> R3[安全RL策略转移/Safe RL Policy Transfer]
    ```

- **[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions**
  - **tags:** [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, Φ-CPDAG, regret-based]
  - **authors:** Federico Baldo, Charles K. Assaad
  - **institution:** Sorbonne Université, INSERM, Institut Pierre Louis d'Epidémiologie et de Santé Publique
  - **link:** https://arxiv.org/pdf/2512.23626
  - **contributions:** 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm's effectiveness and privacy guarantees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Regret-Based Federated Causal Discovery with Unknown Interventions] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 联邦因果发现中客户端存在未知异质干预/Federated causal discovery with unknown, heterogeneous client interventions]
        C[主要方法/Method: 提出I-PERI算法，利用干预差异定向边/Propose I-PERI algorithm, orienting edges using intervention differences]
        D[关键结果/Results: 定义Φ-CPDAG，提供理论与隐私保证/Define Φ-CPDAG, provide theoretical and privacy guarantees]
    ```

- **[arXiv251230] Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE**
  - **tags:** [mlsys], [others], [physics-informed neural networks, circuit simulation, differential-algebraic equations, surrogate modeling, NeuroSPICE]
  - **authors:** Chien-Ting Tung, Chenming Hu
  - **institution:** University of California at Berkeley
  - **link:** https://arxiv.org/pdf/2512.23624
  - **contributions:** 1. Proposes NeuroSPICE, a novel PINN-based framework for solving circuit differential-algebraic equations directly, bypassing traditional time-discretized numerical solvers. 2. Demonstrates the framework's flexibility for modeling emerging devices and multi-physics systems within a single Python environment, lowering the barrier for rapid prototyping. 3. Highlights the potential of the differentiable PINN model as a surrogate for circuit design optimization and inverse problems, despite not outperforming SPICE in raw training speed or accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7d88b0c7c41bb8bbaf5959a08185913698993a5cc330641c604219b2a1c0622_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces NeuroSPICE, a framework that uses Physics-Informed Neural Networks (PINNs) to simulate electronic circuits by solving their governing differential-algebraic equations through backpropagation. It represents circuit waveforms as continuous, differentiable functions of time, enabling the simulation of novel devices like ferroelectric memories. The main conclusion is that while not faster than SPICE for training, NeuroSPICE offers unique advantages as a flexible, differentiable surrogate model for design optimization and complex multi-physics systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE<br>论文标题"] --> B["Problem: Conventional SPICE struggles with emerging, highly nonlinear devices and multi-physics coupling.<br>核心问题: 传统SPICE难以处理新兴的非线性器件和多物理场耦合。"]
        A --> C["Method: NeuroSPICE, a PINN framework that solves circuit DAEs by minimizing equation residuals via backpropagation.<br>主要方法: NeuroSPICE，一个通过反向传播最小化方程残差来求解电路DAE的PINN框架。"]
        A --> D["Results: Provides a flexible, differentiable surrogate model for design optimization, enabling simulation of novel devices like ferroelectric memories.<br>关键结果: 提供了一个灵活的、可微分的代理模型用于设计优化，能够模拟如铁电存储器等新型器件。"]
    ```

- **[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization**
  - **tags:** [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]
  - **authors:** Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong
  - **institution:** Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst
  - **link:** https://arxiv.org/pdf/2512.23631
  - **code:** https://github.com/iamxjy/BOAD-SWE-Agent
  - **contributions:** 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BOAD: 发现分层软件工程代理 / BOAD: Discovering Hierarchical Software Engineering Agents] --> B
        A --> C
        A --> D
        B[核心问题 / Problem: 单一LLM代理在长视野、分布外软件工程任务上泛化能力差 / Single-agent LLMs generalize poorly on long-horizon, out-of-distribution SWE tasks]
        C[主要方法 / Method: 将分层发现建模为多臂老虎机问题，优化子代理协作 / Formulate hierarchy discovery as a multi-armed bandit problem to optimize sub-agent collaboration]
        D[关键结果 / Results: 在SWE-bench上超越单代理和手动设计的多代理系统，36B模型排名第二 / Outperforms single-agent and manual multi-agent systems on SWE-bench, 36B model ranks second]
    ```

- **[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms**
  - **tags:** [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]
  - **authors:** LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão
  - **institution:** Google, Eedi
  - **link:** https://arxiv.org/pdf/2512.23633
  - **contributions:** 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]
        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]
        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]
        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]
        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]
        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]
        Results --> R2[学生表现相当或更好/Student performance equal or better]
        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]
    ```

- **[arXiv251230] Nested Browser-Use Learning for Agentic Information Seeking**
  - **tags:** [mlsys], [agent system], [information-seeking agents, browser interaction, ReAct-style agents, nested framework]
  - **authors:** Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.23647
  - **code:** https://github.com/Alibaba-NLP/DeepResearch
  - **contributions:** 1. Proposes a minimal and complete browser-action framework for agents, 2. Introduces a nested structure to decouple interaction control from page exploration, 3. Demonstrates improved performance on deep information-seeking benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of current information-seeking agents, which rely on simple API calls and cannot perform real browsing. It proposes NestBrowse, a framework that uses a nested structure to enable fine-grained browser control for agents, simplifying reasoning and improving performance on deep search tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Nested Browser-Use Learning for Agentic Information Seeking<br>面向智能信息搜索的嵌套浏览器使用学习"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Agents lack real browsing, limited to APIs."]
        Method["主要方法/Method<br>NestBrowse: nested browser-action framework."]
        Results["关键结果/Results<br>Better performance on deep IS benchmarks."]
    ```

- **[arXiv251230] Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing**
  - **tags:** [sec], [adversarial attacks], [prompt injection, large language models, academic peer review, multilingual, adversarial robustness]
  - **authors:** Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss
  - **institution:** International School of Athens, Idiap Research Institute
  - **link:** https://arxiv.org/pdf/2512.23684
  - **contributions:** 1. Constructed a dataset of ~500 real ICML papers to empirically evaluate hidden prompt injection attacks in a realistic academic reviewing context. 2. Demonstrated that embedding semantically equivalent adversarial instructions in multiple languages (English, Japanese, Chinese, Arabic) can significantly alter LLM-generated review scores and decisions. 3. Revealed notable cross-lingual differences in attack effectiveness, with Arabic injections having minimal impact compared to others.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the vulnerability of LLM-based academic peer review systems to hidden prompt injection attacks. By injecting adversarial instructions in four languages into a dataset of real papers and having an LLM review them, the authors found that such attacks can substantially change review outcomes for English, Japanese, and Chinese, but not Arabic. The results highlight a critical security risk and language-dependent susceptibility in automated reviewing pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>LLM-based reviewing systems are vulnerable to hidden prompt injection attacks.]
        C[主要方法/Method<br>Inject semantically equivalent adversarial prompts in 4 languages into ~500 real papers and review with an LLM.]
        D[关键结果/Results<br>English, Japanese, Chinese injections change scores/decisions; Arabic injections have little effect.]
    ```

- **[arXiv251230] Web World Models**
  - **tags:** [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]
  - **authors:** Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang
  - **institution:** Princeton University, University of California, Los Angeles, University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.23676
  - **code:** https://princeton-ai2-lab.github.io/Web-World-Models/
  - **contributions:** 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world "physics" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Web World Models] --> B["核心问题/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models)."]
        A --> C["主要方法/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top."]
        A --> D["关键结果/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs."]
    ```

- **[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence**
  - **tags:** [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]
  - **authors:** Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang
  - **institution:** Tsinghua University, National Climate Centre, China Meteorological Administration
  - **link:** https://arxiv.org/pdf/2512.22144
  - **contributions:** 1. Introduced an "AI-for-theory" paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an "AI-for-theory" paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI's role as a discovery tool for complex systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Defining MJO lifecycle is challenging due to propagation; classical methods conflate artifacts with physics.]
        C[主要方法/Method<br>AI-for-theory paradigm; Deep learning model (PhysAnchor-MJO-AE) learns similarity-preserving latent representation for objective clustering.]
        D[关键结果/Results<br>First complete six-phase MJO anatomy; New physics-coherent monitoring framework reduces errors by an order of magnitude.]
    ```

- **[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data**
  - **tags:** [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]
  - **authors:** Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet
  - **institution:** IMT Atlantique, Ifremer, CNRS, Mercator Ocean International
  - **link:** https://arxiv.org/pdf/2512.22152
  - **contributions:** 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Neural Ocean Forecasting from Sparse Observations] --> B(核心问题/Problem: 稀疏卫星数据下的短期海洋预报/Short-term ocean forecasting from sparse satellite data)
        A --> C(主要方法/Method: 基于U-Net和4DVarNet的端到端序列预测/End-to-end sequence forecasting using U-Net & 4DVarNet)
        A --> D(关键结果/Results: 神经模型超越业务化基线，在多变区域改进显著/Neural model outperforms operational baseline, notable improvements in high-variability regions)
    ```

- **[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images**
  - **tags:** [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]
  - **authors:** Haozhe Jia
  - **institution:** Boston University
  - **link:** https://arxiv.org/pdf/2512.22209
  - **contributions:** 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis."]
        Method["主要方法/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images."]
        Results["关键结果/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy."]
    ```

- **[arXiv251230] Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [literature mining, large language models, prompt engineering, few-shot prompting, domain adaptation]
  - **authors:** Xinyang Sun, Nipon Sarmah, Miao Guo
  - **institution:** King's College London
  - **link:** https://arxiv.org/pdf/2512.22225
  - **contributions:** 1. Developed a domain-adapted AI system using LLMs and advanced prompt engineering to automate the extraction of nutraceutical-producing microbial strains from unstructured text. 2. Created and validated a structured dataset of 35 nutraceutical-strain associations, spanning multiple compound categories. 3. Demonstrated the system's performance and provided biological insights, identifying dominant microbial contributors and the framework's utility for synthetic biology and precision fermentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c767d67875d1e2e25e95c99a9b826daf52082c66775d52728bda6802558969_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an AI-driven literature mining system that uses large language models and prompt engineering to automatically identify microbes that produce nutraceuticals from scientific text. The system, which performed best with the DeepSeek-V3 model and domain-specific prompts, generated a validated dataset and revealed key microbial strains for biosynthesis. This framework enhances the scalability of literature mining and provides actionable insights for strain selection and fermentation strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Literature Mining System for Nutraceutical Biosynthesis<br/>营养保健品生物合成的文献挖掘系统] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Extracting structured knowledge on microbial strains from literature is a bottleneck.<br/>从文献中提取关于微生物菌株的结构化知识是一个瓶颈。]
        C[主要方法/Method<br/>Domain-adapted system using LLMs and prompt engineering.<br/>使用LLM和提示工程的领域适应系统。]
        D[关键结果/Results<br/>Created dataset of 35 associations; DeepSeek-V3 outperformed LLaMA-2; identified dominant microbial strains.<br/>创建了35个关联的数据集；DeepSeek-V3优于LLaMA-2；识别了主要微生物菌株。]
    ```

- **[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging**
  - **tags:** [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]
  - **authors:** Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger
  - **institution:** Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology
  - **link:** https://arxiv.org/pdf/2512.22176
  - **contributions:** 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>磁共振成像深度学习分析中场强依赖的性能变异性") --> Problem
        Root --> Method
        Root --> Results
    
        Problem("核心问题/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI场强对深度学习模型性能与泛化能力的影响")
        Method("主要方法/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>在1.5T、3.0T及混合数据上训练/评估nnU-Net模型；使用UMAP和影像组学分析")
        Results("关键结果/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>训练数据中的场强显著影响性能，尤其对软组织")
    ```

- **[arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth**
  - **tags:** [ai], [autonomous systems], [autonomous operations, mission planning, in-situ resource utilisation]
  - **authors:** Ziyang Wang
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2512.22399
  - **contributions:** 1. Proposes and defines "Space AI" as a unified interdisciplinary field at the intersection of AI and space science. 2. Consolidates historical and contemporary progress into a systematic four-context framework (AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life). 3. Identifies key application areas where AI advances can translate to societal benefits on Earth, such as in sensing, robotics, and trustworthy AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ab175271eb74821199e8998bba499198a030f7f6b329c43a525f851b07aabe_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces "Space AI" as a new interdisciplinary field and proposes a systematic framework to organize its applications across four mission contexts, from Earth-based planning to multi-planetary life support. It argues that AI is critical for enabling autonomous and resilient space operations under extreme conditions, and that advances in this domain will also yield significant benefits for life on Earth.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Space AI: Leveraging AI for Space to Improve Life on Earth] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>如何在极端不确定和有限监督下<br>实现太空自主弹性操作<br>How to enable autonomous, resilient space operations under extreme uncertainty and limited oversight]
        C[主要方法/Method<br>提出系统化四维框架<br>Propose a systematic four-context framework<br>(AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life)]
        D[关键结果/Results<br>统一了跨学科领域并识别关键应用<br>加速太空探索能力并产生广泛地球影响<br>Unifies interdisciplinary field and identifies key applications<br>Accelerates space exploration capability and yields broad Earth impact]
    ```

- **[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds**
  - **tags:** [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]
  - **authors:** Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra
  - **institution:** Dream Sports, Columbia University
  - **link:** https://arxiv.org/pdf/2512.22473
  - **contributions:** 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[Transformer内部几何结构如何形成?/How is transformer internal geometry formed?]
    C --> C1[推导注意力梯度动态/Derive attention gradient dynamics]
    C --> C2[建立EM算法类比/Establish EM algorithm analogy]
    D --> D1[发现优势路由与责任更新/Discover advantage-based routing & responsibility-weighted update]
    D --> D2[梯度流塑造贝叶斯流形/Gradient flow sculpts Bayesian manifolds]
    ```

- **[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers**
  - **tags:** [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]
  - **authors:** Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan
  - **institution:** Başkent University, Gaziantep University
  - **link:** https://arxiv.org/pdf/2512.22564
  - **contributions:** 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
    
        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]
        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]
    
        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]
        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]
        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]
    
        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]
        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]
    ```

- **[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference**
  - **tags:** [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]
  - **authors:** Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev
  - **institution:** Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22999
  - **contributions:** 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Actively optimizing design variables for parameter estimation] --> Problem_Sub[子问题/Sub-problem: Sequential design and inference are typically treated separately]
        Method[主要方法/Method: Jointly amortize design and inference via end-to-end training] --> Method_Sub1[网络/Networks: Policy, History, and Inference (Diffusion-based) networks]
        Method --> Method_Sub2[损失函数/Loss: Aggregates incremental posterior error reduction]
        Results[关键结果/Results: Superior/competitive performance on standard benchmarks]
    ```

- **[arXiv251230] Deep Learning for Art Market Valuation**
  - **tags:** [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]
  - **authors:** Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang
  - **institution:** Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy
  - **link:** https://arxiv.org/pdf/2512.23078
  - **contributions:** 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]
        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]
        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]
    ```

- **[arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan**
  - **tags:** [other], [scheduling], [constraint programming, biased random-key genetic algorithm, makespan, exact delays, local search]
  - **authors:** Vítor A. Barbosa, Rafael A. Melo
  - **institution:** Institute of Computing, Universidade Federal da Bahia
  - **link:** https://arxiv.org/pdf/2512.23150
  - **contributions:** 1. A Constraint Programming (CP) model for the single-machine coupled task scheduling problem with exact delays, utilizing well-established global constraints. 2. A novel Biased Random-Key Genetic Algorithm (BRKGA) that incorporates an efficient decoder, periodical restarts, shakes, and a local search algorithm for enhanced exploration. 3. An empirical evaluation demonstrating that the BRKGA provides high-quality solutions quickly, while the CP model with extended resources can find best-known solutions for a majority of benchmark instances.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f80cc4c12e23f8eb80b390efdd4a8b62ea37fb03c01008317951d1079c35c319_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the NP-hard single-machine coupled task scheduling problem with exact delays to minimize makespan. It proposes both a Constraint Programming model and a Biased Random-Key Genetic Algorithm (BRKGA) enhanced with local search and shake components. Computational results show the BRKGA finds good solutions quickly, while the CP model with more resources achieves state-of-the-art results on most benchmark instances.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[论文标题/Paper Title: Constraint Programming and BRKGA for Coupled Task Scheduling] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 单机精确延迟耦合任务调度，最小化完工时间/Single-machine coupled task scheduling with exact delays to minimize makespan]
        Method[主要方法/Method: 约束规划模型与带偏置随机密钥遗传算法/Constraint Programming model and Biased Random-Key Genetic Algorithm (BRKGA)]
        Results[关键结果/Results: BRKGA快速提供高质量解，CP模型在充分资源下达到当前最优解/BRKGA provides high-quality solutions quickly; CP model reaches best-known solutions with sufficient resources]
    ```

- **[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making**
  - **tags:** [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]
  - **authors:** Wendyam Eric Lionel Ilboudo, Saori C Tanaka
  - **institution:** Nara Institute of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23144
  - **contributions:** 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Decision Paralysis] --> P1[挑战/Challenge<br>Choice models assume ready-to-compare options]
        Problem --> P2[现象/Phenomenon<br>Hesitation, freezing, failure to act]
        Method[主要方法/Method<br>Computational Account] --> M1[架构/Architecture<br>Hierarchical decision process]
        Method --> M2[形式化/Formalization<br>Intent vs. Affordance selection]
        Method --> M3[目标/Objective<br>Mixture of reverse-KL & forward-KL]
        Results[关键结果/Results<br>Simulation Outcomes] --> R1[行为/Behavior<br>Slow, heavy-tailed response times]
        Results --> R2[失败模式/Failure Modes<br>Intent & Affordance saturation]
        Results --> R3[解释/Interpretation<br>Autism as an extreme regime]
    ```

- **[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation**
  - **tags:** [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]
  - **authors:** Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu
  - **institution:** University of Science and Technology of China, PAII Inc., Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.23185
  - **contributions:** 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[EIR: Enhanced Image Representations for Medical Report Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[报告生成耗时耗力/Report generation is time-consuming]
        Problem --> P2[信息不对称与领域鸿沟/Information asymmetry & domain gap]
        Method[主要方法/Method] --> M1[跨模态Transformer融合元数据/Cross-modal transformer for metadata fusion]
        Method --> M2[医学领域预训练模型/Medical domain pre-trained model]
        Results[关键结果/Results] --> R1[在MIMIC和Open-I数据集上验证/Validated on MIMIC & Open-I datasets]
    ```

- **[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]
  - **authors:** Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua
  - **institution:** Shanghai Jiao Tong University, StepFun, FinStep
  - **link:** https://arxiv.org/pdf/2512.23515
  - **code:** https://github.com/FinStep-AI/Alpha-R1
  - **contributions:** 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning] --> B[核心问题/Problem: Signal decay and regime shifts in non-stationary markets; existing methods overlook semantic rationale for factor relevance.]
        A --> C[主要方法/Method: Alpha-R1, an 8B-parameter LLM trained via RL, reasons over factor logic and real-time news for context-aware alpha screening.]
        A --> D[关键结果/Results: Outperforms benchmark strategies; exhibits improved robustness to alpha decay across multiple asset pools.]
    ```

- **[arXiv251230] PINNs for Electromagnetic Wave Propagation**
  - **tags:** [other], [Scientific Computing / Computational Physics], [Physics-Informed Neural Networks (PINNs), Maxwell's Equations, FDTD, Time Marching, Poynting Regularizer]
  - **authors:** Nilufer K. Bulut
  - **institution:** Izmir, Turkiye (Inferred from author location; no specific institution mentioned in provided content)
  - **link:** https://arxiv.org/pdf/2512.23396
  - **contributions:** 1. Introduces a hybrid training strategy combining time marching and causality-aware weighting to address the causality collapse problem in time-dependent PINNs. 2. Proposes a two-stage interface continuity loss to mitigate discontinuities introduced by time marching. 3. Develops a local Poynting-based regularizer to suppress cumulative energy drift and improve energy conservation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c5894c195aba4dd14bb55cc020098ad00e472ec3df24245a9a20ad4ca74b81c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses accuracy and energy conservation deficiencies in Physics-Informed Neural Networks (PINNs) for electromagnetic wave propagation. It proposes a hybrid training methodology incorporating time marching, causality-aware weighting, and a Poynting-based regularizer. The results show that the enhanced PINNs achieve competitive field accuracy and energy conservation compared to traditional FDTD methods, demonstrating their viability for canonical electromagnetic problems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PINNs for Electromagnetic Wave Propagation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[PINNs在精度和能量守恒上落后于FDTD/PINNs lag behind FDTD in accuracy & energy]
        C --> C1[混合训练策略/Hybrid Training Strategy]
        C1 --> C1_1[时间推进与因果感知加权/Time Marching & Causality-Aware Weighting]
        C1 --> C1_2[两阶段界面连续性损失/Two-Stage Interface Continuity Loss]
        C1 --> C1_3[局部坡印廷正则化器/Local Poynting Regularizer]
        D --> D1[高场精度/High Field Accuracy (0.09% NRMSE)]
        D --> D2[能量守恒/Energy Conservation (0.024% mismatch)]
        D --> D3[与FDTD结果竞争/Competitive with FDTD]
    ```

## 2026-01-01

- **[arXiv260101] Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration**
  - **tags:** [nlp], [information extraction], [OCR, LLM, record linkage, digital humanities, data harmonization]
  - **authors:** Zahra Abedi, Richard M.K. van Dijk, Gijs Wijnholds, Tessa Verhoef
  - **institution:** Leiden University
  - **link:** https://arxiv.org/pdf/2512.23710
  - **contributions:** 1. Designed an automated pipeline integrating OCR, LLM-based interpretation, and database linking for historical document digitization. 2. Demonstrated that generative AI can partially correct low OCR performance during structured data extraction. 3. Developed a record linkage algorithm achieving high accuracy (94% on annotated data, 81% on OCR-derived data) for integrating extracted data with existing databases.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d89795562de4ae19fec35c7e5d2890c5c08f327549b9a9887347777baf5b0c5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an automated pipeline using OCR and generative AI to extract and structure biographical data from historical documents, then links this data to existing database records. The method achieved high OCR accuracy and demonstrated that LLMs can correct some OCR errors, with the final linkage algorithm performing well. The study contributes a practical tool for digital humanities research by addressing challenges like layout variability and terminology differences.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>How to automate the integration of historical document data with existing databases?]
        Method[主要方法/Method<br>OCR + LLM-based interpretation + Database linkage]
        Results[关键结果/Results<br>High OCR accuracy, LLM corrects OCR errors, Effective record linkage]
    ```

- **[arXiv260101] HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate**
  - **tags:** [nlp], [safety alignment], [multi-agent debate, stealthy harmful queries, safety training data, query transformation, adversarial prompting]
  - **authors:** Shenzhe Zhu
  - **institution:** University of Toronto
  - **link:** https://arxiv.org/pdf/2512.23717
  - **contributions:** 1. Introduces HarmTransform, the first multi-agent debate framework designed for transforming harmful queries into stealthier forms while preserving intent. 2. Designs a comprehensive evaluation protocol and provides an in-depth analysis of debate dynamics, identifying its benefits and drawbacks. 3. Demonstrates the framework's potential for generating data to enhance LLM safety alignment, highlighting both the promise and limitations of the approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9bfc9b669283c35e277363d47c398576a9ae941e3c10bc7ea3296632fc0184f_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HarmTransform, a multi-agent debate framework that iteratively refines harmful queries into stealthier forms to expose gaps in LLM safety mechanisms. Experiments show it outperforms baselines in generating effective transformations. The analysis reveals that while debate improves stealth, it can also introduce topic shifts and complexity, highlighting its dual nature for safety data generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM安全机制忽略隐蔽有害查询/LLM safety overlooks covert harmful queries]
        C --> C1[多智能体辩论迭代优化查询/Multi-agent debate iteratively refines queries]
        D --> D1[辩论提升隐蔽性但可能引入复杂性/Debate improves stealth but may add complexity]
    ```

- **[arXiv260101] STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability**
  - **tags:** [nlp], [evaluation & metrics], [STED, consistency scoring, structured output, JSON, semantic equivalence]
  - **authors:** Guanghui Wang, Jinze Yu, Xing Zhang, Dayuan Jiang, Yin Song, Tomal Deb, Xuefeng Liu, Peiyang He
  - **institution:** AWS Generative AI Innovation Center, AWS WWSO SA Field Initiatives
  - **link:** https://arxiv.org/pdf/2512.23712
  - **contributions:** 1. Proposes STED (Semantic Tree Edit Distance), a novel similarity metric for comparing JSON outputs that balances semantic flexibility with structural strictness. 2. Introduces a comprehensive consistency scoring framework that aggregates multiple STED measurements across repeated generations to quantify LLM output reliability. 3. Provides a systematic benchmark of six LLMs using the proposed framework, revealing significant variations in model consistency and enabling practical applications like model selection and prompt refinement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2a404e6faa604b1e48c4c20d60e0b88bb889cfe94b29f60a234af8a87b5d05b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating the consistency of LLM-generated structured outputs (like JSON). It proposes a framework combining a new similarity metric (STED) and a consistency scoring method. The framework effectively benchmarks LLMs, showing Claude-3.7-Sonnet to be highly consistent, and provides tools for improving reliability in production systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability] --> B
        A --> C
        A --> D
        B[核心问题/Problem: LLM生成结构化输出的可靠性评估/Evaluating Reliability of LLM Structured Outputs]
        C[主要方法/Method: STED度量与一致性评分框架/STED Metric & Consistency Scoring Framework]
        D[关键结果/Results: STED优于现有指标，Claude-3.7-Sonnet一致性最佳/STED Outperforms Baselines, Claude-3.7-Sonnet Most Consistent]
    ```

- **[arXiv260101] PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents**
  - **tags:** [nlp], [code generation], [agent-based framework, iterative self-correction, multilingual LLM, Thought-Code-Observation loop, zero-shot]
  - **authors:** Jahidul Islam, Md Ataullha, Saiful Azad
  - **institution:** Green University of Bangladesh
  - **link:** https://arxiv.org/pdf/2512.23713
  - **code:** github.com/jahidulzaid/PyBanglaCodeActAgent
  - **contributions:** 1. Introduced BanglaCodeAct, an agent-based framework for Bangla-to-Python code generation. 2. Leveraged a multilingual LLM in a zero-shot setting without task-specific fine-tuning. 3. Demonstrated the effectiveness of an iterative Thought-Code-Observation loop for dynamic code testing and refinement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6751bddc58c50ee4806ff6af3cccc66715b2e854b912a102cbc99edee59d5c86_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating Python code from Bangla natural language instructions, a low-resource language. It proposes BanglaCodeAct, an agent-based framework that uses a multilingual LLM within an iterative Thought-Code-Observation loop for zero-shot code generation and self-correction. The method, tested with Qwen3-8B, achieves high accuracy on the mHumanEval dataset, setting a new benchmark for Bangla NL-to-Code.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PyBangla at BLP-2025 Task 2<br>论文标题/Paper Title] --> B[LLMs excel in English but not low-resource languages<br>核心问题/Problem]
        A --> C[Introduce BanglaCodeAct with multi-agent & iterative self-correction<br>主要方法/Method]
        A --> D[Qwen3-8B achieves 94.0% (dev) and 71.6% (test) pass@1<br>关键结果/Results]
    ```

- **[arXiv260101] A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation**
  - **tags:** [hpc], [computational geometry], [mesh generation, geometry preparation, CAD-to-mesh, machine learning, large language models]
  - **authors:** Steven Owen, Nathan Brown, Nikos Chrisochoides, Rao Garimella, Xianfeng Gu, Franck Ledoux, Na Lei, Roshan Quadros, Navamita Ray, Nicolas Winovich, Yongjie Jessica Zhang
  - **institution:** Sandia National Laboratories, Old Dominion University, Los Alamos National Laboratory, New York University / Stony Brook University, CEA, Dalian University of Technology, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.23719
  - **contributions:** 1. Surveys the application of AI/ML methods to automate and improve key steps in the CAD-to-mesh pipeline, such as part classification, mesh quality prediction, and defeaturing. 2. Reviews AI techniques for enhancing unstructured/block-structured meshing, volumetric parameterization, and parallel mesh generation. 3. Examines emerging tools like reinforcement learning and large language models for scripting automation in meshing workflows.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c3d8c73084ddd952c2e0c9fd20e1b7fe1d87bb02c884cf6327ca47e6ec442eb_w640_q70.webp
  - **Simple LLM Summary:** This survey paper reviews how artificial intelligence and machine learning are being applied to address bottlenecks in geometry preparation and mesh generation for engineering simulation. It explores a range of methods, from quality prediction to automation with large language models, concluding that AI serves as an assistive technology to extend traditional tools and highlights key challenges for future data-driven workflows.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CAD-to-mesh流程瓶颈 / CAD-to-mesh Pipeline Bottlenecks]
        C --> C1[AI辅助几何与网格生成 / AI-aided Geometry & Meshing]
        C --> C2[机器学习方法 / Machine Learning Methods]
        C --> C3[新兴自动化工具 / Emerging Automation Tools]
        C1 --> C1a[部件分类 / Part Classification]
        C1 --> C1b[网格质量预测 / Mesh Quality Prediction]
        C1 --> C1c[去特征化 / Defeaturing]
        C2 --> C2a[非结构化/块结构化网格 / Unstructured/Block-structured Meshing]
        C2 --> C2b[体积参数化 / Volumetric Parameterizations]
        C2 --> C2c[并行网格生成 / Parallel Mesh Generation]
        C3 --> C3a[强化学习 / Reinforcement Learning]
        C3 --> C3b[大语言模型 / Large Language Models]
        D --> D1[AI作为辅助技术 / AI as Assistive Technology]
        D --> D2[代表性方法与部署 / Representative Methods & Deployments]
        D --> D3[关键研究挑战 / Key Research Challenges]
    ```

- **[arXiv260101] When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection**
  - **tags:** [nlp], [hate speech detection], [collaborative expert judgment, confidence-based routing, class-balanced focal loss]
  - **authors:** Anwar Alajmi, Gabriele Pergola
  - **institution:** University of Warwick, Public Authority of Applied Education and Training (Kuwait)
  - **link:** https://arxiv.org/pdf/2512.23732
  - **contributions:** 1. A two-stage framework combining targeted training for noisy, imbalanced data with selective, reasoning-based inference for ambiguous cases. 2. A novel Collaborative Expert Judgment (CEJ) module that uses multiple LLM personas in a structured debate to resolve uncertain predictions. 3. A dynamic routing mechanism at inference time that directly classifies high-confidence cases and escalates low-confidence ones to the CEJ module.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d2baab37215bb5672da3307e81be99d99c0b9c3a16715fc3d5d0430dfd9273_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenges of detecting subtle, context-dependent sexist content online, which suffers from data noise, class imbalance, and conceptual ambiguity. It proposes a framework that uses robust training techniques and a novel inference module where uncertain cases are routed to a multi-persona LLM debate for judgment. This approach achieves state-of-the-art performance on benchmark sexism detection tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Subtle, ambiguous sexist content<br>Data noise, imbalance, ambiguity]
        C[主要方法/Method<br>Two-stage framework<br>Robust training & CEJ routing]
        D[关键结果/Results<br>SOTA on benchmarks<br>+2.72% F1 on EXIST]
    ```

- **[arXiv260101] Enforcing Temporal Constraints for LLM Agents**
  - **tags:** [mlsys], [agent system], [temporal constraints, SMT solving, constrained generation, formal verification, LLM agents]
  - **authors:** Adharsh Kamath, Sishen Zhang, Calvin Xu, Shubham Ugare, Gagandeep Singh, Sasa Misailovic
  - **institution:** University of Illinois at Urbana-Champaign, Meta
  - **link:** https://arxiv.org/pdf/2512.23738
  - **code:** https://github.com/structuredllm/agent-c
  - **contributions:** 1. A novel framework (Agent-C) providing runtime guarantees for LLM agents to adhere to formal temporal safety properties., 2. A domain-specific language for expressing temporal properties, which are translated to first-order logic and verified via SMT solving during token generation., 3. Demonstration of perfect safety (100% conformance) and improved task utility across real-world applications and multiple LLMs, outperforming state-of-the-art guardrails.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a978adfab8b202d7b971f6b65f8d005235baabb93f5540985ad131638c67354_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLM agents violating temporal safety policies, such as accessing data before authentication. It proposes Agent-C, a framework that uses a domain-specific language, formal logic translation, and SMT solving to enforce constraints during token generation, ensuring compliant actions. The evaluation shows Agent-C achieves 100% safety conformance and improves task utility compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enforcing Temporal Constraints for LLM Agents] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有护栏无法保证时间安全策略/Existing guardrails fail to enforce temporal safety policies]
        C --> C1[提出Agent-C框架/Propose Agent-C framework]
        C1 --> C2[使用DSL和SMT求解进行运行时验证/Use DSL & SMT solving for runtime verification]
        C2 --> C3[采用约束生成确保合规/Achieve compliance via constrained generation]
        D --> D1[100%安全性，0%危害/100% safety, 0% harm]
        D --> D2[在真实应用中提高任务效用/Improve task utility in real-world applications]
    ```

- **[arXiv260101] Towards representation agnostic probabilistic programming**
  - **tags:** [mlsys], [compiler & ir], [factor abstraction, probabilistic programming, hybrid models, representation-agnostic, factor graphs]
  - **authors:** Ole Fenske, Maximilian Popko, Sebastian Bader, Thomas Kirste
  - **institution:** Institute for Visual and Analytic Computing, University of Rostock
  - **link:** https://arxiv.org/pdf/2512.23740
  - **contributions:** 1. Introduces a factor abstraction with five fundamental operations as a universal interface for manipulating probabilistic factors. 2. Enables representation-agnostic probabilistic programming, allowing the mixing of different distribution representations (e.g., discrete tables, Gaussians, samples) within a single framework. 3. Facilitates practical inference in complex hybrid (mixed discrete-continuous) models that current toolkits cannot adequately handle.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97ac8ba28932b58d58cfedc5a8c2d53a706dd206b4f545e3d26852fb3ee19d75_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the tight coupling between model representations and inference algorithms in current probabilistic programming tools, which limits flexibility. It proposes a factor abstraction with a set of core operations to create a representation-agnostic interface. This allows users to mix various distribution representations, enabling inference in complex hybrid models previously difficult to express.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards representation agnostic probabilistic programming] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[PPLs耦合表示与推理算法/PPLs couple representations & inference]
        B --> B2[阻碍混合模型实验/Prevents hybrid model experimentation]
        C --> C1[引入因子抽象/Introduce factor abstraction]
        C --> C2[定义五个基本操作/Define five fundamental operations]
        C --> C3[创建通用接口/Create universal interface]
        D --> D1[实现表示无关编程/Enable representation-agnostic programming]
        D --> D2[支持混合表示/Support mixing representations]
        D --> D3[处理复杂混合模型/Handle complex hybrid models]
    ```

- **[arXiv260101] Break Out the Silverware -- Semantic Understanding of Stored Household Items**
  - **tags:** [ai], [commonsense reasoning], [benchmark dataset, vision-language model, hybrid agent pipeline, storage location prediction, semantic understanding]
  - **authors:** Michaela Levi-Richter, Reuth Mirsky, Oren Glickman
  - **institution:** Bar Ilan University, Tufts University
  - **link:** https://arxiv.org/pdf/2512.23739
  - **contributions:** 1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]
        Method[主要方法/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]
        Results[关键结果/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]
    ```

- **[arXiv260101] AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization**
  - **tags:** [mlsys], [agent system], [TCAD code generation, multi-agent framework, device optimization, LLM fine-tuning, DTCO]
  - **authors:** Guangxi Fan, Tianliang Ma, Xuguang Sun, Xun Wang, Kain Lu Low, Leilai Shao
  - **institution:** Shanghai Jiao Tong University, Xi’an Jiaotong–Liverpool University
  - **link:** https://arxiv.org/pdf/2512.23742
  - **contributions:** 1. Construction of an open-source, expert-curated TCAD dataset and fine-tuning of a domain-specific LLM for TCAD code generation. 2. Proposal of AgenticTCAD, a natural language-driven multi-agent framework for end-to-end automated device design and optimization. 3. Demonstration of the framework's efficiency, achieving target device specifications in 4.2 hours compared to 7.1 days for human experts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9454cdfb5c93c8b3f9587f99a9e8982d695a9783f118909a0fd90f5e347f512e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating valid TCAD simulation code due to a lack of open-source data. It proposes AgenticTCAD, a multi-agent LLM framework that automates device design and optimization from natural language. The system was validated on a 2 nm nanosheet FET design, achieving target specifications significantly faster than human experts.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("AgenticTCAD: LLM多智能体框架 / AgenticTCAD: LLM-based Multi-Agent Framework") --> Problem("TCAD代码生成资源稀缺 / Scarcity of TCAD Code Generation Resources")
        Root --> Method("构建数据集与多智能体框架 / Dataset Construction & Multi-Agent Framework")
        Root --> Results("4.2小时达到IRDS规格 / Achieves IRDS Specs in 4.2 Hours")
    ```

- **[arXiv260101] State-of-the-art Small Language Coder Model: Mify-Coder**
  - **tags:** [mlsys], [llm training], [compute-optimal training, CPT-SFT, synthetic data generation, model quantization, quality filtering]
  - **authors:** Abhinav Parmar, Abhisek Panigrahi, Abhishek Kumar Dwivedi, Abhishek Bhattacharya, Adarsh Ramachandra, Aditya Choudhary, Aditya Garg, Aditya Raj, Alankrit Bhatt, Alpesh Yadav, Anant Vishnu, Ananthu Pillai, Ankush Kumar, Aryan Patnaik, Aswatha Narayanan S, Avanish Raj Singh, Bhavya Shree Gadda, Brijesh Pankajbhai Kachhadiya, Buggala Jahnavi, Chidurala Nithin Krishna, Chintan Shah, Chunduru Akshaya, Debarshi Banerjee, Debrup Dey, Deepa R., Deepika B G, Faiz ur Rahman, Gagan Gayari, Gudhi Jagadeesh Kumar Naidu, Gursimar Singh, Harshal Tyagi, Harshini K, James Mani Vathalloor, Jayarama Nettar, Jayashree Gajjam, Joe Walter Sugil George, Kamalakara Sri Krishna Tadepalli, Kamalkumar Rathinasamy, Karan Chaurasia, Karthikeyan S, Kashish Arora, Kaushal Desai, Khushboo Buwade, Kiran Manjrekar, Malikireddy Venkata Sai Likhitha, Manjunath A, Mitali Mahavir Bedmutha, Mohammed Rafee Tarafdar, Nikhil Tiwari, Nikitha K Gigi, Pavan Ravikumar, Pendyala Swarnanjali, Piyush Anand, Prakash Chandrasekar, Prasanna Bhalchandra Gawade, Prasanth Sivan, Preeti Khurana, Priyanshi Babbar, Rajab Ali Mondal, Rajesh Kumar Vissapragada, Rajeshwari Ganesan, Rajeswari Koppisetti, Ramjee R., Ramkumar Thiruppathisamy, Rani G. S., S Reka, Samarth Gupta, Sandeep Reddy Kothakota, Sarathy K, Sathyanarayana Sampath Kumar, Saurabh Kumar, Shashank Khasare, Shenbaga Devi Venkatesh Kumar, Shiva Rama Krishna Parvatham, Shoeb Shaikh, Shrishanmathi A, Shubham Pathak, Sree Samhita Koppaka, Sreenivasa Raghavan K S, Sreeram Venkatasubramanian, Suprabha Desai Bojja, Swetha R, Syed Ahmed, Chinmai Harshitha Thota, Tushar Yadav, Veeravelly Kusumitha, V V S S Prasanth Patnaik, Vidya Sri Sesetti, Vijayakeerthi K, Vikram Raj Bakshi, Vinay K K, Vinoth Kumar Loganathan, Vipin Tiwari, Vivek Kumar Shrivastav, V Venkata Sri Datta Charan, Wasim Akhtar Khan
  - **institution:** Infosys AI Research
  - **link:** https://arxiv.org/pdf/2512.23747
  - **contributions:** 1. Introduced Mify-Coder, a 2.5B-parameter code model trained with a compute-optimal strategy on 4.2T tokens, demonstrating that compact models can match frontier-grade performance. 2. Developed a training pipeline combining high-quality curated data with agentically generated synthetic data, refined using enterprise-grade evaluations and LLM-based quality filtering for high data density. 3. Showed that disciplined exploration of training objectives and data mixtures within a single continuous trajectory enables competitive accuracy, efficiency, and safety, with quantized variants enabling deployment on standard hardware.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c8af9d7668bbc4ac7ae9ad0ed379feb93f982a5fecdc8491169bd4d6c33d30_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost of large code models by proposing Mify-Coder, a compact 2.5B-parameter model trained using a compute-optimal strategy that integrates curated and synthetic data with quality filtering. It demonstrates that this approach allows a small model to achieve performance comparable to much larger models on coding benchmarks while maintaining safety and enabling efficient desktop deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mify-Coder] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[大模型成本高/High cost of large code models]
        C --> C1[计算最优训练/Compute-optimal training]
        C --> C2[合成数据生成/Synthetic data generation]
        C --> C3[质量过滤/Quality filtering]
        D --> D1[性能可比/Competitive performance]
        D --> D2[高效部署/Efficient deployment]
    ```

- **[arXiv260101] Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding**
  - **tags:** [mlsys], [agent system], [neuro-symbolic AI, multi-agent framework, local inference, hallucination detection, deterministic fallback]
  - **authors:** Yunguo Yu
  - **institution:** Zyter|TruCare
  - **link:** https://arxiv.org/pdf/2512.23743
  - **contributions:** 1. A hybrid neuro-symbolic multi-agent framework (Hybrid-Code) for reliable, on-premise clinical coding that combines an LLM-based Coder with a deterministic fallback and a symbolic Auditor for verification. 2. A privacy-preserving architecture ensuring no patient data leaves the hospital firewall, addressing critical deployment barriers in healthcare. 3. Demonstration that system reliability through architectural redundancy (achieving 0% hallucinations within the knowledge base) is more valuable than pure model performance for production healthcare AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b1a7891d43346fbec0638fdd7970002ba8ac5fe529a855a4c41f35f8cc8ad1e_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Hybrid-Code, a framework for automated clinical coding that runs locally to preserve privacy. It uses a two-agent system where a Coder attempts semantic reasoning with a local LLM but falls back to keyword matching, and an Auditor verifies codes against a knowledge base to prevent hallucinations. The key conclusion is that this redundant, hybrid approach ensures production reliability where failures are unacceptable, even with a moderate coverage rate.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hybrid-Code / Hybrid-Code] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[云LLM存在隐私与延迟风险 / Cloud LLMs pose privacy & latency risks]
        C --> C1[混合神经符号多智能体框架 / Hybrid Neuro-Symbolic Multi-Agent Framework]
        C1 --> C2[编码器: LLM推理 + 确定性回退 / Coder: LLM + Deterministic Fallback]
        C1 --> C3[审计器: 基于知识的验证 / Auditor: Knowledge-Based Verification]
        D --> D1[0% 知识库内幻觉 / 0% Hallucination within KB]
        D --> D2[34.11% 覆盖率 / 34.11% Coverage]
        D --> D3[无数据离开防火墙 / No Data Leaves Firewall]
    ```

- **[arXiv260101] Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents**
  - **tags:** [ai], [one-shot learning], [Coordinate Matrix Machine, structural intelligence, Green AI, lazy learning, glass-box model]
  - **authors:** Amin Sadri, M Maruf Hossain
  - **institution:** Not explicitly stated (email domains are personal: gmail.com)
  - **link:** https://arxiv.org/pdf/2512.23749
  - **code:** GitHub Repository (URL not fully specified in provided text)
  - **contributions:** Proposes the Coordinate Matrix Machine (CM^2) for one-shot document classification, Introduces a structural coordinate-based approach as an alternative to semantic vectorization, Designs a "Green AI" model optimized for CPU use with inherent explainability
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acd9104b41dbc3c7f9d1597d31b940424a078e93beb2b2b21007c741209b006_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of human-level concept learning, where machines require many examples to learn a concept. It proposes the Coordinate Matrix Machine (CM^2), a purpose-built model that learns document structures to classify very similar documents using only one sample per class. The method is presented as a "Green AI" solution that outperforms traditional models while being computationally efficient and explainable.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents] --> B[核心问题/Problem: Human-level Concept Learning Gap]
        A --> C[主要方法/Method: Coordinate Matrix Machine (CM^2)]
        A --> D[关键结果/Results: High Accuracy, One-shot Learning, Green AI]
        B --> B1[人类单样本学习/Human one-shot learning]
        B --> B2[机器需大量样本/Machine needs many samples]
        C --> C1[学习文档结构/Learns document structure]
        C --> C2[识别重要特征/Identifies important features]
        D --> D1[高精度与低数据/High accuracy with minimal data]
        D --> D2[CPU优化与可解释性/CPU-optimized & explainable]
    ```

- **[arXiv260101] Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation**
  - **tags:** [ai], [uncertainty quantification], [Evidential Deep Learning, Subjective Logic, Activation Functions, Regularization, Learning Dynamics]
  - **authors:** Deep Shankar Pandey, Hyomin Choi, Qi Yu
  - **institution:** Rochester Institute of Technology, InterDigital
  - **link:** https://arxiv.org/pdf/2512.23753
  - **contributions:** 1. Theoretically characterizes the activation-dependent "learning-freeze" behavior in evidential deep learning models, where gradients vanish in low-evidence regions. 2. Designs a general family of activation functions and corresponding evidential regularizers to enable consistent evidence updates across different activation regimes. 3. Empirically validates the proposed theory and method through extensive experiments on multiple benchmark classification, few-shot classification, and blind face restoration tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fae8a70028f83294a153c8fd9b9083e99f87bf5f04f2fb8d64ce2fbab74beb82_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies and theoretically analyzes a "learning-freeze" problem in Evidential Deep Learning (EDL) models caused by specific activation functions. To solve this, the authors propose a generalized family of activation functions and regularizers. Extensive experiments show the proposed method improves learning dynamics and effectiveness across various tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Generalized Regularized Evidential Deep Learning Models") --> Problem("核心问题/Problem: Activation functions in EDL cause learning-freeze in low-evidence regions")
        Root --> Method("主要方法/Method: Design a general family of activation functions and evidential regularizers")
        Root --> Results("关键结果/Results: Theory validated; method effective across multiple benchmarks")
    ```

- **[arXiv260101] Geometric Scaling of Bayesian Inference in LLMs**
  - **tags:** [nlp], [interpretability], [Bayesian inference, geometric scaling, attention mechanism, value manifolds, predictive entropy]
  - **authors:** Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra
  - **institution:** Dream Sports, Columbia University
  - **link:** https://arxiv.org/pdf/2512.23752
  - **contributions:** 1. Demonstrates that production-grade LLMs (Pythia, Phi-2, Llama-3, Mistral) preserve a geometric substrate (low-dimensional value manifolds) similar to that enabling exact Bayesian inference in small, controlled "wind-tunnel" models. 2. Shows that the dominant axis of last-layer value representations strongly correlates with predictive entropy, and domain-restricted prompts collapse the structure into the same low-dimensional manifolds. 3. Through targeted interventions on the entropy-aligned axis, reveals that this geometry is a privileged readout of uncertainty rather than a singular computational bottleneck for Bayesian-like behavior.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e11ae0519509ba1ae43d1087dfc56ed0f799df57a2ee4fe7c4a6a7f20eb11c3f_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether the geometric structures that enable exact Bayesian inference in small, controlled transformer models persist in large-scale production language models. The authors find that models like Llama-3 and Mistral organize their value representations along an entropy-correlated axis, forming similar low-dimensional manifolds. They conclude that modern LLMs preserve this geometric substrate for approximate Bayesian updates, though it acts more as a readout mechanism than a sole computational bottleneck.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometric Scaling of Bayesian Inference in LLMs] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Do geometric structures for Bayesian inference persist in production LLMs?]
        C[主要方法/Method<br>Analyze value representations & perform targeted axis interventions]
        D[关键结果/Results<br>Geometry persists as a privileged uncertainty readout]
    ```

- **[arXiv260101] HINTS: Extraction of Human Insights from Time-Series Without External Sources**
  - **tags:** [ai], [time series forecasting], [self-supervised learning, opinion dynamics, attention mechanism, latent factor extraction, residual analysis]
  - **authors:** Sheo Yon Jhin, Noseong Park
  - **institution:** KAIST
  - **link:** https://arxiv.org/pdf/2512.23755
  - **contributions:** 1. Proposes HINTS, a novel self-supervised framework that extracts latent human factors (e.g., sentiment, influence) endogenously from time series residuals without requiring external data sources like news or social media. 2. Introduces the use of the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns within the time series data. 3. Demonstrates that integrating the extracted human factors as an attention map into a state-of-the-art backbone model consistently improves forecasting accuracy across multiple datasets and provides interpretable insights aligned with real-world events.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a97eb229421de0a13cd23a1f8c66b8e7b1ac081ecf63d3e2a393fa375ac5f65_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high cost of using external data to model human factors in time series forecasting. It proposes HINTS, a self-supervised learning framework that extracts latent human insights directly from time series residuals using an opinion dynamics model as inductive bias. The method improves forecasting accuracy and provides interpretable factors aligned with real events, validated on nine real-world and benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HINTS: Extraction of Human Insights from Time-Series] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[外部数据依赖成本高/High cost of external data dependency]
        C --> C1[从残差中自监督提取人类因素/Self-supervised extraction from residuals]
        C --> C2[使用意见动力学作为归纳偏置/Using opinion dynamics as inductive bias]
        C --> C3[集成到注意力机制中/Integrated as attention map]
        D --> D1[预测精度提升/Forecasting accuracy improved]
        D --> D2[可解释性与现实事件对齐/Interpretability aligned with real events]
    ```

- **[arXiv260101] Drift-Based Dataset Stability Benchmark**
  - **tags:** [mlsys], [communication & networking], [concept drift, dataset stability, traffic classification, benchmark, feature weights]
  - **authors:** Dominik Soukup, Richard Plný, Daniel Vašata, Tomáš Čejka
  - **institution:** Czech Technical University in Prague, CESNET a.l.e.
  - **link:** https://arxiv.org/pdf/2512.23762
  - **contributions:** 1. A novel methodology for evaluating dataset stability based on concept drift detection and ML feature weights. 2. A benchmark workflow for comparing datasets and identifying their weak points. 3. A demonstration and initial benchmark of the framework on the CESNET-TLS-Year22 dataset, showing its use for dataset optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc4badd4590db607d7056063e44a30285afee3c4bd25f3f6b231fbc0932dd8de_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of model degradation in network traffic classification due to data/concept drift. It proposes a new framework that uses a concept drift detection method enhanced with ML feature weights to benchmark dataset stability. The method is demonstrated on a real-world TLS dataset, providing insights for dataset optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Drift-Based Dataset Stability Benchmark] --> B[核心问题/Problem: Model degradation from data drift in network traffic classification]
        A --> C[主要方法/Method: Concept drift detection boosted by ML feature weights for dataset benchmarking]
        A --> D[关键结果/Results: Initial stability benchmark for CESNET-TLS-Year22 dataset, showing optimization impact]
    ```

- **[arXiv260101] Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning**
  - **tags:** [mlsys], [llm inference], [speculative decoding, entropy penalty, training-free, reasoning acceleration, draft-model verification]
  - **authors:** Tiancheng Su, Meicong Zhang, Guoxiu He
  - **institution:** East China Normal University
  - **link:** https://arxiv.org/pdf/2512.23765
  - **contributions:** 1. Proposes Entropy-Aware Speculative Decoding (EASD), a training-free method that introduces a dynamic entropy-based penalty to reject low-confidence draft tokens, 2. Enables speculative decoding to potentially surpass the target model's performance by incorporating draft-model verification and preventing error propagation, 3. Demonstrates that EASD maintains efficiency comparable to standard speculative decoding while improving reasoning accuracy across multiple benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/326c86ae03e220a7cb48737a0a6fe149bd4384ccc08f3113a051c3548bc2d30e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of speculative decoding being constrained by the target model's performance. It proposes Entropy-Aware Speculative Decoding (EASD), which uses entropy to quantify uncertainty and reject low-confidence draft tokens. Experiments show EASD outperforms existing methods and can surpass the target LLM's performance while maintaining comparable efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Entropy-Aware Speculative Decoding] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[SD性能受限于目标模型/SD performance capped by target model]
        Method[主要方法/Method] --> M1[引入动态熵惩罚/Introduce dynamic entropy penalty]
        Method --> M2[基于不确定性拒绝低置信度令牌/Reject low-confidence tokens based on uncertainty]
        Method --> M3[目标模型重采样/Target model re-sampling]
        Results[关键结果/Results] --> R1[超越现有SD方法/Outperforms existing SD methods]
        Results --> R2[常超越目标LLM本身/Often surpasses target LLM]
        Results --> R3[效率与SD相当/Efficiency comparable to SD]
    ```

- **[arXiv260101] Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory**
  - **tags:** [mlsys], [agent system], [skill graph, verifiable rewards, continual memory, experience synthesis, audit logging]
  - **authors:** Ken Huang, Jerry Huang
  - **institution:** DistributedApps.ai, OWASP, Kleiner Perkins
  - **link:** https://arxiv.org/pdf/2512.23760
  - **contributions:** 1. Proposes the Audited Skill-Graph Self-Improvement (ASG-SI) framework, which treats agent self-improvement as the iterative compilation of an auditable, growing skill graph. 2. Introduces a verifier-auditor mechanism that uses replayable evidence and decomposed rewards to gate skill promotion, enabling independent audit and governance. 3. Integrates experience synthesis for scalable testing and continual memory control to manage context growth and preserve long-horizon performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd2b1c94a27644963b1a77d560eb3715fa72ea6468dc6c4e39e42eb5e040187_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses security and governance challenges in self-improving AI agents, such as reward hacking and opaque behavioral drift. It proposes the ASG-SI framework, which compiles agent improvements into an auditable skill graph verified by replayable evidence. The approach reframes self-improvement as the accumulation of verifiable, reusable capabilities for reproducible evaluation and operational governance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Audited Skill-Graph Self-Improvement (ASG-SI) / 审计技能图自我改进"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem"] --> P1["部署的自我改进代理存在安全与治理挑战 / Deployed self-improving agents pose security & governance challenges"]
        P1 --> P2["奖励黑客行为、行为漂移、不透明的更新 / Reward hacking, behavioral drift, opaque updates"]
        Method["主要方法/Method"] --> M1["ASG-SI 框架 / ASG-SI Framework"]
        M1 --> M2["将改进编译为可审计技能图 / Compile improvements into auditable skill graph"]
        M2 --> M3["基于验证器的证据和可分解奖励进行技能提升 / Verifier-backed evidence & decomposed rewards gate skill promotion"]
        M3 --> M4["集成经验合成和持续记忆控制 / Integrate experience synthesis & continual memory control"]
        Results["关键结果/Results"] --> R1["提供可验证、可重用的能力积累 / Provides accumulation of verifiable, reusable capabilities"]
        R1 --> R2["为自我改进AI提供可复现评估和操作治理的路径 / Offers path for reproducible evaluation & operational governance of self-improving AI"]
    ```

- **[arXiv260101] Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics**
  - **tags:** [mlsys], [on-device ai], [FPGA acceleration, model recovery, hardware-software co-design, GRU, Neural ODE]
  - **authors:** Bin Xu, Ayan Banerjee, Sandeep Gupta
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.23767
  - **contributions:** 1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enabling Physical AI at the Edge<br>在边缘实现物理人工智能] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Model recovery methods (Neural ODEs) are inefficient for edge hardware<br>模型恢复方法在边缘硬件上效率低下]
        C[主要方法/Method<br>MERINDA: FPGA-accelerated, hardware-friendly framework<br>MERINDA: FPGA加速的硬件友好框架]
        D[关键结果/Results<br>114x lower energy, 28x smaller memory, 1.68x faster training<br>能耗降低114倍, 内存占用减少28倍, 训练速度提升1.68倍]
    ```

- **[arXiv260101] Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations**
  - **tags:** [ai], [algorithmic fairness], [discrimination clustering, individual fairness, hybrid verification, SMT solver, MILP solver]
  - **authors:** Ranit Debnath Akash, Ashish Kumar, Verya Monjezi, Ashutosh Trivedi, Gang, Saeid Tizpaz-Niari
  - **institution:** University of Illinois Chicago, University of Colorado Boulder, Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.23769
  - **contributions:** 1. Introduced the concept of "discrimination clustering" as a generalization of individual fairness to uncover systematic bias patterns. 2. Proposed HyFair, a hybrid technique combining formal symbolic analysis (SMT/MILP) and randomized search for both certification and violation discovery. 3. Developed a novel explanation method to generate interpretable, decision-tree-style artifacts for inputs exhibiting high discrimination.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05da1223f6a1c9a93634f021d221ac5175d00dee272ded0b8782834941db9c55_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a limitation in individual fairness, which only detects isolated unfairness, and proposes the concept of "discrimination clustering" to uncover systematic bias patterns. It introduces HyFair, a hybrid method combining formal verification and randomized search to detect these clusters and generate explanations. Experiments show HyFair outperforms existing fairness verification and explanation methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Uncovering Discrimination Clusters") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("个体公平性检查的局限性/Limitations of individual fairness checks")
        P1 --> P2("无法捕捉系统性歧视模式/Fails to capture systematic bias patterns")
        Method --> M1("提出歧视聚类概念/Propose discrimination clustering concept")
        Method --> M2("开发HyFair混合技术/Develop HyFair hybrid technique")
        M2 --> M3("结合形式分析与随机搜索/Combine formal analysis & randomized search")
        Results --> R1("优于现有方法/Outperforms state-of-the-art methods")
        Results --> R2("揭示系统性偏差/Reveals substantial discrimination clustering")
        Results --> R3("提供可解释的说明/Provides intuitive explanations")
    ```

- **[arXiv260101] Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions**
  - **tags:** [ai], [safe reinforcement learning], [constrained MDP, trust region policy optimization, natural policy gradient, safety gymnasium, hard constraints]
  - **authors:** Ankit Kanwar, Dominik Wagner, Luke Ong
  - **institution:** Sony Corporation, Nanyang Technological University (NTU Singapore)
  - **link:** https://arxiv.org/pdf/2512.23770
  - **contributions:** 1. Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new algorithm for hard-constrained RL that adaptively biases policy updates towards safety while seeking reward improvement. 2. Introduces a trust-region update using a convex combination of the natural policy gradients of cost and reward to ensure a fixed fraction of optimal cost reduction per step. 3. Provides a theoretical guarantee of local progress towards safety and demonstrates superior balance of safety and task performance on Safety Gymnasium benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea6c2a84f6cc7b4f3144847fc78a84736f7247b99f773ed23dd1861f2ff0760_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of reinforcement learning under hard safety constraints, where existing methods struggle to avoid violations without sacrificing reward. It proposes SB-TRPO, an algorithm that performs trust-region updates by combining reward and cost gradients to bias updates towards safety. Experiments show that SB-TRPO achieves a better balance of safety and task completion than state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Safety-Biased Policy Optimisation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: RL in safety-critical domains requires strict constraint adherence without sacrificing reward performance.]
        Method[主要方法/Method: SB-TRPO uses convex combination of natural policy gradients for cost and reward in trust-region updates.]
        Results[关键结果/Results: Achieves best balance of safety and task completion on Safety Gymnasium benchmarks.]
    ```

- **[arXiv260101] A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms**
  - **tags:** [ai], [anomaly detection], [Graph Neural Networks (GNNs), Fraud Detection, Class Imbalance, Fraudulent Camouflage, Ride-Hailing Platforms]
  - **authors:** Kanishka Hewageegana, Janani Harischandra, Nipuna Senanayake, Gihan Danansuriya, Kavindu Hapuarachchi, Pooja Illangarathne
  - **institution:** Informatics Institute of Technology, Rajarata University, University of Sri Jayewardenepura
  - **link:** https://arxiv.org/pdf/2512.23777
  - **contributions:** 1. Provides a structured overview and comparison of existing Graph Neural Network (GNN) architectures and methodologies for fraud detection in ride-hailing platforms. 2. Highlights and analyzes key challenges in the domain, specifically class imbalance and fraudulent camouflage, within the ride-hailing ecosystem. 3. Identifies significant methodological progress and research gaps, calling for further exploration into real-world applicability and technical improvements.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65fc7f6ff0330c22a6dc35373a15256baac5eef984a9100cce967991d93a1e67_w640_q70.webp
  - **Simple LLM Summary:** This survey investigates the use of Graph Neural Networks (GNNs) for detecting fraud in ride-hailing platforms. It analyzes and compares various GNN models, focusing on their effectiveness in handling complex relational data and challenges like class imbalance. The paper concludes by identifying progress and gaps in the field, advocating for more research on real-world applications and technical enhancements.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms"] --> Problem["核心问题/Problem: Fraud detection in ride-hailing platforms"]
        Root --> Method["主要方法/Method: Survey and analysis of Graph Neural Networks (GNNs)"]
        Root --> Results["关键结果/Results: Identifies progress, gaps, and calls for future work"]
    ```

- **[arXiv260101] Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark**
  - **tags:** [sec], [adversarial attacks on llms], [denial-of-service, over-generation, black-box attack, evolutionary search, reinforcement learning]
  - **authors:** Manu, Yi Guo, Jo Plested, Tim Lynar, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jack Yang, Wangli Yang
  - **institution:** Western Sydney University, University of New South Wales Canberra, The University of Sydney, University of Wollongong
  - **link:** https://arxiv.org/pdf/2512.23779
  - **contributions:** 1. Introduces a black-box, query-only benchmark for evaluating prompt-induced denial-of-service attacks on LLMs. 2. Proposes two novel prompt-only attackers: an evolutionary search method (EOGen) and a goal-conditioned reinforcement learning method (RL-GOAL). 3. Defines the Over-Generation Factor (OGF) as a key metric to quantify attack success and characterize model vulnerability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3002b15ce3957d88befc241c59f1be73a9e49f4e8ad4c345e9d472f11883059e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of denial-of-service attacks on large language models via prompt-induced over-generation. It proposes a standardized black-box benchmark and two automated attack methods, EOGen and RL-GOAL, to find adversarial prefixes that delay model termination. The results show that the RL-GOAL attacker is particularly effective at forcing models to generate excessively long outputs, highlighting a significant vulnerability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Prompt-Induced Over-Generation as Denial-of-Service<br/>提示诱导过度生成作为拒绝服务攻击] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>LLM过度生成导致服务拒绝、延迟和成本增加]
        C[主要方法/Method<br/>黑盒基准与两种攻击者: EOGen(进化搜索)和RL-GOAL(强化学习)]
        D[关键结果/Results<br/>RL-GOAL攻击者实现更高的平均过度生成因子(OGF)]
    ```

- **[arXiv260101] FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading**
  - **tags:** [ai], [reinforcement learning], [ensemble reinforcement learning, selective update, variational autoencoder, high-frequency trading, risk management]
  - **authors:** Molei Qin, Xinyu Cai, Yewen Li, Haochong Xia, Chuqiao Zong, Shuo Sun, Xinrun Wang, Bo An
  - **institution:** Nanyang Technological University, Singapore Management University, Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.23773
  - **contributions:** 1. A selective update mechanism for ensemble Q-learners using ensemble TD errors to stabilize training and improve convergence in high-leverage environments. 2. A risk-aware filtering and routing mechanism that uses VAEs to model market state dynamics and identify agent capability boundaries, enabling dynamic policy selection to mitigate risk. 3. A novel three-stage ensemble RL framework (FineFT) that integrates stable training and risk management, demonstrating superior profitability and over 40% risk reduction in crypto futures trading.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c036ba975592c2c3be9068e742ccd28ed5b9722ff62085fbcc37e9f3627fe370_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FineFT, a three-stage ensemble reinforcement learning framework designed to address the challenges of high leverage and unseen market states in futures trading. The method uses selective updates for stable training and VAEs for risk-aware policy routing, achieving higher profitability and significantly lower risk compared to state-of-the-art baselines in high-frequency crypto futures experiments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FineFT: Efficient and Risk-Aware Ensemble RL for Futures Trading] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[高杠杆放大波动/High leverage amplifies reward fluctuations]
        B --> B2[缺乏能力边界意识/Lack of self-awareness of capability boundaries]
        C --> C1[阶段I: 选择性更新/Stage I: Selective Update]
        C --> C2[阶段II: 过滤与VAE训练/Stage II: Filtering & VAE Training]
        C --> C3[阶段III: 动态路由/Stage III: Dynamic Routing]
        D --> D1[超越12个SOTA基线/Outperforms 12 SOTA baselines]
        D --> D2[风险降低超40%/Risk reduced by >40%]
        D --> D3[实现更高盈利/Achieves superior profitability]
    ```

- **[arXiv260101] Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems**
  - **tags:** [mlsys], [federated learning], [Zero-Trust Architecture, SHAP-weighted aggregation, TPM-based attestation]
  - **authors:** Samaresh Kumar Singh, Joyjit Roy, Martin So
  - **institution:** Independent Researchers (based on provided affiliations: IEEE Senior Member in Texas, IEEE Member in Texas, Independent Researcher in Canada)
  - **link:** https://arxiv.org/pdf/2512.23809
  - **contributions:** 1) Proposed a hierarchical edge-fog-cloud zero-trust federated learning architecture for trusted agent participation. 2) Introduced a novel SHAP-weighted aggregation algorithm for explainable Byzantine detection in non-IID environments. 3) Integrated TPM-based cryptographic attestation and on-device adversarial training into a defense-in-depth framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baa785fc442fcbc6a80214c4fdc6361e67a8e34e5a9bb6f5dd8fb34baf21bb68_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses security vulnerabilities in Federated Learning for Industrial IoT by proposing ZTA-FL, a framework combining zero-trust agent authentication, explainable Byzantine-resilient aggregation, and on-device adversarial training. It demonstrates high detection accuracy and robustness against attacks on intrusion detection benchmarks while reducing communication overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: IIoT安全漏洞与联邦学习攻击 / IIoT Security Gaps & FL Attacks]
        Method[主要方法/Method: 零信任认证与可解释聚合 / Zero-Trust Attestation & Explainable Aggregation]
        Results[关键结果/Results: 高检测精度与抗攻击鲁棒性 / High Detection Accuracy & Attack Robustness]
    ```

- **[arXiv260101] StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection**
  - **tags:** [nlp], [mental health text classification], [transfer learning, continual training, RoBERTa, cross-condition, stress detection]
  - **authors:** Amal Alqahtani, Efsun Kayi, Mona Diab
  - **institution:** The George Washington University, King Saud University, Johns Hopkins University Applied Physics Laboratory, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.23813
  - **contributions:** 1. Proposes StressRoBERTa, a cross-condition transfer learning approach for detecting self-reported chronic stress in tweets. 2. Demonstrates that continual training on a focused set of clinically related mental health conditions (depression, anxiety, PTSD) improves stress detection over general models. 3. Shows effective transfer from clinical mental health contexts to situational stress discussions via evaluation on the Dreaddit dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a57224b94917298d3e6f94d2c9255d2d054abcab5c5051070f928ff8ddd8bc6b_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces StressRoBERTa, a method that continually trains a RoBERTa model on social media text from users with depression, anxiety, and PTSD before fine-tuning it for chronic stress detection. The approach outperforms the previous best system on the SMM4H 2022 shared task by 3% F1-score, showing that focused cross-condition transfer learning from related disorders provides stronger representations for stress detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[StressRoBERTa: Cross-Condition Transfer Learning] --> B(核心问题/Problem: 检测社交媒体中的慢性压力/Detect chronic stress on social media)
        A --> C(主要方法/Method: 从相关心理健康状况进行跨条件迁移学习/Cross-condition transfer learning from related mental health conditions)
        A --> D(关键结果/Results: 性能超越最佳共享任务系统，F1分数达82%/Outperforms best shared task system with 82% F1)
    ```

- **[arXiv260101] Improved Bounds for Private and Robust Alignment**
  - **tags:** [ai], [preference learning], [private alignment, robust alignment, uniform convergence, log loss, adversarial corruption]
  - **authors:** Wenqian Weng, Yi He, Xingyu Zhou
  - **institution:** Wayne State University
  - **link:** https://arxiv.org/pdf/2512.23816
  - **contributions:** 1. Showed that standard private MLE-type log loss can achieve near-optimal rates for private alignment, contrary to prior belief. 2. Demonstrated that existing offline algorithms for joint privacy-and-corruption provide stronger guarantees than previously known, leading to improved bounds for corruption-only settings. 3. Presented the first set of theoretical results for private and robust online alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07d766123762454b64f45852c98c882b263a3fe86efdee6b0c39b70b0d888215_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the theoretical alignment of language models under privacy constraints and adversarial corruption. It shows that a standard MLE-style log loss can achieve near-optimal rates for private alignment and provides improved bounds for joint private-and-robust settings, including the first online results, enabled by new uniform convergence guarantees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improved Bounds for Private and Robust Alignment] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[语言模型对齐/Language Model Alignment]
        B --> B2[隐私与噪声标签/Private & Noisy Labels]
        C --> C1[理论分析/Theoretical Analysis]
        C --> C2[统一收敛/Uniform Convergence]
        D --> D1[私有MLE达到最优/Private MLE Near-Optimal]
        D --> D2[离线和在线改进界限/Improved Offline & Online Bounds]
    ```

- **[arXiv260101] Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments**
  - **tags:** [cv], [human pose estimation and action analysis], [video-based assessment, 2D skeleton extraction, Cognitive Task Analysis (CTA), performance metrics, synthetic training environments]
  - **authors:** Surya Rayala, Marcos Quinones-Grueiro, Naveeduddin Mohammed, Ashwin T S, Benjamin Goldberg, Randall Spain, Paige Lawton, Gautam Biswas
  - **institution:** Vanderbilt University, US Army DEVCOM Soldier Center
  - **link:** https://arxiv.org/pdf/2512.23819
  - **contributions:** 1. A video-based assessment pipeline that extracts performance analytics (2D skeletons, gaze vectors, trajectories) from training videos without extra hardware. 2. Development of task-specific metrics for psychomotor fluency, situational awareness, and team coordination, integrated into an extended Cognitive Task Analysis hierarchy. 3. Demonstration of the approach via a case study on real-world Enter and Clear the Room drills and discussion of its integration into After Action Review systems like Gamemaster and GIFT.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33d390da3c0f68240f7cda6efac9e31e941e99ab611d61cd2837f0352453d12c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automatically and objectively assessing soldier performance in synthetic training environments. It proposes a video-based pipeline using computer vision to extract movement and gaze data, from which it derives specific performance metrics for cognitive and teamwork skills. The method is demonstrated on real-world drills and shows potential for scalable, hardware-free evaluation to support training feedback.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Video-Based Performance Evaluation for ECR Drills") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("传统评估方法受限/Traditional assessment limited")
        P1 --> P1_1("依赖昂贵传感器/Relies on costly sensors")
        P1 --> P1_2("主观人为观察/Subjective human observation")
        Method --> M1("视频分析管道/Video-based pipeline")
        M1 --> M1_1("提取2D骨架、视线、轨迹/Extract 2D skeletons, gaze, trajectories")
        M1 --> M1_2("开发任务特定指标/Develop task-specific metrics")
        M1 --> M1_3("扩展认知任务分析/Extended Cognitive Task Analysis")
        Results --> R1("案例研究验证/Case study validation")
        Results --> R2("支持行动后评估/Supports After Action Reviews")
        Results --> R3("未来: 3D分析/Future: 3D analysis")
    ```

- **[arXiv260101] Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation**
  - **tags:** [nlp], [adversarial robustness], [mechanistic interpretability, attention layers, adversarial examples, LLM evaluation, token substitution]
  - **authors:** Kaustubh Dhole
  - **institution:** Emory University
  - **link:** https://arxiv.org/pdf/2512.23837
  - **contributions:** 1. Proposes a novel adversarial example generation method that exploits intermediate attention layer token distributions, contrasting with prompt-based or gradient-based attacks. 2. Introduces two specific attention-based generation techniques: attention-based token substitution and attention-based conditional generation. 3. Empirically demonstrates that such adversarial examples can degrade performance on an evaluation task (argument quality assessment) while maintaining semantic similarity, highlighting both the promise and limitations (e.g., grammatical degradation) of the approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new method to generate adversarial examples by extracting token predictions from the intermediate attention layers of LLMs, leveraging their iterative refinement property. The approach is used to stress-test LLM-based evaluation pipelines, showing it can cause performance drops on an argument quality task while preserving semantics, though grammatical issues can arise. The findings illustrate the potential and current constraints of using internal model representations for adversarial testing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Can intermediate attention layers be used to generate adversarial examples for LLM evaluation?]
        Method[主要方法/Method: Leverage attention-layer token distributions for token substitution/conditional generation]
        Results[关键结果/Results: Adversarial examples cause performance drop but may introduce grammatical issues]
    ```

- **[arXiv260101] Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms**
  - **tags:** [nlp], [bias detection], [SHAP, transformer, interpretability, false positives, domain adaptation]
  - **authors:** Himel Ghosh
  - **institution:** Technical University of Munich, Sapienza University of Rome
  - **link:** https://arxiv.org/pdf/2512.23835
  - **contributions:** 1. Conducted a comparative interpretability study of two transformer-based bias detection models using SHAP to analyze their decision mechanisms. 2. Revealed that a standard bias detector model exhibits a misalignment between attribution strength and prediction correctness, leading to systematic over-flagging, while a domain-adapted model produces significantly fewer false positives. 3. Demonstrated that model errors, particularly false positives, arise from discourse-level ambiguity rather than explicit bias cues, highlighting distinct linguistic failure modes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cbe893cabdbb4c14e3f0b2a14a91011067d2ee0ee4225b0a232eb5591a1b743_w640_q70.webp
  - **Simple LLM Summary:** This paper compares how two transformer models detect bias in news text using SHAP-based explanations. It finds that while both models focus on similar evaluative language, a domain-adapted model integrates these signals more reliably, producing far fewer false positives than a standard bias detector. The study concludes that interpretability analysis is crucial for evaluating bias detection systems and that architectural choices critically impact their reliability for journalistic use.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Explaining News Bias Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[How do bias detection models make decisions?]
        C --> C1[Comparative SHAP analysis of two transformer models]
        D --> D1[Domain-adapted model has better alignment and fewer false positives]
        D --> D2[False positives driven by discourse ambiguity]
    ```

- **[arXiv260101] Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [adaptive prompting, context window, open-domain QA, retrieval-augmented generation, LLM ignorance]
  - **authors:** Dingmin Wang, Ji Ma, Shankar Kumar
  - **institution:** Google Research, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.23836
  - **contributions:** 1. Proposes an adaptive prompting strategy for RAG that splits retrieved information into smaller chunks for sequential processing, mitigating the noise from irrelevant information in long contexts. 2. Demonstrates experimentally that this strategy matches or outperforms standard prompting on open-domain QA datasets while using fewer tokens. 3. Identifies and analyzes a key failure mode where LLMs generate incorrect answers instead of declining when information is insufficient, highlighting a critical area for future research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that longer context windows in Retrieval-Augmented Generation (RAG) introduce irrelevant information, degrading LLM performance. It proposes an adaptive prompting strategy that processes retrieved text in smaller, sequential chunks, achieving comparable accuracy with lower token usage. The study concludes that a major source of error is the LLM's tendency to generate wrong answers rather than admit ignorance, pointing to the need for improved refusal capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[长上下文引入无关信息，降低LLM性能/Long contexts introduce irrelevant info, degrading LLM performance]
        C --> C1[自适应提示策略：分块顺序处理/Adaptive prompting: sequential chunk processing]
        D --> D1[性能相当，使用更少token/Matches performance, uses fewer tokens]
        D --> D2[LLM常生成错误答案而非拒绝/LLM often generates wrong answers instead of declining]
    ```

- **[arXiv260101] Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education**
  - **tags:** [other], [AI in Education], [AI literacy, teacher perceptions, quantitative survey, ethics, infrastructure challenges]
  - **authors:** Bruno Florentino, Camila Sestito, Wellington Cruz, André de Carvalho, Robson Bonidia
  - **institution:** University of São Paulo, Federal University of Technology-Paraná (UTFPR), Instituto Significare
  - **link:** https://arxiv.org/pdf/2512.23834
  - **contributions:** 1. Provides empirical data on the AI literacy levels and application interests of Brazilian K-12 teachers, revealing a high interest despite low knowledge. 2. Identifies key structural barriers (lack of training, technical support, and infrastructure) to AI adoption in Brazilian public education. 3. Highlights the critical importance teachers place on discussing ethics, digital citizenship, and responsible AI use within the pedagogical context.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9fd8e32651f0740b552d9443daf7c424819558f55a942221ff741ca45c296c9_w640_q70.webp
  - **Simple LLM Summary:** This study quantitatively analyzes Brazilian K-12 teachers' perceptions of AI in education through a survey of 346 educators. The results show strong teacher interest in using AI for pedagogical tasks despite limited knowledge, while identifying significant structural challenges and emphasizing the need for ethical discussions. The study concludes that effective AI integration in Brazil requires integrated public policies, teacher training, and equitable access to technology.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(巴西教师对AI的认知与态度/Brazilian Teachers' Perceptions and Attitudes towards AI)
        Problem --> P2(AI在教育中的伦理与公平挑战/Ethical and Equity Challenges of AI in Education)
        Method --> M1(定量问卷调查/Quantitative Questionnaire Survey)
        M1 --> M1_1(346名巴西K-12教师/346 Brazilian K-12 Teachers)
        Results --> R1(高兴趣但知识有限/High Interest but Limited Knowledge)
        Results --> R2(关注伦理与结构挑战/Concerns on Ethics and Structural Challenges)
        Results --> R3(需要政策与培训支持/Need for Policy and Training Support)
    ```

- **[arXiv260101] From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering**
  - **tags:** [se], [Human-AI Collaboration], [AI Agent Evaluation, Behavioral Taxonomy, Context-Adaptive Behavior Framework]
  - **authors:** Tao Dong, Harini Sampath, Ja Young Lee, Sherry Y. Shi, Andrew Macvean
  - **institution:** Google LLC
  - **link:** https://arxiv.org/pdf/2512.23844
  - **contributions:** 1. A foundational taxonomy of desirable AI agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. 2. The Context-Adaptive Behavior (CAB) Framework, which models how behavioral expectations shift based on context. 3. An empirical derivation of two key axes (Time Horizon and Type of Work) that drive behavioral expectation shifts in the CAB Framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7ee9d0c9d0b72de4dc6af2921706818c987d2f7c644977698965be6b535d20c_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current AI evaluation benchmarks focus too narrowly on code correctness and fail to assess the collaborative behaviors needed for AI to be an effective partner in software engineering. To address this, the authors propose a taxonomy of desirable agent behaviors and a Context-Adaptive Behavior (CAB) Framework that models how these expectations change with context. These contributions provide a human-centered foundation for evaluating and designing collaborative AI agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering<br/>从正确性到协作：评估软件工程中AI智能体行为的人本框架"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br/>Current benchmarks fail to capture collaborative AI agent behavior.<br/>当前基准测试无法评估AI智能体的协作行为。"]
        Method["主要方法/Method<br/>1. Taxonomy of agent behaviors.<br/>智能体行为分类法。<br/>2. Context-Adaptive Behavior (CAB) Framework.<br/>上下文自适应行为框架。"]
        Results["关键结果/Results<br/>Provides a human-centered foundation for evaluating collaborative AI agents.<br/>为评估协作型AI智能体提供了人本基础。"]
    ```

- **[arXiv260101] The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models**
  - **tags:** [nlp], [language model evaluation], [epistemic robustness, semantic compression, adversarial fabrication, two-system cognitive model, comprehension integrity]
  - **authors:** Rahul Baxi
  - **institution:** Independent Researcher (affiliation inferred from email domain: alumni.cmu.edu, Carnegie Mellon University)
  - **link:** https://arxiv.org/pdf/2512.23850
  - **contributions:** 1. Introduces the Drill-Down and Fabricate Test (DDFT), a novel protocol for measuring epistemic robustness in language models under stress from semantic compression and adversarial fabrication. 2. Proposes a two-system cognitive model (Semantic System and Epistemic Verifier) to explain and analyze LLM behavior. 3. Provides empirical evidence that epistemic robustness is orthogonal to model scale and architecture, identifying error detection as the critical bottleneck.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a gap in current language model evaluations, which fail to measure how robustly models maintain factual knowledge under stress. It introduces the Drill-Down and Fabricate Test (DDFT) to measure epistemic robustness by applying semantic compression and adversarial fabrication. The key finding is that epistemic robustness is not predicted by model size or architecture but by a model's internal verification mechanisms, challenging assumptions about scaling and reliability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Drill-Down and Fabricate Test (DDFT)] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有评估无法衡量知识鲁棒性/Current evaluations fail to measure knowledge robustness]
        C --> C1[DDFT协议: 语义压缩与对抗伪造/DDFT Protocol: Semantic Compression & Adversarial Fabrication]
        C --> C2[双系统认知模型/Two-System Cognitive Model]
        D --> D1[鲁棒性与模型规模/架构无关/Robustness orthogonal to model size/architecture]
        D --> D2[错误检测能力是关键瓶颈/Error detection is the critical bottleneck]
    ```

- **[arXiv260101] Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense**
  - **tags:** [sec], [IoT Security], [Economic Denial Security, Stackelberg Game, Cost Asymmetry, Computational Puzzles]
  - **authors:** Samaresh Kumar Singh, Joyjit Roy
  - **institution:** IEEE (Inferred from author affiliations as IEEE members; specific institutional affiliation not provided in the excerpt)
  - **link:** https://arxiv.org/pdf/2512.23849
  - **contributions:** 1. Proposed the Economic Denial Security (EDS) framework, a detection-independent defense that exploits the defender's environmental control to impose economic infeasibility on attackers., 2. Formally modeled EDS as a Stackelberg game, deriving optimal parameters and proving that the composition of its four mechanisms yields superlinear (2.1x) cost amplification., 3. Demonstrated practical efficacy with a lightweight (&lt;12KB) implementation, validated on a 20-device IoT testbed and against IoT-23 malware, showing significant attack slowdown, cost asymmetry, and improved mitigation rates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53020dd5fb969c1980dd7f764afe5f97440c1ef68620bdcd3383e97bf39600fc_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the failure of detection-based security in resource-constrained IoT/edge environments. It proposes Economic Denial Security (EDS), a framework that uses mechanisms like computational puzzles and bandwidth taxation to make attacks economically infeasible by amplifying attacker costs. The method is proven to be lightweight, effective in significantly slowing attacks and reducing success rates, and provides a detection-independent layer of defense.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[检测安全在资源受限的IoT/边缘环境中失效/Detection-based security fails in resource-constrained IoT/edge]
        C --> C1[经济拒绝安全框架 / Economic Denial Security (EDS) Framework]
        C1 --> C2[四种机制组合 / Four Mechanism Composition]
        C2 --> C3[计算谜题 / Computational Puzzles]
        C2 --> C4[交互熵 / Interaction Entropy]
        C2 --> C5[时间拉伸 / Temporal Stretching]
        C2 --> C6[带宽征税 / Bandwidth Taxation]
        C --> C7[斯塔克尔伯格博弈建模 / Stackelberg Game Modeling]
        D --> D1[32-560倍攻击减速 / 32-560x Attack Slowdown]
        D --> D2[85-520:1 成本不对称 / 85-520:1 Cost Asymmetry]
        D --> D3[内存占用<12KB / <12KB Memory Footprint]
        D --> D4[94% 恶意软件缓解 / 94% Malware Mitigation]
    ```

- **[arXiv260101] Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis**
  - **tags:** [nlp], [conversational ai], [mental health crisis, stages of change model, human-AI interaction, testimonial survey, expert interviews]
  - **authors:** Leah Hope Ajmani, Arka Ghosh, Benjamin Kaveladze, Eugenia Kim, Keertana Namuduri, Theresa Nguyen, Ebele Okoli, Jessica Schleider, Denae Ford, Jina Suh
  - **institution:** University of Minnesota, Northwestern University, Dartmouth College, Microsoft, Microsoft Research, Mental Health America
  - **link:** https://arxiv.org/pdf/2512.23859
  - **contributions:** 1. Provides first-person experiential data on using conversational AI during mental health crises via a testimonial survey (n=53). 2. Contrasts user experiences with mental health expert perspectives (n=16) to highlight the essential role of human connection in crisis management. 3. Proposes a responsible design framework for AI crisis intervention, positioning AI as a bridge to human support using the stages of change model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/557b0c2624da79d40758f334c7d781c4558951ee96a27d54b04a81b3f20ec2ea_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how people use conversational AI (e.g., ChatGPT) during mental health crises through a survey and expert interviews. It finds users turn to AI due to gaps in human support, but experts emphasize human connection is crucial. The study concludes that responsible AI should act as a bridge to human help, increasing preparedness for positive action and de-escalating crises.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis] --> B(核心问题/Problem: Can conversational AI responsibly support mental health crises?)
        A --> C(主要方法/Method: Testimonial survey (n=53) & expert interviews (n=16))
        A --> D(关键结果/Results: AI fills gaps in human support; Human connection is essential; Design AI as a bridge to human help)
    ```

- **[arXiv260101] Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining**
  - **tags:** [mlsys], [llm training], [Infini-attention, compressive memory, small language models (SLMs), long-context extrapolation, pretraining]
  - **authors:** Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu
  - **institution:** Huawei Technologies Canada Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.23862
  - **code:** https://github.com/RRaAy-H/nanotron-infini
  - **contributions:** 1. Replaced standard attention in a 300M-parameter LLaMA model with Infini-attention to study compressive memory behavior under short-sequence pretraining. 2. Analyzed the training dynamics of SLMs with Infini-attention, revealing characteristics like loss fluctuations, gradient volatility, and early-layer memory concentration. 3. Demonstrated that Infini-attention improves long-context extrapolation over a baseline model, with supervised fine-tuning further boosting performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether the Infini-attention mechanism, which combines local attention with compressive memory, can enhance long-context capabilities in Small Language Models (SLMs) during small-scale pretraining. The authors empirically study a 300M-parameter LLaMA model equipped with Infini-attention and find it improves long-context retrieval accuracy over a baseline, despite some degradation over very long sequences. The conclusion is that architectural memory like Infini-attention is beneficial for achieving robust long-context performance in SLMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Enhancing long-context extrapolation for Small Language Models (SLMs)]
        C[主要方法/Method: Using Infini-attention (compressive memory + local attention) in small-scale pretraining]
        D[关键结果/Results: Improves long-context retrieval; Identifies balance factor importance; Shows performance degradation over very long sequences but still outperforms baseline]
    ```

- **[arXiv260101] Lifelong Domain Adaptive 3D Human Pose Estimation**
  - **tags:** [cv], [human pose estimation], [lifelong domain adaptation, catastrophic forgetting, generative adversarial network, pose-aware knowledge, temporal-aware knowledge]
  - **authors:** Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen
  - **institution:** University of Central Florida, University of North Carolina at Charlotte
  - **link:** https://arxiv.org/pdf/2512.23860
  - **contributions:** 1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Lifelong Domain Adaptive 3D Human Pose Estimation] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[3D HPE泛化挑战/3D HPE Generalization Challenge]
    B --> B2[非平稳目标域/Non-stationary Target Domains]
    B --> B3[灾难性遗忘/Catastrophic Forgetting]
    C --> C1[终身域适应任务/Lifelong DA Task]
    C --> C2[GAN框架/GAN Framework]
    C --> C3[3D姿态生成器/3D Pose Generator]
    C2 --> C2a[3D姿态生成器/3D Pose Generators]
    C2 --> C2b[2D姿态判别器/2D Pose Discriminator]
    C2 --> C2c[3D姿态估计器/3D Pose Estimator]
    C3 --> C3a[姿态感知/Pose-aware]
    C3 --> C3b[时序感知/Temporal-aware]
    C3 --> C3c[域感知/Domain-aware]
    D --> D1[缓解域偏移/Mitigates Domain Shifts]
    D --> D2[对齐姿态/Aligns Poses]
    D --> D3[卓越性能/Superior Performance]
    ```

- **[arXiv260101] Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack**
  - **tags:** [sec], [adversarial attacks], [universal adversarial perturbation, latent-space attack, audio-language models, encoder-level vulnerability, targeted attack]
  - **authors:** Roee Ziv, Raz Lapid, Moshe Sipper
  - **institution:** Ben Gurion University of the Negev, Deepkeep
  - **link:** https://arxiv.org/pdf/2512.23881
  - **contributions:** 1. Proposes a universal targeted latent-space attack against audio-language models, focusing solely on the audio encoder. 2. Introduces an attack method that learns a single perturbation effective across different inputs and speakers, without needing access to the downstream language model. 3. Demonstrates high attack success rates with minimal perceptual distortion on a state-of-the-art model, revealing a critical new attack surface in multimodal systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aec73155c062c3c66b8e33c0e7892e17d374292349cfa71e3389850584cf8195_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a security vulnerability in audio-language models where adversarial attacks can be launched by manipulating only the audio encoder's latent representations. The proposed method learns a universal perturbation that forces the model to generate attacker-specified text outputs, and experiments show it is highly effective and stealthy. This reveals a significant and previously underexplored attack surface at the encoder level of multimodal AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Breaking Audio Large Language Models by Attacking Only the Encoder<br>仅攻击编码器来攻破音频大语言模型"] --> Problem["核心问题/Problem<br>Audio-language models have new security vulnerabilities.<br>音频-语言模型存在新的安全漏洞"]
        Root --> Method["主要方法/Method<br>Universal targeted latent-space attack on the encoder.<br>针对编码器的通用目标潜空间攻击"]
        Root --> Results["关键结果/Results<br>High attack success with minimal distortion.<br>高攻击成功率，最小感知失真"]
    ```

- **[arXiv260101] CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution**
  - **tags:** [mlsys], [agent system], [self-evolving agent, skill acquisition, autonomous development, knowledge graph, scientific research agent]
  - **authors:** Xu Huang, Junwu Chen, Yuxing Fei, Zhuohan Li, Philippe Schwaller, Gerbrand Ceder
  - **institution:** University of California, Berkeley; Lawrence Berkeley National Laboratory; École Polytechnique Fédérale de Lausanne (EPFL)
  - **link:** https://arxiv.org/pdf/2512.23880
  - **contributions:** 1. Introduces CASCADE, a self-evolving agentic framework that transitions from static "LLM + tool use" to dynamic "LLM + skill acquisition". 2. Proposes meta-skills for continuous learning (via web search/code extraction) and self-reflection (via introspection/knowledge graph exploration) to master complex external tools. 3. Demonstrates high effectiveness on scientific tasks (93.3% success rate on SciSkillBench) and real-world applications like computational analysis and autonomous lab experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8104399440bdcb0943addb2985b2e27c755af9a6ab5a4f36894d7a82db9d0e00_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of current LLM agents that rely on predefined or brittle tools, which hinders their adaptability in complex scientific tasks. It proposes CASCADE, a self-evolving framework that enables agents to autonomously acquire and codify skills through meta-skills like continuous learning and self-reflection. The method achieves a high success rate on a materials science and chemistry benchmark and shows promise for scalable AI-assisted scientific research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CASCADE: Cumulative Agentic Skill Creation<br>累积智能体技能创造] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM agents depend on predefined/brittle tools<br>LLM智能体依赖预定义/脆弱工具]
        B --> B2[Constrained capability for complex scientific tasks<br>处理复杂科学任务能力受限]
        C --> C1[Self-evolving agentic framework<br>自进化智能体框架]
        C --> C2[Meta-skills: Continuous Learning & Self-Reflection<br>元技能：持续学习与自我反思]
        C --> C3[Transition: LLM+Tool Use → LLM+Skill Acquisition<br>转变：从工具使用到技能获取]
        D --> D1[93.3% success rate on SciSkillBench (116 tasks)<br>在SciSkillBench上成功率93.3%]
        D --> D2[Real-world applications demonstrated<br>演示了实际应用]
        D --> D3[Enables scalable AI-assisted research<br>实现可扩展的AI辅助研究]
    ```

- **[arXiv260101] How Large Language Models Systematically Misrepresent American Climate Opinions**
  - **tags:** [nlp], [large language model evaluation], [large language models, public opinion simulation, intersectionality, bias evaluation, climate policy]
  - **authors:** Sola Kim, Jieshu Wang, Marco A. Janssen, John M. Anderies
  - **institution:** Arizona State University, Stony Brook University
  - **link:** https://arxiv.org/pdf/2512.23889
  - **contributions:** 1. Conducted the first comparative study of LLM-generated public opinion against real human survey responses across intersecting demographic identities (race and gender). 2. Identified a systematic "compression" bias in LLMs, where they flatten the diversity of climate opinions by overestimating concern in less-concerned groups and underestimating it in more-concerned groups. 3. Revealed that this bias is intersectional, showing that LLMs apply uniform gender assumptions that fail for specific racial groups (e.g., misrepresenting gender patterns among Black Americans), a flaw potentially invisible to standard audits.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0e5dfdc0fba7038277e876cd0c81062b3c5735782d5df732873aec991a84b3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how six large language models (LLMs) represent U.S. climate opinions by prompting them with profiles from a real national survey and comparing their generated responses to actual human answers. The study finds that LLMs systematically compress opinion diversity and misrepresent intersectional patterns, particularly for Black Americans, which could undermine equitable policy-making.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["How Large Language Models Systematically Misrepresent American Climate Opinions<br>论文标题"] --> B["Problem: LLMs used for public opinion analysis may misrepresent diverse, intersectional views.<br>核心问题：用于公众意见分析的LLM可能歪曲多样化的交叉性观点。"]
        A --> C["Method: Prompt 6 LLMs with real survey respondent profiles and compare outputs to human answers.<br>主要方法：用真实调查受访者档案提示6个LLM，并将输出与人类答案比较。"]
        A --> D["Results: LLMs compress opinion diversity and misapply gender assumptions across racial groups.<br>关键结果：LLM压缩了意见多样性，并在不同种族群体中误用了性别假设。"]
    ```

- **[arXiv260101] Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City**
  - **tags:** [ai], [time series forecasting], [Transformer, Mamba, Knowledge Distillation]
  - **authors:** Tin Hoang
  - **institution:** University of Surrey
  - **link:** https://arxiv.org/pdf/2512.23898
  - **code:** github.com/Tin-Hoang/solar-timeseries-forecasting
  - **contributions:** 1. Conducted a comprehensive benchmark of ten deep learning architectures for short-term solar irradiance forecasting, identifying the Transformer as the best-performing model. 2. Used SHAP analysis to reveal and contrast the distinct temporal reasoning patterns of different architectures (e.g., Transformer's recency bias vs. Mamba's periodic dependency). 3. Demonstrated that Knowledge Distillation can effectively compress the high-performance Transformer model, reducing its size by 23.5% while improving accuracy for edge deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8685dbd2386bb45fd799c168962048f6a243c0b8c836a5be5978ff64d0c2e184_w640_q70.webp
  - **Simple LLM Summary:** This paper benchmarks ten deep learning models for 1-hour ahead solar irradiance forecasting in Ho Chi Minh City. The Transformer model achieved the highest accuracy, and the study used explainable AI to analyze model behavior and successfully compressed the model via Knowledge Distillation for efficient edge deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题 / Paper Title: Efficient Deep Learning for Short-Term Solar Irradiance Forecasting] --> B
        A --> C
        A --> D
        B[核心问题 / Problem: 预测全球水平辐照度(GHI)以缓解太阳能波动 / Forecasting GHI to mitigate solar energy variability]
        C[主要方法 / Method: 对十种深度学习架构进行基准测试与可解释性分析 / Benchmarking 10 DL architectures with explainability analysis]
        D[关键结果 / Results: Transformer性能最优；知识蒸馏实现高效压缩 / Transformer best; Knowledge Distillation enables efficient compression]
    ```

- **[arXiv260101] Interactive Machine Learning: From Theory to Scale**
  - **tags:** [ai], [interactive machine learning], [active learning, contextual bandits, model selection, sequential decision making, partial feedback]
  - **authors:** Yinglun Zhu
  - **institution:** University of Wisconsin–Madison
  - **link:** https://arxiv.org/pdf/2512.23924
  - **contributions:** 1. Developed computationally efficient active learning algorithms that achieve exponential label savings without requiring low-noise assumptions., 2. Introduced the first efficient, general-purpose contextual bandit algorithms whose performance guarantees are independent of the action space size., 3. Provided the first tight characterizations of the fundamental cost of model selection in sequential decision-making settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d3c8fc2dd2145126e7321fa8de265f0c2652dbd2dc7df8c387585634498e335_w640_q70.webp
  - **Simple LLM Summary:** This dissertation addresses the high cost of data labeling and trial-and-error in machine learning by developing new algorithms for interactive learning. It proposes statistically optimal and computationally efficient methods for active learning, contextual bandits with large action spaces, and model selection under partial feedback. The work advances the theoretical foundations of interactive learning and provides guidance for its deployment in large-scale, real-world applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Interactive Machine Learning: From Theory to Scale<br>交互式机器学习：从理论到规模")
        Root --> Problem("Problem: High cost of labeled data & trial-and-error in ML<br>核心问题：机器学习中标注数据和试错的高成本")
        Root --> Method("Method: Develop algorithms for interactive learning<br>主要方法：开发交互式学习算法")
        Root --> Results("Results: Statistically optimal & computationally efficient algorithms<br>关键结果：统计最优且计算高效的算法")
        Problem --> P1("Active learning with noisy data<br>含噪声数据的主动学习")
        Problem --> P2("Sequential decision making with large action spaces<br>大动作空间的序列决策")
        Problem --> P3("Model selection under partial feedback<br>部分反馈下的模型选择")
        Method --> M1("New algorithmic principles<br>新算法原理")
        Method --> M2("Establish fundamental limits<br>建立基本极限")
        Results --> R1("Exponential label savings in active learning<br>主动学习中的指数级标签节省")
        Results --> R2("Contextual bandit guarantees independent of action space size<br>与动作空间大小无关的上下文赌博机保证")
        Results --> R3("Tight characterization of model selection cost<br>模型选择成本的紧致刻画")
    ```

- **[arXiv260101] A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming**
  - **tags:** [ai], [neuro-symbolic ai], [Answer Set Programming, Large Language Models, Explainable AI, Knowledge Base Construction, Disease Diagnosis]
  - **authors:** Ioanna Gemou, Evangelos Lamprou
  - **institution:** University of Patras
  - **link:** https://arxiv.org/pdf/2512.23932
  - **contributions:** 1. Proposes McCoy, a novel framework that integrates LLMs and Answer Set Programming for automated disease diagnosis. 2. Automates the labor-intensive construction of medical knowledge bases by using an LLM to translate medical literature into ASP code. 3. Delivers an interpretable and robust diagnostic system that provides logical justifications for its predictions, achieving high accuracy on preliminary tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9000f6d3b0daa6fb6bc9ea6b6b2f1ca56befb92d47f3f4f67b25d12dc6ca996_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to automate the creation of diagnostic knowledge bases and perform explainable disease diagnosis. The LLM translates medical literature into ASP rules, which are then combined with patient data and solved to produce a diagnosis with logical justifications. Preliminary results show the framework achieves high predictive accuracy on small-scale tasks while providing transparency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming<br>可解释疾病诊断的概念验证：使用大语言模型与回答集编程"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Symbolic AI adoption limited by manual knowledge base construction.<br>符号AI因需手动构建知识库而应用受限。"]
        Method["主要方法/Method<br>Combine LLM (translates literature) with ASP (logical reasoning).<br>结合LLM（翻译文献）与ASP（逻辑推理）。"]
        Results["关键结果/Results<br>High accuracy, interpretable diagnosis framework.<br>高准确性、可解释的诊断框架。"]
    ```

- **[arXiv260101] An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System**
  - **tags:** [mlsys], [agent system], [agentic AI, recommendation system, KYC (Know Your Customer), nDCG, multi-stage architecture]
  - **authors:** Junjie H. Xu
  - **institution:** Hechu Tech
  - **link:** https://arxiv.org/pdf/2512.23961
  - **contributions:** 1. Proposes a novel agentic AI-based recommendation system specifically designed for integrating KYC (Know Your Customer) processes. 2. Conducts a comparative performance evaluation across five distinct content verticals (Ad, News, Gossip, Sharing, Tech) using the nDCG metric. 3. Synthesizes experimental data with industry benchmarks to provide engineering insights for building large-scale agentic recommendation systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e7c4d7572522a69f7c0db05b6553014273403aa44576ea9c0de823750c5368_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new recommendation system that uses agentic AI to incorporate KYC (Know Your Customer) information. It evaluates the system's performance across five different content types and compares it against standard benchmarks. The study concludes by providing practical insights for engineering large-scale agentic recommendation systems based on the experimental results.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System] --> B[核心问题/Problem: Transition from passive ranking to agentic AI in RecSys]
        A --> C[主要方法/Method: Agentic AI for KYC, evaluated across 5 content verticals using nDCG@k]
        A --> D[关键结果/Results: Performance comparison of 4 KYC usage groups, insights for large-scale engineering]
    ```

- **[arXiv260101] Physics-informed Graph Neural Networks for Operational Flood Modeling**
  - **tags:** [ai], [graph neural networks], [physics-informed neural networks, graph neural networks, flood modeling, curriculum learning, message-passing]
  - **authors:** Carlo Malapad Acosta, Herath Mudiyanselage Viraj Vidura Herath, Jia Yu Lim, Abhishek Saha, Sanka Rasnayaka, Lucy Marshall
  - **institution:** National University of Singapore, The University of Sydney, Delft University of Technology
  - **link:** https://arxiv.org/pdf/2512.23964
  - **code:** https://github.com/acostacos/dual_flood_gnn
  - **contributions:** 1. Proposes DUALFloodGNN, a novel GNN architecture that embeds physical constraints at both global and local scales through explicit loss terms. 2. Introduces a model that jointly predicts water volume at nodes and flow along edges using a shared message-passing framework. 3. Enhances autoregressive inference performance via multi-step loss training with dynamic curriculum learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6bbddb2010c8550ce3ea4a09f24c04abc0993a7ee2a722f960fc0852c4f049_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of physics-based flood models by proposing DUALFloodGNN, a physics-informed graph neural network architecture. The model incorporates physical constraints into its loss function and uses a multi-step training strategy with curriculum learning. It achieves improved prediction accuracy for hydrologic variables while maintaining high computational efficiency compared to existing GNN models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-informed Graph Neural Networks for Operational Flood Modeling] --> B
        A --> C
        A --> D
        B[核心问题/Problem: High computational cost of physics-based flood models limits operational use]
        C[主要方法/Method: DUALFloodGNN embeds physical constraints via loss terms and uses multi-step training with curriculum learning]
        D[关键结果/Results: Achieves improved accuracy and maintains high computational efficiency]
    ```

- **[arXiv260101] Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [hypergraph memory, multi-step reasoning, global sense-making, long-context modeling, retrieval-augmented generation]
  - **authors:** Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu
  - **institution:** The Chinese University of Hong Kong, WeChat AI
  - **link:** https://arxiv.org/pdf/2512.23959
  - **code:** https://github.com/Encyclomen/HGMem
  - **contributions:** 1. Proposes HGMem, a novel hypergraph-based memory mechanism that models memory as a dynamic structure with higher-order interactions, moving beyond passive storage. 2. Addresses the limitation of existing multi-step RAG memory in capturing complex relational structures and providing strong guidance for subsequent reasoning steps. 3. Demonstrates through extensive experiments that the method consistently improves multi-step RAG performance and substantially outperforms strong baselines on global sense-making tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of static, passive memory in multi-step RAG systems, which leads to fragmented reasoning in long-context tasks. It proposes HGMem, a dynamic hypergraph-based memory mechanism that captures high-order correlations among facts to form an integrated knowledge structure for stronger reasoning guidance. The method is shown to consistently and substantially outperform baseline systems across diverse global sense-making tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improving Multi-step RAG with Hypergraph-based Memory<br>改进多步RAG的超图记忆] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有记忆模块是被动的静态存储<br>Existing memory is passive static storage]
        B1 --> B2[忽略了高阶关联，导致碎片化推理<br>Ignores high-order correlations, causing fragmented reasoning]
        C --> C1[提出超图记忆机制 HGMem<br>Propose hypergraph memory mechanism HGMem]
        C1 --> C2[将记忆表示为动态超图<br>Represent memory as a dynamic hypergraph]
        C2 --> C3[超边形成高阶交互，构建集成知识结构<br>Hyperedges form high-order interactions, building integrated knowledge]
        D --> D1[在多步RAG上取得一致改进<br>Achieves consistent improvement on multi-step RAG]
        D1 --> D2[在全局理解任务上显著超越基线<br>Substantially outperforms baselines on global sense-making tasks]
    ```

- **[arXiv260101] Efficient Context Scaling with LongCat ZigZag Attention**
  - **tags:** [mlsys], [llm inference], [sparse attention, long-context, mid-training, ZigZag Attention]
  - **authors:** Chen Zhang, Yang Bai, Jiahuan Li, Anchun Gui, Keheng Wang, Feifan Liu, Guanyu Wu, Yuwei Jiang, Defei Bu, Li Wei, Haihang Jing, Hongyin Tang, Xin Chen, Xiangzhou Huang, Fengcun Li, Rongxiang Weng, Yulei Qian, Yifan Lu, Yerui Sun, Jingang Wang, Yuchen Xie, Xunliang Cai
  - **institution:** Meituan
  - **link:** https://arxiv.org/pdf/2512.23966
  - **contributions:** 1. Proposes LongCat ZigZag Attention (LoZA), a sparse attention scheme to convert full-attention models into sparse versions with limited compute. 2. Demonstrates LoZA's effectiveness for speed-up in both prefill-intensive (e.g., RAG) and decode-intensive (e.g., tool use) long-context scenarios. 3. Applies LoZA to create LongCat-Flash-Exp, a foundation model capable of efficiently processing up to 1 million tokens.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46b353da1c2cbb89962a2e909144fbcc8c92d821f35157049a111e1855b4242c_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces LongCat ZigZag Attention (LoZA), a sparse attention method designed to efficiently transform standard full-attention language models into sparse models suitable for long-context tasks. This approach enables significant speed improvements for both prefill and decoding phases. The resulting model, LongCat-Flash-Exp, can process up to 1 million tokens, facilitating efficient long-term reasoning and agentic capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Efficient Context Scaling with LongCat ZigZag Attention] --> B[核心问题/Problem: 长上下文场景下全注意力计算开销大/High computational cost of full attention in long-context scenarios]
        A --> C[主要方法/Method: 提出LoZA稀疏注意力方案/Propose LoZA sparse attention scheme]
        A --> D[关键结果/Results: 实现显著加速，支持百万token高效处理/Achieve significant speed-up, enable efficient processing of 1M tokens]
    ```

- **[arXiv260101] Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing**
  - **tags:** [mlsys], [others], [streaming machine learning, directed acyclic graph (DAG), point-in-time idempotency, temporal tiling, causality enforcement]
  - **authors:** Giacinto Paolo Saggese, Paul Smith
  - **institution:** Not explicitly stated. Could be inferred from author names and arXiv submission, but no clear affiliation is provided in the given content.
  - **link:** https://arxiv.org/pdf/2512.23977
  - **contributions:** 1. A unified DAG-based execution model with point-in-time idempotency, ensuring identical model behavior in batch and streaming modes without code changes. 2. Automatic causality enforcement by tracking knowledge time across transformations, eliminating future-peeking bugs. 3. Flexible temporal and feature dimension tiling, allowing models to operate at different frequencies and memory profiles via configuration alone.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4105add65c1e7a9960d7e21b6bda977778c41365cfff18c04d16a1af3773b5c4_w640_q70.webp
  - **Simple LLM Summary:** The paper presents DataFlow, a framework for building high-performance ML systems on streaming time-series data. It uses a DAG-based model with point-in-time idempotency to bridge the gap between batch prototyping and streaming production, ensuring causality and reproducibility. The framework demonstrates effectiveness in domains like financial trading and IoT analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing"] --> Problem["核心问题/Problem: Gap between batch ML prototypes and streaming production systems causes causality violations and poor reproducibility."]
        Root --> Method["主要方法/Method: Unified DAG execution model with point-in-time idempotency and automatic causality tracking."]
        Root --> Results["关键结果/Results: Enables identical batch/stream execution, flexible tiling, and effective deployment in financial, IoT, and fraud detection domains."]
    ```

- **[arXiv260101] A Community-Aware Framework for Influence Maximization with Explicit Accounting for Inter-Community Influence**
  - **tags:** [ai], [social network analysis], [influence maximization, community structure, inter-community diffusion, progressive budgeting, community-based diffusion degree]
  - **authors:** Eliot W. Robson, Abhishek K. Umrawal
  - **institution:** Narmi, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.23973
  - **contributions:** 1. Proposes Community-IM++, a scalable framework that explicitly models cross-community influence, overcoming a key limitation of prior community-based methods. 2. Introduces a principled heuristic based on community-based diffusion degree (CDD) and a progressive budgeting strategy to prioritize bridging nodes and allocate seeds adaptively. 3. Demonstrates through experiments on large real-world networks that the method achieves near-greedy influence spread with up to 100x speedup, outperforming baseline heuristics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b25bc24c3cf4c404605618e8019a2659fc31bfeb9d29ad19d3af7a7ca5cc7a4c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the Influence Maximization problem in social networks, where existing community-based methods often overlook inter-community influence. The authors propose Community-IM++, a scalable framework that explicitly models cross-community diffusion using a new heuristic (CDD) and a progressive budgeting strategy. The method achieves near-optimal performance with significantly lower runtime, making it practical for large-scale applications like viral marketing and public health campaigns.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("A Community-Aware Framework for Influence Maximization<br>影响力最大化社区感知框架") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("社区方法忽略社区间影响<br>Community methods ignore inter-community influence")
        Method --> M1("提出Community-IM++框架<br>Propose Community-IM++ framework")
        M1 --> M2("使用社区扩散度(CDD)启发式<br>Use Community-based Diffusion Degree (CDD) heuristic")
        M1 --> M3("渐进预算与惰性评估<br>Progressive budgeting & lazy evaluation")
        Results --> R1("接近贪婪算法的传播范围<br>Near-greedy influence spread")
        Results --> R2("运行时间降低高达100倍<br>Up to 100x lower runtime")
        Results --> R3("优于基线方法<br>Outperforms baseline methods")
    ```

- **[arXiv260101] Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education**
  - **tags:** [se], [AI-assisted Software Engineering], [vibe coding, agentic coding, LLM-based coding, code review, curricular shift]
  - **authors:** Hung-Fu Chang, MohammadShokrolah Shirazi, Lizhou Cao, Supannika Koolmanojwong Mobasser
  - **institution:** University of Indianapolis, Marian University, University of Maryland Eastern Shore, The Boehm Center for Systems and Software Engineering
  - **link:** https://arxiv.org/pdf/2512.23982
  - **contributions:** 1. Provides an industry-grounded investigation of LLM coding practices (vibe, AI-assisted, agentic coding) and their impact on professional workflows, based on qualitative analysis of practitioner reflections. 2. Identifies key risks and concerns associated with AI-based coding, including shifts in development bottlenecks to code review, code quality issues, security vulnerabilities, and skill erosion. 3. Proposes implications and guidance for computer science and software engineering education, advocating for curricular shifts toward problem-solving, architectural thinking, and early integration of LLM tools.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d32f497e3b36008891d37440a49e031ae3f31d7dcba8585c229352e091d83a3a_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how large language models (LLMs) are used in professional software development by qualitatively analyzing 57 YouTube videos from practitioners. The study identifies new coding paradigms, productivity gains, and associated risks like quality and security concerns. It concludes by discussing the need for educational reforms in computer science to align with these evolving industrial practices.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Coding With AI: From Industrial Practices to Future Education") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("工业实践中LLM编码工具的使用与风险未充分探索/LLM coding use & risks in industry underexplored")
        Method --> M1("对57个YouTube视频进行定性分析/Qualitative analysis of 57 YouTube videos")
        Results --> R1("定义AI编码实践，发现生产力提升与风险/Defines AI coding practices, finds productivity gains & risks")
        Results --> R2("提出计算机科学教育的课程改革建议/Proposes curricular shifts for CS education")
    ```

- **[arXiv260101] MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning**
  - **tags:** [sec], [malware detection], [Model-Agnostic Meta-Learning (MAML), Chunk-wise Feature Selection (CFSGB), Gradient Boosting]
  - **authors:** Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D
  - **institution:** National Institute of Technology Calicut
  - **link:** https://arxiv.org/pdf/2512.23987
  - **contributions:** 1. Proposed MeLeMaD, a novel malware detection framework leveraging Model-Agnostic Meta-Learning (MAML) for adaptability and generalization. 2. Introduced a novel Chunk-wise Feature Selection based on Gradient Boosting (CFSGB) technique to handle large-scale, high-dimensional datasets efficiently. 3. Demonstrated state-of-the-art performance on benchmark datasets (CIC-AndMal2020, BODMAS) and a custom dataset (EMBOD), achieving high accuracy and robustness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1509044c1c0bdfbb4a075c799e3c0cafd035f0b7c579548b445cf04caee2975d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes MeLeMaD, a novel malware detection framework that combines a new chunk-wise feature selection method (CFSGB) with meta-learning (MAML) to improve adaptability and efficiency on large-scale datasets. It achieves high accuracy on benchmark and custom datasets, outperforming existing state-of-the-art approaches and demonstrating robustness against evolving threats.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MeLeMaD: Adaptive Malware Detection] --> B[核心问题/Problem: Malware detection needs robustness & adaptability]
        A --> C[主要方法/Method: Meta-Learning (MAML) + Chunk-wise Feature Selection (CFSGB)]
        A --> D[关键结果/Results: High accuracy on benchmarks (98.04%, 99.97%) & custom dataset]
    ```

- **[arXiv260101] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process**
  - **tags:** [nlp], [mechanistic interpretability], [sparse auto-encoder, reasoning vectors, chain-of-thought]
  - **authors:** Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang
  - **institution:** Google DeepMind, The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.23988
  - **contributions:** 1. Proposes RISE, an unsupervised framework using sparse auto-encoders (SAEs) to discover "reasoning vectors" that encode distinct reasoning behaviors from step-level LLM activations. 2. Demonstrates that these discovered vectors correspond to interpretable behaviors (e.g., reflection, backtracking) and can be used for targeted intervention to controllably steer the reasoning process without retraining. 3. Shows SAEs can uncover novel, human-undefined reasoning behaviors and structural properties, such as controlling response confidence, highlighting the potential of unsupervised latent discovery.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of interpreting the internal reasoning process of large language models (LLMs). It proposes RISE, an unsupervised framework that uses sparse auto-encoders to discover disentangled "reasoning vectors" from chain-of-thought activations. The method enables the identification, visualization, and controllable intervention of specific reasoning behaviors, revealing novel insights beyond supervised analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"]
        Root --> Problem["核心问题/Problem<br>LLM推理内部机制不明确<br>Supervised methods are limited"]
        Root --> Method["主要方法/Method<br>RISE框架: 无监督稀疏自编码器<br>Unsupervised SAEs on step-level activations"]
        Root --> Results["关键结果/Results<br>发现可解释推理行为向量<br>可控干预推理轨迹<br>Discover novel behaviors"]
    ```

- **[arXiv260101] PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation**
  - **tags:** [cv], [audio-visual generation], [text-to-audio-video, physics-sensitivity, benchmark, audio-physics grounding, contrastive physical response score]
  - **authors:** Tianxin Xie, Wentao Lei, Guanjie Huang, Pengfei Zhang, Kai Jiang, Chunhui Zhang, Fengji Ma, Haoyu He, Han Zhang, Jiangshan He, Jinting Wang, Linghan Fang, Lufei Gao, Orkesh Ablet, Peihua Zhang, Ruolin Hu, Shengyu Li, Weilin Lin, Xiaoyang Feng, Xinyue Yang, Yan Rong, Yanyun Wang, Zihang Shao, Zelin Zhao, Chenxing Li, Shan Yang, Wenfu Wang, Meng Yu, Dong Yu, Li Liu
  - **institution:** HKUST(GZ), Tencent, Shanghai Jiao Tong University, Technical University of Munich
  - **link:** https://arxiv.org/pdf/2512.23994
  - **code:** https://imxtx.github.io/PhyAVBench/
  - **contributions:** 1. Introduces PhyAVBench, a novel benchmark for evaluating the audio physics-sensitivity of T2AV models., 2. Proposes the Audio-Physics Sensitivity Test (APST) paradigm using paired prompts with controlled physical variables., 3. Defines the Contrastive Physical Response Score (CPRS) to quantitatively measure a model's understanding of physical principles.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eec12bf437e946a08a88614c7454f1315e7f14f21f2ebe631265de8a77d352a_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that current text-to-audio-video (T2AV) models lack physical plausibility in generated sounds. To address this, it introduces PhyAVBench, a challenging benchmark designed to systematically evaluate models' audio physics grounding through a novel Audio-Physics Sensitivity Test (APST). The authors argue that this benchmark will stimulate progress in generating physically consistent audio-visual content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有T2AV模型无法生成物理合理的声音 / Existing T2AV models generate physically implausible sounds]
        C --> C1[提出PhyAVBench基准与APST评估范式 / Propose PhyAVBench benchmark & APST evaluation paradigm]
        C --> C2[使用成对提示控制物理变量 / Use paired prompts with controlled physical variables]
        C --> C3[引入对比物理响应分数(CPRS) / Introduce Contrastive Physical Response Score (CPRS)]
        D --> D1[系统性评估模型对声学物理的理解 / Systematically evaluate models' understanding of acoustic physics]
        D --> D2[推动物理基础T2AV生成的研究 / Stimulate research in physically-grounded T2AV generation]
    ```

- **[arXiv260101] TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems**
  - **tags:** [ai], [metaheuristics], [simulation optimization, tabu search, elite memory, noisy black-box, aspiration criterion]
  - **authors:** Bulent Soykan, Sean Mondesire, Ghaith Rabadi
  - **institution:** University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.24007
  - **code:** github.com/bulentsoykan/TESO
  - **contributions:** 1. Proposes TESO, a novel metaheuristic framework that integrates adaptive search with memory-based strategies for simulation optimization. 2. Introduces a dual-memory mechanism combining a short-term Tabu List for diversification and a long-term Elite Memory for intensification. 3. Demonstrates the framework's effectiveness and reliability on a queue optimization problem, showing improved performance over benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00ac2df6705adb84c43e6fe1380cb528473aee412ea23024864f40a91dfd7ec9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TESO, a metaheuristic framework for noisy, expensive black-box simulation optimization. It combines a Tabu List and an Elite Memory with an aspiration criterion to balance exploration and exploitation. The method is validated on a queue optimization problem, showing improved performance and reliability compared to benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[TESO: Tabu-Enhanced Simulation Optimization] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Noisy, expensive, multimodal simulation optimization] --> P1[挑战/Challenges: Noisy evaluations, high cost, complex landscapes]
        Method[主要方法/Method: Memory-based metaheuristic framework] --> M1[组件/Components: Tabu List, Elite Memory, Aspiration Criterion]
        Results[关键结果/Results: Validated on queue optimization] --> R1[结论/Conclusion: Improved performance & reliability]
    ```

- **[arXiv260101] SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing**
  - **tags:** [mlsys], [agent system], [multi-agent systems, retrieval-augmented generation, persona-based agents, long-term memory, coordination protocols]
  - **authors:** Gaurab Chhetri, Subasish Das, Tausif Islam Chowdhury
  - **institution:** Texas State University
  - **link:** https://arxiv.org/pdf/2512.24008
  - **contributions:** 1. Proposes SPARK, a novel framework that uses coordinated, persona-based LLM agents for personalized search, moving beyond static user profiles. 2. Formalizes a persona space and introduces a Persona Coordinator to dynamically activate specialized agents based on query interpretation. 3. Facilitates inter-agent collaboration through structured protocols like shared memory and iterative debate, enabling emergent personalization from distributed behaviors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522362a9d0a8ab25d8cf01e3f9989591d57b2f5595d731ca2b3bdd84da90a098_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SPARK, a framework that uses coordinated, persona-based LLM agents to perform personalized search. It dynamically activates specialized agents for retrieval and uses structured communication for collaboration. The approach aims to model the fluid and complex nature of human information needs better than traditional static-profile systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        SPARK_Title[SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing] --> Problem[核心问题/Problem]
        SPARK_Title --> Method[主要方法/Method]
        SPARK_Title --> Results[关键结果/Results]
        Problem --> P1[静态用户画像限制个性化搜索/Static user profiles limit personalized search]
        Problem --> P2[单一检索流程难以捕捉复杂需求/Monolithic retrieval fails to capture complex needs]
        Method --> M1[定义角色化智能体空间/Define persona-based agent space]
        Method --> M2[引入智能体协调器/Introduce Persona Coordinator]
        Method --> M3[智能体协作与知识共享/Agent collaboration & knowledge-sharing]
        Results --> R1[实现涌现的个性化/Achieve emergent personalization]
        Results --> R2[为下一代搜索系统提供见解/Provide insights for next-gen search systems]
    ```

- **[arXiv260101] iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning**
  - **tags:** [nlp], [reasoning], [latent planning, implicit cognition, vector-quantized autoencoder, chain-of-thought]
  - **authors:** Sijia Chen, Di Niu
  - **institution:** Hong Kong University of Science and Technology (Guangzhou), University of Alberta
  - **link:** https://arxiv.org/pdf/2512.24014
  - **code:** https://github.com/AgenticFinLab/latent-planning
  - **contributions:** 1. Proposes iCLP, a novel framework inspired by human Implicit Cognition to enable LLMs to generate and use compact latent plans for reasoning. 2. Introduces a method to distill explicit plans from reasoning trajectories and learn their discrete representations via a vector-quantized autoencoder. 3. Demonstrates that fine-tuning LLMs on latent plans improves reasoning accuracy, efficiency, and cross-domain generalization while preserving interpretability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dce54c76575e0ccf630d7de1d6cf89260c30415f1817167f368dcb00b0d64bd_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of LLMs generating unreliable explicit textual plans for reasoning. It proposes the iCLP framework, which enables LLMs to learn and use compact latent plans, inspired by human subconscious cognition. Experiments show this approach improves reasoning performance and generalization on mathematical and coding tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[iCLP: 大语言模型推理与隐式认知潜在规划<br>iCLP: LLM Reasoning with Implicit Cognition Latent Planning] --> B[核心问题/Problem: 显式文本规划生成困难<br>Challenges in generating explicit textual plans]
        A --> C[主要方法/Method: 学习并使用潜在规划<br>Learn and use latent plans via VQ-VAE and fine-tuning]
        A --> D[关键结果/Results: 提升准确率、效率与泛化能力<br>Improves accuracy, efficiency, and generalization]
    ```

- **[arXiv260101] FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing**
  - **tags:** [cv], [vision-language models], [multi-feature fusion, recurrent visual injection, remote sensing vision-language model]
  - **authors:** Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao
  - **institution:** Nanjing University
  - **link:** https://arxiv.org/pdf/2512.24022
  - **code:** https://github.com/Yunkaidang/RSVLM
  - **contributions:** 1. Proposes a Multi-Feature Fusion Remote Sensing Vision-Language Model (MF-RSVLM) that extracts and fuses multi-scale visual features to better capture small and complex structures in remote sensing scenes., 2. Introduces a recurrent visual feature injection scheme to keep the language model grounded in visual evidence and mitigate visual forgetting during text generation., 3. Demonstrates state-of-the-art or highly competitive performance on diverse remote sensing benchmarks, including classification, image captioning, and VQA tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51db66b7af59969350cb2c0f2ca84f598178ab4f9ea4040dfc08f46586ef7fe0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of applying general vision-language models to remote sensing data by proposing MF-RSVLM, a model that fuses multi-scale visual features and uses recurrent visual injection to reduce forgetting. It achieves strong results on remote sensing classification, captioning, and VQA tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLMs struggle with fine-grained features and visual forgetting in remote sensing]
        C[主要方法/Method: Multi-feature fusion and recurrent visual injection]
        D[关键结果/Results: SOTA/competitive performance on RS benchmarks]
    ```

- **[arXiv260101] Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective**
  - **tags:** [ai], [self-supervised learning], [electrocardiogram (ECG), self-supervised learning (SSL), cardiac conduction, sparse attention, hierarchical diagnosis]
  - **authors:** Tan Pan, Yixuan Sun, Chen Jiang, Qiong Gao, Rui Sun, Xingmeng Zhang, Zhenqi Yang, Limei Han, Yixiu Liang, Yuan Cheng, Kaiyu Guo
  - **institution:** Fudan University, Shanghai Academy of Artificial Intelligence for Science
  - **link:** https://arxiv.org/pdf/2512.24002
  - **code:** https://github.com/Ashespt/CLEAR-HUG
  - **contributions:** 1. Identifies a key limitation in prior ECG self-supervised learning (eSSL) methods: they overlook inherent heartbeat differences rooted in cardiac conduction and neglect the sequential logic of clinical ECG diagnosis. 2. Proposes a novel two-stage framework (CLEAR-HUG), where the first stage (CLEAR) is an eSSL model that uses a sparse attention mechanism to reconstruct signals by treating each heartbeat as a distinct entity, capturing subtle conduction variations. 3. Introduces a Hierarchical lead-Unified Group head (HUG) for the downstream diagnosis stage, which mirrors the clinical workflow from heartbeats to leads to lead combinations, aligning model patterns with expert guidelines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4316ae9f56d525621d461087803f69e54c99676d3863c02d5df1d1ad32dab6a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CLEAR-HUG, a two-stage framework for ECG representation learning. The method first uses a self-supervised model (CLEAR) with sparse attention to learn from cardiac conduction variations, then applies a hierarchical diagnosis head (HUG) aligned with clinical guidelines. Experiments across six tasks show a 6.84% performance improvement, validating its effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[追踪心路：从心脏传导视角的ECG表征学习<br>Tracing the Heart's Pathways: ECG Representation Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有eSSL方法忽视心脏传导导致的细微差异<br>Prior eSSL overlooks conduction-based heartbeat differences]
        B --> B2[模型未遵循从心跳到导联的临床诊断逻辑<br>Models neglect clinical diagnostic sequence]
        C --> C1[两阶段框架CLEAR-HUG<br>Two-stage framework CLEAR-HUG]
        C1 --> C2[阶段一: CLEAR (自监督学习)<br>Stage 1: CLEAR (eSSL)]
        C2 --> C3[稀疏注意力重构信号<br>Sparse attention for reconstruction]
        C1 --> C4[阶段二: HUG (分层诊断头)<br>Stage 2: HUG (hierarchical head)]
        C4 --> C5[模仿临床工作流<br>Mirrors clinical workflow]
        D --> D1[六项任务性能提升6.84%<br>6.84% improvement across six tasks]
        D --> D2[验证了方法的有效性<br>Validates effectiveness]
    ```

- **[arXiv260101] RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations**
  - **tags:** [cv], [text-guided segmentation], [agentic MLLM, multi-turn tool invocation, iterative mask refinement]
  - **authors:** Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang
  - **institution:** Fudan University, Shanghai Jiao Tong University School of Medicine
  - **link:** https://arxiv.org/pdf/2512.24023
  - **contributions:** 1. Proposes RSAgent, an agentic MLLM that interleaves reasoning and action for segmentation via multi-turn tool invocations, enabling iterative refinement. 2. Builds a data pipeline to synthesize multi-turn reasoning segmentation trajectories for training. 3. Introduces a two-stage training framework combining cold-start supervised fine-tuning with agentic reinforcement learning using fine-grained, task-specific rewards.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492283683a8b1ec7673cb4aa97997d0e1ab70ed4a074c17cfa6a9a5e87c60998_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of one-shot methods in text-guided segmentation, where initial errors cannot be corrected. It proposes RSAgent, an agentic multimodal LLM that iteratively uses a segmentation toolbox, observes feedback, and refines its spatial hypotheses over multiple turns. Experiments show RSAgent achieves state-of-the-art performance on benchmarks like ReasonSeg and RefCOCOg.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RSAgent: Learning to Reason and Act for Text-Guided Segmentation] --> B[核心问题/Problem: One-shot grounding methods lack verification and refinement capabilities]
        A --> C[主要方法/Method: Agentic MLLM with multi-turn tool invocation for iterative reasoning and mask refinement]
        A --> D[关键结果/Results: Achieves SOTA on benchmarks (66.5% gIoU on ReasonSeg, 81.5% cIoU on RefCOCOg)]
    ```

- **[arXiv260101] PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing**
  - **tags:** [mlsys], [diffusion models], [DDIM inversion, motion analysis, pipelined scheduling, frame interpolation, long-form video editing]
  - **authors:** Mustafa Munir, Md Mostafijur Rahman, Kartikeya Bhardwaj, Paul Whatmough, Radu Marculescu
  - **institution:** The University of Texas at Austin, Qualcomm AI Research
  - **link:** https://arxiv.org/pdf/2512.24026
  - **contributions:** 1. A motion-aware frame selection method using SSIM and Optical Flow to skip editing of low-motion frames. 2. A pipelined task scheduling algorithm that splits videos into segments for parallel DDIM inversion and joint editing based on GPU memory. 3. A neural network-based interpolation technique to smooth border frames and interpolate skipped frames.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3787a383f3a5b9871e80fb8c9aaad731151ef891a01b7640701576d610e23860_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost of long-form video editing with diffusion models. It proposes PipeFlow, a method that uses motion analysis to skip frames, parallel pipelined processing, and interpolation to achieve linear scaling with video length. The method achieves significant speedups (up to 31.7x) over prior work while maintaining quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing] --> B[核心问题/Problem: Long-form video editing is computationally expensive due to DDIM inversion and joint editing.]
        A --> C[主要方法/Method: 1. Motion-aware frame skipping. 2. Pipelined parallel processing. 3. Neural interpolation.]
        A --> D[关键结果/Results: Achieves linear scaling, up to 31.7x speedup over baselines.]
    ```

- **[arXiv260101] ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment**
  - **tags:** [mlsys], [agent system], [prompt optimization, multi-agent architecture, decision tree protocols, zero-shot alignment, automated debugging]
  - **authors:** Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn
  - **institution:** Amity AI Research and Application Center
  - **link:** https://arxiv.org/pdf/2512.24040
  - **contributions:** 1. Introduces ROAD, a novel framework that treats prompt optimization as a dynamic debugging investigation, eliminating the need for curated gold-standard datasets. 2. Proposes a specialized multi-agent architecture (Analyzer, Optimizer, Coach) to convert unstructured failure logs into structured Decision Tree Protocols. 3. Demonstrates high sample efficiency and performance improvements on both academic benchmarks and a live production system, offering a data-efficient alternative to RL-based methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2d710802d4ef2f4627a2a20e4c9834618953664f1165bf78a33477b890319f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of optimizing LLM agents without requiring large, labeled datasets, which are often unavailable in real-world software engineering. It proposes ROAD, a framework that uses a multi-agent architecture to perform automated debugging on failure logs, converting them into structured protocols for improvement. The results show that ROAD is highly sample-efficient and significantly improves agent performance, providing a practical alternative to resource-intensive training methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ROAD: Reflective Optimization via Automated Debugging] --> B[核心问题/Problem: APO methods need large labeled datasets, but real-world has messy logs]
        A --> C[主要方法/Method: Multi-agent debugging (Analyzer, Optimizer, Coach) to create Decision Tree Protocols]
        A --> D[关键结果/Results: Sample-efficient, +5.6% success rate, +19% performance on complex tasks]
    ```

- **[arXiv260101] Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?**
  - **tags:** [sec], [adversarial attacks], [jailbreaking, content safety filters, LLM safety alignment, input/output filtering]
  - **authors:** Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang
  - **institution:** CISPA Helmholtz Center for Information Security, Max Planck Institute for Intelligent Systems, Southern University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24044
  - **contributions:** 1. First systematic evaluation of jailbreak attacks across the full LLM inference pipeline including input and output safety filters, 2. Demonstration that nearly all jailbreak techniques can be detected by at least one safety filter, challenging prior overestimations of attack success, 3. Identification of gaps in balancing recall and precision for optimizing protection and user experience in safety systems
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e40bd85a824936f762c47f0b98464b3b30e7733690440363f10ee46695920a8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in evaluating jailbreak attacks by systematically testing them against both LLM safety alignment and external content filters in the full deployment pipeline. The study finds that most jailbreaks can be detected by safety filters, suggesting prior success rates were overestimated, and highlights the need for better precision-recall balance in filter design.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?] --> B[核心问题/Problem: Jailbreak attacks bypass LLM safety alignment, prior evaluations neglect full deployment pipeline with content filters]
    A --> C[主要方法/Method: First systematic evaluation of jailbreak attacks across full inference pipeline including input/output filtering stages]
    A --> D[关键结果/Results: Most jailbreaks detectable by safety filters, prior success overestimated; need better recall-precision balance]
    ```

- **[arXiv260101] Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models**
  - **tags:** [nlp], [llm evaluation], [reliability, calibration, robustness, uncertainty quantification, composite score]
  - **authors:** Rohit Kumar Salla, Manoj Saravanan, Shrikar Reddy Kota
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.24058
  - **code:** https://github.com/rohitsalla/CRS.git
  - **contributions:** 1. A unified reliability metric (CRS) integrating calibration, robustness, and uncertainty. 2. A large-scale evaluation of ten open-source LLMs on five QA datasets. 3. The demonstration that CRS provides stable model rankings and uncovers hidden failure modes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the fragmented evaluation of Large Language Model (LLM) reliability by proposing the Composite Reliability Score (CRS), a unified metric that integrates calibration, robustness, and uncertainty quantification. Through experiments on ten open-source LLMs, the authors show that CRS provides consistent model rankings and reveals trade-offs between reliability dimensions. The main conclusion is that the most dependable LLM systems balance accuracy, robustness, and calibrated uncertainty.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Beyond Hallucinations: A Composite Score for Measuring Reliability] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM可靠性评估碎片化/Fragmented LLM Reliability Evaluation]
        C --> C1[提出CRS复合分数/Propose Composite Reliability Score (CRS)]
        D --> D1[CRS提供稳定模型排名/CRS Delivers Stable Model Rankings]
        D --> D2[揭示隐藏的失败模式/Uncovers Hidden Failure Modes]
    ```

- **[arXiv260101] Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds**
  - **tags:** [other], [parameterized complexity], [kidney exchange, FPT algorithm, W[1]-hardness, pathwidth, treewidth]
  - **authors:** Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu
  - **institution:** National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur
  - **link:** https://arxiv.org/pdf/2512.24037
  - **contributions:** 1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) ≈ O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>肾脏交换：更快的参数化算法与更紧的下界"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Kidney exchange is NP-complete<br>肾脏交换问题是NP完全问题"] --> P1["限制/Constraint<br>Exchange via small cycles & paths<br>通过小环和路径交换"]
        Method["主要方法/Method<br>Parameterized Complexity<br>参数化复杂度"] --> M1["参数/Parameter<br>Number of patients (t)<br>患者数量(t)"]
        Method --> M2["参数/Parameter<br>Graph pathwidth<br>图路径宽度"]
        Results["关键结果/Results"] --> R1["算法改进/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT算法: O*((4e)^t)"]
        Results --> R2["下界/Lower Bound<br>W[1]-hard for pathwidth<br>对路径宽度是W[1]-难的"]
    ```

- **[arXiv260101] AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives**
  - **tags:** [ai], [multi-modal reasoning], [audio-language models, hallucination mitigation, counterfactual hard negatives, preference alignment, temporal reasoning]
  - **authors:** Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang
  - **institution:** Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley
  - **link:** https://arxiv.org/pdf/2512.24052
  - **code:** https://github.com/LLM-VLM-GSL/AHA
  - **contributions:** 1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[Large Audio-Language Models (LALMs) suffer from hallucinations / 大型音频语言模型存在幻觉问题]
        Method[主要方法/Method] --> M1[Propose AHA framework with counterfactual hard negative mining / 提出AHA框架，使用反事实硬负例挖掘]
        Method --> M2[Construct preference dataset for alignment / 构建用于对齐的偏好数据集]
        Results[关键结果/Results] --> R1[13.7% improvement on AHA-Eval benchmark / 在AHA-Eval基准上提升13.7%]
        Results --> R2[Gains on public benchmarks (MMAU-Test, MMAR) / 在公开基准(MMAU-Test, MMAR)上取得提升]
    ```

- **[arXiv260101] Pathology Context Recalibration Network for Ocular Disease Recognition**
  - **tags:** [cv], [medical image analysis], [Pathology Recalibration Module, expert prior Guidance Adapter, Integrated Loss]
  - **authors:** Zunjie Xiao, Xiaoqing Zhang, Risa Higashita, Jiang Liu
  - **institution:** Southern University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24066
  - **contributions:** 1. Proposed a novel Pathology Recalibration Module (PRM) to leverage pathology context prior via pixel-wise context compression and pathology distribution concentration. 2. Introduced an expert prior Guidance Adapter (EPGA) to highlight significant pixel-wise regions by mining expert experience prior. 3. Designed an Integrated Loss (IL) to boost performance by considering sample-wise loss distributions and training label frequencies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2286b84bd9cbd7682cb726e99010af22979d096fd6391215bb5d0212bb1875c5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes PCRNet, a network for ocular disease recognition that incorporates a Pathology Recalibration Module and an expert prior Guidance Adapter to integrate clinical pathology context and expert experience priors into a DNN. An Integrated Loss is also introduced to handle sample and label imbalances. Experiments on three datasets show PCRNet's superiority over state-of-the-art methods, and visualizations explain its decision-making process.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Pathology Context Recalibration Network for Ocular Disease Recognition] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: DNNs ignore pathology context & expert experience priors for ocular disease recognition] --> P1[问题1/Sub-Problem: Lack of pathology context utilization]
        Problem --> P2[问题2/Sub-Problem: Lack of expert experience integration]
        Method[主要方法/Method: PCRNet] --> M1[模块1/Module: Pathology Recalibration Module (PRM)]
        Method --> M2[模块2/Module: expert prior Guidance Adapter (EPGA)]
        Method --> M3[组件/Component: Integrated Loss (IL)]
        Results[关键结果/Results] --> R1[结果1/Result: Superior performance on three datasets]
        Results --> R2[结果2/Result: Better than SOTA attention networks & loss methods]
        Results --> R3[结果3/Result: Visualization explains decision-making]
    ```

- **[arXiv260101] LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm**
  - **tags:** [mlsys], [agent system], [evolutionary search, plan-execute-summarize, MAP-Elites, multi-island model, adaptive Boltzmann selection]
  - **authors:** Chunhui Wan, Xunan Dai, Zhuo Wang, Minglei Li, Yanpeng Wang, Yinan Mao, Yu Lan, Zhiwen Xiao
  - **institution:** Baidu Inc.
  - **link:** https://arxiv.org/pdf/2512.24077
  - **code:** https://github.com/baidu-baige/LoongFlow
  - **contributions:** 1. Introduces the LoongFlow framework, a self-evolving agent that integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm to guide evolutionary search with structured reasoning. 2. Proposes a hybrid evolutionary memory system combining Multi-Island models, MAP-Elites, and adaptive Boltzmann selection to balance exploration-exploitation and maintain long-term architectural coherence. 3. Demonstrates state-of-the-art performance, outperforming baselines by up to 60% in evolutionary efficiency on benchmarks like AlphaEvolve and Kaggle competitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf0c647c1b7ee2e5581b15e60884e9d1c6f9da783e58c0ac3ab7c483db365b6_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces LoongFlow, a self-evolving agent framework that uses a cognitive "Plan-Execute-Summarize" paradigm and a hybrid memory system to guide evolutionary search with LLMs. This approach addresses issues like premature convergence in traditional methods. Evaluations show LoongFlow achieves superior solution quality with significantly reduced computational cost compared to existing baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LoongFlow: Directed Evolutionary Search<br>LoongFlow: 定向进化搜索] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统进化方法缺乏结构化推理<br>Traditional evolutionary methods lack structured reasoning]
        B --> B2[过早收敛与低效探索<br>Premature convergence & inefficient exploration]
        C --> C1[认知范式: 计划-执行-总结<br>Cognitive Paradigm: Plan-Execute-Summarize]
        C --> C2[混合进化记忆系统<br>Hybrid Evolutionary Memory System]
        D --> D1[进化效率提升高达60%<br>Evolutionary efficiency improved by up to 60%]
        D --> D2[发现更优解决方案<br>Discovers superior solutions]
    ```

- **[arXiv260101] Random Multiplexing**
  - **tags:** [sys], [wireless communication], [random multiplexing, AMP detection, power allocation, replica optimality, constrained capacity]
  - **authors:** Lei Liu, Yuhao Chi, Shunqi Huang, Zhaoyang Zhang
  - **institution:** Zhejiang University, Xidian University, Japan Advanced Institute of Science and Technology (JAIST)
  - **link:** https://arxiv.org/pdf/2512.24087
  - **contributions:** 1. Proposes a random multiplexing technique decoupled from physical channel structures, enabling application to arbitrary norm-bounded and spectrally convergent channel matrices. 2. Introduces a low-complexity cross-domain memory AMP (CD-MAMP) detector and derives optimal power allocations to minimize BER and maximize constrained capacity. 3. Investigates the optimal coding principle and proves the replica constrained-capacity optimality of the CD-MAMP detector for random multiplexing systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2f6d7e30422bda561811ec881c0aa17f79a2f4e7bf13203955c43c9c8fab17_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a random multiplexing technique to overcome the limitations of traditional and emerging multiplexing schemes (like OFDM and OTFS) which rely on specific channel structures. The method decouples from the physical channel, uses a random transform to create an input-isotropic equivalent channel, and employs a low-complexity AMP-type detector to achieve near-optimal performance for arbitrary norm-bounded channels. The authors validate the approach with theoretical analysis and numerical results, demonstrating its robustness and versatility in dynamic wireless environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Random Multiplexing] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统复用技术依赖特定信道结构/Traditional multiplexing relies on specific channel structures]
        B --> B2[在动态真实环境中鲁棒性有限/Limited robustness in dynamic real-world environments]
        C --> C1[随机复用技术/Random Multiplexing Technique]
        C --> C2[构建输入各向同性等效信道/Construct input-isotropic equivalent channel]
        C --> C3[CD-MAMP检测器/CD-MAMP Detector]
        D --> D1[保证渐近最优BER/Guarantees asymptotic optimal BER]
        D --> D2[推导最优功率分配/Derives optimal power allocation]
        D --> D3[验证理论结果/Validates theoretical findings]
    ```

- **[arXiv260101] FedLiTeCAN : A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection**
  - **tags:** [mlsys], [federated learning], [Controller Area Network (CAN), Intrusion Detection System (IDS), Transformer, Federated Learning, Lightweight Model]
  - **authors:** Devika S, Pratik Narang, Tejasvi Alladi
  - **institution:** BITS Pilani, Pilani Campus
  - **link:** https://arxiv.org/pdf/2512.24088
  - **code:** https://github.com/Transformer IDS
  - **contributions:** 1. Proposes FedLiTeCAN, a supervised intrusion detection framework using a two-layer encoder-only transformer in a Federated Learning environment. 2. Achieves a highly lightweight model (0.4MB) and fast real-time inference (0.608 ms per message), significantly outperforming baselines in size and speed. 3. Demonstrates strong generalization capability through cross-dataset analysis, achieving high accuracy on unseen cyber threats.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fccdaefce6abd0f6935fde57aedf3ec8956dfd7f60385f88dafe7a2ba8e6ef09_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes FedLiTeCAN, a lightweight Transformer-based Intrusion Detection System for CAN bus security, deployed using Federated Learning. The model is designed to be fast, small, and robust, achieving high accuracy and rapid inference on resource-constrained hardware like Jetson Nano. The results show it is an effective solution for real-time intrusion detection in vehicular networks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FedLiTeCAN: A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection] --> B[核心问题/Problem: CAN协议缺乏内置安全，需要轻量、快速、鲁棒的入侵检测系统]
        A --> C[主要方法/Method: 在联邦学习环境中使用轻量级两层编码器Transformer]
        A --> D[关键结果/Results: 模型小(0.4MB)，检测快(0.608ms)，精度高(98.5%)，泛化能力强]
    ```

- **[arXiv260101] Factorized Learning for Temporally Grounded Video-Language Models**
  - **tags:** [cv], [video-language models], [temporal grounding, factorized learning, preference optimization, evidence tokens, video understanding]
  - **authors:** Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng
  - **institution:** National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24097
  - **code:** https://github.com/nusnlp/d2vlm
  - **contributions:** 1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a "grounding then answering with evidence referencing" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Factorized Learning for Temporally Grounded Video-Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Existing models struggle with accurate temporal grounding for event-level perception. 现有模型在事件级感知的精确时间定位上存在困难。]
        C[主要方法/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). 提出D2VLM框架和因子化偏好优化算法。]
        D[关键结果/Results: Demonstrates clear advantage on various tasks. 在多种任务上展现出明显优势。]
    ```

- **[arXiv260101] Enhancing LLM Planning Capabilities through Intrinsic Self-Critique**
  - **tags:** [ai], [planning], [self-critique, few-shot learning, many-shot learning, iterative refinement, planning benchmarks]
  - **authors:** Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, Noah Fiedel
  - **institution:** Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.24103
  - **contributions:**  1. Proposes an intrinsic self-critique method for LLMs to improve their own planning outputs without external verifiers. 2. Demonstrates significant performance gains on established planning benchmarks (Blocksworld, Logistics, Mini-grid) over strong baselines. 3. Shows the method's applicability across different models and datasets, achieving new state-of-the-art results for the considered model class.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1646fd60ae3c2dbada7b54640bd9b507a010326fc6e75dd48733e4e2612eeefd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an intrinsic self-critique approach where LLMs iteratively critique and refine their own plans. The method, building upon few-shot and many-shot learning, significantly improves planning performance on benchmarks like Blocksworld without needing external verification. The results set a new state-of-the-art, demonstrating that self-critique can effectively enhance LLM planning capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enhancing LLM Planning Capabilities through Intrinsic Self-Critique] --> B[核心问题/Problem: LLM规划能力不足，传统自批判方法效果受质疑/LLM planning capability is limited, effectiveness of self-critique is questioned]
        A --> C[主要方法/Method: 内在自批判与迭代精炼/Intrinsic Self-Critique and Iterative Refinement]
        A --> D[关键结果/Results: 在规划基准测试中取得显著性能提升，达到新SOTA/Significant performance gains on planning benchmarks, achieving new SOTA]
    ```

- **[arXiv260101] Multilevel Fair Allocation**
  - **tags:** [ai], [fair division], [multilevel allocation, hierarchical fairness, matroid-rank utilities, Yankee Swap, tree-structured agents]
  - **authors:** Maxime Lucet, Nawal Benabbou, Aurélie Beynier, Nicolas Maudet
  - **institution:** LIP6, Sorbonne Université
  - **link:** https://arxiv.org/pdf/2512.24105
  - **contributions:** 1. Introduces the novel concept of multilevel fair allocation for hierarchical agent structures, 2. Proposes a generic polynomial-time sequential algorithm with theoretical fairness/efficiency guarantees, 3. Extends the General Yankee Swap algorithm to the multilevel setting with efficiency guarantees and demonstrated practical fairness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/931310fb9395b1c90ff2e48d3f63019e189cb72d20da462c04eb32b29d032896_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the problem of multilevel fair allocation, where resources are distributed among agents organized in a tree hierarchy. It proposes two algorithms: a generic top-down sequential algorithm with theoretical guarantees and an extension of the General Yankee Swap algorithm for the multilevel setting. The work provides both theoretical and practical solutions for achieving fairness and efficiency in hierarchical resource allocation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multilevel Fair Allocation<br>多级公平分配"] --> Problem["核心问题/Problem<br>Allocating resources in a tree-structured agent hierarchy<br>在树状代理层次结构中分配资源"]
        Root --> Method["主要方法/Method<br>1. Generic sequential top-down algorithm<br>通用顺序自上而下算法<br>2. Extended General Yankee Swap<br>扩展的通用Yankee Swap算法"]
        Root --> Results["关键结果/Results<br>Polynomial-time algorithms with fairness/efficiency guarantees<br>具有公平性/效率保证的多项式时间算法"]
    ```

- **[arXiv260101] Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design**
  - **tags:** [mlsys], [llm training], [Neural Architecture Search, Few-Shot Prompting, Code Deduplication, Automated Architecture Design, Lightweight Validation]
  - **authors:** Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte
  - **institution:** Computer Vision Lab, CAIDAS & IFI, University of Würzburg
  - **link:** https://arxiv.org/pdf/2512.24120
  - **contributions:** 1. Few-Shot Architecture Prompting (FSAP), a systematic study determining n=3 examples as optimal for LLM-based architecture generation in vision tasks. 2. Whitespace-Normalized Hash Validation, a lightweight (&lt;1ms) deduplication method providing 100x speedup over AST parsing. 3. A dataset-balanced evaluation methodology for comparing architectures across heterogeneous vision benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c601e530777a60f08e6c3607d74352f9046689f8debe4c26a2d893b33b09df97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses challenges in using LLMs for automated neural network architecture design in computer vision. It introduces a systematic few-shot prompting strategy (FSAP) and a fast deduplication method to prevent redundant training. The main conclusion is that using three examples in prompts best balances diversity and focus, and the lightweight validation enables efficient large-scale generation of unique architectures.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Enhancing LLM-Based Neural Network Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("自动化架构设计的挑战 / Challenges in Automated Architecture Design")
        Problem --> P2("LLM提示与验证策略未系统研究 / LLM Prompting & Validation Not Systematically Studied")
        Method --> M1("少样本架构提示 / Few-Shot Architecture Prompting (FSAP)")
        Method --> M2("空白标准化哈希验证 / Whitespace-Normalized Hash Validation")
        Results --> R1("n=3示例为最优 / n=3 Examples is Optimal")
        Results --> R2("验证速度提升100倍 / 100x Speedup in Validation")
        Results --> R3("生成1900个独特架构 / Generated 1,900 Unique Architectures")
    ```

- **[arXiv260101] CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation**
  - **tags:** [ai], [recommender systems], [cognitive architecture, Soar, large language models, explainable recommendation, online learning]
  - **authors:** Jiaxin Hu, Tao Wang, Bingsan Yang, Hongrun Wang
  - **institution:** Sun Yat-Sen University
  - **link:** https://arxiv.org/pdf/2512.24113
  - **contributions:** 1. Proposes CogRec, a novel cognitive recommender agent that synergizes the strengths of Large Language Models (LLMs) and the Soar cognitive architecture. 2. Introduces a learning paradigm where Soar's symbolic reasoning is initialized and dynamically augmented by an LLM via chunking, enabling robust online learning. 3. Demonstrates that the agent provides highly interpretable rationales and shows advantages in accuracy, explainability, and addressing the long-tail problem on public datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6840c5950eff3f651f3ba24036f583190638dc59f4b4c5d9c32d2ca2079cb860_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CogRec, a cognitive recommender agent that fuses Large Language Models (LLMs) with the Soar cognitive architecture to address the black-box nature and limited online learning of LLMs. The agent uses Soar for structured reasoning and dynamically queries an LLM to generate new symbolic rules when needed, enabling continuous knowledge evolution. Evaluations show CogRec improves recommendation accuracy, explainability, and performance on long-tail items.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CogRec: A Cognitive Recommender Agent] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLMs: 黑盒, 幻觉, 难在线学习/LLMs: Black-Box, Hallucination, Limited Online Learning]
        B --> B2[认知架构: 知识获取困难/Cognitive Architectures: Laborious Knowledge Acquisition]
        C --> C1[融合LLM与Soar/Fuse LLM and Soar]
        C --> C2[感知-认知-行动循环/PCA Cycle]
        C --> C3[LLM初始化与动态查询/LLM for Initialization & Dynamic Query]
        C --> C4[Soar组块化在线学习/Soar Chunking for Online Learning]
        D --> D1[提升推荐准确性/Improved Recommendation Accuracy]
        D --> D2[增强可解释性/Enhanced Explainability]
        D --> D3[有效处理长尾问题/Effective Long-Tail Handling]
    ```

- **[arXiv260101] Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training**
  - **tags:** [ai], [embodied ai], [Embodied Reasoning, Action Tokenization, Vision-Language-Action Models, Flow Matching, Discrete Control]
  - **authors:** Yi Liu, Sukai Wang, Dafeng Wei, Xiaowei Cai, Linqing Zhong, Jiange Yang, Guanghui Ren, Jinyu Zhang, Maoqing Yao, Chuankang Li, Xindong He, Liliang Chen, Jianlan Luo
  - **institution:** AgiBot Research, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.24125
  - **contributions:** 1. Introduces ERIQ, a large-scale benchmark for decoupled evaluation of embodied reasoning in robotic manipulation. 2. Proposes FACT, a flow-matching-based action tokenizer for high-fidelity discretization of continuous control. 3. Presents GenieReasoner, a unified model that jointly optimizes reasoning and action in a discrete space, outperforming baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1a2e4506fbecb3e90de4ef501197f9125c51ac19b6cfc789fdfcb6e035edf72_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of combining broad generalization with precise execution in general-purpose robotics. It introduces a benchmark (ERIQ) to diagnose reasoning capabilities and a method (FACT) to tokenize actions, leading to a unified model (GenieReasoner) that improves performance on real-world tasks. The work provides a framework to overcome the reasoning-precision trade-off in robotic manipulation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>VLA模型难以兼顾泛化与精确执行"] --> P1["泛化与精度权衡<br>Reasoning-Precision Trade-off"]
        Method["主要方法/Method"] --> M1["评估基准: ERIQ<br>Benchmark: ERIQ"]
        Method --> M2["动作分词器: FACT<br>Action Tokenizer: FACT"]
        Method --> M3["统一模型: GenieReasoner<br>Unified Model: GenieReasoner"]
        Results["关键结果/Results"] --> R1["揭示了推理能力与泛化的正相关<br>Revealed positive correlation"]
        Results --> R2["在真实任务中超越基线<br>Outperformed baselines in real-world tasks"]
    ```

- **[arXiv260101] OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [post-training quantization, weight outliers, rotation, GPTQ, data-free]
  - **authors:** Advait Gadhikar, Riccardo Grazzi, James Hensman
  - **institution:** CISPA Helmholtz Center for Information Security, Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.24124
  - **contributions:** 1. Proposes OptRot, a data-free method that learns fusible rotations by minimizing a principled, cheap proxy objective (element-wise fourth power of weights) to reduce weight outliers for quantization. 2. Demonstrates that OptRot outperforms existing rotation methods (Hadamard, SpinQuant, OSTQuant) for weight quantization and improves W4A8 activation quantization. 3. Introduces OptRot+, a data-dependent variant that incorporates activation covariance information for further performance gains, while highlighting a trade-off between weight and activation quantization in the W4A4 setting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of quantizing Large Language Models (LLMs) by mitigating weight outliers. It proposes OptRot, a data-free method that learns efficient rotations to minimize a proxy for weight quantization error, and shows it outperforms existing techniques for weight and W4A8 activation quantization. The work also introduces an enhanced data-dependent variant and reveals a performance trade-off in more aggressive quantization settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization] --> B[核心问题/Problem: LLM权重和激活中的异常值使量化困难/Outliers in LLM weights & activations make quantization difficult]
        A --> C[主要方法/Method: 通过最小化旋转后权重的四阶矩学习可融合的旋转/Learn fusible rotations by minimizing element-wise fourth power of rotated weights (OptRot)]
        A --> D[关键结果/Results: OptRot在权重量化上优于现有方法，改进W4A8激活量化，W4A4下存在权衡/OptRot outperforms existing methods for weight quant., improves W4A8 activation quant., trade-off in W4A4 setting]
    ```

- **[arXiv260101] GARDO: Reinforcing Diffusion Models without Reward Hacking**
  - **tags:** [ai], [reinforcement learning], [reward hacking, diffusion models, regularization, mode collapse, online RL]
  - **authors:** Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan
  - **institution:** Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.24138
  - **code:** https://tinnerhrhe.github.io/gardo_project
  - **contributions:** 1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GARDO: Reinforcing Diffusion Models without Reward Hacking] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Reward Hacking in RL for Diffusion Models/扩散模型RL中的奖励破解]
        B --> B2[Proxy Reward Mismatch & Mode Collapse/代理奖励不匹配与模式崩溃]
        C --> C1[Gated & Adaptive Regularization/门控自适应正则化]
        C --> C2[Diversity-aware Reward Optimization/多样性感知奖励优化]
        D --> D1[Mitigates Reward Hacking/缓解奖励破解]
        D --> D2[Enhances Diversity & Maintains Efficiency/提升多样性并保持效率]
    ```

- **[arXiv260101] Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks**
  - **tags:** [ai], [interactive reasoning], [graph-based exploration, state-space exploration, visual salience, training-free, ARC-AGI-3]
  - **authors:** Evgenii Rudakov, Jonathan Shock, Benjamin Ultan Cowley
  - **institution:** University of Helsinki, University of Cape Town
  - **link:** https://arxiv.org/pdf/2512.24156
  - **code:** https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore
  - **contributions:** 1. Proposes a training-free, graph-based method for systematic state-space exploration in interactive reasoning tasks. 2. Introduces a strategy that segments visual frames and prioritizes actions based on visual salience and shortest paths to untested state-action pairs. 3. Demonstrates the method's strong performance on the ARC-AGI-3 benchmark, significantly outperforming state-of-the-art LLM-based agents and establishing a strong non-learning baseline.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5463388c7bf347baae8d407681f498a8378e19ba7ae57ea056451d79518546_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a training-free, graph-based exploration method for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. The method uses visual frame segmentation and maintains a graph of states to prioritize exploration, solving a median of 30 out of 52 levels and outperforming leading LLMs. The results show that explicit, structured exploration is a powerful baseline for tasks where current LLMs fail.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks] --> B[核心问题/Problem: LLMs fail at interactive reasoning in sparse-feedback environments like ARC-AGI-3]
        A --> C[主要方法/Method: Training-free graph-based exploration with visual segmentation and action prioritization]
        A --> D[关键结果/Results: Solves median 30/52 levels, ranks 3rd, outperforms frontier LLM agents]
    ```

- **[arXiv260101] Developing controlled natural language for formal specification patterns using AI assistants**
  - **tags:** [se], [requirements engineering], [controlled natural language, formal specification patterns, AI assistant, temporal requirements, syntax formalization]
  - **authors:** Natalia Garanina, Vladimir Zyubin, Igor Anureev
  - **institution:** Institute of Automation and Electrometry, Siberian Branch of the Russian Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.24159
  - **contributions:** 1. A novel three-stage method for systematically constructing a Controlled Natural Language (CNL) for requirements using an AI assistant. 2. A prompt engineering approach that leverages a generalized template and formal semantics to generate a diverse corpus of natural language patterns. 3. Formalization of CNL syntax based on grammatical analysis of AI-generated patterns, specifically validated for event-driven temporal requirements.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dd04888cb9a22a839acc0700d35470498034dd8676cff5e34d6cb903fd6ab9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method to systematically develop a controlled natural language (CNL) for formal requirements specification using an AI assistant. The method involves creating a generalized pattern, using an AI to generate a corpus of natural language variants, and then formalizing the CNL syntax from the results. The approach was successfully tested for specifying event-driven temporal requirements, producing a language with built-in formal semantics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Developing controlled natural language for formal specification patterns using AI assistants] --> B(核心问题/Problem: How to systematically construct a Controlled Natural Language (CNL) for formal requirements specification?);
        A --> C(主要方法/Method: Three-stage method using AI assistant: 1. Compile generalized pattern, 2. Generate corpus via prompt, 3. Formalize syntax from grammar analysis.);
        A --> D(关键结果/Results: Method successfully tested for event-driven temporal requirements, yielding a CNL with formal semantics by design.);
    ```

- **[arXiv260101] PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds**
  - **tags:** [cv], [3D point cloud regression], [PointRAFT, partial point clouds, object height embedding, PointNet++, RGB-D]
  - **authors:** Pieter M. Blok, Haozhou Wang, Hyun Kwon Suh, Peicheng Wang, James Burridge, Wei Guo
  - **institution:** The University of Tokyo, Sejong University
  - **link:** https://arxiv.org/pdf/2512.24193
  - **code:** https://github.com/pieterblok/pointraft.git
  - **contributions:** 1. Proposed PointRAFT, a high-throughput point cloud regression network for directly predicting continuous 3D shape properties from partial point clouds. 2. Introduced a novel object height embedding as an architectural component to incorporate tuber height, improving regression performance under occlusion. 3. Demonstrated superior performance and real-time capability on a large-scale agricultural dataset, achieving high accuracy for potato tuber weight prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c71915dfc1ce9e8203a646c9242d84788697d59124a23181fa869b682fd4cdf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of systematically underestimating potato tuber weight from incomplete 3D point clouds captured on harvesters. It proposes PointRAFT, a deep learning network that directly regresses weight from partial point clouds using a novel object height embedding. The method significantly outperforms baselines and achieves real-time processing speeds suitable for commercial harvesters.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds"] --> Problem["核心问题/Problem: Incomplete point clouds from RGB-D lead to weight underestimation"]
        Root --> Method["主要方法/Method: PointRAFT network with object height embedding for direct regression"]
        Root --> Results["关键结果/Results: Low error (MAE 12.0g), high speed (150 tubers/sec)"]
    ```

- **[arXiv260101] SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents**
  - **tags:** [mlsys], [agent system], [Science Context Protocol, autonomous scientific agents, unified resource integration, experiment lifecycle management, federated servers]
  - **authors:** Yankai Jiang, Wenjie Lou, Lilong Wang, Zhenyu Tang, Shiyang Feng, Jiaxuan Lu, Haoran Sun, Yaning Pan, Shuang Gu, Haoyang Su, Feng Liu, Wangxu Wei, Pan Tan, Dongzhan Zhou, Fenghua Ling, Cheng Tan, Bo Zhang, Xiaosong Wang, Lei Bai, Bowen Zhou
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.24189
  - **code:** https://github.com/InternScience/scp
  - **contributions:** 1. Proposes SCP, an open-source protocol-level standard for universally describing and invoking heterogeneous scientific resources (tools, models, datasets, instruments)., 2. Introduces a secure service architecture (centralized Hub & federated Servers) for managing the complete, traceable experiment lifecycle and enforcing fine-grained access control., 3. Demonstrates a functional platform built on SCP, integrating over 1,600 tool resources to facilitate secure, large-scale, multi-institution collaboration between AI agents and human researchers, reducing integration overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8885d3eeb276d5044f777dc33201481a07be8831aaff9d61726e4a6c13e821be_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces the Science Context Protocol (SCP), an open-source standard designed to address the fragmentation and bespoke nature of current autonomous scientific agent systems. SCP provides a universal specification for resource integration and a secure service architecture for experiment orchestration, enabling seamless, large-scale collaboration across platforms. The authors conclude that SCP establishes essential infrastructure for scalable, reproducible, and agent-driven science by standardizing context and tool orchestration at the protocol level.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Bespoke, isolated agent systems; Lack of shared protocol for heterogeneous resources"] --> Problem_Detail["具体挑战/Specific Challenges<br>Difficult to deploy beyond single lab; Hard to reuse components & reproduce workflows"]
        Method["主要方法/Method<br>Science Context Protocol (SCP)"] --> Method_Pillar1["支柱1: 统一资源集成/Unified Resource Integration<br>Universal spec for describing/invoking tools, models, data, instruments"]
        Method --> Method_Pillar2["支柱2: 实验生命周期管理/Experiment Lifecycle Management<br>Secure architecture (Hub & Servers) for registration, execution, monitoring"]
        Results["关键结果/Results<br>Enables global web of autonomous agents"] --> Results_Outcome1["成果1: 大规模生态系统/Large-scale Ecosystem<br>1,600+ integrated tool resources"]
        Results --> Results_Outcome2["成果2: 促进协作/Facilitates Collaboration<br>Reduces integration overhead; Enhances reproducibility"]
    ```

- **[arXiv260101] Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem**
  - **tags:** [ai], [reinforcement learning], [Fleet Size and Mix Vehicle Routing Problem (FSMVRP), deep reinforcement learning (DRL), Markov Decision Process (MDP), fleet-and-route integrated policy network (FRIPN), remaining graph embedding]
  - **authors:** Pengfu Wan, Jiawei Chen, Gangyan Xu
  - **institution:** The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.24251
  - **contributions:** 1. Formulates the Fleet Size and Mix Vehicle Routing Problem (FSMVRP) as a Markov Decision Process (MDP) for a deep reinforcement learning approach. 2. Proposes a novel policy network (FRIPN) that integrates fleet composition and routing decisions into a single model. 3. Introduces specialized input embeddings, including a remaining graph embedding, to enhance decision-making for vehicle employment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3969891b48accce35280355d106951820196973739958b782a307a2a3df23aa3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a deep reinforcement learning method to solve the complex Fleet Size and Mix Vehicle Routing Problem (FSMVRP). The core innovation is a policy network called FRIPN that jointly decides on fleet composition and routing. Experiments show the method is computationally efficient and scalable, producing near-optimal solutions quickly, especially for large-scale problems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem] --> B(核心问题/Problem: FSMVRP - simultaneous fleet composition & routing)
        A --> C(主要方法/Method: DRL-based MDP formulation with FRIPN policy network & remaining graph embedding)
        A --> D(关键结果/Results: Near-optimal solutions in seconds, high computational efficiency & scalability)
    ```

- **[arXiv260101] Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment**
  - **tags:** [nlp], [safety alignment], [risk-aware optimization, constrained policy optimization, nested risk measures, stepwise alignment, tail risk suppression]
  - **authors:** Lijun Zhang, Lin Li, Wei Wei, Yajie Qi, Huizhong Song, Jun Wang, Yaodong Yang, Jiye Liang
  - **institution:** Shanxi University, University College London, Peking University
  - **link:** https://arxiv.org/pdf/2512.24263
  - **contributions:** 1. Proposes Risk-aware Stepwise Alignment (RSA), a novel method that explicitly incorporates risk awareness into LLM policy optimization using nested risk measures. 2. Formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it via a stepwise procedure for token-level policy updates. 3. Provides theoretical analysis on policy optimality and demonstrates experimentally that the method ensures strong safety and suppresses low-probability, high-impact harmful behaviors while maintaining helpfulness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4111a1181bf60ea84ea5a2f52c3748bc1804913ef18afecfc6e9f7bd9f15f5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of risk-neutral safety alignment methods for large language models, which struggle with risks from policy deviation and rare catastrophic outputs. It proposes Risk-aware Stepwise Alignment (RSA), a method that uses nested risk measures for token-level constrained policy optimization. Experimental results show RSA achieves high helpfulness while ensuring strong safety and significantly suppressing tail risks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment] --> B[核心问题/Problem: Risk-neutral alignment insufficient for policy deviation & rare catastrophic harms]
        A --> C[主要方法/Method: Risk-aware Stepwise Alignment (RSA) using nested risk measures for token-level policy optimization]
        A --> D[关键结果/Results: High helpfulness, strong safety, significant tail risk suppression]
    ```

- **[arXiv260101] One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training**
  - **tags:** [cv], [medical image synthesis], [one-shot synthesis, language-guided concept disentanglement, data augmentation, rare disease, generative framework]
  - **authors:** Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Zhihua Wang, Fei Wu, Quanlin Li, Pinghong Zhou, Shuo Wang, Xian Yang
  - **institution:** Zhejiang University, Fudan University, Imperial College London, The University of Manchester
  - **link:** https://arxiv.org/pdf/2512.24278
  - **contributions:** 1. Proposed EndoRare, a one-shot, retraining-free generative framework for synthesizing diverse, high-fidelity images of rare gastrointestinal lesions from a single reference image., 2. Introduced a language-guided concept disentanglement method to separate pathognomonic lesion features from non-diagnostic attributes, ensuring diversity while preserving diagnostic fidelity., 3. Demonstrated that synthetic images improve both AI classifier performance (true positive rate) and novice clinician diagnostic accuracy (recall and precision) for rare pathologies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf643227c51e8cd7c3969dfe80d336673878c555397635a5c9831a5997ed5bd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data scarcity problem for rare gastrointestinal lesions in AI development and clinical training. It proposes EndoRare, a one-shot generative framework that uses language-guided concept disentanglement to synthesize diverse and clinically plausible lesion images from a single example. The results show that these synthetic images significantly enhance the performance of AI classifiers and improve the diagnostic accuracy of novice endoscopists.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[One-shot synthesis of rare gastrointestinal lesions] --> B[核心问题/Problem: Rare lesions are infrequent, limiting AI model data and clinician training.]
        A --> C[主要方法/Method: EndoRare framework uses one-shot, language-guided concept disentanglement to generate diverse, high-fidelity synthetic images.]
        A --> D[关键结果/Results: Improves AI classifier true positive rate and novice clinician recall & precision.]
    ```

- **[arXiv260101] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation**
  - **tags:** [mlsys], [multi-modal training], [counterfactual video generation, visual hallucination, diffusion-based video editing, advantage normalization, contrastive training]
  - **authors:** Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang
  - **institution:** Tsinghua University, Beihang University, AMAP (Alibaba Group)
  - **link:** https://arxiv.org/pdf/2512.24271
  - **code:** https://amap-ml.github.io/Taming-Hallucinations/
  - **contributions:** 1. Introduces DualityForge, a framework for automatically synthesizing counterfactual video QA data using controllable diffusion-based video editing. 2. Presents DualityVidQA, a large-scale video dataset built using DualityForge to mitigate MLLM hallucinations. 3. Proposes DNA-Train, a two-stage SFT-RL training regime with pair-wise advantage normalization for stable and efficient policy optimization on contrastive data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa10c3c7d7f412fdbab92df4afa93ab6b3653346b89fc3443bf25d8615391b81_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of visual hallucinations in Multimodal Large Language Models (MLLMs) when processing counterfactual videos. The proposed solution, DualityForge, synthesizes counterfactual video QA data for training, and a novel training method, DNA-Train, leverages this data to improve grounding. Experiments show the method significantly reduces hallucinations and improves performance on both hallucination and general benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Taming Hallucinations: Boosting MLLMs' Video Understanding<br>驯服幻觉：通过反事实视频生成提升MLLM视频理解"] --> B["核心问题/Problem<br>MLLMs over-rely on language priors, causing visual hallucinations on counterfactual videos."]
        A --> C["主要方法/Method<br>DualityForge: Counterfactual video & QA synthesis.<br>DNA-Train: Contrastive SFT-RL training."]
        A --> D["关键结果/Results<br>24.0% hallucination reduction.<br>Strong generalization on benchmarks."]
    ```

- **[arXiv260101] DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments**
  - **tags:** [ai], [reinforcement learning], [temporal graph attention, hierarchical graph pooling, multi-modal fusion, UGV navigation, deep reinforcement learning]
  - **authors:** Ruitong Li, Lin Zhang, Yuenan Zhao, Chengxin Liu, Ran Song, Wei Zhang
  - **institution:** The affiliations are not explicitly provided in the given content. Based on the author names, it is not possible to reliably infer the main research institution(s).
  - **link:** https://arxiv.org/pdf/2512.24284
  - **contributions:** 1. Proposed a DRL-based navigation framework (DRL-TH) that integrates historical observations and adaptively fuses multi-modal information. 2. Introduced a Temporal-Guided Graph Attention Network (TG-GAT) to capture temporal context and scene evolution between consecutive frames. 3. Designed a Graph Hierarchical Abstraction Module (GHAM) to dynamically and balance multi-scale representations from RGB and LiDAR features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83abe1de24f9a7e490d93db45dec754f2a2ff7f3de84d6e6983a358e1d9dff40_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes DRL-TH, a deep reinforcement learning framework for UGV navigation in crowded environments. It addresses limitations of single-frame observation and simple fusion by introducing a temporal graph attention network and a hierarchical graph pooling module for adaptive multi-modal feature integration. Experiments and real-world deployment show that DRL-TH outperforms existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DRL-TH: UGV导航框架] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[单帧观测/Single-frame observation]
        Problem --> P2[简单多模态融合/Simple multi-modal fusion]
        P1 --> P1_Sub[限制动态适应性/Limits dynamic adaptability]
        P2 --> P2_Sub[难以捕捉时序上下文/Hard to capture temporal context]
        Method[主要方法/Method] --> M1[时序引导图注意力网络/Temporal-Guided GAT (TG-GAT)]
        Method --> M2[图层次抽象模块/Graph Hierarchical Abstraction Module (GHAM)]
        M1 --> M1_Sub[捕捉连续帧关联/Captures correlations between consecutive frames]
        M2 --> M2_Sub[动态融合RGB与LiDAR特征/Dynamically fuses RGB & LiDAR features]
        Results[关键结果/Results] --> R1[性能超越现有方法/Outperforms existing methods]
        Results --> R2[真实UGV上表现良好/Performs well on real UGV]
    ```

- **[arXiv260101] Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction**
  - **tags:** [cv], [medical image analysis], [CT preprocessing, quality control, foundation models, lung cancer screening, validation]
  - **authors:** Md. Enamul Hoq, Linda Larson-Prior, Fred Prior
  - **institution:** University of Arkansas for Medical Sciences
  - **link:** https://arxiv.org/pdf/2512.24294
  - **contributions:** 1. Development and validation of Virtual-Eyes, a novel, anatomically targeted 16-bit CT quality-control pipeline for lung cancer screening. 2. Quantitative demonstration that such preprocessing significantly improves the performance and calibration of generalist foundation models (e.g., RAD-DINO) for cancer risk prediction. 3. Discovery that specialist models (e.g., Sybil) can degrade with the same preprocessing, revealing their potential reliance on contextual shortcuts in raw clinical data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6f230cd5094d46243a5c6fe260cf946e5540833e1a82b86423bbe80a292b76c_w640_q70.webp
  - **Simple LLM Summary:** This paper develops Virtual-Eyes, a quality-control pipeline for lung CT scans that standardizes resolution and extracts lung regions. The study finds that this preprocessing significantly boosts the cancer risk prediction performance of generalist foundation models but can harm specialist models that have adapted to raw, unprocessed clinical data.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Virtual-Eyes: 肺癌CT质量控制流程验证] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>LDCT预处理影响未量化] --> P1[缺乏量化/Lack of Quantification]
        Problem --> P2[通用vs专用模型差异/Generalist vs. Specialist]
        Method[主要方法/Method<br>Virtual-Eyes Pipeline] --> M1[质量控制/Quality Control<br>512x512, 过滤系列]
        Method --> M2[肺块提取/Lung Block Extraction<br>HU过滤, 覆盖评分]
        Method --> M3[模型评估/Model Evaluation<br>RAD-DINO, Sybil等]
        Results[关键结果/Results<br>预处理效果不同] --> R1[提升通用模型/Improves Generalist FMs<br>RAD-DINO AUC↑]
        Results --> R2[损害专用模型/Harms Specialist Models<br>Sybil AUC↓]
        Results --> R3[揭示捷径学习/Reveals Shortcut Learning<br>上下文依赖]
    ```

- **[arXiv260101] Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction**
  - **tags:** [mlsys], [multi-modal inference], [reliability-aware dynamic weighting, cross-modal contrastive learning, semantic-aware beam prediction, low-altitude UAV, multi-modal learning]
  - **authors:** Haojin Li, Anbang Zhang, Chen Sun, Chenyuan Feng, Kaiqian Qu, Tony Q. S. Quek, Haijun Zhang
  - **institution:** University of Science and Technology Beijing, Sony China Research Laboratory, Shandong University, Southeast University, University of Exeter, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.24324
  - **contributions:** 1. Proposes a reliability-aware dynamic weighting scheme that adaptively allocates contributions across different modalities (e.g., visual, posture, geospatial) based on their instantaneous reliability, moving beyond fixed-weight approaches. 2. Introduces a semantic-aware multi-modal beam prediction framework (SaM²B) that uses cross-modal contrastive learning to align multi-source representations into a shared semantic space, enhancing robustness to noise and distribution shifts. 3. Validates the proposed SaM²B framework on real-world low-altitude UAV datasets, demonstrating superior performance over baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fa219c5db4bfd0eeac8ee93679e8eba9ceb6e2e5ce7336d45afc6686f1d8162_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of unreliable beam prediction in multi-modal UAV communications caused by static weighting and modal misalignment. It proposes SaM²B, a framework that uses reliability-aware dynamic weighting and cross-modal contrastive learning to adaptively fuse modalities and align their semantics. Experiments on real-world datasets show SaM²B outperforms existing baseline methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[静态权重与模态失配/Static Weighting & Modal Mismatch]
        B --> B2[跨场景泛化弱/Weak Cross-Scenario Generalization]
        C --> C1[可靠性感知动态加权/Reliability-Aware Dynamic Weighting]
        C --> C2[跨模态对比学习/Cross-Modal Contrastive Learning]
        C --> C3[语义感知多模态框架/Semantic-Aware Multi-Modal Framework (SaM²B)]
        D --> D1[真实数据集验证/Validated on Real-World UAV Datasets]
        D --> D2[优于基线方法/Superior to Baseline Methods]
    ```

- **[arXiv260101] DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images**
  - **tags:** [cv], [medical image analysis], [visual question answering, lesion segmentation, multimodal models]
  - **authors:** Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy
  - **institution:** Microsoft, University of Washington, Stanford University, Hospital Clinic of Barcelona
  - **link:** https://arxiv.org/pdf/2512.24340
  - **code:** https://osf.io/72rp3
  - **contributions:** 1. Introduction of the Dermatology Assessment Schema (DAS), a novel expert-developed framework for structured dermatological feature assessment. 2. Release of DermaVQA-DAS, an extended dataset supporting closed-ended question answering and lesion segmentation on patient-generated images. 3. Comprehensive benchmarking of state-of-the-art multimodal models on the new tasks, analyzing the impact of prompt design on segmentation performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of patient-centered benchmarks in dermatology by introducing DermaVQA-DAS, a dataset extension built upon a novel expert-developed assessment schema (DAS) for structured feature annotation. It supports two tasks—closed-ended visual question answering and lesion segmentation—on patient-generated images and queries. The study benchmarks modern multimodal models, finding strong QA performance and demonstrating that prompt design significantly impacts segmentation results.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DermaVQA-DAS] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[现有数据集缺乏患者视角/Existing datasets lack patient perspective]
        P1 --> P2[限制以患者为中心的护理应用/Limits patient-centered care applications]
    
        Method[主要方法/Method] --> M1[提出皮肤病评估框架(DAS)/Propose Dermatology Assessment Schema (DAS)]
        M1 --> M2[扩展DermaVQA数据集/Extend DermaVQA dataset]
        M2 --> M3[支持两项任务:封闭式问答与分割/Support two tasks: closed QA & segmentation]
    
        Results[关键结果/Results] --> R1[提示设计影响分割性能/Prompt design impacts segmentation performance]
        R1 --> R2[模型在QA上表现强劲/Models perform strongly on QA]
        R2 --> R3[公开数据集与评估协议/Publicly release dataset & evaluation protocols]
    ```

- **[arXiv260101] FedSecureFormer: A Fast, Federated and Secure Transformer Framework for Lightweight Intrusion Detection in Connected and Autonomous Vehicles**
  - **tags:** [mlsys], [federated learning], [Transformer, Federated Learning, Differential Privacy, Intrusion Detection, Connected and Autonomous Vehicles]
  - **authors:** Devika S, Vishnu Hari, Pratik Narang, Tejasvi Alladi, F. Richard Yu
  - **institution:** BITS Pilani, Pilani Campus; Carleton University
  - **link:** https://arxiv.org/pdf/2512.24345
  - **contributions:** 1. Proposed FedSecureFormer, a lightweight encoder-only Transformer model with only 1.7M parameters for efficient intrusion detection in CAVs. 2. Integrated the model within a Federated Learning framework with Differential Privacy to enhance data privacy and enable collaborative training. 3. Demonstrated high performance (93.69% accuracy on 19 attacks) and fast inference (3.7775 ms on Jetson Nano), making it 100x faster than SOTA models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7914a9d504b1c6d9fb36172de5c5c3554e336d7411316a13768fea0423324240_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses cybersecurity threats in Connected and Autonomous Vehicles (CAVs) by proposing FedSecureFormer, a lightweight Transformer model trained using Federated Learning and Differential Privacy. The model achieves high accuracy for intrusion detection while ensuring data privacy and demonstrates extremely fast inference on edge devices, making it suitable for real-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[FedSecureFormer: A Fast, Federated and Secure Transformer Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: CAV网络安全威胁 / Cybersecurity threats in CAVs]
        Method[主要方法/Method: 轻量Transformer + 联邦学习 + 差分隐私 / Lightweight Transformer + FL + DP]
        Results[关键结果/Results: 高精度 & 快速推理 / High Accuracy & Fast Inference]
    ```

- **[arXiv260101] Skim-Aware Contrastive Learning for Efficient Document Representation**
  - **tags:** [nlp], [document representation], [contrastive learning, natural language inference, long document, self-supervised learning, hierarchical transformer]
  - **authors:** Waheed Ahmed Abro, Zied Bouraoui
  - **institution:** Univ Artois, CNRS
  - **link:** https://arxiv.org/pdf/2512.24373
  - **contributions:** 1. A novel self-supervised contrastive learning framework inspired by human skimming behavior for long document representation. 2. A method that uses random section masking and an NLI-based contrastive objective to align relevant parts and distance unrelated ones. 3. Demonstrated significant improvements in both accuracy and efficiency on legal and biomedical document tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae38f3776c3982e71d1fd2fda48c20229eb63f1918544341ca35ab3cb48a02af_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of efficiently representing long documents like legal and medical texts. It proposes a self-supervised contrastive learning method that mimics human skimming by masking sections and using an NLI objective to relate document parts. Experiments show the method achieves better accuracy and computational efficiency compared to existing approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Skim-Aware Contrastive Learning for Efficient Document Representation] --> B[核心问题/Problem: 长文档表示困难/Inefficient long document representation]
        A --> C[主要方法/Method: 基于NLI的对比学习/NLI-based contrastive learning with section masking]
        A --> D[关键结果/Results: 在准确性和效率上取得显著提升/Significant gains in accuracy and efficiency]
    ```

- **[arXiv260101] Tubular Riemannian Laplace Approximations for Bayesian Neural Networks**
  - **tags:** [ai], [bayesian deep learning], [Laplace approximation, Riemannian geometry, uncertainty quantification, Bayesian neural networks, model calibration]
  - **authors:** Rodrigo Pereira David
  - **institution:** National Institute of Metrology, Technology and Quality (Inmetro)
  - **link:** https://arxiv.org/pdf/2512.24381
  - **contributions:** 1. Introduces the Tubular Riemannian Laplace (TRL) approximation, a novel method that models the posterior as a probabilistic tube following low-loss valleys induced by functional symmetries. 2. Proposes using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty, adapting to the anisotropic, curved loss surfaces of deep models. 3. Demonstrates empirically that TRL achieves calibration comparable to Deep Ensembles on ResNet-18 (CIFAR-10/100) at a fraction (1/5) of the training cost, bridging single-model efficiency with ensemble-grade reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbef449cd138ae804891e277de76405797dfc3e78511bcb03bf13e56823c9746_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the poor calibration of traditional Euclidean Laplace approximations in Bayesian Neural Networks. It proposes the Tubular Riemannian Laplace (TRL) approximation, which models the posterior as a tube using a Riemannian metric to better capture parameter space geometry. The method achieves excellent uncertainty calibration on image classification tasks, matching Deep Ensembles' reliability with significantly lower computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Tubular Riemannian Laplace Approximations<br>管状黎曼拉普拉斯近似] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统拉普拉斯近似在深度模型中校准不佳<br>Traditional Laplace approximations struggle with calibration in deep models]
        C --> C1[提出管状黎曼拉普拉斯(TRL)近似<br>Propose Tubular Riemannian Laplace (TRL) approximation]
        C1 --> C2[使用Fisher/Gauss-Newton度量建模概率管<br>Model probabilistic tube using Fisher/Gauss-Newton metric]
        D --> D1[在ResNet-18上实现优秀校准<br>Achieves excellent calibration on ResNet-18]
        D1 --> D2[匹配集成方法可靠性，成本仅1/5<br>Matches ensemble reliability at 1/5 training cost]
    ```

- **[arXiv260101] Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack**
  - **tags:** [mlsys], [fault-tolerance], [Functional Mockup Unit (FMU), fault injection, Continuous Integration/Continuous Delivery (CI/CD)]
  - **authors:** Giovanni Lambertini, Matteo Pini, Eugenio Mascaro, Francesco Moretti, Ayoub Raji, Marko Bertogna
  - **institution:** University of Modena and Reggio Emilia
  - **link:** https://arxiv.org/pdf/2512.24402
  - **contributions:** 1. An automated simulation and reporting pipeline for an autonomous racing stack that can execute up to three times faster than real-time, locally or on GitHub for CI/CD. 2. A fault injection module capable of introducing sensor delays, perturbations, and modifying outputs of any node in the software stack. 3. A design for an automated reporting process aimed at maximizing the effectiveness of simulation analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d172287ab1ffde29e52c3f7cde1a986f7a5d73ac371395320ced490d39f5dac_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an automated simulation and reporting pipeline for validating an autonomous racing stack. The method uses a high-fidelity vehicle model as a Functional Mockup Unit (FMU) and includes a fault injection module to test system robustness. The pipeline enables fast, realistic scenario testing and automated reporting, which is crucial for efficiently validating critical autonomous driving functions like high-speed overtaking.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack"] --> Problem["核心问题/Problem: Need for efficient validation of autonomous racing stack modules, especially for high-speed maneuvers and localization."]
        Root --> Method["主要方法/Method: Automated simulation pipeline using high-fidelity FMU model, scenario initialization, and fault injection."]
        Root --> Results["关键结果/Results: Pipeline executes up to 3x faster than real-time, supports CI/CD, and includes automated reporting."]
    ```

- **[arXiv260101] FAST-IDS: A Fast Two-Stage Intrusion Detection System with Hybrid Compression for Real-Time Threat Detection in Connected and Autonomous Vehicles**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [hybrid model compression, two-stage IDS, BiGAN, CNN-LSTM, real-time inference]
  - **authors:** Devika S, Vishnu Hari, Pratik Narang, Tejasvi Alladi, Vinay Chamola
  - **institution:** BITS Pilani, Pilani Campus
  - **link:** https://arxiv.org/pdf/2512.24391
  - **contributions:** 1. A novel two-stage Intrusion Detection System (IDS) architecture combining a coarse-grained BiGAN-CNN for anomaly detection and a fine-grained CNN-LSTM for attack classification. 2. The application of hybrid model compression (structural pruning and static quantization) to achieve a 77.2% model size reduction while maintaining performance. 3. Demonstrated real-time, efficient deployment on resource-constrained edge devices (e.g., Jetson Nano) with low per-vehicle inference latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e22580ef1d7646dbfb5cbb4038c5aaef82d4f9a515521bd0405cb40b44178ee2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes FAST-IDS, a fast two-stage intrusion detection system for Connected and Autonomous Vehicles (CAVs). It uses a hybrid model compression technique to create a lightweight system that combines anomaly detection (BiGAN-CNN) and attack classification (CNN-LSTM) for efficient real-time threat detection. The compressed model achieves significant size reduction and faster inference, making it suitable for deployment on edge devices like the Jetson Nano.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FAST-IDS: 面向CAV的快速入侵检测系统] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CAV网络安全威胁 / CAV Cybersecurity Threats]
        B --> B2[资源受限环境部署 / Deployment in Resource-Constrained Environments]
        C --> C1[两阶段IDS / Two-Stage IDS]
        C1 --> C1_1[阶段1: BiGAN-CNN异常检测 / Stage 1: BiGAN-CNN Anomaly Detection]
        C1 --> C1_2[阶段2: CNN-LSTM攻击分类 / Stage 2: CNN-LSTM Attack Classification]
        C --> C2[混合模型压缩 / Hybrid Model Compression]
        C2 --> C2_1[结构化剪枝 / Structural Pruning]
        C2 --> C2_2[静态量化 / Static Quantization]
        D --> D1[模型大小减少77.2% / 77.2% Model Size Reduction]
        D --> D2[推理时间减少50.05% / 50.05% Inference Time Reduction]
        D --> D3[高检测准确率 / High Detection Accuracy]
    ```

- **[arXiv260101] Comparing Approaches to Automatic Summarization in Less-Resourced Languages**
  - **tags:** [nlp], [text summarization], [less-resourced languages, multilingual transfer, data augmentation, LLM prompting, mT5]
  - **authors:** Chester Palen-Michel, Constantine Lignos
  - **institution:** Brandeis University
  - **link:** https://arxiv.org/pdf/2512.24410
  - **contributions:** 1. A comprehensive comparative study of multiple summarization approaches for less-resourced languages, including zero-shot LLMs, fine-tuned mT5, and a translation pipeline. 2. Exploration and evaluation of three data augmentation methods using Wikipedia to generate synthetic training data for low-resource settings. 3. An analysis showing that a fine-tuned multilingual mT5 baseline often outperforms zero-shot LLMs and that LLM-as-judge evaluation may be unreliable for less-resourced languages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14f0d3397d12fd7c93ad9814a09fa7bd5c030b47c51091b59d65cc5392446ba_w640_q70.webp
  - **Simple LLM Summary:** This paper compares various methods for automatic text summarization in less-resourced languages, including prompting large language models (LLMs), fine-tuning multilingual models like mT5 with data augmentation, and a translation pipeline. The evaluation across multiple metrics finds that a fine-tuned multilingual mT5 model generally outperforms zero-shot LLMs, and highlights potential issues with using LLMs as evaluators for these languages.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Comparing Approaches to Automatic Summarization in Less-Resourced Languages] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["高资源语言性能好，低资源语言研究不足 / High performance in high-resourced languages, less attention to less-resourced languages"]
        C --> C1["比较多种方法 / Compare various approaches"]
        C1 --> C1_1["零样本提示LLM / Zero-shot prompting LLMs"]
        C1 --> C1_2["微调mT5（含数据增强） / Fine-tuning mT5 (with data augmentation)"]
        C1 --> C1_3["翻译-总结-翻译流程 / Translate-summarize-translate pipeline"]
        D --> D1["微调mT5优于大多数方法 / Fine-tuned mT5 outperforms most approaches"]
        D --> D2["LLM作为评估者可能不可靠 / LLM as judge may be less reliable"]
    ```

- **[arXiv260101] PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression**
  - **tags:** [mlsys], [llm inference], [KV cache, lossy compression, memory footprint, GPU kernels, attention mechanism]
  - **authors:** Bo Jiang, Taolue Yang, Youyuan Liu, Xubin He, Sheng Di, Sian Jin
  - **institution:** Temple University, Argonne National Laboratory
  - **link:** https://arxiv.org/pdf/2512.24449
  - **code:** https://github.com/BoJiang03/PackKV
  - **contributions:** 1. Proposes PackKV, a generic KV cache management framework featuring novel lossy compression techniques specifically tailored to KV cache data characteristics. 2. Presents a careful co-design of compression algorithms and system architecture that is compatible with the dynamically growing KV cache while preserving high computational efficiency. 3. Achieves significantly higher memory reduction rates and execution throughput compared to state-of-the-art methods, effectively eliminating decompression overhead and accelerating matrix-vector multiplication.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42e07bd3aaf7626174ef50b03ee71ac8de443f8fb5560e374d8293b4df52b84f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high memory footprint of the KV cache during long-context LLM inference by proposing PackKV, a framework that uses LLM-aware lossy compression. PackKV co-designs compression algorithms and system architecture to reduce memory usage while maintaining computational efficiency. The results show that PackKV achieves superior memory reduction and higher throughput compared to existing quantization methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PackKV: Reducing KV Cache Memory Footprint] --> B[核心问题/Problem: KV缓存内存占用大/Large KV Cache Memory Footprint]
        A --> C[主要方法/Method: LLM感知的有损压缩/LLM-Aware Lossy Compression]
        A --> D[关键结果/Results: 更高内存减少与吞吐量/Higher Memory Reduction & Throughput]
    ```

- **[arXiv260101] Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations**
  - **tags:** [sec], [semantic communications], [min-max optimization, adversarial perturbations, multi-task learning]
  - **authors:** Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus
  - **institution:** Nexcepta, The Ohio State University, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.24452
  - **contributions:** 1. A deep learning-based semantic communication framework that jointly supports multiple receiver tasks (e.g., inference and reconstruction) while explicitly limiting semantic information leakage to an eavesdropper. 2. Formulation of the privacy problem as an iterative min-max optimization, where the legitimate transmitter-receiver pair is trained to degrade an adaptive eavesdropper's semantic inference performance. 3. Introduction of an auxiliary adversarial perturbation layer that superimposes a crafted signal on the transmitted waveform to degrade eavesdropper performance, even when the legitimate link is not co-trained against it.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e9c394c6a473f54b07d7337b43fb693f3ebef1de80e7b275ea3c91df03de11_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses privacy leakage in semantic communications, where task-optimized representations can be exploited by eavesdroppers. The proposed method uses a min-max adversarial training framework and an auxiliary perturbation layer to protect semantic information. Evaluations on image datasets show the approach significantly reduces eavesdropper inference accuracy without harming legitimate receiver performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Semantic representations leak sensitive information to eavesdroppers] --> Problem_Detail[语义泄露/Semantic Leakage]
        Method[主要方法/Method: Deep learning framework with min-max optimization and adversarial perturbations] --> Method_Detail1[对抗训练/Min-Max Optimization]
        Method --> Method_Detail2[扰动层/Perturbation Layer]
        Results[关键结果/Results: Reduces eavesdropper accuracy, maintains legitimate performance] --> Results_Detail[有效隐私保护/Effective Privacy Preservation]
    ```

- **[arXiv260101] Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents**
  - **tags:** [ai], [embodied ai], [partial observability, belief refinement, information gain, exploratory inference, test-time adaptation]
  - **authors:** Seohui Bae, Jeonghye Kim, Youngchul Sung, Woohyung Lim
  - **institution:** LG AI Research, KAIST
  - **link:** https://arxiv.org/pdf/2512.24461
  - **contributions:** 1. A test-time adaptive agent architecture that performs exploratory inference through posterior-guided belief refinement without gradient updates or extra training. 2. A method to estimate information gain for action selection using a lightweight LLM-based surrogate. 3. A novel reward metric to assess world alignment by quantifying consistency between posterior belief and ground-truth environment configuration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/234ff05b49010da656582805168140c0ed145a8c4a3d1ca5e4147e19f57db21f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a belief-guided agent for embodied AI that operates under partial observability. The agent refines its structured belief about the world at test time and selects actions to maximize predicted information gain, using a lightweight LLM to estimate it. Experiments show this method outperforms inference-time scaling baselines in aligning with latent world states while using significantly fewer tokens.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Align While Search: Belief-Guided Exploratory Inference<br/>对齐搜索：信念引导的探索性推理] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>LLM agents struggle with adaptive reasoning in partially observable environments.<br/>LLM智能体在部分可观测环境中难以进行自适应推理。]
        C[主要方法/Method<br/>Test-time agent with structured belief, updated via observations, selects actions to maximize predicted information gain.<br/>具有结构化信念的测试时智能体，通过观察更新信念，选择行动以最大化预测信息增益。]
        D[关键结果/Results<br/>Outperforms inference-time baselines (e.g., prompt-augmented LLMs) with lower token usage.<br/>以更低的令牌使用量优于推理时基线（如提示增强的LLM）。]
    ```

- **[arXiv260101] HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors**
  - **tags:** [ai], [causal discovery], [sheaf theory, large language models, natural gradient descent, algebraic latent projection, presheaf]
  - **authors:** Hyunjun Kim
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), École Polytechnique Fédérale de Lausanne (EPFL)
  - **link:** https://arxiv.org/pdf/2512.24478
  - **code:** https://github.com/hyunjun1121/holograph
  - **contributions:** 1. A sheaf-theoretic framework formalizing LLM-guided causal discovery as a presheaf satisfaction problem. 2. A natural gradient descent algorithm on the belief manifold for principled optimization. 3. The introduction of Algebraic Latent Projection to handle hidden confounders.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eaf3f0002711b7e42d725e2a453cb23c5db4c9ca0b3a10f0290cfce9779f2e_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HOLOGRAPH, a framework that uses sheaf theory to formally integrate Large Language Model priors for causal discovery, addressing issues of coherence and hidden confounders. It proposes novel methods like Algebraic Latent Projection and natural gradient optimization. The approach provides a rigorous mathematical foundation and shows competitive performance, while analysis reveals a failure of the Locality axiom in larger graphs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HOLOGRAPH] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[因果发现受可识别性限制/Causal discovery limited by identifiability]
        B --> B2[现有LLM方法缺乏理论基础/Existing LLM methods lack theory]
        C --> C1[层理论框架/Sheaf-theoretic framework]
        C --> C2[代数潜在投影/Algebraic Latent Projection]
        C --> C3[自然梯度下降/Natural Gradient Descent]
        D --> D1[提供数学基础/Provides mathematical foundation]
        D --> D2[性能有竞争力/Achieves competitive performance]
        D --> D3[局部性公理失效/Locality axiom fails for large graphs]
    ```

- **[arXiv260101] F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model**
  - **tags:** [cv], [image super-resolution], [diffusion models, DINOv2, feature conditioning, hallucination control, real-world images]
  - **authors:** Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh
  - **institution:** MPI Lab, Samsung Research America
  - **link:** https://arxiv.org/pdf/2512.24473
  - **contributions:** 1. Proposes F2IDiff, a new Feature-to-Image Diffusion Foundation Model for SISR that uses lower-level DINOv2 features for conditioning instead of text. 2. Demonstrates that this approach provides stricter and richer conditioning for small patches, enabling controlled generation and higher fidelity, especially for high-fidelity smartphone LR images. 3. Shows that the model can be trained effectively with a much smaller dataset (38K images) and a smaller U-Net than large text-to-image models like SD2.1, while achieving superior performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccded9f21c9c6de980a50bfd5c308052e1cce34fc21b16a1a1b0cbd3a8c8c57d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of undesirable hallucinations in generative super-resolution for high-fidelity smartphone images. It proposes F2IDiff, a diffusion foundation model conditioned on DINOv2 features instead of text, which allows for stricter control and richer description of image patches. The method achieves better fidelity and performance than text-conditioned models while requiring significantly less data and a smaller network.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[F2IDiff: Real-world Image Super-resolution] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[文本特征对细节描述不足<br/>Text features lack detail]
        B --> B2[智能手机高分辨率LR图像需要无幻觉生成<br/>Smartphone LR needs hallucination-free generation]
        C --> C1[使用DINOv2特征进行条件控制<br/>Use DINOv2 features for conditioning]
        C --> C2[构建特征到图像扩散基础模型(F2IDiff)<br/>Build Feature-to-Image Diffusion FM (F2IDiff)]
        D --> D1[比基于文本的模型保真度更高<br/>Higher fidelity than text-based models]
        D --> D2[使用更小的数据集和网络实现<br/>Achieved with smaller dataset & network]
    ```

- **[arXiv260101] Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model (VLM), fallback maneuver, semantic hazard detection, autonomous surface vessel (ASV), IMO MASS Code]
  - **authors:** Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert
  - **institution:** NTNU (Norwegian University of Science and Technology), Stanford University, NVIDIA Research
  - **link:** https://arxiv.org/pdf/2512.24470
  - **contributions:** 1. Proposes Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector for maritime autonomy. 2. Introduces a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback to meet IMO MASS Code requirements. 3. Demonstrates that sub-10-second VLM models retain semantic awareness and outperform geometry-only baselines in hazard scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd093268c75343899a9f72b23f4d22948c1b634539b5e39f8d26296c9f122886_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of semantic hazard detection for autonomous maritime vessels, which is required by the draft IMO MASS Code. It proposes Semantic Lookout, a vision-language model (VLM) based system that selects safe fallback maneuvers by understanding scene semantics. The results show that this approach is effective within practical latency budgets and outperforms traditional geometry-only methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Foundation models on the bridge / 论文标题"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["自主船舶需检测语义危害 / Autonomous vessels need semantic hazard detection"]
        Problem --> P2["传统系统难以处理语义异常 / Classical stacks struggle with semantic OOD situations"]
        Method --> M1["引入Semantic Lookout / Introduce Semantic Lookout"]
        Method --> M2["基于VLM的备用机动选择器 / VLM-based fallback maneuver selector"]
        Method --> M3["快速-慢速异常处理流程 / Fast-slow anomaly pipeline"]
        Results --> R1["10秒内模型保持语义感知 / Sub-10 s models retain semantic awareness"]
        Results --> R2["优于几何基线 / Outperforms geometry-only baselines"]
        Results --> R3["支持IMO MASS法规 / Supports draft IMO MASS Code"]
    ```

- **[arXiv260101] What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?**
  - **tags:** [ai], [world models], [joint-embedding predictive architecture, representation space planning, model-based reinforcement learning]
  - **authors:** Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun
  - **institution:** Meta FAIR, INRIA Paris, Ecole normale supérieure/PSL, New York University
  - **link:** https://arxiv.org/pdf/2512.24497
  - **code:** https://github.com/facebookresearch/jepa-wms
  - **contributions:** 1. Proposes a comprehensive characterization and study of Joint-Embedding Predictive Architecture World Models (JEPA-WMs) for physical planning. 2. Systematically investigates the impact of model architecture, training objective, and planning algorithm on planning success in simulated and real-world robotic tasks. 3. Combines the findings to propose a new model that outperforms established baselines (DINO-WM and V-JEPA-2-AC) in navigation and manipulation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the key factors for successful physical planning using Joint-Embedding Predictive World Models (JEPA-WMs). It conducts a systematic study of architectural and algorithmic choices within this family of methods and proposes a new model that achieves superior performance on navigation and manipulation tasks compared to existing baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[What Drives Success in Physical Planning with JEPA-WMs?] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>How to build agents that generalize to new physical tasks?]
        Method[主要方法/Method<br>Study JEPA-WMs: architecture, objective, planning algorithm]
        Results[关键结果/Results<br>Proposed model outperforms baselines (DINO-WM, V-JEPA-2-AC)]
    ```

- **[arXiv260101] Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice**
  - **tags:** [mlsys], [llm training], [proxy models, data curation, hyperparameter tuning, learning rate, pretraining]
  - **authors:** Jiachen T. Wang, Tong Wu, Kaifeng Lyu, James Zou, Dawn Song, Ruoxi Jia, Prateek Mittal
  - **institution:** Princeton University, Tsinghua University, Stanford University, UC Berkeley, Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.24503
  - **contributions:** 1. Identifies a critical flaw in the standard proxy-model evaluation protocol, showing that using a fixed training configuration for all data recipes leads to unreliable conclusions that can flip with minor hyperparameter changes. 2. Proposes a simple and effective patch to the protocol: training proxy models with reduced learning rates, which preserves the relative performance ranking of data recipes and correlates strongly with fully-tuned large-scale training. 3. Provides theoretical justification for the proposed method by proving it preserves dataset ordering for random-feature models, and validates it empirically across 23 data recipes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0253b877f5ec90fd3f09b9d1a8f7d0967a88407fbbd25eb1e8a8bfcdf23081c4_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that the standard practice of using small proxy models with identical hyperparameters to evaluate data recipes is unreliable because optimal training configurations are data-dependent. To fix this, the authors propose training proxy models with reduced learning rates, a simple change that makes small-scale experiment rankings strongly correlate with those from fully-tuned large-scale LLM pretraining. This method is theoretically justified and empirically validated, dramatically improving the reliability of data curation guidance from small training runs.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[固定配置评估不可靠/Fixed-config evaluation unreliable]
        Problem --> P2[结论随超参翻转/Conclusions flip with hyperparams]
        Method[主要方法/Method] --> M1[降低学习率训练代理模型/Train proxy models with reduced LR]
        Method --> M2[数据特定调优目标/Data-specific tuning objective]
        Results[关键结果/Results] --> R1[与大规模训练强相关/Strong correlation with large-scale training]
        Results --> R2[理论证明与实证验证/Theoretical proof & empirical validation]
    ```

- **[arXiv260101] Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments**
  - **tags:** [ai], [spatial reasoning], [foundation model agents, interactive evaluation, spatial memory, graph-based representation, path planning]
  - **authors:** Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu
  - **institution:** Hunan Normal University, Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.24504
  - **contributions:** 1. Proposes an interactive evaluation framework to assess how foundation model agents explore, remember, and reason in partially observable symbolic map environments., 2. Systematically analyzes the distinct functional roles of exploration strategies, memory representations, and reasoning schemes on spatial task performance., 3. Reveals that spatial reasoning performance saturates with model scale, indicating the need for tailored spatial representation mechanisms beyond scaling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f22bb0bbc4529875e6217599c85097728b0d0b467695dfc3508153e6cab59ae2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an interactive framework to evaluate how foundation model agents understand maps by exploring, remembering, and reasoning in partially observable grid environments. The study finds that structured memory representations, like graphs, are crucial for complex tasks like path planning, and that performance improvements require specialized spatial mechanisms, not just scaling model size.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(现有评估忽视交互性/Existing evaluations overlook interactive, experience-driven spatial understanding)
        C --> C1(提出交互式评估框架/Propose interactive evaluation framework)
        C1 --> C2(智能体增量探索部分可观测地图/Agents incrementally explore partially observable maps)
        C2 --> C3(评估六类空间任务/Evaluate six kinds of spatial tasks)
        D --> D1(结构化记忆提升性能/Structured memory (e.g., graph) substantially improves performance)
        D --> D2(探索影响经验获取/Exploration affects experience acquisition)
        D --> D3(性能随规模饱和/Reasoning performance saturates with model scale)
    ```

- **[arXiv260101] Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems**
  - **tags:** [nlp], [mathematical reasoning], [large language models, mathematical reasoning, benchmark evaluation, error analysis, underrepresented datasets]
  - **authors:** Samuel Golladay, Majid Bani-Yaghoub
  - **institution:** University of Missouri: Kansas City
  - **link:** https://arxiv.org/pdf/2512.24505
  - **contributions:** 1. Evaluated LLMs on underrepresented mathematics competition problems (Missouri Collegiate Mathematics Competition) to address dataset contamination and generalizability issues. 2. Conducted a detailed analysis of reasoning quality and error patterns across three LLMs (GPT-4o-mini, Gemini-2.0-Flash, DeepSeek-V3) beyond just final answer accuracy. 3. Identified distinct error profiles for each model and highlighted Geometry as a persistent challenge for LLMs' structured reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0233f7d7f1efe2775a15812915de42cb0ed0d928ad7ea3ea8e06fe4ee277303_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the reasoning abilities of three LLMs on underrepresented mathematics competition problems. The results show DeepSeek-V3 performed best, and all models struggled with Geometry, revealing distinct error patterns. The work concludes that using such datasets provides deeper insights into LLMs' reasoning limitations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Evaluating LLMs on Underrepresented Math Problems<br/>评估LLM在代表性不足数学问题上的表现] --> B
        A --> C
        A --> D
        B[Problem: Limited generalizability of LLM math benchmarks<br/>问题: LLM数学基准测试泛化性不足]
        C[Method: Test LLMs on Missouri Collegiate Math Competition problems<br/>方法: 在密苏里大学数学竞赛问题上测试LLMs]
        D[Results: DeepSeek-V3 best; Geometry is a weak point; distinct error patterns<br/>结果: DeepSeek-V3最优; 几何是弱点; 不同的错误模式]
    ```

- **[arXiv260101] From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [spatial reasoning, LoRA, GRPO, supervised fine-tuning, reinforcement learning]
  - **authors:** Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera
  - **institution:** Purdue University
  - **link:** https://arxiv.org/pdf/2512.24532
  - **contributions:** 1. A two-stage approach for multi-step spatial reasoning that first fine-tunes an LLM on atomic spatial transformations and then trains lightweight LoRA adapters via RL to compose these blocks for planning. 2. The creation of a synthetic ASCII-art dataset and a corresponding ASCII-based RL environment to support training and evaluation. 3. Demonstration that the proposed method outperforms baselines in both dynamic and static environments, with faster convergence and more stable training than end-to-end RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19427bf120d4b89a11879461b589b7313caa689fdec51f99f516c6485aef495_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of multi-step spatial reasoning in LLMs by proposing a two-stage method: first, supervised fine-tuning on basic spatial transformations to build physics awareness, and then training LoRA adapters with reinforcement learning (GRPO) to learn planning policies. The approach is evaluated using a custom ASCII-art environment and is shown to outperform various baselines, converging faster and more stably than training from scratch with RL.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>LLMs struggle with spatial transformations and multi-step planning]
        C[主要方法/Method<br>Two-stage: SFT on spatial blocks, then RL (GRPO) with LoRA for planning]
        D[关键结果/Results<br>Outperforms baselines, faster convergence, stable training]
    ```

- **[arXiv260101] More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [extreme quantization, double binary factorization, low-bit LLM, post-training quantization, binary matrix multiplication]
  - **authors:** Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa
  - **institution:** Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University
  - **link:** https://arxiv.org/pdf/2512.24545
  - **contributions:** 1. Proposed Multi-Envelope Double Binary Factorization (MDBF), which replaces the single magnitude envelope in DBF with a rank-l envelope to enhance magnitude expressiveness while maintaining a shared binary sign carrier. 2. Introduced a closed-form initialization and an alternating refinement method to effectively optimize the MDBF parameters. 3. Demonstrated that MDBF improves perplexity and zero-shot accuracy over prior binary formats on LLaMA and Qwen models at matched bit budgets while preserving the same efficient inference primitive.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the performance saturation of Double Binary Factorization (DBF) in extreme low-bit quantization of LLMs, where a single magnitude envelope limits expressiveness. It proposes Multi-Envelope DBF (MDBF), which uses multiple envelope components to allocate more expressivity to magnitudes while keeping binary sign matrices shared. Experiments on LLaMA and Qwen families show MDBF outperforms previous binary formats in accuracy and perplexity at the same bit rate without changing the inference primitive.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>DBF scaling too restrictive,<br>single envelope causes<br>performance saturation"]
        Method["主要方法/Method<br>Propose MDBF: shared 1-bit sign bases,<br>replace single envelope with<br>rank-l envelope"]
        Results["关键结果/Results<br>Better perplexity & accuracy<br>over previous binary formats,<br>same inference primitive"]
    ```

- **[arXiv260101] Localized Calibrated Uncertainty in Code Language Models**
  - **tags:** [se], [code generation], [calibrated uncertainty, minimal intent aligning patches, white-box probing, Brier Skill Score, AI oversight]
  - **authors:** David Gros, Prem Devanbu
  - **institution:** University of California, Davis
  - **link:** https://arxiv.org/pdf/2512.24560
  - **contributions:** 1. Creation of a dataset of "Minimal Intent Aligning Patches" for LLM-generated code repairs. 2. Proposal and evaluation of techniques (white-box probing, black-box reflective, self-consistency) for assigning well-calibrated, localized uncertainty to code segments. 3. Demonstration that a small supervisor probe can effectively estimate edit likelihood on code from much larger models and shows preliminary signs of generalization to natural language errors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e04860e78fa5e50071d58627500e751c212bb40e58f4d01943c9ad0eda5c5a9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of LLM-generated code potentially deviating from user intent by proposing methods to localize where edits are likely needed. It introduces a dataset of minimal repair patches and compares techniques for assigning calibrated probabilities to code lines. The key finding is that a small white-box probing model can effectively estimate which lines will be edited, achieving good calibration and a Brier Skill Score of ~0.2, and shows some generalizability beyond code.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Localized Calibrated Uncertainty in Code Language Models] --> B[核心问题/Problem: LLM生成的代码可能偏离用户意图/LLM-generated code may deviate from user intent]
        A --> C[主要方法/Method: 创建最小意图对齐补丁数据集，比较白盒探测与黑盒方法/Create Minimal Intent Aligning Patches dataset, compare white-box probing vs. black-box methods]
        A --> D[关键结果/Results: 小监督模型可实现低校准误差，Brier Skill Score约0.2/Small supervisor model achieves low calibration error, Brier Skill Score ~0.2]
    ```

- **[arXiv260101] MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use**
  - **tags:** [mlsys], [agent system], [Model Context Protocol (MCP), LLM Agent, Benchmark, Tool Use, Dynamic Sandbox]
  - **authors:** Wenrui Liu, Zixiang Liu, Elsie Dai, Wenhan Yu, Lei Yu, Tong Yang
  - **institution:** Peking University, Columbia University
  - **link:** https://arxiv.org/pdf/2512.24565
  - **code:** Github (2025)
  - **contributions:** 1. Proposes MCPAgentBench, a benchmark based on real-world MCP definitions to evaluate agent tool-use capabilities. 2. Constructs a dataset with authentic tasks and simulated MCP tools, employing a dynamic sandbox environment with distractor tools. 3. Introduces comprehensive metrics to measure both task completion rates and execution efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088dcfaedc1cf43a859436433c7e392c2a6c7d8ccaf29c406871b6b794f77a2d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of existing MCP evaluation sets by introducing MCPAgentBench, a benchmark that uses simulated MCP tools and a dynamic sandbox environment with distractors to test LLM agents' tool selection and execution abilities. Experiments on various LLMs reveal significant performance differences in handling complex, multi-step tool invocations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有MCP评估集依赖外部服务且缺乏难度感知/Current MCP evaluations rely on external services and lack difficulty awareness]
        C --> C1[基于真实MCP定义构建基准/Benchmark based on real-world MCP definitions]
        C --> C2[使用含干扰项的动态沙盒环境评估/Evaluation using dynamic sandbox with distractors]
        C --> C3[引入任务完成率和执行效率综合指标/Introducing comprehensive metrics for completion rate and efficiency]
        D --> D1[主流大模型在复杂多步工具调用上表现差异显著/Significant performance differences among LLMs on complex multi-step invocations]
    ```

- **[arXiv260101] SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [SIEM, query generation, platform-agnostic specification, cross-platform, LLM framework]
  - **authors:** Md Hasan Saju, Austin Page, Akramul Azim, Jeff Gardiner, Farzaneh Abazari, Frank Eargle
  - **institution:** Ontario Tech University, GlassHouse Systems Inc.
  - **link:** https://arxiv.org/pdf/2512.24571
  - **contributions:** 1. Introduces SynRAG, a unified LLM framework for generating executable queries for heterogeneous SIEM systems from a single high-level specification. 2. Enables seamless threat detection and incident investigation across different SIEM platforms, reducing the need for specialized training. 3. Demonstrates superior performance compared to state-of-the-art base LLMs (GPT, Llama, etc.) in generating platform-specific queries for systems like Qradar and SecOps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d87255708c71cdba68b8c5ca7054cc8216b56e3021254a7bbbd7c64c9a53687_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SynRAG, a framework that uses large language models to automatically generate platform-specific SIEM queries from a single, high-level, platform-agnostic specification. This addresses the challenge of monitoring diverse SIEM systems with different query languages. Evaluation shows SynRAG outperforms base LLMs in generating accurate queries for cross-platform threat detection and investigation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SynRAG: 异构SIEM系统的可执行查询生成框架<br>SynRAG: Executable Query Generation in Heterogeneous SIEM Systems] --> B[核心问题/Problem: SIEM平台多样性导致查询语言不同，分析师监控多平台困难<br>Problem: SIEM platform diversity leads to different query languages, making multi-platform monitoring difficult for analysts]
        A --> C[主要方法/Method: 提出SynRAG框架，从平台无关的高级描述自动生成特定平台查询<br>Method: Proposes SynRAG framework to auto-generate platform-specific queries from a platform-agnostic high-level specification]
        A --> D[关键结果/Results: SynRAG生成的查询优于GPT、Llama等先进基础模型<br>Results: SynRAG generates better queries than SOTA base models like GPT, Llama]
    ```

- **[arXiv260101] Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time**
  - **tags:** [mlsys], [llm inference], [chain-of-thought reasoning, attention heads, test-time intervention, computational efficiency, reasoning steering]
  - **authors:** Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun
  - **institution:** University of Texas at Austin, Together AI, University of Sydney
  - **link:** https://arxiv.org/pdf/2512.24574
  - **code:** https://github.com/togethercomputer/CREST
  - **contributions:** 1. Identified specialized attention heads in LLMs that correlate with distinct cognitive reasoning behaviors (e.g., verification, backtracking). 2. Proposed CREST, a training-free method for Cognitive REasoning Steering at Test-time, which involves offline calibration to find steering vectors and inference-time rotation to suppress unproductive reasoning. 3. Demonstrated that CREST improves reasoning accuracy and reduces token usage across diverse benchmarks, offering a pathway to faster and more reliable LLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency and instability of long chain-of-thought reasoning in LLMs, which leads to high latency and alternating underthinking/overthinking. The authors propose CREST, a training-free method that identifies and steers specific attention heads at test-time to suppress unproductive cognitive behaviors. The method improves accuracy by up to 17.5% and reduces token usage by 37.6%, enabling faster and more reliable reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM推理轨迹低效且不稳定/Inefficient & Unstable LLM Reasoning Trajectories]
        B1 --> B2[过度思考与思考不足/Overthinking & Underthinking]
        B1 --> B3[高延迟与高令牌消耗/High Latency & Token Usage]
        C --> C1[识别与认知行为相关的注意力头/Identify Cognitive Attention Heads]
        C --> C2[提出CREST方法: 测试时认知推理引导/Propose CREST: Test-time Cognitive REasoning Steering]
        C2 --> C3[离线校准获取引导向量/Offline Calibration for Steering Vectors]
        C2 --> C4[推理时旋转隐藏表示/Inference-time Representation Rotation]
        D --> D1[准确率显著提升/Accuracy Improved Up to 17.5%]
        D --> D2[令牌使用大幅减少/Token Usage Reduced by 37.6%]
        D --> D3[实现更快更可靠的推理/Enables Faster, More Reliable Reasoning]
    ```

- **[arXiv260101] Recursive Language Models**
  - **tags:** [mlsys], [llm inference], [recursive language models, long-context processing, inference-time scaling, context condensation, out-of-core algorithms]
  - **authors:** Alex L. Zhang, Tim Kraska, Omar Khattab
  - **institution:** MIT CSAIL
  - **link:** https://arxiv.org/pdf/2512.24601
  - **contributions:** 1. Proposes Recursive Language Models (RLMs), a general inference strategy that allows LLMs to programmatically examine, decompose, and recursively call themselves over long prompts. 2. Demonstrates that RLMs can handle inputs up to two orders of magnitude beyond standard model context windows. 3. Shows RLMs outperform base LLMs and existing long-context methods across diverse tasks while maintaining comparable or lower cost per query.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b76e3870cfbc09a08f87a31e423d89e08fb4d1e100fa580e4e6b58754309af5_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLMs struggling with arbitrarily long prompts due to limited context windows and context rot. It introduces Recursive Language Models (RLMs), an inference-time method that treats long prompts as an external environment, enabling recursive decomposition and processing. The results show RLMs effectively scale to inputs far beyond standard context limits and outperform baseline approaches in quality and cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Recursive Language Models] --> B[核心问题/Problem: LLMs have limited context lengths and suffer from context rot with long prompts]
        A --> C[主要方法/Method: Recursive Language Models (RLMs) treat long prompts as an external environment, allowing programmatic examination and recursive self-calls]
        A --> D[关键结果/Results: RLMs handle inputs up to 100x beyond context windows, outperform base LLMs and long-context scaffolds, with comparable or cheaper cost]
    ```

- **[arXiv260101] Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization**
  - **tags:** [mlsys], [agent system], [Dec-POMDP, CTDE, GRPO]
  - **authors:** Dong Qiu, Duo Xu, Limengxi Yue
  - **institution:** New England College, Northeastern University, University of Massachusetts Amherst
  - **link:** https://arxiv.org/pdf/2512.24609
  - **contributions:** 1. A reinforcement learning-augmented LLM agent framework that formulates multi-agent collaboration as a Dec-POMDP and uses CTDE. 2. The introduction of Group Relative Policy Optimization (GRPO) for jointly optimizing agent policies with global training signals. 3. A simplified joint reward function that balances task quality, speed, and coordination cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/251d4cfbf0941a0b6ad2b2a8b7158ec06789c4a7a60653f447034ce562d8c91d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of collaborative awareness in LLMs for multi-agent settings by proposing a framework that combines reinforcement learning with LLMs, using a Dec-POMDP formulation and CTDE. It introduces GRPO for policy optimization and a balanced reward function. The method significantly outperforms baselines in collaborative writing and coding tasks, demonstrating improved speed, consistency, and success rates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement Learning-Augmented LLM Agents<br/>强化学习增强的LLM智能体] --> B[核心问题/Problem<br/>LLMs lack collaborative awareness in multi-agent settings<br/>LLM在多智能体环境中缺乏协作意识]
        A --> C[主要方法/Method<br/>Dec-POMDP & CTDE framework with GRPO and a simplified joint reward<br/>基于Dec-POMDP和CTDE的框架，使用GRPO和简化联合奖励]
        A --> D[关键结果/Results<br/>3x speedup, 98.7% writing consistency, 74.6% coding pass rate<br/>3倍速度提升，98.7%写作一致性，74.6%编码通过率]
    ```

- **[arXiv260101] Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning**
  - **tags:** [mlsys], [agent system], [multi-agent dialogue, role-based architecture, self-game mechanism, retrieval enhancement, proximal policy optimization]
  - **authors:** Zheyu Shi, Dong Qiu, Shanlong Yu
  - **institution:** Brown University, New England College, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.24613
  - **contributions:** 1. Proposed a three-level role division architecture (generation-verification-integration) for structured multi-agent collaboration in complex reasoning. 2. Introduced a self-game mechanism to expand multi-path reasoning trajectories and a retrieval enhancement module for dynamic external knowledge supplementation. 3. Designed a composite reward function and applied an improved proximal policy optimization strategy for collaborative training of the multi-agent system.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e250eb290183b556b4931328b4b20eb01ed043b916f7156301013ceb7995ab66_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a group deliberation oriented multi-agent conversational model to enhance complex reasoning. The model employs a three-agent architecture for opinion generation, evidence verification, and consistency arbitration, augmented by a self-game mechanism and retrieval enhancement. Experiments show significant improvements in multi-hop reasoning accuracy and consistency over baseline models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Limitations of single LLMs in complex reasoning tasks]
        C[主要方法/Method: Three-level role division, self-game mechanism, retrieval enhancement, composite reward & PPO training]
        D[关键结果/Results: Improved multi-hop reasoning accuracy & consistency on HotpotQA, 2WikiMultihopQA, MeetingBank]
    ```

- **[arXiv260101] Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space**
  - **tags:** [mlsys], [llm inference], [hierarchical compression, compression-aware scaling law, decoupled µP parametrization, concept space, adaptive semantic boundaries]
  - **authors:** Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang
  - **institution:** ByteDance Seed, University of Manchester, Mila - Quebec AI Institute, Tsinghua University, M-A-P
  - **link:** https://arxiv.org/pdf/2512.24617
  - **contributions:** 1. Proposed Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns variable-length semantic concepts end-to-end and shifts computation from tokens to a compressed concept space for more efficient reasoning. 2. Introduced the first compression-aware scaling law that disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. 3. Developed a decoupled µP parametrization for stable training of the heterogeneous architecture, supporting zero-shot hyperparameter transfer across model widths and compression regimes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dff9c61b225b5a86d535cfc752b13f390ae301473e649284a6815c3eaf80b24_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of uniform token-level computation in LLMs by proposing Dynamic Large Concept Models (DLCM), which learns adaptive semantic concepts and reallocates compute to a higher-capacity reasoning backbone in a compressed concept space. This approach achieves a +2.69% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space] --> B[核心问题/Problem: LLMs apply uniform computation to tokens, wasting capacity on predictable spans and under-allocating to critical transitions]
        A --> C[主要方法/Method: Hierarchical framework learns semantic boundaries, shifts computation to compressed concept space, introduces compression-aware scaling law and decoupled µP parametrization]
        A --> D[关键结果/Results: +2.69% average improvement on 12 zero-shot benchmarks under matched inference FLOPs with R=4 compression]
    ```

- **[arXiv260101] Chat-Driven Optimal Management for Virtual Network Services**
  - **tags:** [mlsys], [communication & networking], [intent-based networking, virtual network allocation, integer linear programming, Sentence-BERT, large language model]
  - **authors:** Yuya Miyaoka, Masaki Inoue, Kengo Urata, Shigeaki Harada
  - **institution:** Keio University, NTT Inc.
  - **link:** https://arxiv.org/pdf/2512.24614
  - **contributions:** 1. Proposes a novel two-stage chat-driven framework that integrates NLP-based intent extraction with optimization-based allocation for virtual network management. 2. Introduces and compares two intent extractors: a low-latency Sentence-BERT with SVM classifier and a high-accuracy LLM-based model. 3. Demonstrates the framework's ability to dynamically and feasibly update VM placement and routing in both single-user and multi-user settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dd7600e3aadf99d8f0d2549d3d65d3c00bce80acbf6105aef79f0c8a797c126_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of conventional intent-based networking, which cannot guarantee feasible configurations, by proposing a chat-driven framework. The framework uses an NLP Interpreter to extract user intent and an optimization-based Optimizer to compute feasible VM placement and routing. The results show that combining NLP with optimization enables safe, interpretable, and user-friendly network management, with an LLM-based extractor offering higher accuracy and a Sentence-BERT/SVM extractor providing lower latency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Chat-Driven Optimal Management for Virtual Network Services] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Conventional IBN cannot guarantee configuration feasibility] --> P1[传统IBN方法/Traditional IBN Methods]
        P1 --> P2[依赖统计语言模型/Rely on statistical language models]
        P2 --> P3[无法保证可行性/Cannot guarantee feasibility]
        Method[主要方法/Method: Two-stage NLP + Optimization Framework] --> M1[阶段1: 解释器/Stage 1: Interpreter]
        M1 --> M2[提取用户意图/Extract user intent from chat]
        M2 --> M3[使用NLP模型/Use NLP models (Sentence-BERT+SVM or LLM)]
        Method --> M4[阶段2: 优化器/Stage 2: Optimizer]
        M4 --> M5[计算可行配置/Compute feasible configuration]
        M5 --> M6[使用整数线性规划/Use Integer Linear Programming]
        Results[关键结果/Results] --> R1[动态更新VM放置和路由/Dynamically updates VM placement & routing]
        R1 --> R2[保持可行性/Preserves feasibility]
        Results --> R3[LLM提取器: 高精度/LLM Extractor: High accuracy with few samples]
        Results --> R4[Sentence-BERT+SVM: 低延迟/Sentence-BERT+SVM: Low latency for real-time]
    ```

- **[arXiv260101] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization**
  - **tags:** [mlsys], [agent system], [LLM agent framework, automated agent generation, hybrid policy optimization, in-context optimization, reinforcement learning]
  - **authors:** Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun
  - **institution:** Tencent (inferred from "TencentCloudADP" in GitHub URL)
  - **link:** https://arxiv.org/pdf/2512.24615
  - **code:** https://github.com/TencentCloudADP/youtu-agent
  - **contributions:** 1. A modular LLM agent framework (Youtu-Agent) with a structured configuration system for decoupling components and enabling automated synthesis. 2. Two agent generation paradigms: Workflow mode for standard tasks and Meta-Agent mode for complex tasks, capable of auto-generating tools and prompts. 3. A hybrid policy optimization system combining an in-context learning "Agent Practice" module and a scalable reinforcement learning "Agent RL" module for continuous agent evolution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0414a5769d966ffb5a9f4428d4a182e5095ba0b107862d50c8224788df0caa46_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Youtu-Agent, a framework to automate the generation and continuous optimization of LLM agents, addressing high configuration costs and static capabilities. It introduces structured configuration, automated generation paradigms, and a hybrid optimization system combining in-context learning and reinforcement learning. Experiments show state-of-the-art performance on several benchmarks and significant improvements in agent capabilities through automated synthesis and optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Youtu-Agent] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[高配置成本 / High Configuration Cost]
        B --> B2[静态能力 / Static Capabilities]
        C --> C1[结构化配置系统 / Structured Configuration System]
        C --> C2[自动化生成 / Automated Generation]
        C --> C21[工作流模式 / Workflow Mode]
        C --> C22[元智能体模式 / Meta-Agent Mode]
        C --> C3[混合策略优化 / Hybrid Policy Optimization]
        C --> C31[智能体实践 / Agent Practice]
        C --> C32[智能体强化学习 / Agent RL]
        D --> D1[SOTA性能 / SOTA Performance]
        D --> D2[高工具合成率 / High Tool Synthesis Rate]
        D --> D3[能力显著提升 / Significant Capability Improvement]
    ```

- **[arXiv260101] AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt**
  - **tags:** [mlsys], [federated learning], [personalized federated learning, prompt learning, traffic prediction, non-IID data, hyper-parameter tuning]
  - **authors:** Zijian Zhao, Yitong Shang, Sen Li
  - **institution:** The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24625
  - **code:** https://github.com/RS2002/AutoFed
  - **contributions:** 1. Proposes AutoFed, a novel Personalized Federated Learning (PFL) framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. 2. Introduces a federated representor with a client-aligned adapter to distill local data into a compact, globally shared prompt matrix, inspired by prompt learning. 3. Demonstrates through extensive experiments that AutoFed consistently achieves superior performance across diverse real-world traffic prediction scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e572784e0dff554ff14fce989febaa7869bdfb488d60b6fd86ebd7e98cdf0bf7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AutoFed, a manual-free Personalized Federated Learning framework for traffic prediction that uses a client-aligned adapter to generate a shared prompt matrix, enabling knowledge sharing while preserving local specificity. Experiments on real-world datasets show that AutoFed achieves superior performance without requiring manual hyper-parameter tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据孤岛与隐私问题 / Data Silos & Privacy]
        B --> B2[非独立同分布数据 / Non-IID Data]
        B --> B3[手动超参数调优 / Manual Hyper-parameter Tuning]
        C --> C1[个性化联邦学习 / Personalized Federated Learning (PFL)]
        C --> C2[提示学习 / Prompt Learning]
        C --> C3[联邦表征器与客户端对齐适配器 / Federated Representor & Client-Aligned Adapter]
        D --> D1[性能优越 / Superior Performance]
        D --> D2[无需手动调参 / No Manual Tuning]
        D --> D3[真实数据集验证 / Validated on Real-world Datasets]
    ```

- **[arXiv260101] AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels**
  - **tags:** [ai], [medical audio classification], [hierarchical classification, acoustic biomarkers, mel-spectrograms, voice disorders, sustained vowels]
  - **authors:** Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang
  - **institution:** Weill Cornell Medicine
  - **link:** https://arxiv.org/pdf/2512.24628
  - **contributions:** 1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework's potential as a scalable tool for early voice disorder screening and diagnostic triage.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[良性喉部嗓音疾病分类/Benign Laryngeal Voice Disorder Classification]
        C --> C1[三级分层机器学习框架/Three-Stage Hierarchical ML Framework]
        C1 --> C1_1[阶段1: 病理筛查/Stage 1: Pathological Screening]
        C1 --> C1_2[阶段2: 粗粒度分层/Stage 2: Coarse Stratification]
        C1 --> C1_3[阶段3: 细粒度分类/Stage 3: Fine-Grained Classification]
        C1_1 --> C1_1a[融合CNN梅尔谱特征与21种声学生物标志物/Integrates CNN Mel-Spectrogram & 21 Acoustic Biomarkers]
        D --> D1[性能优于平面多类分类器与预训练模型/Outperforms Flat Classifiers & Pre-trained Models (HuBERT, HeAR)]
        D --> D2[结合深度表征与可解释特征，增强临床可操作性/Enhances Transparency & Clinical Alignment via Deep & Interpretable Features]
    ```

- **[arXiv260101] DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information**
  - **tags:** [se], [automated program repair], [large language models, dynamic analysis, iterative repair, execution-level information, Defects4J]
  - **authors:** Zhili Huang, Ling Xu, Chao Liu, Weifeng Sun, Xu Zhang, Yan Lei, Meng Yan, Hongyu Zhang
  - **institution:** Chongqing University
  - **link:** https://arxiv.org/pdf/2512.24635
  - **contributions:** 1. Proposes DynaFix, a novel APR method that iteratively leverages fine-grained, execution-level dynamic information (e.g., variable states, control-flow) to guide LLMs in patch generation. 2. Introduces an iterative repair loop where failed patches trigger re-execution to collect updated runtime feedback, mimicking human stepwise debugging. 3. Demonstrates significant effectiveness and efficiency improvements, repairing 186 bugs (10% more than SOTA) and reducing patch search space by 70% on Defects4J benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ca5f82c2ff7a5874dfaa7c320c9097716fe0d1514eca3eb69291b75f1a981d7_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes DynaFix, an automated program repair method that iteratively uses execution-level dynamic information (like variable states) to guide large language models in generating patches. This approach mimics human debugging by refining patches based on runtime feedback from failed attempts. Evaluation on Defects4J shows it repairs more bugs and reduces the search space more effectively than existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有APR方法依赖静态分析或粗粒度反馈，难以模拟人类逐步调试/Existing APR relies on static or coarse feedback, failing to simulate stepwise debugging]
        C --> C1[迭代捕获执行级动态信息（变量状态、控制流等）指导LLM生成补丁/Iteratively captures execution-level info (variable states, control flow) to guide LLM patch generation]
        D --> D1[修复Defects4J中186个bug，性能提升10%/Repairs 186 bugs on Defects4J, 10% improvement over SOTA]
        D --> D2[将补丁搜索空间减少70%/Reduces patch search space by 70%]
    ```

- **[arXiv260101] Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation**
  - **tags:** [ai], [robot navigation], [hybrid motion planning, deep reinforcement learning, entity-aware reward, graph-based global planner, collision avoidance]
  - **authors:** Yury Kolomeytsev, Dmitry Golembiovsky
  - **institution:** Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2512.24651
  - **contributions:** 1. Proposes HMP-DRL, a hybrid framework integrating a graph-based global planner with a local DRL policy via checkpoints. 2. Introduces an entity-aware reward structure for the local planner to ensure social compliance by adjusting safety based on agent type. 3. Validates the method in a realistic simulation, showing superior performance in success rate, collision rate, and time to goal.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes HMP-DRL, a hybrid motion planning framework that combines a graph-based global planner for long-range pathfinding with a local Deep Reinforcement Learning policy for reactive, socially-compliant navigation. The method uses checkpoints to integrate the global path and an entity-aware reward function to dynamically adjust to different moving agents. Experiments in realistic simulation show it outperforms other methods in key navigation metrics, enhancing safety and reliability in complex environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统图规划器缺乏反应性/Traditional graph planners lack reactivity]
        B --> B2[深度强化学习方法缺乏全局上下文/DRL methods lack global context]
        C --> C1[混合框架HMP-DRL/Hybrid framework HMP-DRL]
        C1 --> C2[图规划器生成路径/Graph planner generates path]
        C1 --> C3[局部DRL策略使用检查点和实体感知奖励/Local DRL policy uses checkpoints & entity-aware reward]
        D --> D1[更高的成功率/Higher success rate]
        D --> D2[更低的碰撞率/Lower collision rate]
        D --> D3[更短的到达时间/Shorter time to goal]
    ```

- **[arXiv260101] Do Large Language Models Know What They Are Capable Of?**
  - **tags:** [nlp], [llm evaluation], [in-advance confidence, overconfidence, capability awareness, decision-making, agentic tasks]
  - **authors:** Casey O. Barkan, Sid Black, Oliver Sourbut
  - **institution:** RAND Corporation, UK AI Security Institute, The Future of Life Foundation
  - **link:** https://arxiv.org/pdf/2512.24661
  - **contributions:** 1. Evaluates LLMs' in-advance confidence and its impact on decision-making in costly-failure scenarios, a less studied area compared to after-the-fact calibration. 2. Investigates how LLMs' confidence and overconfidence evolve during multi-step agentic tasks and with in-context failure experiences. 3. Demonstrates that while LLMs' decisions are rational given their self-estimates, their systematic overconfidence leads to poor task pursuit decisions, highlighting a lack of capability awareness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c758274cef27c9ad79258c27a063b8420cef801a9f25ff7610a7c4c12509a926_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether large language models (LLMs) can accurately predict their own success on tasks, especially when failure is costly. It evaluates their in-advance confidence, how it changes during multi-step tasks and with in-context failure, and its impact on decision-making. The main finding is that current LLMs are generally overconfident, which impairs their decision-making despite rational behavior based on their flawed self-assessments, indicating a lack of self-awareness of their capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Do Large Language Models Know What They Are Capable Of?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs能否预测自身任务成功率?<br/>Can LLMs predict their task success?]
        B --> B2[失败成本高时如何决策?<br/>How to decide when failure is costly?]
        C --> C1[评估事前置信度<br/>Evaluate in-advance confidence]
        C --> C2[分析多步骤任务中的信心变化<br/>Analyze confidence change in multi-step tasks]
        C --> C3[研究上下文失败经验的影响<br/>Study impact of in-context failure]
        D --> D1[LLMs普遍过度自信<br/>LLMs are generally overconfident]
        D --> D2[新/大模型判别力未显著提升<br/>Newer/larger models don't have greater discriminatory power]
        D --> D3[部分模型能从失败中学习<br/>Some models learn from failure]
        D --> D4[决策理性但估计过于乐观<br/>Decisions rational but estimates overly optimistic]
    ```

- **[arXiv260101] Renormalization Group Guided Tensor Network Structure Search**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [tensor network structure search, renormalization group, multi-scale optimization, edge gates, node tension]
  - **authors:** Maolin Wang, Bowen Yu, Sheng Zhang, Linjie Mi, Wanyu Wang, Yiqi Wang, Pengyue Jia, Xuetao Wei, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao
  - **institution:** City University of Hong Kong, National University of Defense Technology, Southern University of Science and Technology, Fudan University, Intuit AI
  - **link:** https://arxiv.org/pdf/2512.24663
  - **code:** https://github.com/Applied-Machine-Learning-Lab/RGTN
  - **contributions:** 1. Proposes RGTN, a physics-inspired framework that uses multi-scale renormalization group flows for continuous tensor network structure evolution, overcoming the limitations of fixed-scale, discrete search methods. 2. Introduces learnable edge gates for dynamic topology modification and intelligent proposals based on physical quantities like node tension and edge information flow to guide the search. 3. Demonstrates state-of-the-art performance, achieving superior compression ratios and running 4-600 times faster than existing methods on tasks like light field data and video completion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9fdccc30b2aad8da00c1fdf132004ba3347f8992425dfb71406c3fa11d073c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing Tensor Network Structure Search (TN-SS) methods, which struggle with computational tractability and structure adaptivity. The authors propose RGTN, a novel framework guided by renormalization group theory, which enables multi-scale, continuous structure optimization. Experiments show RGTN achieves better compression and is significantly faster than prior methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Renormalization Group Guided Tensor Network Structure Search] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有TN-SS方法局限/Limitations of existing TN-SS]
        P1 --> P1_1[单尺度优化/Single-scale optimization]
        P1 --> P1_2[离散搜索空间/Discrete search space]
        P1 --> P1_3[分离的结构-参数优化/Separated structure-parameter optimization]
        Method[主要方法/Method] --> M1[RGTN框架/RGTN Framework]
        M1 --> M1_1[多尺度重整化群流/Multi-scale RG flows]
        M1 --> M1_2[可学习边门/Learnable edge gates]
        M1 --> M1_3[智能提案(节点张力, 边信息流)/Intelligent proposals (node tension, edge info flow)]
        Results[关键结果/Results] --> R1[SOTA压缩比/State-of-the-art compression ratio]
        Results --> R2[4-600倍加速/4-600x speedup]
    ```

- **[arXiv260101] Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions**
  - **tags:** [ai], [domain generalization], [multi-modal fusion, feature disentanglement, domain-invariant representation, cross-domain mixing, unseen working conditions]
  - **authors:** Pengcheng Xia, Yixiang Huang, Chengjin Qin, Chengliang Liu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.24679
  - **code:** https://github.com/xiapc1996/MMDG
  - **contributions:** 1. A dual disentanglement framework to separate modality-invariant/specific and domain-invariant/specific features. 2. A cross-domain mixed fusion strategy to augment data diversity by randomly mixing modality information across domains. 3. A triple-modal fusion mechanism to adaptively integrate heterogeneous multi-modal information.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd4bf3933b539ef0ff25f07453c8a5bf8bac90477ba90fdbe4c834d24627de_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-modal fusion model with dual feature disentanglement to address the problem of fault diagnosis under unseen working conditions. The method decouples modality and domain features and uses cross-domain mixing to improve generalization. Experiments on motor fault diagnosis show it outperforms existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multi-modal Cross-domain Mixed Fusion Model / 多模态跨域混合融合模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["性能下降在未见工况 / Performance decline under unseen conditions"]
        Problem --> P2["单模态信息局限 / Single-modal limitation"]
        Method --> M1["双重解耦框架 / Dual Disentanglement Framework"]
        Method --> M2["跨域混合融合 / Cross-domain Mixed Fusion"]
        Method --> M3["三模态融合 / Triple-modal Fusion"]
        Results --> R1["优于先进方法 / Outperforms advanced methods"]
        Results --> R2["消融验证有效性 / Ablation verifies effectiveness"]
    ```

- **[arXiv260101] VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots**
  - **tags:** [mlsys], [multi-modal inference], [VLA models, action chunking, trajectory smoothing, asynchronous inference, robot motion control]
  - **authors:** Yongsheng Zhao, Lei Zhao, Baoping Cheng, Gongxin Yao, Xuanzhang Wen, Han Gao
  - **institution:** China Mobile (Hangzhou) Information Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.24673
  - **contributions:** 1. Proposes VLA-RAIL, a framework for asynchronous inference and motion control to enable smooth, continuous robot action execution. 2. Introduces a Trajectory Smoother using polynomial fitting to filter noise and jitter within an action chunk. 3. Designs a Chunk Fuser to ensure position, velocity, and acceleration continuity between successive action chunks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/886e2a128c843b55a605e8cbe2a7b174fc4b0e84225f1908b0b180891d09bdad_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of motion jitter, stalling, and pauses when deploying Vision-Language-Action (VLA) models on robots due to sequential inference and execution. It proposes VLA-RAIL, a framework that decouples model inference from robot control via a Trajectory Smoother and Chunk Fuser. Experiments show it reduces jitter, increases speed, and improves task success rates for robotic manipulation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLA-RAIL: A Real-Time Asynchronous Inference Linker] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法导致机器人动作抖动、卡顿/Existing methods cause jitter, stalling in robot actions]
        C --> C1[异步推理与运动控制/Asynchronous inference & motion control]
        C1 --> C2[轨迹平滑器/Trajectory Smoother]
        C1 --> C3[块融合器/Chunk Fuser]
        D --> D1[减少运动抖动/Reduces motion jitter]
        D --> D2[提升执行速度/Enhances execution speed]
        D --> D3[提高任务成功率/Improves task success rates]
    ```

- **[arXiv260101] R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory**
  - **tags:** [nlp], [computational argumentation], [retrieval-augmented generation, argumentative memory, multi-turn debate, agentic framework, rhetorical grounding]
  - **authors:** Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu
  - **institution:** University of Auckland, Wuhan College of Communication
  - **link:** https://arxiv.org/pdf/2512.24684
  - **code:** https://anonymous.4open.science/r/R-debater-E87F/
  - **contributions:** 1. Proposes R-Debater, a novel agentic framework for multi-turn debate generation grounded in the concept of "argumentative memory" from rhetoric and memory studies. 2. Integrates a debate knowledge base for retrieving evidence and prior arguments with a role-based agent to ensure stance consistency and coherent multi-turn composition. 3. Demonstrates superior performance over strong LLM baselines in both single-turn and multi-turn debate tasks through automated metrics (InspireScore, Debatrix) and human evaluation with experienced debaters.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d8c7f9f1e27feb6f7eff24aacab0f5dd1eb3b83c76e27f254c97c97b4c0b3_w640_q70.webp
  - **Simple LLM Summary:** The paper presents R-Debater, a framework that generates multi-turn debates by retrieving and adapting arguments from a knowledge base ("argumentative memory") using a role-based agent. Evaluated on ORCHID debates, it outperforms LLM baselines in producing more consistent, evidence-grounded, and coherent debates across turns.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[R-Debater: Retrieval-Augmented Debate Generation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: LLMs generate fluent but shallow, ungrounded debates with weak stance fidelity]
        C[主要方法/Method: Agentic framework with argumentative memory for retrieval & role-based utterance composition]
        D[关键结果/Results: Higher scores than LLM baselines; more faithful, stance-aligned, coherent debates]
    ```

- **[arXiv260101] BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis**
  - **tags:** [mlsys], [agent system], [physics-informed features, SHAP, Gradient Boosting Decision Trees, LLM reasoning, fault diagnosis]
  - **authors:** Songqi Zhou, Ruixue Liu, Boman Su, Jiazhou Wang, Yixing Wang, Benben Jiang
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.24686
  - **contributions:** 1. Proposes BatteryAgent, a hierarchical framework integrating physics-based features with LLM reasoning for interpretable battery fault diagnosis. 2. Introduces a "numerical-semantic" bridge using SHAP attributions and a knowledge base to generate comprehensive diagnostic reports with root cause analysis. 3. Demonstrates superior performance (AUROC 0.986) and extends binary detection to multi-type, interpretable diagnosis, shifting from passive detection to intelligent diagnosis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3462edc6bdbdfb2b93fa0ef560df4483fbef65600fac566e9e76c08059dd4f31_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes BatteryAgent, a framework that combines physics-informed features, Gradient Boosting Decision Trees with SHAP, and an LLM agent to diagnose lithium-ion battery faults. It achieves high accuracy (AUROC 0.986) and provides interpretable reports with root causes and maintenance suggestions, moving beyond simple binary classification.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[黑盒模型缺乏可解释性/Black-box models lack interpretability]
        B --> B2[二元分类无法提供根因分析/Binary classification lacks root cause analysis]
        C --> C1[物理感知层提取特征/Physical Perception Layer extracts features]
        C --> C2[检测归因层量化贡献/Detection & Attribution Layer quantifies contributions]
        C --> C3[推理诊断层生成报告/Reasoning & Diagnosis Layer generates reports]
        D --> D1[高精度AUROC 0.986/High accuracy AUROC 0.986]
        D --> D2[纠正困难样本/Misclassification correction on hard samples]
        D --> D3[实现智能诊断/Enables intelligent diagnosis]
    ```

- **[arXiv260101] Nested Learning: The Illusion of Deep Learning Architectures**
  - **tags:** [ai], [learning theory], [nested learning, in-context learning, continual learning, associative memory, self-modifying model]
  - **authors:** Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni
  - **institution:** Google Research (inferred from authors Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni, who are affiliated with Google)
  - **link:** https://arxiv.org/pdf/2512.24695
  - **contributions:** 1. Expressive Optimizers: Shows gradient-based optimizers are associative memory modules and proposes more expressive variants with deeper memory and learning rules. 2. Self-Modifying Learning Module: Presents a sequence model that learns to modify itself by learning its own update algorithm. 3. Continuum Memory System: Introduces a new memory formulation generalizing long/short-term memory, which is combined with the self-modifying model to create "Hope", a continual learning module.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac4e338b76a10773a96b12a072d809a81c99a79e63d8b40927cb078da7b7fdb_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a new learning paradigm called Nested Learning (NL), which frames machine learning models as nested optimization problems. This view explains the emergence of in-context learning and is used to design more expressive optimizers, a self-modifying model, and a new memory system, culminating in a continual learning module named "Hope" that shows promising results on various tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Nested Learning: The Illusion of Deep Learning Architecture] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[如何实现持续学习与自我改进/How to achieve continual learning and self-improvement]
        C --> C1[嵌套学习范式/Nested Learning Paradigm]
        C --> C2[设计表达性优化器/Design Expressive Optimizers]
        C --> C3[自修改学习模块/Self-Modifying Learning Module]
        C --> C4[连续体记忆系统/Continuum Memory System]
        D --> D1[提出持续学习模块Hope/Propose continual learning module Hope]
        D --> D2[在多个任务上展示潜力/Show potential on multiple tasks]
    ```

- **[arXiv260101] BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework**
  - **tags:** [ai], [multi-task learning], [multi-armed bandit, negative transfer, auxiliary task selection, multi-bandit framework, drug-target interaction]
  - **authors:** András Millinghoffer, András Formanek, András Antos, Péter Antal
  - **institution:** Budapest University of Technology and Economics, E-Group ICT Software Zrt., KU Leuven
  - **link:** https://arxiv.org/pdf/2512.24708
  - **contributions:** 1. A three-stage method (BandiK) for efficient auxiliary task subset selection in multi-task learning, 2. Reduction of candidate auxiliary sets from exponential to linear complexity using pairwise transfer estimations, 3. A novel multi-bandit framework that exploits semi-overlapping arms across tasks to improve computational efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e49fedc6c9b07cd38435ef8daff0ba16207d6088f86e049c77cd192822bd2cf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces BandiK, a three-stage method using multi-armed bandits to efficiently select beneficial auxiliary task subsets in multi-task learning, reducing computational cost by estimating pairwise transfers and leveraging a multi-bandit structure. It is validated on a drug-target interaction benchmark, showing scalable performance for complex multi-task scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework] --> B[核心问题/Problem: 多任务学习中负迁移和辅助任务选择的高计算成本与复杂性]
        A --> C[主要方法/Method: 三阶段多臂老虎机框架，估计任务间转移、构建线性候选集、利用半重叠臂的多老虎机结构]
        A --> D[关键结果/Results: 在药物-靶点相互作用基准上验证，实现高效可扩展的任务分解]
    ```

- **[arXiv260101] Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting**
  - **tags:** [cv], [reasoning segmentation], [evolutionary prompting, zero-shot learning, visual arena, semantic mutation, heterogeneous arena]
  - **authors:** Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao
  - **institution:** Xiamen University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24702
  - **code:** https://github.com/AHideoKuzeA/Evol-SAM3
  - **contributions:** 1. Proposes EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. 2. Introduces a "Generate-Evaluate-Evolve" loop with a Visual Arena for reference-free fitness assessment and a Semantic Mutation operator for diversity and error correction. 3. Designs a Heterogeneous Arena module that integrates geometric priors with semantic reasoning for robust final selection.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b60e24e2f5e5668a6d7d977acfb32177365388575df74e72ccb5a8b3d4e7f7e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of static, training-free methods for reasoning segmentation by proposing EVOL-SAM3, a zero-shot framework that uses an evolutionary prompting strategy to iteratively refine prompt hypotheses at inference time. The method outperforms both static baselines and fully supervised state-of-the-art methods on the ReasonSeg benchmark without any training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EVOL-SAM3: 零样本推理分割的进化提示 / EVOL-SAM3: Zero-Shot Reasoning Segmentation via Evolutionary Prompting] --> B
        A --> C
        A --> D
        B[核心问题 / Problem] --> B1[静态推理范式 / Static Inference Paradigm]
        B1 --> B2[推理深度不足 / Insufficient Reasoning Depth]
        B1 --> B3[无法自我纠正 / Lack of Self-Correction]
        C[主要方法 / Method] --> C1[进化搜索 / Evolutionary Search]
        C1 --> C2[生成-评估-进化循环 / Generate-Evaluate-Evolve Loop]
        C2 --> C3[视觉竞技场 / Visual Arena]
        C2 --> C4[语义突变 / Semantic Mutation]
        C2 --> C5[异构竞技场 / Heterogeneous Arena]
        D[关键结果 / Results] --> D1[超越静态基线 / Outperforms Static Baselines]
        D --> D2[超越全监督SOTA / Surpasses Fully Supervised SOTA]
        D --> D3[零样本设置 / Zero-Shot Setting]
    ```

- **[arXiv260101] LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving**
  - **tags:** [mlsys], [multi-modal inference], [latent semantic rule encoding, recurrent world model, language-guided latent classification, semantic risk detection, autonomous driving]
  - **authors:** Qian Cheng, Weitao Zhou, Cheng Jing, Nanshan Deng, Junze Wen, Zhaoyang Liu, Kun Jiang, Diange Yang
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.24712
  - **contributions:** 1. Proposed LSRE, a framework that encodes sparse VLM judgments into decision boundaries within a recurrent world model's latent space for real-time semantic risk assessment. 2. Demonstrated that LSRE achieves detection accuracy comparable to a per-frame VLM baseline while enabling earlier hazard anticipation and operating at 10 Hz. 3. Showed that the learned latent classifier generalizes to rarely seen, semantically similar test cases, indicating its effectiveness for semantic safety monitoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d95e05edf41980725845c43ad581e04cc0d1877fc56167a5c4d2c47f7e9516_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of real-time semantic rule compliance in autonomous driving, where explicit encoding of complex social rules is difficult. It proposes LSRE, a framework that uses sparsely sampled VLM outputs to train a lightweight latent classifier within a recurrent world model, enabling efficient semantic risk detection. The method achieves accuracy comparable to a VLM baseline with much lower latency and better anticipation, showing promise for deployable semantic safety systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving] --> B(核心问题/Problem: Real-time semantic rule compliance in autonomous driving is difficult to encode explicitly and VLM inference is too slow.)
        A --> C(主要方法/Method: LSRE converts sparse VLM judgments into decision boundaries in a recurrent world model's latent space.)
        A --> D(关键结果/Results: Achieves VLM-comparable accuracy, earlier anticipation, 10 Hz operation, and generalization to unseen cases.)
    ```

- **[arXiv260101] Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow**
  - **tags:** [ai], [robotic manipulation], [3D object flow, video generation, zero-shot manipulation, trajectory optimization, reinforcement learning]
  - **authors:** Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.24766
  - **contributions:** 1. Proposes Dream2Flow, a framework that bridges video generation and robotic control using 3D object flow as an intermediate representation. 2. Demonstrates the ability to reconstruct 3D object motions from generated videos and formulate manipulation as object trajectory tracking, overcoming the embodiment gap. 3. Shows that the method enables zero-shot guidance from pre-trained video models to manipulate diverse object categories (rigid, articulated, deformable, granular) without task-specific demonstrations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Dream2Flow, a framework that uses 3D object flow extracted from videos generated by off-the-shelf models as an interface for robotic manipulation. It translates these generated motions into executable robot actions via trajectory optimization or reinforcement learning, enabling zero-shot manipulation of diverse objects in open-world settings. The results demonstrate 3D object flow as a general and scalable bridge between video generation models and robotic control.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow] --> B(核心问题/Problem: Translating human-like motions from video models into low-level robot actions)
        A --> C(主要方法/Method: Use 3D object flow as intermediate representation, reconstruct motions from videos, track trajectories)
        A --> D(关键结果/Results: Enables zero-shot manipulation of diverse objects, bridges video generation to robot control)
    ```

- **[arXiv260101] HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment**
  - **tags:** [mlsys], [llm inference], [slate recommendation, generative recommendation, residual quantization, hierarchical planning, listwise preference alignment]
  - **authors:** Yunsheng Pang, Zijian Liu, Yudong Li, Shaojie Zhu, Zijian Luo, Chenyun Yu, Sikai Wu, Shichen Shen, Cong Xu, Bin Wang, Kai Jiang, Hongyong Yu, Chengxiang Zhuo, Zang Li
  - **institution:** Tencent, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.24787
  - **contributions:** 1. Proposed an auto-encoder with residual quantization and contrastive constraints for semantically structured item tokenization. 2. Introduced a hierarchical generation framework that decouples list-level planning from item-level decoding for efficient slate generation. 3. Designed a listwise preference alignment objective to directly optimize slate quality using implicit user feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df7bb0829f12a7ec59faf3d297069dfbce2bbcb60223c10f7e7a4162d6e7a4e0_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes HiGR, an efficient generative framework for slate recommendation. It addresses the limitations of existing autoregressive methods by using hierarchical planning and a listwise alignment objective. Experiments on a commercial platform show HiGR significantly outperforms state-of-the-art methods in both offline quality and online metrics while being 5x faster at inference.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiGR: Efficient Generative Slate Recommendation] --> B[核心问题/Problem: 现有自回归方法存在语义纠缠和低效解码]
        A --> C[主要方法/Method: 分层规划与列表偏好对齐]
        C --> D[语义结构化ID / Semantically Structured IDs]
        C --> E[分层生成 / Hierarchical Generation]
        C --> F[列表偏好对齐 / Listwise Preference Alignment]
        A --> G[关键结果/Results: 离线质量提升>10%, 推理加速5倍, 在线指标显著增长]
    ```

- **[arXiv260101] LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)**
  - **tags:** [ai], [theorem proving], [formal verification, category theory, benchmark, Lean, large language models]
  - **authors:** Rongge Xu, Hui Dai, Yiming Fu, Jiedong Jiang, Tianjiao Nie, Hongwei Wang, Junkai Wang, Holiverse Yang, Jiatong Yang, Zhi-Hao Zhang
  - **institution:** Tsinghua University, Southern University of Science and Technology, Westlake University, Xi'an Jiaotong-Liverpool University, The Chinese University of Hong Kong, Yanqi Lake Beijing Institute of Mathematical Sciences and Applications (BIMSA)
  - **link:** https://arxiv.org/pdf/2512.24796
  - **code:** https://github.com/sciencraft/LeanCat
  - **contributions:** 1. Introduces LeanCat, a benchmark for formal category theory in Lean, designed to stress-test abstraction and library-mediated reasoning. 2. Presents a curated dataset of 100 tasks with topic families and difficulty tiers, created via an LLM-assisted human grading process. 3. Demonstrates the benchmark's utility by evaluating models and the LeanBridge method, showing current AI capabilities and providing a checkpoint for tracking progress.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bb17bb33fa46697baca7f3b3a6262916453dd0a4bf5f92ff26cebdd7d681ffe_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces LeanCat, a benchmark for formalizing category theory in Lean to better evaluate AI's ability for abstract, library-based reasoning in mathematics. It presents a curated set of 100 tasks and evaluates models, finding low success rates, especially on harder problems, while showing that retrieval-augmented methods like LeanBridge can improve performance. The benchmark serves as a compact checkpoint for tracking progress in research-level formal theorem proving.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LeanCat: A Benchmark Suite for Formal Category Theory in Lean] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>现有基准未能充分衡量抽象和基于库的推理/Current benchmarks under-measure abstraction and library-mediated reasoning]
        C[主要方法/Method<br>为Lean创建形式化范畴论基准，包含100个分级任务/Create a Lean benchmark for formal category theory with 100 graded tasks]
        D[关键结果/Results<br>最佳模型pass@1为8.25%，检索增强方法有提升/Best model pass@1 is 8.25%, retrieval-augmented methods show gains]
    ```

- **[arXiv260101] Practising responsibility: Ethics in NLP as a hands-on course**
  - **tags:** [nlp], [ethics in nlp], [ethics education, active learning, curriculum development, hands-on activities, learning by teaching]
  - **authors:** Malvina Nissim, Viviana Patti, Beatrice Savoldi
  - **institution:** University of Groningen, University of Turin, Fondazione Bruno Kessler
  - **link:** https://arxiv.org/pdf/2512.24825
  - **contributions:** 1. Introduction of a dedicated course "Ethical Aspects in NLP" designed to integrate ethics into NLP education, 2. Development of a pedagogical approach based on active learning, interactive sessions, hands-on activities, and "learning by teaching" methods, 3. Creation and refinement of the course over four years, adapting it across different institutions, educational levels, and interdisciplinary backgrounds, yielding reusable teaching materials and student-made educational products.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad8d866d1d641ab747e97be881ac0eec09fdae4762445938b33c32d5a452c3e5_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of integrating ethical considerations into NLP education by proposing a hands-on course. The method employs active learning through interactive sessions, practical activities, and "learning by teaching". The main conclusion is that this approach successfully fosters critical thinking and produces reusable educational resources, providing a model for educators to incorporate social impact into curricula.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Practising responsibility: Ethics in NLP as a hands-on course") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("NLP系统普及/NLP systems pervasive")
        Problem --> P2("伦理教育挑战/Ethics education challenges")
        Method --> M1("主动学习/Active learning")
        Method --> M2("实践与互动/Hands-on & interactive")
        Method --> M3("以教促学/Learning by teaching")
        Results --> R1("课程优化与适应/Course refined & adapted")
        Results --> R2("产出可复用产品/Reusable products created")
    ```

- **[arXiv260101] Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences**
  - **tags:** [ai], [human-robot interaction], [object rearrangement, human preference modeling, Monte Carlo Tree Search, psychological constructs, user study]
  - **authors:** Emmanuel Fashae, Michael Burke, Leimin Tian, Lingheng Meng, Pamela Carreno-Medrano
  - **institution:** Monash University, CSIRO Robotics
  - **link:** https://arxiv.org/pdf/2512.24829
  - **contributions:** 1. Proposes a novel, interpretable formulation of human object arrangement preferences based on four psychological constructs (spatial practicality, habitual convenience, semantic coherence, commonsense appropriateness). 2. Designs and validates a self-report questionnaire to capture these constructs through a 63-participant online study. 3. Demonstrates the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner to generate arrangements that align with human preferences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0c9201484194f4f746f05eae7a288356c0e53c3a3a9d4683db5313924455a5a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of interpretability in robotic object rearrangement models by identifying four explicit psychological constructs that guide human organizational preferences. The authors designed a questionnaire to measure these constructs and integrated them into a Monte Carlo Tree Search planner. The results show that the planner, guided by these interpretable preferences, can generate arrangements closely matching those created by human participants.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Explaining Why Things Go Where They Go<br>解释物品为何归位] --> B(Problem: 机器人重排模型缺乏可解释性<br>Problem: Robotic rearrangement models lack interpretability)
        A --> C(Method: 提出四个可解释偏好构念与问卷<br>Method: Four interpretable preference constructs & questionnaire)
        A --> D(Results: 基于MCTS的规划器能生成符合人类偏好的布局<br>Results: MCTS planner generates human-aligned arrangements)
    ```

- **[arXiv260101] GenZ: Foundational models as latent variable generators within traditional statistical models**
  - **tags:** [ai], [hybrid statistical modeling], [latent variable model, generalized EM algorithm, semantic feature discovery, cold-start collaborative filtering, hedonic regression]
  - **authors:** Marko Jojic, Nebojsa Jojic
  - **institution:** Arizona State University, Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.24834
  - **contributions:** 1. Proposes GenZ, a hybrid model that integrates frozen foundational models as latent variable generators within traditional statistical models. 2. Introduces an iterative, generalized EM algorithm that jointly discovers interpretable semantic feature descriptors and optimizes statistical model parameters from dataset errors. 3. Demonstrates the method's effectiveness on real-world tasks, significantly outperforming pure LLM baselines and matching performance that requires extensive traditional data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11807fe38101f2230e94348a6c074eab9e6cece7f18336e076aa76c0c5604232_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes GenZ, a method that uses a frozen foundational model to generate latent semantic features within a statistical model, optimized via a generalized EM algorithm. It shows this hybrid approach significantly outperforms using the foundational model's general knowledge alone for house price prediction and matches traditional collaborative filtering performance for cold-start movie recommendations using only semantic descriptions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["GenZ: Foundational models as latent variable generators"] --> Problem["核心问题/Problem: Foundational models lack dataset-specific patterns for prediction"]
        Root --> Method["主要方法/Method: Hybrid model with generalized EM for joint semantic feature & statistical parameter optimization"]
        Root --> Results["关键结果/Results: Outperforms GPT-5 on house prices; Matches CF with 4000 ratings using semantics"]
    ```

- **[arXiv260101] Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control**
  - **tags:** [mlsys], [multi-modal inference], [derivative-free optimisation, regret minimisation, multivariate mutual information, in-scene camera control, vision-language models]
  - **authors:** Jason Armitage, Rico Sennnrich
  - **institution:** University of Zurich
  - **link:** https://arxiv.org/pdf/2512.24826
  - **contributions:** 1. A new method that improves multivariate mutual information estimates using regret minimisation with derivative-free optimisation. 2. An algorithm enabling off-the-shelf 2D-trained cross-modal systems to adapt online to object occlusions and differentiate features in 3D scenes. 3. A pipeline that controls an in-scene camera to learn directly from noisy VLM outputs, improving performance on 3D multi-object scenes without pretraining or finetuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66c8b5c775488157feb95c4650ebd70cf6bfc3b2373d13cfde2db9f88f119e8d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the dimensional shift when 2D-trained vision-language models process 3D scenes. It proposes a method using derivative-free optimisation and regret minimisation to improve mutual information estimates and control an in-scene camera, allowing the system to adapt online and improve performance on cross-modal tasks for 3D multi-object scenes without additional training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("2D系统处理3D场景的维度偏移/Dimensional shift for 2D systems on 3D scenes")
        Problem --> P2("需要学习相机控制模块/Need to learn an in-scene camera control module")
        Method --> M1("基于遗憾最小化的多元互信息估计/Multivariate mutual information estimates via regret minimisation")
        Method --> M2("使用无导数优化/Using derivative-free optimisation")
        Results --> R1("使现成的2D系统能在线适应3D场景/Enables off-the-shelf 2D systems to adapt online to 3D scenes")
        Results --> R2("无需预训练或微调即可提升性能/Improves performance without pretraining or finetuning")
    ```

- **[arXiv260101] PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI**
  - **tags:** [sec], [Privacy protections], [PrivacyBench, Retrieval-Augmented Generation (RAG), secret leakage, privacy-aware prompt, privacy-by-design]
  - **authors:** Srija Mukhopadhyay, Sathwik Reddy, Shruthi Muthukumar, Jisun An, Ponnurangam Kumaraguru
  - **institution:** International Institute of Information Technology Hyderabad, Indiana University
  - **link:** https://arxiv.org/pdf/2512.24848
  - **contributions:** 1. Introduces PrivacyBench, a novel conversational benchmark with socially grounded datasets containing embedded secrets for evaluating privacy in AI assistants. 2. Provides a multi-turn conversational evaluation framework to measure secret preservation capabilities of personalized AI systems. 3. Empirically demonstrates that current RAG-based assistants leak secrets in up to 26.56% of interactions and identifies a critical architectural flaw where the retrieval mechanism is a single point of failure for privacy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7685e487e221610623a5fd5b2084368051b6218a6804a6749e97cc93aef26acb_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces PrivacyBench, a benchmark to evaluate privacy leakage in personalized AI agents that access a user's digital footprint. Testing shows RAG-based assistants leak secrets frequently, and while privacy-aware prompts help, the fundamental architecture is unsafe, highlighting the need for structural, privacy-by-design solutions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Personalized AI agents risk exposing sensitive user data from their digital footprint.]
        C[主要方法/Method: Introduce PrivacyBench benchmark with datasets containing secrets and multi-turn conversational evaluation.]
        D[关键结果/Results: RAG assistants leak secrets; privacy prompts partially mitigate; need for privacy-by-design safeguards.]
    ```

- **[arXiv260101] A study on constraint extraction and exception exclusion in care worker scheduling**
  - **tags:** [ai], [constraint learning], [constraint extraction, exception exclusion, constraint programming, shift scheduling, care worker scheduling]
  - **authors:** Koki Suenaga, Tomohiro Furuta, Satoshi Ono
  - **institution:** Kagoshima University
  - **link:** https://arxiv.org/pdf/2512.24853
  - **contributions:**  1. Proposes a method using constraint templates to automatically extract facility-specific scheduling constraints from interview data, reducing the need for manual specification. 2. Introduces a novel mechanism to identify and exclude exceptional constraints from the extraction process, improving the quality of the learned constraints. 3. Demonstrates the effectiveness of the approach by successfully generating schedules that satisfy all hard constraints and reduce soft constraint violations in care worker scheduling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e005f244d37b26b715bffe3d3edce37668aa27d5541ef7201258ae5746901d94_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automatically generating work schedules for long-term care facilities, where constraints vary widely. It proposes a method that uses customizable constraint templates to extract relevant rules from manager interviews while excluding exceptional cases. Experiments show the method successfully creates schedules that meet all hard constraints and reduces soft constraint violations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Study on Constraint Extraction and Exception Exclusion in Care Worker Scheduling] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[设施条件各异，需人工访谈定义约束/Facility-specific constraints require manual interviews]
        C --> C1[使用约束模板提取组合/Use constraint templates to extract combinations]
        C --> C2[引入例外约束排除机制/Incorporate mechanism to exclude exceptional constraints]
        D --> D1[满足所有硬约束/Satisfied all hard constraints]
        D --> D2[减少软约束违规/Reduced soft constraint violations]
    ```

- **[arXiv260101] Big AI is accelerating the metacrisis: What can we do?**
  - **tags:** [nlp], [ethics & society], [metacrisis, language engineers, human flourishing, planetary boundaries, technofeudalism]
  - **authors:** Steven Bird
  - **institution:** Charles Darwin University
  - **link:** https://arxiv.org/pdf/2512.24863
  - **contributions:** 1. Identifies and critiques the role of "Big AI" and language engineers in accelerating converging global crises (ecological, meaning, language). 2. Highlights the ethical conflict between professional obligations (e.g., ACL Code of Ethics) and the harms caused by current NLP/AI development practices. 3. Proposes a paradigm shift for NLP, advocating for a future centered on human flourishing and amplifying social networks rather than scaling through large, polluting models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the current trajectory of "Big AI," particularly in NLP, is accelerating a global metacrisis. It critiques the field's focus on scalability and value-neutral technology development, which benefits powerful interests at the expense of the public good and the planet. The paper concludes by urgently calling for an alternative, life-affirming future for NLP centered on human flourishing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Big AI is accelerating the metacrisis: What can we do?] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Big AI加速生态、意义和语言危机/Big AI accelerates ecological, meaning, and language crises]
        B --> B2[语言工程师的伦理困境/Ethical dilemma of language engineers]
        C --> C1[批判当前可扩展性叙事/Critique current scalability narrative]
        C --> C2[呼吁探索替代方案/Call to explore alternatives]
        D --> D1[需要以人类繁荣为中心的未来/NLP future must center human flourishing]
        D --> D2[利用集体智慧设计生命肯定的NLP/Design life-affirming NLP with collective intelligence]
    ```

- **[arXiv260101] Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements**
  - **tags:** [nlp], [llm evaluation], [benchmark, knowledge statements, dynamic composition, data contamination, multi-knowledge assessment]
  - **authors:** Yiming Liang, Yizhi Li, Yantao Du, Ge Zhang, Jiayi Zhou, Yuchen Wu, Yinzhu Piao, Denghui Cao, Tong Sun, Ziniu Li, Li Du, Bo Lei, Jiaheng Liu, Chenghua Lin, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang
  - **institution:** University of Chinese Academy of Sciences, Chinese Academy of Sciences, Bytedance, Nanjing University, M-A-P, BAAI, The University of Manchester
  - **link:** https://arxiv.org/pdf/2512.24867
  - **code:** https://encyclo-k.github.io
  - **contributions:** 1. Proposes a novel statement-based benchmark (Encyclo-K) that uses knowledge statements as the fundamental curation unit instead of pre-defined questions., 2. Introduces a dynamic evaluation method where questions are composed by randomly sampling multiple statements at test time, mitigating data contamination and enabling periodic refresh., 3. Demonstrates a scalable, low-cost annotation process that requires only formatting verification, not domain expertise, while enabling comprehensive multi-knowledge assessment per question.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf5e4785262ae916ad666afc0e0e212833641ae7597f48f69f148b35cfc4807b_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Encyclo-K, a new benchmark for evaluating LLMs that constructs questions by dynamically combining multiple knowledge statements extracted from textbooks at test time. This approach addresses key limitations of existing benchmarks, such as data contamination and single-point assessment. Experiments show it poses a significant challenge to state-of-the-art models, with top accuracy at only 62.07%, validating its effectiveness for assessing comprehensive, multi-statement understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>现有基准的局限性/Limitations of Existing Benchmarks]
        C[主要方法/Method<br>基于知识陈述的动态组合/Dynamic Composition from Knowledge Statements]
        D[关键结果/Results<br>强区分性，模型表现梯度分布/Strong Discriminative Power, Gradient Performance]
        B --> B1[数据污染/Data Contamination]
        B --> B2[单知识点评估/Single-Knowledge Assessment]
        B --> B3[高标注成本/High Annotation Cost]
        C --> C1[从权威教材提取陈述/Extract Statements from Textbooks]
        C --> C2[测试时随机组合/Compose Questions at Test Time]
        D --> D1[GPT-5.1准确率62.07%/GPT-5.1 Accuracy 62.07%]
        D --> D2[推理模型16.04%-62.07%/Reasoning Models 16.04%-62.07%]
        D --> D3[聊天模型9.71%-50.40%/Chat Models 9.71%-50.40%]
    ```

- **[arXiv260101] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem**
  - **tags:** [mlsys], [agent system], [Agentic Learning Ecosystem (ALE), Interaction-based Policy Alignment (IPA), Terminal Bench Pro, post-training, trajectory generation]
  - **authors:** Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng
  - **institution:** ROCK & ROLL & IFLOW & DT Joint Team (Inferred from the author list and likely represents a collaboration, but no specific university or company is named. The domain is unclear from the provided text.)
  - **link:** https://arxiv.org/pdf/2512.24873
  - **contributions:** 1. Introduces the Agentic Learning Ecosystem (ALE), an end-to-end infrastructure for developing agent LLMs, comprising the ROLL post-training framework, the ROCK sandbox manager, and the iFlow CLI agent framework. 2. Proposes a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks to improve long-horizon training stability. 3. Releases the ROME agent model, trained on over one million trajectories, and introduces the Terminal Bench Pro benchmark with improved scale and contamination control for rigorous evaluation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408c7a8de1b2fe7261d28a9d7eb1d3df3afe81fc17a03cc9b2c09e332ac8782_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a principled, end-to-end ecosystem for developing agentic LLMs by introducing the Agentic Learning Ecosystem (ALE). ALE streamlines the agent development pipeline, and the authors use it to build and release the ROME agent model, which demonstrates strong performance on benchmarks, validating the effectiveness of their infrastructure.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Let It Flow: Agentic Crafting on Rock and Roll<br/>构建ROME模型于开放智能体学习生态中] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Open-source community lacks a principled, end-to-end ecosystem for agent LLM development.] --> B1[阻碍/Block<br/>Hinders practical development and production adoption of agents.]
        C[主要方法/Method<br/>Introduce Agentic Learning Ecosystem (ALE)] --> C1[组件/Components<br/>ROLL (post-training), ROCK (sandbox), iFlow CLI (agent framework)]
        C --> C2[模型/Model<br/>Release ROME agent trained on 1M+ trajectories]
        C --> C3[算法/Algorithm<br/>Propose Interaction-based Policy Alignment (IPA)]
        D[关键结果/Results<br/>ROME achieves strong benchmark performance.] --> D1[基准/Benchmarks<br/>24.72% on Terminal-Bench 2.0, 57.40% on SWE-bench Verified]
        D --> D2[新基准/New Benchmark<br/>Introduce Terminal Bench Pro for rigorous evaluation.]
    ```

- **[arXiv260101] Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing**
  - **tags:** [cv], [3D object detection], [semi-automated annotation, human-in-the-loop, 3D object detection, data anonymization, domain adaptation]
  - **authors:** Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter
  - **institution:** Warsaw University of Technology
  - **link:** https://arxiv.org/pdf/2512.24896
  - **contributions:** 1. A semi-automated annotation pipeline that combines AI-generated initial annotations with human verification to reduce cost and time. 2. A system architecture supporting iterative model retraining and incorporating data anonymization and domain adaptation techniques. 3. A methodology and toolset that accelerates the creation of a large-scale, multimodal autonomous driving dataset tailored to Polish road conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/575844a7a43a73f8b8d00aa31dfa90a80dbb02b7129fa4cb48b23a397afe6e78_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the costly and time-consuming problem of manually annotating large-scale, multimodal datasets for autonomous vehicles. It proposes a semi-automated, human-in-the-loop annotation pipeline that uses 3D object detection to generate initial labels, enabling iterative retraining and incorporating anonymization and adaptation techniques. The developed solution significantly reduces annotation time while ensuring high-quality, consistent labels, directly supporting the creation of a Polish-specific autonomous driving dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing"] --> Problem["核心问题/Problem: Manual annotation of multimodal AV datasets is costly and time-consuming."]
        Root --> Method["主要方法/Method: A semi-automated, human-in-the-loop pipeline using 3D object detection for initial annotations."]
        Root --> Results["关键结果/Results: Substantial time savings and consistent, high-quality annotations."]
    ```

- **[arXiv260101] mHC: Manifold-Constrained Hyper-Connections**
  - **tags:** [mlsys], [llm training], [Hyper-Connections, residual connection, identity mapping, manifold constraint, training stability]
  - **authors:** Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang
  - **institution:** DeepSeek-AI
  - **link:** https://arxiv.org/pdf/2512.24880
  - **contributions:** 1. Proposes Manifold-Constrained Hyper-Connections (mHC), a framework that projects the residual connection space onto a specific manifold to restore the identity mapping property compromised by Hyper-Connections (HC). 2. Incorporates rigorous infrastructure optimization to address the memory access overhead and ensure training efficiency. 3. Demonstrates that mHC enables effective large-scale training with tangible performance improvements and superior scalability, offering a flexible and practical extension of HC.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that Hyper-Connections (HC), while improving performance, lose the identity mapping property of standard residual connections, leading to training instability and memory overhead. To solve this, the authors propose Manifold-Constrained Hyper-Connections (mHC), which projects HC's connection space onto a manifold to restore identity mapping and includes infrastructure optimizations. Empirical results show mHC is effective for scalable training, offering better performance and stability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[mHC: Manifold-Constrained Hyper-Connections] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["HC 破坏了恒等映射，导致训练不稳定/HC compromises identity mapping, causing instability"]
        B --> B2["HC 带来内存开销/HC incurs memory overhead"]
        C --> C1["将残差连接空间投影到特定流形/Project residual space onto a manifold"]
        C --> C2["恢复恒等映射属性/Restore identity mapping property"]
        C --> C3["结合基础设施优化/Incorporate infrastructure optimization"]
        D --> D1["实现可扩展的有效训练/Enables effective training at scale"]
        D --> D2["提供性能改进和可扩展性/Offers performance improvements & scalability"]
    ```

- **[arXiv260101] AI-Driven Cloud Resource Optimization for Multi-Cluster Environments**
  - **tags:** [mlsys], [cluster infrastructure], [multi-cluster systems, resource optimization, predictive learning, policy-aware decision-making, cross-cluster telemetry]
  - **authors:** Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Abhirup Mazumder, Kabilan Kannan, Sumit Saha
  - **institution:** IEEE (affiliations indicate authors are IEEE Senior Members, with industry affiliations from Albertsons and East West Bank, USA)
  - **link:** https://arxiv.org/pdf/2512.24914
  - **contributions:** 1. An AI-driven framework for adaptive resource optimization across multi-cluster cloud systems, moving beyond reactive, cluster-centric approaches. 2. Integration of predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management. 3. A prototype demonstrating improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02143e20715d24c6ab11a29c3710ca28e3f39c48ce36f9862a254d3564252726_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an AI-driven framework to address the problem of inefficient and reactive resource management in multi-cluster cloud environments. The method uses predictive learning and policy-aware decision-making on cross-cluster telemetry to proactively optimize resource allocation for performance, cost, and reliability. The results show the framework improves resource efficiency and system stability compared to traditional approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AI-Driven Cloud Resource Optimization for Multi-Cluster Environments] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法反应式且集群中心化 / Existing approaches are reactive and cluster-centric]
        B --> B2[导致资源利用低效和延迟适应 / Causes inefficient resource utilization and delayed adaptation]
        C --> C1[AI驱动框架集成预测学习 / AI-driven framework integrates predictive learning]
        C --> C2[策略感知决策与持续反馈 / Policy-aware decision-making and continuous feedback]
        C --> C3[分析跨集群遥测数据 / Analyzes cross-cluster telemetry]
        D --> D1[提高资源效率 / Improved resource efficiency]
        D --> D2[更快稳定于工作负载波动 / Faster stabilization during workload fluctuations]
        D --> D3[减少性能变异 / Reduced performance variability]
    ```

- **[arXiv260101] Iterative Deployment Improves Planning Skills in LLMs**
  - **tags:** [ai], [reinforcement learning], [iterative deployment, implicit reward, data curation, planning, fine-tuning]
  - **authors:** Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal
  - **institution:** University of Oxford, AI Sequrity Company, UFRGS
  - **link:** https://arxiv.org/pdf/2512.24940
  - **contributions:** 1. Demonstrates that iterative deployment and fine-tuning on curated user data significantly improves LLM planning skills, including emergent generalization to longer plans. 2. Provides a theoretical analysis showing iterative deployment effectively implements an outer-loop reinforcement learning process with an implicit reward function. 3. Highlights the AI safety implications of this implicit training regime and positions it as an alternative to explicit RL training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp
  - **Simple LLM Summary:** The paper shows that repeatedly deploying LLMs and fine-tuning them on curated data from previous deployments significantly improves their planning capabilities. This process is analyzed as an implicit form of reinforcement learning, which raises safety concerns due to the undefined reward function and offers an alternative training paradigm based on data curation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Iterative Deployment Improves Planning Skills in LLMs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM规划能力/LLM Planning Skills]
        C --> C1[迭代部署与微调/Iterative Deployment & Fine-tuning]
        C1 --> C2[用户数据筛选/User Data Curation]
        D --> D1[规划能力提升/Improved Planning Skills]
        D --> D2[发现隐式RL/Discovering Implicit RL]
        D2 --> D3[AI安全影响/AI Safety Implications]
    ```

- **[arXiv260101] RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment**
  - **tags:** [ai], [information retrieval], [relevance assessment, benchmark, long-tail, visual salience, e-commerce]
  - **authors:** Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng
  - **institution:** Taobao & Tmall Group of Alibaba
  - **link:** https://arxiv.org/pdf/2512.24943
  - **contributions:** 1. Proposes RAIR, a comprehensive Chinese benchmark for e-commerce relevance assessment derived from real-world scenarios. 2. Establishes a standardized evaluation framework with universal rules to address the lack of standardized metrics. 3. Introduces a dataset with three specialized subsets (general, long-tail hard, visual salience) to evaluate fundamental, challenging, and multimodal capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes RAIR, a rule-aware benchmark for e-commerce search relevance assessment, to address the lack of complex and standardized evaluation datasets. It introduces a comprehensive dataset with three subsets to test different model capabilities. Experiments on 14 models show RAIR is challenging, with GPT-5 performing best, and it serves as a new industry benchmark.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RAIR: 一个用于电子商务相关性评估的规则感知基准 / RAIR: A Rule-Aware Benchmark for E-commerce Relevance Assessment]
        A --> B[核心问题/Problem: 现有基准缺乏复杂性，缺少标准化评估 / Existing benchmarks lack complexity and standardized evaluation]
        A --> C[主要方法/Method: 提出包含通用、长尾、视觉显著性子集的基准和规则框架 / Propose benchmark with general, long-tail, visual-salience subsets and rule framework]
        A --> D[关键结果/Results: 对14个模型构成挑战，GPT-5表现最佳，可作为行业基准 / Presents challenge to 14 models, GPT-5 performs best, serves as industry benchmark]
    ```

- **[arXiv260101] MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control**
  - **tags:** [ai], [reinforcement learning], [Lyapunov certificates, exponential stability, multi-step learning, actor-critic, maximum entropy RL]
  - **authors:** Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She
  - **institution:** Beihang University
  - **link:** https://arxiv.org/pdf/2512.24955
  - **contributions:** 1. Proposes a novel framework (MSACL) that integrates exponential stability theory with maximum entropy RL via multi-step Lyapunov certificate learning, using off-policy data to learn certificates that satisfy theoretical stability conditions. 2. Introduces Exponential Stability Labels (ESL) and a λ-weighted aggregation mechanism to effectively balance the bias-variance trade-off in multi-step learning. 3. Guides policy optimization with a stability-aware advantage function to ensure the learned policy promotes rapid Lyapunov descent, achieving provable stability and robustness under simple rewards.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes MSACL, a model-free reinforcement learning framework that ensures provable exponential stability by learning Lyapunov certificates from multi-step data and guiding policy optimization with a stability-aware advantage. It demonstrates superior performance over baseline and state-of-the-art Lyapunov-based RL methods across six benchmarks, achieving rapid convergence and robustness with simple rewards. The work establishes a link between Lyapunov theory and actor-critic frameworks for verifiably safe control.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Provable Stability in Model-Free RL / 模型无关RL的可证明稳定性]
        C --> C1[Multi-Step Lyapunov Certificate Learning / 多步李雅普诺夫证书学习]
        C --> C2[Stability-Aware Advantage Function / 稳定性感知优势函数]
        D --> D1[Superiority over SOTA / 优于现有最优方法]
        D --> D2[Exponential Stability & Robustness / 指数稳定性与鲁棒性]
    ```

- **[arXiv260101] Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning**
  - **tags:** [ai], [multi-armed bandits], [semi-overlapping multi-bandit, best arm identification, sequential support network learning, GapE algorithm, sample complexity]
  - **authors:** András Antos, András Millinghoffer, Péter Antal
  - **institution:** Budapest University of Technology and Economics, E-Group ICT Software Zrt.
  - **link:** https://arxiv.org/pdf/2512.24959
  - **contributions:** 1. Proposes a new pure-exploration model called the semi-overlapping multi-bandit (SOMMAB) for Sequential Support Network Learning (SSNL)., 2. Develops a generalized GapE algorithm for the SOMMAB setting., 3. Derives new exponential error bounds that improve the best-known constant in the exponent and scale linearly with the degree of overlap, showing sample complexity gains from shared evaluations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1ccf2e1312507d052da8a3c6c2b6fb042432d3f13e5efa2572b5d2cd1dff292_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new framework called Sequential Support Network Learning (SSNL) and models it as a semi-overlapping multi-bandit (SOMMAB) problem, where a single evaluation provides feedback to multiple bandits. The authors develop a generalized GapE algorithm for SOMMABs and prove new, improved error bounds that demonstrate significant sample-complexity reductions due to structural overlap.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Selecting beneficial partners via shared, asymmetric evaluations"] --> P1["问题领域/Application Domains: MTL, ATL, FL, MAS"]
        Method["主要方法/Method: SOMMAB model & generalized GapE algorithm"] --> M1["模型/Model: Semi-overlapping multi-bandit"]
        Results["关键结果/Results: Improved error bounds & sample complexity gains"] --> R1["理论保证/Theoretical: Exponential bounds scale with overlap"]
    ```

- **[arXiv260101] AMAP Agentic Planning Technical Report**
  - **tags:** [mlsys], [agent system], [tool-integrated reasoning, spatio-temporal understanding, cascaded training, hierarchical data curation, stable tool environment]
  - **authors:** Yulan Hu, Xiangwen Zhang, Sheng Ouyang, Hao Yi, Lu Xu, Qinglin Lang, Lide Tan, Xiang Cheng, Tianchen Ye, Zhicong Li, Ge Chen, Wenjin Yang, Zheng Pan, Shaopan Xiong, Siran Yang, Ju Huang, Yan Zhang, Jiamang Wang, Yong Liu, Yinfeng Huang, Tucheng Lin, Xin Li, Ning Guo
  - **institution:** Alibaba (AMAP AI Agent LLM Team)
  - **link:** https://arxiv.org/pdf/2512.24957
  - **contributions:** 1. A stable tool environment supporting over ten domain-specific tools for asynchronous rollout and training. 2. A hierarchical data curation framework that filters high-quality queries with a 1:10,000 ratio, emphasizing diversity and difficulty. 3. A cascaded training recipe involving a seed SFT stage to measure query difficulty, a second SFT stage on high-certainty data, and an RL stage on low-certainty data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b6f557a1e8b6dbdc7fc3030b4817b52b9eae922a5f44ccb37d965540ecf2229_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces STAgent, an agentic LLM specialized for spatio-temporal reasoning tasks like itinerary planning. It is empowered by a stable tool environment, a hierarchical data curation framework, and a cascaded training recipe. The model, initialized from Qwen3-30B-A3B, shows strong performance on TravelBench while maintaining general capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AMAP Agentic Planning Technical Report] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[缺乏解决现实世界时空推理任务的工具集成方案/Lack of TIR solutions for real-world spatio-temporal reasoning]
        C --> C1[STAgent: 专用于时空理解的智能体模型/STAgent: Agentic LLM for spatio-temporal understanding]
        C --> C2[稳定工具环境/Stable Tool Environment]
        C --> C3[分层数据管理框架/Hierarchical Data Curation Framework]
        C --> C4[级联训练方案/Cascaded Training Recipe]
        D --> D1[在TravelBench上表现优异/Promising performance on TravelBench]
        D --> D2[保持了广泛的通用能力/Maintains general capabilities across benchmarks]
    ```

- **[arXiv260101] HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films**
  - **tags:** [cv], [video restoration], [diffusion model, film restoration, high-resolution video, patch-wise training, global-local frequency module]
  - **authors:** Rongji Xun, Junjie Yuan, Zhongjie Wang
  - **institution:** Tongji University, Shanghai Film Restoration Laboratory
  - **link:** https://arxiv.org/pdf/2512.24946
  - **code:** https://anonymous.4open.science/r/HaineiFRDM
  - **contributions:** 1. Proposed HaineiFRDM, a film restoration framework leveraging diffusion models for content understanding to restore indistinguishable film defects. 2. Introduced a patch-wise training/testing strategy with a position-aware Global Prompt and Frame Fusion Module and a global-local frequency module to enable high-resolution restoration on a single 24GB GPU and ensure texture consistency. 3. Constructed a new film restoration dataset containing restored real-degraded films and realistic synthetic data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49ddd01a0523d32c1f20822a4a1d270659f5454212011a90e04a39fe796d1ab_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes HaineiFRDM, a diffusion model-based framework for restoring high-resolution, real-world films. It addresses limitations of existing open-source methods by using a patch-wise strategy and novel modules to handle high-resolution videos efficiently and introduces a new dataset. Experiments show the model outperforms existing open-source methods in defect restoration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HaineiFRDM: Explore Diffusion to Restore High-resolution Real-World Films] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[开源方法性能有限/Open-source methods have limited performance]
        B --> B2[高分辨率电影未探索/High-resolution films unexplored]
        C --> C1[基于扩散模型的修复框架/Diffusion-based restoration framework]
        C --> C2[分块训练与测试策略/Patch-wise training & testing]
        C --> C3[全局-局部频率模块/Global-local frequency module]
        C --> C4[构建新数据集/Construct new dataset]
        D --> D1[缺陷修复能力优越/Superior defect restoration ability]
        D --> D2[代码与数据集将开源/Code & dataset to be released]
    ```

- **[arXiv260101] ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands**
  - **tags:** [ai], [human-computer interaction], [flow-based generative model, GUI automation, continuous trajectory prediction, unified discrete-continuous actions, ScreenDrag benchmark]
  - **authors:** Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou
  - **institution:** Show Lab, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24965
  - **code:** https://github.com/showlab/showui-pi
  - **contributions:** 1. Proposed ShowUI-π, the first flow-based generative model for GUI dexterous manipulation, unifying discrete clicks and continuous drags in a shared model. 2. Introduced a flow-based action generation method for drag modeling, predicting incremental cursor adjustments from continuous visual observations. 3. Created ScreenDrag, a benchmark with 20K drag trajectories across five domains and comprehensive evaluation protocols to assess GUI agents' drag capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6391f609bd9b67bc717f5e3756501bf8f4dedd5a207352ff5b4f02bc902207_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing GUI agents that only perform discrete clicks, lacking the ability for continuous, closed-loop drag interactions. The authors propose ShowUI-π, a flow-based generative model that unifies discrete and continuous actions and generates smooth drag trajectories from visual observations. Experiments show ShowUI-π outperforms proprietary GUI agents on the new ScreenDrag benchmark, demonstrating effective dexterous control for GUI automation.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands] --> B[核心问题/Problem: Existing GUI agents only support discrete clicks, lacking continuous drag capability for closed-loop trajectories]
    A --> C[主要方法/Method: Flow-based generative model with unified discrete-continuous actions and incremental trajectory prediction]
    A --> D[关键结果/Results: Outperforms proprietary agents on ScreenDrag benchmark (score 26.98), demonstrating effective dexterous control]
    ```

- **[arXiv260101] Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantization, pruning, weight clustering, robustness, multiobjective assessment]
  - **authors:** Itallo Patrick Castro Alves Da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza, Baldoino Fonseca dos Santos Neto, Marcio de Medeiros Ribeiro
  - **institution:** Federal University of Alagoas, Federal University of Rio Grande do Norte
  - **link:** https://arxiv.org/pdf/2512.24971
  - **contributions:** 1. Conducted a comprehensive evaluation of individual and combined compression techniques (quantization, pruning, weight clustering) on CNNs for robustness under natural corruptions. 2. Demonstrated that certain compression strategies can preserve or even improve model robustness, especially on complex architectures. 3. Utilized multiobjective assessment to identify optimal compression configurations that balance robustness, accuracy, and compression ratio for real-world deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8640f8930863d5573a7df0bb7287b8800d92c67a60523b6b0dab2eb044c58bb_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how compression techniques like quantization, pruning, and weight clustering affect the robustness of CNNs (ResNet-50, VGG-19, MobileNetV2) against natural image corruptions on CIFAR-10-C/100-C. It finds that specific compression strategies, particularly when combined, can maintain or enhance robustness, with multiobjective analysis revealing the best trade-offs for efficient and robust model deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Evaluating the Impact of Compression Techniques on CNNs under Natural Corruptions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[压缩模型在自然损坏下的鲁棒性/Robustness of compressed models under natural corruptions]
        C --> C1[评估量化、剪枝、权重聚类技术/Evaluate quantization, pruning, weight clustering]
        C --> C2[使用CIFAR-10/100-C数据集/Use CIFAR-10/100-C datasets]
        C --> C3[多目标评估/Multiobjective assessment]
        D --> D1[特定策略保持或提升鲁棒性/Certain strategies preserve or improve robustness]
        D --> D2[定制组合产生有益结果/Customized combinations yield beneficial results]
    ```

- **[arXiv260101] DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments**
  - **tags:** [cv], [embodied vision-language reasoning], [low-light vision, embodied question answering, vision-language models, image enhancement, benchmark]
  - **authors:** Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)
  - **link:** https://arxiv.org/pdf/2512.24985
  - **contributions:** 1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --> B[核心问题/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]
        A --> C[主要方法/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]
        A --> D[关键结果/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]
    ```

- **[arXiv260101] Classifying long legal documents using short random chunks**
  - **tags:** [nlp], [document classification], [DeBERTa V3, LSTM, random chunks, Temporal, long document processing]
  - **authors:** Luis Adrián Cabrera-Diego
  - **institution:** Jus Mundi
  - **link:** https://arxiv.org/pdf/2512.24997
  - **contributions:** 1. A novel legal document classifier architecture combining DeBERTa V3 with an LSTM that processes only 48 randomly selected short chunks (max 128 tokens) per document, enabling efficient handling of long texts. 2. A robust deployment pipeline built using Temporal, a durable execution framework, ensuring reliable and fault-tolerant processing workflows. 3. Demonstrated effective performance on a multilingual legal document dataset with a weighted F-score of 0.898 and quantified processing efficiency (498 seconds per 100 files on CPU).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92192ce5f7c11c2d86da5e296a17a8e7ae1b0794ecb76a29cb686c4b5b4f5f12_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of classifying long legal documents by proposing a model that uses DeBERTa V3 and an LSTM to process only 48 randomly selected short text chunks per document. The method avoids the computational expense of processing full documents with Transformers and is deployed via a reliable Temporal-based pipeline. The system achieves a weighted F-score of 0.898 and processes 100 files in a median time of 498 seconds on CPU.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Classifying long legal documents using short random chunks] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Long legal documents are expensive/slow to process with full Transformers]
        C[主要方法/Method<br>Classifier: DeBERTa V3 + LSTM on 48 random short chunks (128 tokens max)]
        D[关键结果/Results<br>Weighted F-score: 0.898, Median time: 498s per 100 files (CPU)]
    ```

- **[arXiv260101] A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts**
  - **tags:** [other], [modal logic], [weighted modal logic, possibilistic reasoning, formal concept analysis, fuzzy formal contexts, rough set theory]
  - **authors:** Prosenjit Howlader, Churn-Jung Liau
  - **institution:** Institute of Information Science, Academia Sinica
  - **link:** https://arxiv.org/pdf/2512.24980
  - **contributions:** 1. Introduces a novel two-sort weighted modal logic with necessity and sufficiency operators for possibilistic reasoning in fuzzy formal contexts. 2. Provides a sound and complete axiomatization for the logic and its fragments with respect to fuzzy context models. 3. Shows the logic can represent generalized formal, object-oriented, and property-oriented concepts in fuzzy FCA and can be extended to multi-relational contexts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0eedb37864559f4df3443ff6f46099ca8b0528be3387a300ecd0f5068e2a25f9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new two-sort weighted modal logic designed for possibilistic reasoning with fuzzy formal contexts, featuring necessity and sufficiency operators. It provides a sound and complete axiomatization for this logic and demonstrates its expressive power by showing it can represent key generalized concepts from Formal Concept Analysis (FCA) in the fuzzy setting. The work also indicates the logic's potential for extension to reasoning with multi-relational fuzzy contexts.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[论文标题: A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Reasoning with uncertainty in fuzzy formal contexts] --> P1[模糊形式背景中的可能性推理/Possibilistic reasoning in fuzzy formal contexts]
        Method[主要方法/Method: A two-sort weighted modal logic] --> M1[引入两种加权模态算子/Introduces two weighted modal operators]
        M1 --> M1a[必要性算子/Necessity (□)]
        M1 --> M1b[充分性算子/Sufficiency (⊟)]
        Results[关键结果/Results] --> R1[逻辑是可靠且完备的/Logic is sound and complete]
        Results --> R2[可表示FCA中的广义概念/Can represent generalized FCA concepts]
        Results --> R3[可扩展至多关系模糊背景/Extensible to multi-relational contexts]
    ```

- **[arXiv260101] Modeling Language as a Sequence of Thoughts**
  - **tags:** [nlp], [language modeling], [recurrent transformer, thought gestalt, reversal curse, cross-attention, scaling efficiency]
  - **authors:** Nasim Borazjanizadeh, James McClelland
  - **institution:** Independent Researcher, Stanford University
  - **link:** https://arxiv.org/pdf/2512.25026
  - **contributions:** 1. Introduced the Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels (tokens and sentence-level "thought" states). 2. Proposed a unified training scheme where token and sentence representations are generated with the same parameters and a single next-token objective, enabling gradient flow through memory. 3. Demonstrated improved data and parameter efficiency over GPT-2 and better performance on relational direction generalization (e.g., reversal curse).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8e6d868bbc8b6b68328f1680dd184805a29e94d0e7b58137122aad5c0a6e9d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of standard Transformers, which rely on surface-level token statistics and lack globally consistent representations. It proposes the Thought Gestalt model, a recurrent Transformer that generates tokens while cross-attending to a memory of prior sentence-level "thought" states, trained with a unified next-token objective. The model shows improved scaling efficiency and reduces errors on relational generalization tasks like the reversal curse.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Modeling Language as a Sequence of Thoughts") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Transformer 依赖表层统计/Transformers rely on surface-level statistics")
        Problem --> P2("缺乏全局一致表示/Lack globally consistent representations")
        Problem --> P3("导致逆转诅咒等问题/Leads to issues like reversal curse")
        Method --> M1("提出思想完形模型/Propose Thought Gestalt (TG) model")
        Method --> M2("双层建模: Token + 句子级思想/Two-level modeling: tokens & sentence-level thoughts")
        Method --> M3("循环Transformer + 跨注意力记忆/Recurrent Transformer with cross-attention memory")
        Results --> R1("比GPT-2更高效/More efficient than GPT-2")
        Results --> R2("减少逆转诅咒错误/Reduces reversal curse errors")
        Results --> R3("统一参数与目标训练/Unified parameter & objective training")
    ```

- **[arXiv260101] Generative Classifiers Avoid Shortcut Solutions**
  - **tags:** [ai], [generative models], [generative classifiers, spurious correlations, distribution shift, diffusion models, autoregressive models]
  - **authors:** Alexander C. Li, Ananya Kumar, Deepak Pathak
  - **institution:** Carnegie Mellon University, Stanford University
  - **link:** https://arxiv.org/pdf/2512.25034
  - **code:** https://github.com/alexlioralexli/generative-classifiers
  - **contributions:** 1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]
        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]
        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]
        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]
        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]
    ```

- **[arXiv260101] Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings**
  - **tags:** [mlsys], [agent system], [Large Language Model, Building Energy Management System, AI Agents, Human-Building Interaction, Context-aware]
  - **authors:** Tianzhi He, Farrokh Jazizadeh
  - **institution:** The University of Texas at San Antonio, Virginia Polytechnic Institute and State University
  - **link:** https://arxiv.org/pdf/2512.25055
  - **contributions:** 1. Proposes a conceptual framework for LLM-based AI agents in BEMS, featuring a closed-loop system with perception, central control, and action modules. 2. Develops and benchmarks a prototype using real-world datasets and diverse metrics (latency, functionality, accuracy, cost-effectiveness), formalizing the assessment of such agents. 3. Demonstrates the framework's performance and generalizability, identifying strengths (e.g., high accuracy in device control) and areas for improvement (e.g., complex cost estimation).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d46efc47f789043b6987c8068e21aecb4ec53b645c2c1a61c1880ed06029103b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a framework for LLM-based AI agents to manage energy in smart buildings through natural language. The agent uses a closed-loop system to analyze data and control devices, and its evaluation shows promising accuracy in tasks like device control but highlights challenges in complex cost estimation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings"] --> Problem["核心问题/Problem: Existing BEMS lack context-aware, natural language interaction for energy management"]
        Root --> Method["主要方法/Method: Proposes a three-module LLM-based AI agent framework (perception, central control, action) for closed-loop management"]
        Root --> Results["关键结果/Results: Prototype shows high accuracy in device control (86%) and memory tasks (97%), but lower accuracy in cost estimation (49%)"]
    ```

- **[arXiv260101] AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [redundancy-aware selection, token-budgeted RAG, greedy selection, submodular optimization, adaptive calibration]
  - **authors:** Chao Peng, Bin Wang, Zhilei Long, Jinfang Sheng
  - **institution:** Central South University, Yizhi Intelligent (YZInt)
  - **link:** https://arxiv.org/pdf/2512.25052
  - **contributions:** 1. Proposes AdaGReS, a redundancy-aware context selection framework that optimizes a set-level objective combining query relevance and intra-set redundancy penalties under a token-budget constraint. 2. Introduces a closed-form, instance-adaptive calibration method for the relevance-redundancy trade-off parameter, eliminating manual tuning and adapting to candidate-pool statistics and budget limits. 3. Provides a theoretical analysis showing the proposed objective exhibits ε-approximate submodularity, yielding near-optimality guarantees for the greedy selection algorithm.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85c85942576679b5e5fe4c0066c0977620d02d122c659a34dfe900bfed59c445_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of redundant context in token-budgeted RAG systems, which wastes budget and degrades generation quality. It proposes AdaGReS, an adaptive greedy selection framework that scores and selects chunks by balancing relevance and redundancy, with a theoretically-backed near-optimal guarantee. Experiments on QA and biomedical datasets show it improves redundancy control and end-to-end answer quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AdaGReS: Adaptive Greedy Context Selection] --> B[核心问题/Problem: Top-k检索返回冗余块，浪费token预算并降低生成质量/Top-k retrieval returns redundant chunks, wasting token budget and degrading generation]
        A --> C[主要方法/Method: 冗余感知的贪婪选择框架，结合相关性得分与冗余惩罚，并进行自适应参数校准/Redundancy-aware greedy selection framework with relevance-redundancy trade-off and adaptive calibration]
        A --> D[关键结果/Results: 在开放域QA和生物医学语料上，冗余控制和上下文质量得到改善，提升了端到端答案质量/Improved redundancy control and context quality on open-domain QA and biomedical corpus, leading to better end-to-end answer quality]
    ```

- **[arXiv260101] Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search**
  - **tags:** [mlsys], [memory & caching], [heuristic synthesis, evolutionary search, instance-optimal, LLM code generation, cache eviction]
  - **authors:** Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.25065
  - **contributions:** 1. Proposes Vulcan, a framework that recasts heuristic design as an automated search problem using LLMs to synthesize instance-optimal heuristics tailored to specific deployment contexts. 2. Introduces LLM-friendly, task-agnostic interfaces that separate policy and mechanism, making the synthesis tractable and enabling even small LLMs to generate correct code. 3. Demonstrates the framework's effectiveness by synthesizing heuristics for cache eviction and memory tiering that outperform state-of-the-art human-designed algorithms.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8daf2ac9cf0548fb6b41e5cb643f8b78cc3001bcb2d8b95b8ade5ced5201e53a_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Vulcan, a framework that uses LLM-driven evolutionary search to automatically synthesize instance-optimal system heuristics, tailored to specific workloads and hardware. It introduces task-agnostic interfaces to separate policy from mechanism, enabling efficient code generation. The synthesized heuristics for cache eviction and memory tiering were shown to outperform existing state-of-the-art algorithms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Manual heuristic design is slow and cannot adapt to changing hardware and workloads.]
        Method[主要方法/Method: Use LLM-driven evolutionary search over task-agnostic interfaces to synthesize instance-optimal heuristics.]
        Results[关键结果/Results: Synthesized heuristics outperform state-of-the-art algorithms in cache eviction and memory tiering.]
    ```

- **[arXiv260101] Coordinated Humanoid Manipulation with Choice Policies**
  - **tags:** [ai], [imitation learning], [humanoid robot, teleoperation, choice policy, multimodal behavior, whole-body coordination]
  - **authors:** Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik
  - **institution:** UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.25072
  - **code:** https://choice-policy.github.io
  - **contributions:** 1. A modular teleoperation interface that decomposes humanoid control into intuitive submodules (e.g., hand-eye coordination, locomotion) for efficient, high-quality data collection. 2. The Choice Policy, a novel imitation learning architecture that generates multiple candidate actions and learns to score them, enabling fast inference and effective modeling of multimodal behaviors. 3. Empirical validation on real-world tasks (dishwasher loading, whiteboard wiping) showing superior performance over diffusion policies and behavior cloning, and highlighting the critical role of hand-eye coordination.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of achieving robust whole-body coordination for humanoid robots in unstructured environments. It proposes a system combining a modular teleoperation interface for data collection with a novel "Choice Policy" for imitation learning, which scores multiple candidate actions. Experiments on real-world tasks demonstrate that this approach outperforms baseline methods and that hand-eye coordination is crucial for success in long-horizon manipulation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Coordinated Humanoid Manipulation with Choice Policies] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["实现人形机器人头、手、腿的鲁棒全身协调/Robust whole-body coordination for humanoids"]
        C --> C1["模块化遥操作接口/Modular teleoperation interface"]
        C --> C2["选择策略：生成并评估候选动作/Choice Policy: generate & score candidate actions"]
        D --> D1["在洗碗机装载、白板擦拭任务上超越基线/Outperforms baselines on dishwasher loading & whiteboard wiping"]
        D --> D2["手眼协调对长时域任务至关重要/Hand-eye coordination is critical for long-horizon tasks"]
    ```

- **[arXiv260101] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time**
  - **tags:** [cv], [video generation], [video diffusion model, space-time disentanglement, temporal-warping training, camera-conditioning, generative rendering]
  - **authors:** Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang
  - **institution:** University of Cambridge, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.25075
  - **code:** https://github.com/zheninghuang/Space-Time-Pilot
  - **contributions:** 1. Introduced an animation time-embedding mechanism for explicit motion sequence control in video diffusion. 2. Proposed a temporal-warping training scheme to repurpose multi-view datasets for learning temporal variations. 3. Created the CamxTime synthetic dataset and an improved camera-conditioning mechanism for precise dual space-time control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SpaceTimePilot, a video diffusion model that independently controls camera viewpoint and motion sequence to re-render dynamic scenes from a monocular video. The method uses a novel time-embedding mechanism and a temporal-warping training strategy to achieve robust space-time disentanglement. Experiments show the model enables continuous exploration across space and time, outperforming prior work.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time] --> B[核心问题/Problem: 如何从单目视频中解耦空间和时间以进行可控生成渲染/How to disentangle space and time from a monocular video for controllable generative rendering]
        A --> C[主要方法/Method: 引入动画时间嵌入机制和时域扭曲训练方案/Introduce animation time-embedding and temporal-warping training scheme]
        A --> D[关键结果/Results: 实现鲁棒的时空解耦与连续可控渲染/Achieve robust space-time disentanglement and continuous controllable rendering]
    ```

- **[arXiv260101] q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI using Physics-Informed Diffusion Models**
  - **tags:** [cv], [medical imaging], [diffusion models, quantitative MRI, data consistency, physics-informed, multi-parametric mapping]
  - **authors:** Shishuai Wang, Florian Wiesinger, Noemi Sgambelluri, Carolin Pirkl, Stefan Klein, Juan A. Hernandez-Tamames, Dirk H.J. Poot
  - **institution:** Erasmus MC (Erasmus University Medical Center)
  - **link:** https://arxiv.org/pdf/2512.23726
  - **contributions:** 1. Proposes a diffusion model-based method (q3-MuPa) for quantitative MRI mapping that combines a deep generative model with a physics-based data consistency constraint., 2. Enables high-quality mapping from a fourfold-accelerated, nearly silent MRI scan (MuPa-ZTE), reducing acquisition time to ~1 minute., 3. Demonstrates successful training on synthetic data from digital phantoms alone, with strong generalization to real patient and phantom scans.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e081389f251cbbacaf7c704cd1a34c3a032b2173e63192833db976abd6541d5e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes q3-MuPa, a method that uses a physics-informed diffusion model to generate high-quality quantitative MRI maps (T1, T2, proton density) from accelerated, silent scans. The method integrates a denoising diffusion model with the MRI signal physics as a constraint during inference. It achieves accurate mapping from 1-minute scans and generalizes well to real data despite being trained only on synthetic phantoms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI] --> B(核心问题/Problem: Need for fast, quiet, and accurate quantitative MRI mapping)
        A --> C(主要方法/Method: Physics-informed diffusion model with data consistency)
        A --> D(关键结果/Results: High-accuracy maps from 1-min scans, trained on synthetic data)
    ```

- **[arXiv260101] Leveraging Machine Learning for Early Detection of Lung Diseases**
  - **tags:** [cv], [medical image analysis], [deep learning, convolutional neural networks, chest x-ray, disease classification, VGG16]
  - **authors:** Bahareh Rahmani, Harsha Reddy Bindela, Rama Kanth Reddy Gosula, Krishna Yedubati, Mohammad Amir Salari, Leslie Hinyard, Payam Norouzzadeh, Eli Snir, Martin Schoen
  - **institution:** Saint Louis University, Washington University at Saint Louis
  - **link:** https://arxiv.org/pdf/2512.23757
  - **contributions:** 1. Proposes a diagnostic framework combining traditional image processing with advanced neural networks for lung disease detection. 2. Trains and validates multiple deep learning models (CNNs, VGG16, InceptionV3, EfficientNetB0) on chest x-rays for COVID-19, lung cancer, and pneumonia. 3. Demonstrates high accuracy, precision, recall, and F1 scores, highlighting the potential for real-world, non-invasive diagnostic applications in resource-limited settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5eaabf315a0348dff119282fff830baf854bc7edaf57b6601b94745f8aa312_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using deep learning models, including CNNs, VGG16, InceptionV3, and EfficientNetB0, to diagnose lung diseases like COVID-19, lung cancer, and pneumonia from chest x-rays. The method combines traditional image processing with neural networks to create a rapid, non-invasive diagnostic tool. The study concludes that these models achieve high performance metrics, showing reliability and potential for real-world healthcare applications, especially where radiologists are scarce.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Leveraging Machine Learning for Early Detection of Lung Diseases<br>利用机器学习进行肺部疾病早期检测] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Limited access to radiologists & resources<br>放射科医生和资源有限]
        B --> B2[Need for rapid, non-invasive diagnosis<br>需要快速、无创诊断]
        C --> C1[Combine image processing & neural networks<br>结合图像处理和神经网络]
        C --> C2[Train models (CNN, VGG16, etc.) on chest X-rays<br>在胸部X光片上训练模型]
        D --> D1[High accuracy, precision, recall, F1 scores<br>高准确率、精确率、召回率、F1分数]
        D --> D2[Potential for real-world diagnostic applications<br>具有实际诊断应用潜力]
    ```

- **[arXiv260101] Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware**
  - **tags:** [mlsys], [others], [quantum error mitigation, attention graph neural network, NISQ hardware, Burgers equation, zero-noise extrapolation]
  - **authors:** Seyed Mohamad Ali Tousi, Adib Bazgir, Yuwen Zhang, G. N. DeSouza
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.23817
  - **contributions:** 1. A hybrid quantum-classical framework for solving the viscous Burgers equation on NISQ hardware, using the Cole-Hopf transformation and Trotterized quantum circuits. 2. The creation of a large parametric dataset of noisy, ZNE-corrected, hardware, and classical solutions with circuit metadata for data-driven error mitigation. 3. A novel attention-based graph neural network model that ingests circuit features and noisy outputs to predict error-mitigated solutions, outperforming ZNE alone.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57bb32110f5a354755f846f1b5a7ab41ac3fc4a701e97b9b25486f2bab0c72eb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hybrid quantum-classical framework enhanced with a learned error mitigation model to solve the Burgers equation on noisy quantum hardware. The method uses an attention graph neural network trained on a dataset of noisy quantum simulations to predict corrected solutions. The results show the learned model consistently reduces errors beyond standard zero-noise extrapolation techniques.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[在含噪声量子硬件上求解Burgers方程/Solving Burgers Equation on Noisy Quantum Hardware]
        C --> C1[混合量子-经典框架与注意力图神经网络/Hybrid Quantum-Classical Framework with Attention GNN]
        D --> D1[学习模型超越ZNE，减少量子-经典解差异/Learned Model Outperforms ZNE, Reduces Quantum-Classical Discrepancy]
    ```

- **[arXiv260101] Autoregressive long-horizon prediction of plasma edge dynamics**
  - **tags:** [ai], [surrogate modeling], [transformer, autoregressive prediction, plasma edge dynamics, surrogate model, long-horizon training]
  - **authors:** Hunor Csala, Sebastian De Pascuale, Paul Laiu, Jeremy Lore, Jae-Sun Park, Pei Zhang
  - **institution:** Oak Ridge National Laboratory
  - **link:** https://arxiv.org/pdf/2512.23884
  - **contributions:** 1. Proposes a transformer-based, autoregressive surrogate model for fast, long-horizon prediction of 2D plasma edge state fields (electron temperature, density, radiated power). 2. Demonstrates that training with longer autoregressive horizons systematically improves model rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of time steps. 3. Shows the surrogate model is orders of magnitude faster than the high-fidelity SOLPS-ITER simulator, enabling rapid parameter exploration for fusion device design.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f71c9629a318094d2cde5d048b18c5b331d3a235097acfb1e70f8caf9d3c603_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of high-fidelity plasma edge simulations (SOLPS-ITER) by proposing a transformer-based autoregressive surrogate model. The model is trained on simulation data to predict key plasma state fields over long time horizons, and longer-horizon training is shown to improve prediction stability. The resulting surrogate is much faster than the original simulator, enabling rapid scenario exploration for fusion reactor design.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Autoregressive long-horizon prediction of plasma edge dynamics") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("高保真模拟计算成本高/High-fidelity simulation is computationally expensive")
        Method --> M1("基于Transformer的自回归代理模型/Transformer-based autoregressive surrogate model")
        Method --> M2("长时域训练/Long-horizon training")
        Results --> R1("预测稳定，误差累积减少/Stable prediction, reduced error accumulation")
        Results --> R2("速度比原模拟快数个数量级/Orders of magnitude faster than original simulator")
    ```

- **[arXiv260101] A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe**
  - **tags:** [cv], [remote sensing image analysis], [InSAR, Transformer, ground deformation forecasting, cross-site generalization, multimodal learning]
  - **authors:** Wendong Yao, Binhua Huang, Soumyabrata Dev
  - **institution:** ADAPT SFI Research Centre, University College Dublin
  - **link:** https://arxiv.org/pdf/2512.23906
  - **contributions:** 1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[论文标题: A multimodal Transformer for InSAR-based ground deformation forecasting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 如何利用历史InSAR数据预测未来的地表形变?] --> P1[挑战/Challenges: 长期趋势、季节周期、突变事件的叠加]
        Method[主要方法/Method: 多模态Transformer] --> M1[输入/Inputs: 近期形变图、静态运动学指标、时间编码]
        Method --> M2[任务/Task: 单步、固定间隔的下一时期临近预报]
        Results[关键结果/Results] --> R1[性能/Performance: RMSE=0.90mm, R²=0.97 (爱尔兰测试集)]
        Results --> R2[泛化/Generalization: 跨欧洲5个未见区域，R²≥0.93]
    ```

- **[arXiv260101] Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission**
  - **tags:** [mlsys], [communication & networking], [generative video compression, task-oriented communication, AI Flow, compression-computation trade-off, Level C Shannon-Weaver]
  - **authors:** Xiangyu Chen, Jixiang Luo, Jingyu Xu, Fangqiu Yi, Chi Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.24300
  - **contributions:** 1. Proposes Generative Video Compression (GVC), a novel framework leveraging generative video models to achieve extreme compression rates as low as 0.02%, 2. Introduces a paradigm shift by trading computation for bandwidth, shifting the reconstruction burden to the receiver using generative priors, 3. Presents a practical compression-computation trade-off strategy for fast inference on consumer-grade GPUs, enabling deployment within the AI Flow framework for constrained environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0223eada26a73d6b029f6d32188e4919884b4dea1b6854eeb80d3b61f3d6ac_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Generative Video Compression (GVC), a framework that uses generative video models to achieve extreme compression rates by transmitting compact representations and reconstructing video at the receiver. It shifts the computational burden from transmission to inference and proposes a practical trade-off strategy for deployment. The work demonstrates a viable path for efficient video communication in bandwidth-constrained scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("能否实现0.01%极端压缩率?/Achieve 0.01% extreme compression rate?")
        Problem --> P2("如何权衡计算与压缩?/Trade computation for compression?")
        Problem --> P3("是否实用可部署?/Practical and deployable?")
        Method --> M1("生成式视频压缩框架/GVC Framework")
        Method --> M2("利用生成先验重建/Use generative priors for reconstruction")
        Method --> M3("压缩-计算权衡策略/Compression-computation trade-off")
        Results --> R1("实现~0.02%压缩率/Achieved ~0.02% compression rate")
        Results --> R2("为AI Flow框架赋能/Enables AI Flow framework")
        Results --> R3("开辟高效视频通信新范式/Opens new practical video communication paradigm")
    ```

- **[arXiv260101] Automated Classification of First-Trimester Fetal Heart Views Using Ultrasound-Specific Self-Supervised Learning**
  - **tags:** [cv], [medical image classification], [self-supervised learning, masked autoencoder, vision transformer, ultrasound foundation model, fetal echocardiography]
  - **authors:** Youssef Megahed, Aylin Erman, Robin Ducharme, Mark C. Walker, Steven Hawken, Adrian D. C. Chan
  - **institution:** Carleton University, University of Ottawa, Ottawa Hospital Research Institute
  - **link:** https://arxiv.org/pdf/2512.24492
  - **contributions:** 1. Evaluated a self-supervised ultrasound foundation model (USF-MAE) for the challenging task of first-trimester fetal heart view classification. 2. Demonstrated that ultrasound-specific pretraining on a large, unlabeled dataset yields more transferable representations than models pretrained on natural images (ImageNet) or standard supervised CNNs. 3. Showed robust performance without the need for aggressive image preprocessing or region-of-interest cropping, and improved discrimination of non-diagnostic frames.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f73d9d90b381ff2a6f244e15b64b3e24d9b28e8c2fbe78fa50c79ef8f58d1c66_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated first-trimester fetal heart view classification in ultrasound by fine-tuning a self-supervised ultrasound foundation model (USF-MAE). The model, pretrained on over 370,000 unlabeled ultrasound images, outperformed supervised CNN baselines and a natural image-pretrained Vision Transformer, achieving over 90% accuracy. The results indicate that ultrasound-specific self-supervised pretraining enables more generalizable representations for early fetal cardiac imaging.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated Classification of First-Trimester Fetal Heart Views<br>早孕期胎儿心脏视图自动分类] --> B
        A --> C
        A --> D
        B[Problem: Early detection of congenital heart disease is challenging<br>核心问题: 先天性心脏病早期检测困难]
        C[Method: Fine-tune USF-MAE, an ultrasound-specific self-supervised model<br>主要方法: 微调超声专用自监督模型USF-MAE]
        D[Results: Achieved SOTA 90.57% accuracy, outperforming baselines<br>关键结果: 达到90.57%准确率，超越基线模型]
    ```

- **[arXiv260101] Generative AI-enhanced Sector-based Investment Portfolio Construction**
  - **tags:** [ai], [quantitative finance], [portfolio optimization, large language models, sector-based investment, Sharpe ratio, regime shift]
  - **authors:** Alina Voronina, Oleksandr Romanko, Ruiwen Cao, Roy H. Kwon, Rafael Mendoza-Arriaga
  - **institution:** Ukrainian Catholic University, SS&C Algorithmics, University of Toronto, Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.24526
  - **contributions:** 1. Conducts a multi-model, cross-provider evaluation of LLMs (OpenAI, Google, Anthropic, DeepSeek, xAI) for stock selection in quantitative portfolio construction. 2. Demonstrates a strong temporal dependence in LLM portfolio performance, showing outperformance in stable markets but underperformance in volatile periods. 3. Proposes and validates a hybrid framework that improves performance and consistency by combining LLM-based stock selection with classical portfolio optimization techniques.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76999779560abd5e04b9d83f5cf53223a1c444ffacf2142f6df9f0e236e87dd0_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates using LLMs from multiple providers to select and weight stocks for sector-based portfolios. The study finds that while LLM-weighted portfolios can outperform sector indices in stable markets, they struggle during volatile periods; however, combining LLM selection with traditional optimization improves outcomes. The results highlight the potential and current limitations of LLMs in investment management, advocating for hybrid AI-quantitative frameworks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative AI-enhanced Sector-based Investment Portfolio Construction] --> B
        A --> C
        A --> D
        B[核心问题/Problem: How can LLMs be applied to quantitative sector-based portfolio construction?]
        C[主要方法/Method: Prompt LLMs to select/weight stocks, combine with classical portfolio optimization, evaluate across stable and volatile periods.]
        D[关键结果/Results: LLM performance is market-dependent; hybrid frameworks (LLM + optimization) improve performance and consistency.]
    ```

- **[arXiv260101] An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction**
  - **tags:** [cv], [medical image reconstruction], [disentangled representation, latent diffusion model, self-supervised learning, multidimensional MRI, zero-shot adaptation]
  - **authors:** Ruiyang Zhao, Fan Lam
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.24674
  - **contributions:** 1. A novel learned feature-based image representation that disentangles features like geometry and contrast into distinct latent spaces. 2. The integration of a latent diffusion model to impose stronger constraints on the disentangled feature spaces. 3. New reconstruction formulations and algorithms that combine the learned representation with zero-shot self-supervised learning adaptation and subspace modeling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f858a20b731ce7ed66cd4e854bc6e6bdd5ae01548b1060d4814f643684ce73a0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new method for reconstructing multidimensional MRI data by learning a disentangled image representation that separates features like geometry and contrast into distinct latent spaces, enhanced by a latent diffusion model. The approach integrates this representation with zero-shot self-supervised learning, enabling improved reconstruction without task-specific training. It demonstrates superior performance on accelerated T1 and T2 parameter mapping compared to state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction"] --> Problem["核心问题/Problem: Limited data for task-specific training in multidimensional MRI reconstruction"]
        Root --> Method["主要方法/Method: Disentangled representation + Latent diffusion model + Zero-shot self-supervised adaptation"]
        Root --> Results["关键结果/Results: Improved performance on T1/T2 mapping without task-specific training"]
    ```

- **[arXiv260101] AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement**
  - **tags:** [mlsys], [agent system], [multi-agent framework, automated peer review, LLM-driven, reasoning traces, iterative refinement]
  - **authors:** Yutong Wang, Yunxiang Xiao, Yonglin Tian, Junyong Li, Jing Wang, Yisheng Lv
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.24754
  - **contributions:** 1. An open-source, multi-agent framework (AstroReview) that automates telescope proposal review in three structured stages (novelty/merit, feasibility/yield, meta-review/verification). 2. A design employing task isolation and explicit reasoning traces to curb LLM hallucinations and improve review transparency and auditability. 3. An "AstroReview in Action" module that demonstrates an iterative review-refinement loop, increasing proposal acceptance rates by 66% after two revision cycles.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74800e2a96855acce45d842bc6dfc7d6ea4c32d6def9f45717a69483ec603078_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AstroReview, an LLM-driven multi-agent framework designed to automate and scale the peer review process for astronomical telescope proposals. The system operates in three stages to assess scientific merit, feasibility, and review reliability, using task isolation and reasoning traces to improve transparency. The results show it can identify accepted proposals with 87% accuracy and, through an iterative feedback loop, significantly improve the quality and acceptance rate of revised proposals.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement] --> B[核心问题/Problem: 望远镜提案评审成为瓶颈/Proposal review is a bottleneck]
        A --> C[主要方法/Method: 三阶段多智能体框架/Three-stage multi-agent framework]
        A --> D[关键结果/Results: 87%准确率，接受率提升66%/87% accuracy, 66% acceptance rate increase]
        B --> B1[提案量超过可用时间/Proposal volume > telescope time]
        C --> C1[新颖性与科学价值/Novelty & scientific merit]
        C --> C2[可行性与预期产出/Feasibility & expected yield]
        C --> C3[元评审与可靠性验证/Meta-review & reliability verification]
        D --> D1[正确识别已接受提案/Correctly identifies accepted proposals]
        D --> D2[迭代反馈提升提案质量/Iterative feedback improves proposal quality]
    ```

- **[arXiv260101] The Impact of LLMs on Online News Consumption and Production**
  - **tags:** [ai], [ai economics], [staggered difference-in-differences, synthetic difference-in-differences, robots.txt]
  - **authors:** Hangcheng Zhao, Ron Berman
  - **institution:** Rutgers Business School, The Wharton School of the University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.24968
  - **contributions:** 1. Quantified a moderate decline in news publisher website traffic following the rise of generative AI. 2. Demonstrated that blocking GenAI bots via robots.txt can paradoxically reduce total and real consumer traffic for large publishers. 3. Provided empirical evidence that, contrary to predictions, LLMs have not yet reduced editorial hiring and have shifted publisher content strategy towards rich media and advertising.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7836df9e705d8a023a351c30d3595b1ff6e9614cf1d805218347987098aa3882_w640_q70.webp
  - **Simple LLM Summary:** This paper empirically investigates the impact of Large Language Models (LLMs) on online news publishers using high-frequency data and causal inference methods like difference-in-differences. It finds that blocking LLM crawlers reduces publisher traffic, LLMs have not yet replaced editorial jobs, and publishers are shifting to rich content and advertising tech. The results reveal unforeseen consequences of LLM adoption on the news ecosystem.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Impact of LLMs on Online News Consumption and Production] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs如何影响新闻生产与消费/How LLMs affect news production and consumption]
        C --> C1[高频数据与因果推断/High-frequency data & Causal inference]
        D --> D1[流量下降/Traffic decline]
        D --> D2[屏蔽爬虫反效果/Blocking bots backfires]
        D --> D3[编辑岗位未减少/Editorial jobs not reduced]
        D --> D4[内容转向富媒体/Shift to rich content]
    ```

- **[arXiv260101] SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets**
  - **tags:** [ai], [sequence learning], [formal language theory, symbolic sequences, benchmark suite, cognitive modeling, sequence processing]
  - **authors:** Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte
  - **institution:** Jülich Research Centre, RWTH Aachen University, University of Groningen, University of Coimbra
  - **link:** https://arxiv.org/pdf/2512.24977
  - **contributions:** 1. Introduces SymSeq, a tool for the rigorous generation and analysis of structured symbolic sequences. 2. Introduces SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks for evaluating AI systems. 3. Provides a unified, domain-agnostic framework (SymSeqBench) based on Formal Language Theory to standardize experiments across cognitive science and AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SymSeqBench, a unified software framework combining a symbolic sequence generator/analyzer (SymSeq) and a benchmark suite (SeqBench) for evaluating sequence learning. It is based on Formal Language Theory to provide a domain-agnostic, formal link between computation and cognition. The main conclusion is that this modular, open-source tool offers a versatile and standardized way to investigate sequential structure across diverse fields like psycholinguistics, cognitive psychology, and AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SymSeqBench: 统一框架] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[评估序列学习/Evaluating Sequence Learning]
        Problem --> P2[领域无关的评估/Domain-Agnostic Evaluation]
        Problem --> P3[连接形式理论与认知/Linking Formal Theory & Cognition]
        Method --> M1[SymSeq: 生成与分析/SymSeq: Generation & Analysis]
        Method --> M2[SeqBench: 基准测试套件/SeqBench: Benchmark Suite]
        Method --> M3[基于形式语言理论/Based on Formal Language Theory]
        Results --> R1[跨领域多功能/Versatile Across Domains]
        Results --> R2[标准化实验/Standardizes Experiments]
        Results --> R3[模块化开源工具/Modular Open-Source Tool]
    ```
