---
slug: /daily/csai/20251229-20260104
---
# 20251229-20260104 (cs.AI)

## 2025-12-29

- **[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation**
  - **tags:** [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]
  - **authors:** Santhosh Kumar Ravindran
  - **institution:** Microsoft Corporation
  - **link:** https://arxiv.org/pdf/2512.21351
  - **contributions:** 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as "genomes" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp
  - **Simple LLM Summary:** CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM代码生成缺乏适应性，难以应对API变化/LLM code generation lacks adaptability to API changes]
        C --> C1[将RL轨迹视为基因组进行进化操作/Treat RL trajectories as genomes for evolutionary operations]
        C --> C2[在夜间回放阶段进行突变与选择/Mutation and selection during nocturnal replay]
        D --> D1[解决方案新颖性提升35%/35% higher novelty in solutions]
        D --> D2[适应速度加快25%/25% faster adaptation]
    ```

- **[arXiv251229] EcoNet: Multiagent Planning and Control Of Household Energy Resources Using Active Inference**
  - **tags:** [mlsys], [agent system], [active inference, multi-agent systems, home energy management systems (HEMS), distributed energy resources (DER), Bayesian inference]
  - **authors:** John C. Boik, Kobus Esterhuysen, Jacqueline B. Hynes, Axel Constant, Ines Hipolito, Mahault Albarracin, Alex B. Kiefer, Karl Friston
  - **institution:** VERSES, University of Sussex, Macquarie University, UCL (University College London)
  - **link:** https://arxiv.org/pdf/2512.21343
  - **contributions:** 1. Proposes EcoNet, a novel Bayesian framework for household and neighborhood energy management based on active inference. 2. Addresses the challenge of planning under uncertainty (e.g., weather, solar forecasts) while handling complex, conditional, and conflicting household goals. 3. Demonstrates the approach through simulations for multiagent planning and control of distributed energy resources.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b885d3c6c9f392a494063522c79cde9a59fead8ab6b04010259b6485f007cec8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces EcoNet, a multiagent planning and control system for household energy resources using active inference, a Bayesian approach, to manage uncertainty and conflicting goals. The method aims to optimize energy use, costs, and emissions while maintaining comfort. Simulation results demonstrate its potential for improved energy management and coordination.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EcoNet: 多智能体家庭能源规划与控制 / EcoNet: Multiagent Household Energy Planning & Control] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[复杂且冲突的家庭目标 / Complex & Conflicting Household Goals]
        B --> B2[决策存在不确定性 / Decision-making Under Uncertainty]
        C --> C1[基于主动推理的贝叶斯方法 / Active Inference-based Bayesian Approach]
        C --> C2[多智能体规划与控制 / Multiagent Planning & Control]
        D --> D1[模拟结果展示 / Simulation Results Presented]
        D --> D2[改善能源管理与协调 / Improved Energy Management & Coordination]
    ```

- **[arXiv251229] Multi-Agent LLM Committees for Autonomous Software Beta Testing**
  - **tags:** [se], [automated software testing], [multi-agent system, large language model, vision-language model, consensus voting, beta testing]
  - **authors:** Sumanth Bharadwaj Hachalli Karanam, Dhiwahar Adhithya Kennady
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2512.21352
  - **contributions:** 1. A novel multi-agent committee framework that uses a three-round voting protocol for consensus-based decision-making in software testing. 2. Integration of vision-enabled LLMs and diverse testing personas to systematically explore and understand web application user interfaces. 3. Demonstrated significant performance improvements over single-agent baselines in task success, bug detection (F1 score), and security vulnerability coverage on established benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost of manual software beta testing and the limitations of single-agent LLM approaches by proposing a multi-agent committee framework. The method employs diverse, vision-enabled LLMs that collaborate through a structured voting protocol and persona-driven behavior to autonomously test web applications. The results show that this multi-agent approach significantly outperforms single-agent baselines in task success rates, bug detection, and security testing coverage, making it suitable for real-time CI/CD integration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Agent LLM Committees for Autonomous Software Beta Testing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动测试成本高，单智能体LLM存在幻觉/Manual testing costly, single-agent LLM hallucinates]
        C --> C1[多智能体委员会与三轮投票协议/Multi-agent committee & three-round voting]
        C --> C2[视觉LLM与角色多样性/Vision LLMs & persona diversity]
        D --> D1[任务成功率89.5%，超越基线/Task success 89.5%, beats baseline]
        D --> D2[动作延迟0.71秒，适合CI/CD/Action latency 0.71s, suitable for CI/CD]
        D --> D3[覆盖8/10 OWASP漏洞类别/Covers 8/10 OWASP Top 10]
    ```

- **[arXiv251229] Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software**
  - **tags:** [se], [software fairness], [correlation tuning, phi-coefficient, multi-objective optimization, pre-processing, bias mitigation]
  - **authors:** Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang
  - **institution:** King’s College London, National University of Defense Technology, Southern University of Science and Technology, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.21348
  - **contributions:** 1. Proposes a novel pre-processing bias mitigation method called Correlation Tuning (CoT) that adjusts data correlations. 2. Introduces the Phi-coefficient as an intuitive measure to quantify correlation between sensitive attributes and labels. 3. Employs multi-objective optimization to address proxy biases, demonstrating superior effectiveness over state-of-the-art methods in single and multiple attribute scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Correlation Tuning (CoT), a novel pre-processing method to mitigate bias in ML software by adjusting data correlations using the Phi-coefficient and multi-objective optimization. It frames fairness as a core software quality issue. Extensive evaluation shows CoT significantly improves performance for unprivileged groups and reduces key bias metrics, outperforming existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统公平研究忽视软件质量维度/Traditional fairness research neglects software quality dimension]
        B --> B2[预处理方法效果不足/Pre-processing methods lack effectiveness]
        C --> C1[提出相关性调优 (CoT)/Propose Correlation Tuning (CoT)]
        C --> C2[使用Phi系数量化相关性/Use Phi-coefficient to quantify correlation]
        C --> C3[采用多目标优化/Employ multi-objective optimization]
        D --> D1[提高弱势群体TPR 17.5%/Increase unprivileged group TPR by 17.5%]
        D --> D2[关键偏差指标降低 >50%/Key bias metrics reduced by >50%]
        D --> D3[超越SOTA方法 3-10个百分点/Outperform SOTA by 3-10 percentage points]
    ```

- **[arXiv251229] Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks**
  - **tags:** [db], [text-to-SQL], [unanswerable question detection, few-shot prompting, biomedical databases]
  - **authors:** Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger
  - **institution:** Zurich University of Applied Sciences, University of Zurich
  - **link:** https://arxiv.org/pdf/2512.21345
  - **contributions:** 1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem") --> P1("Text-to-SQL对不可回答查询生成可执行SQL/Text-to-SQL generates executable SQL for unanswerable queries")
        P1 --> P2("生物医学领域风险高/High risk in biomedical contexts")
        Method("主要方法/Method") --> M1("Query Carefully 管道/Query Carefully pipeline")
        M1 --> M2("LLM (llama3.3:70b) + 模式感知提示 + 少样本/LLM (llama3.3:70b) + schema-aware prompts + few-shot")
        M2 --> M3("包含可回答与不可回答示例/Includes answerable and unanswerable examples")
        Results("关键结果/Results") --> R1("构建OncoMX-NAQ基准/Built OncoMX-NAQ benchmark")
        R1 --> R2("不可回答检测准确率0.8/Unanswerable-detection accuracy 0.8")
        R2 --> R3("结构性问题检测好，语义模糊挑战大/Good for structural, challenging for semantic ambiguity")
    ```

- **[arXiv251229] A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers**
  - **tags:** [ai], [reinforcement learning], [Relevance-Zone Based Search, AlphaZero, Life-and-Death problems, heuristic search, pattern table]
  - **authors:** Chung-Chin Shih, Ti-Rong Wu, Ting Han Wei, Yu-Shan Hsu, Hung Guei, I-Chen Wu
  - **institution:** Academia Sinica, National Yang Ming Chiao Tung University, Kochi University of Technology
  - **link:** https://arxiv.org/pdf/2512.21365
  - **code:** https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ
  - **contributions:** 1. Applied and analyzed Relevance-Zone Based Search (RZS) and relevance-zone pattern tables to solve Go Life-and-Death problems, identifying critical relevance-zones. 2. Discovered that solvers can find rare patterns and even alternative solutions differing from established human grandmaster answers. 3. Identified and analyzed key limitations of current solvers, such as misjudging rare patterns and prioritizing direct survival over territory maximization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6376caf4e3dced23991e86eb4d5b0f512ce3623019adeaf18ee253d5fd00c507_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the performance of state-of-the-art computer Go solvers using Relevance-Zone Based Search on classic Life-and-Death problems. The study finds that while these solvers can identify critical areas and discover rare patterns, they exhibit limitations like misjudging pattern values and having a non-human preference for direct survival over territory. The authors suggest future approaches to address these solver issues.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers<br/>使用基于相关区域求解器解决围棋死活问题的研究"] --> B["核心问题/Problem<br/>Analyzing solver behavior on Go Life-and-Death problems<br/>分析求解器在围棋死活问题上的行为"]
        A --> C["主要方法/Method<br/>Using Relevance-Zone Based Search (RZS) and pattern tables<br/>使用基于相关区域的搜索和模式表"]
        A --> D["关键结果/Results<br/>Identifies relevance-zones, finds rare/alternative solutions, reveals solver limitations<br/>识别相关区域，发现罕见/替代解法，揭示求解器局限"]
    ```

- **[arXiv251229] From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration**
  - **tags:** [ai], [computational psychology], [multimodal large language model, multi-agent collaboration, cosine similarity, projective assessment, psychological report generation]
  - **authors:** Shuide Wen, Yu Sun, Beier Ku, Zhi Gao, Lijun Ma, Yang Yang, Can Jiao
  - **institution:** Tsinghua University, Shenzhen University, University of Oxford, Guangzhou University of Chinese Medicine, Harbin Institute of Technology, Shenzhen Institute of Education Sciences
  - **link:** https://arxiv.org/pdf/2512.21360
  - **contributions:** 1. Proposed a novel multi-agent collaboration framework to automate the interpretation of House-Tree-Person drawings, decoupling visual feature recognition from psychological inference. 2. Demonstrated that multimodal large language models (MLLMs) can achieve expert-level baseline comprehension in interpreting projective drawings, with high semantic similarity to human expert interpretations. 3. Introduced a destigmatizing narrative and social-psychological perspective integration to correct visual hallucinations and enhance the ecological validity and coherence of automated psychological reports.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/681955eb18a63880e0327c5debb6e992188de12ca52cef0c9c34258f09c3a91d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an automated assessment framework for the House-Tree-Person drawing test using multimodal LLMs and multi-agent collaboration to address issues of subjective scoring and lack of standardization. The framework effectively interprets drawings with high similarity to expert analysis and generates coherent psychological reports. The results confirm the potential of multimodal models as standardized tools for projective psychological assessment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration<br>从视觉感知到深度共情：基于多模态大模型与多智能体协作的房树人绘画自动评估框架] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>HTP测试评分标准不一，依赖主观经验，缺乏统一量化系统] --> B1
        C[主要方法/Method<br>多模态大语言模型与多智能体协作框架] --> C1
        D[关键结果/Results<br>模型解释与专家解释语义相似度高，多智能体系统生成有效心理报告] --> D1
        B1[HTP test has heterogeneous scoring, relies on subjective experience, lacks unified quantitative coding]
        C1[Multimodal LLMs and multi-agent collaboration framework]
        D1[High semantic similarity to expert interpretations; multi-agent system produces reports with high ecological validity]
    ```

- **[arXiv251229] Reflection-Driven Control for Trustworthy Code Agents**
  - **tags:** [mlsys], [agent system], [reflection-driven control, secure code generation, trustworthy agents, reflective memory, safety control]
  - **authors:** Bin Wang, Jiazheng Quan, Xingrui Yu, Hansen Hu, Yuhao, Ivor Tsang
  - **institution:** Peking University, Xiamen University, Agency for Science, Technology and Research (A*STAR)
  - **link:** https://arxiv.org/pdf/2512.21354
  - **contributions:** 1. Introduces Reflection-Driven Control, a standardized and pluggable control module that integrates self-reflection as an explicit, internal step in an agent's reasoning process. 2. Instantiates the method for secure code generation, using a reflection loop to monitor decisions and retrieve repair examples/guidelines from an evolving reflective memory to inject constraints. 3. Empirically demonstrates that the approach substantially improves security and policy compliance of generated code while preserving functional correctness, with minimal overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of reliable safety controls in LLM agents by proposing Reflection-Driven Control, a module that makes self-reflection an explicit, continuous part of the agent's reasoning to monitor and constrain its decisions using evidence from a reflective memory. Evaluated on security-critical code generation tasks, the method significantly improves code security and compliance while maintaining functionality, offering a practical path toward trustworthy AI coding agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Reflection-Driven Control for Trustworthy Code Agents] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[LLM代理缺乏可靠的安全控制/LLM agents lack reliable safety controls]
        Problem --> P2[可能产生有害输出/Can produce harmful outputs]
        Method --> M1[将自我反思作为推理的显式步骤/Elevates self-reflection to an explicit reasoning step]
        Method --> M2[内部反思循环监控决策路径/Internal reflection loop monitors decision path]
        Method --> M3[从反思记忆中检索修复示例/Retrieves repair examples from reflective memory]
        Results --> R1[显著提高生成代码的安全性和合规性/Substantially improves security & policy compliance]
        Results --> R2[基本保持功能正确性/Largely preserves functional correctness]
        Results --> R3[运行时和token开销最小/Minimal runtime & token overhead]
    ```

- **[arXiv251229] AInsteinBench: Benchmarking Coding Agents on Scientific Repositories**
  - **tags:** [se], [software engineering], [benchmark, scientific computing, code generation, pull requests, test-driven verification]
  - **authors:** Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng
  - **institution:** ByteDance Seed, Princeton University
  - **link:** https://arxiv.org/pdf/2512.21373
  - **code:** https://github.com/ByteDance-Seed/AInsteinBench
  - **contributions:** 1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents' ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI's role in computational scientific research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AInsteinBench: Benchmarking Coding Agents on Scientific Repositories] --> B[核心问题/Problem: Can LLM agents operate as scientific computing development agents?]
        A --> C[主要方法/Method: End-to-end evaluation using tasks from real scientific pull requests]
        A --> D[关键结果/Results: Measures ability beyond surface-level code generation]
    ```

- **[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks**
  - **tags:** [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]
  - **authors:** Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li
  - **institution:** University of Electronic Science and Technology of China, North China University of Technology
  - **link:** https://arxiv.org/pdf/2512.21375
  - **contributions:** 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --> B
    A --> C
    A --> D
    B[核心问题/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]
    C[主要方法/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]
    D[关键结果/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]
    ```

- **[arXiv251229] LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors**
  - **tags:** [sec], [adversarial attacks], [adversarial attack, large language model, retrieval-augmented generation, Android malware detection, adversarial training]
  - **authors:** Tianwei Lan, Farid Naït-Abdesselam
  - **institution:** Université Paris Cité
  - **link:** https://arxiv.org/pdf/2512.21404
  - **contributions:** 1. Proposes LAMLAD, a novel adversarial attack framework that uses a dual-agent LLM architecture (manipulator and analyzer) to generate feature-level perturbations for evading Android malware detectors., 2. Integrates Retrieval-Augmented Generation (RAG) into the LLM pipeline to improve the efficiency and contextual awareness of the attack., 3. Proposes and evaluates an adversarial training-based defense strategy to enhance model robustness against the proposed LAMLAD-style attacks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6061210b194ba5cf79f70b8959faa3abe6b3e91ffad512d9cfd319de948593bb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LAMLAD, a novel adversarial attack framework that leverages the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based Android malware classifiers. The method uses a dual-agent LLM architecture with RAG to generate realistic, functionality-preserving feature perturbations, achieving a high attack success rate. The paper also demonstrates that adversarial training can significantly reduce the effectiveness of such attacks, enhancing model robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors] --> B[核心问题/Problem: ML-based Android malware detectors are vulnerable to adversarial attacks.]
        A --> C[主要方法/Method: Proposes LAMLAD, a dual-agent LLM framework with RAG for generating stealthy perturbations.]
        A --> D[关键结果/Results: Achieves up to 97% attack success rate; adversarial training defense reduces ASR by >30%.]
    ```

- **[arXiv251229] Feasible strategies in three-way conflict analysis with three-valued ratings**
  - **tags:** [ai], [conflict analysis], [three-way conflict analysis, feasible strategy, consistency measure, non-consistency measure, weighted agent-issue evaluation]
  - **authors:** Jing Liu, Mengjun Hu, Guangming Lang
  - **institution:** Changsha University of Science and Technology, Saint Mary's University
  - **link:** https://arxiv.org/pdf/2512.21420
  - **contributions:** 1. Proposes a novel framework for identifying feasible strategies in conflict resolution from the perspectives of consistency and non-consistency. 2. Introduces weighted consistency and non-consistency measures that incorporate the importance of both agents and issues. 3. Develops algorithms to systematically identify feasible strategies, L-order feasible strategies, and optimal solutions, demonstrating superior performance over existing approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4b2e48e756e98608f4388b841aa7940f0ba7b237737fa314abc4db83fbf680f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in formulating actionable strategies for conflict resolution within three-way conflict analysis. It proposes a method that computes agent clique ratings and uses novel weighted consistency and non-consistency measures to identify feasible and optimal strategies. The approach is validated through case studies and shown to outperform conventional conflict analysis models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Feasible strategies in three-way conflict analysis<br>三向冲突分析中的可行策略] --> B(Problem: Lack of focus on conflict resolution strategies<br>问题: 缺乏对冲突解决策略的关注)
        A --> C(Method: Weighted consistency/non-consistency measures & algorithms<br>方法: 加权一致/非一致性度量与算法)
        A --> D(Results: Outperforms conventional approaches, identifies optimal solutions<br>结果: 优于传统方法，识别最优解)
    ```

- **[arXiv251229] Three-way conflict analysis based on alliance and conflict functions**
  - **tags:** [ai], [decision theory], [three-way decision, conflict analysis, alliance function, conflict function, alliance set]
  - **authors:** Junfang Luo, Mengjun Hu, Guangming Lang, Xin Yang, Keyun Qin
  - **institution:** Southwestern University of Finance and Economics, University of Regina, Changsha University of Science and Technology, Southwest Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.21419
  - **contributions:** 1. Proposes a novel separation of the traditional auxiliary function into distinct alliance and conflict functions to clarify semantic interpretation in conflict analysis. 2. Introduces a framework for trisecting agents, issues, and agent pairs based on the new alliance and conflict functions. 3. Explores and applies new concepts such as alliance sets and strategies to solve crucial questions in conflict analysis, demonstrating the model with a real-world application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54277009925f58600c765060d6cbc575e96e562e3c9748aa2f54e97b83024e0b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the semantic ambiguity in aggregating traditional three-way conflict analysis functions by proposing a separation into distinct alliance and conflict functions. The method enables clearer trisection of agents, issues, and agent pairs, leading to the exploration of alliance sets and strategies. The main conclusion is that this separation provides a more interpretable and applicable framework for conflict analysis, as illustrated by a real-world example.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Three-way Conflict Analysis Based on Alliance and Conflict Functions] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统辅助函数聚合语义模糊/Semantic ambiguity in aggregating traditional auxiliary functions]
        C --> C1[分离为联盟与冲突函数/Separate into alliance and conflict functions]
        C --> C2[基于新函数进行三分/Trisec based on new functions]
        D --> D1[提出联盟集与策略概念/Propose alliance sets and strategies]
        D --> D2[提供真实应用案例/Provide a real-world application]
    ```

- **[arXiv251229] Teaching People LLM's Errors and Getting it Right**
  - **tags:** [nlp], [human-ai interaction], [overreliance, failure patterns, mental models, user study, meta-labels]
  - **authors:** Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasović
  - **institution:** University of Utah
  - **link:** https://arxiv.org/pdf/2512.21422
  - **contributions:** 1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness—assessing a user's ability to anticipate LLM errors using taught patterns—which showed a positive effect, unlike traditional human-AI team accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Teaching People LLM’s Errors and Getting it Right] --> B[核心问题/Problem: Users overrely on LLMs due to inaccurate mental models]
        A --> C[主要方法/Method: Analyze failure pattern existence, test discovery methods, propose new evaluation metric]
        A --> D[关键结果/Results: Patterns exist, discovery methods are mixed, new metric shows teaching is effective]
    ```

- **[arXiv251229] Three-way decision with incomplete information based on similarity and satisfiability**
  - **tags:** [ai], [rough set theory], [three-way decision, incomplete information, similarity degree, satisfiability degree, approximability]
  - **authors:** Junfang Luo, Mengjun Hu, Keyun Qin
  - **institution:** Southwest Jiaotong University, University of Regina
  - **link:** https://arxiv.org/pdf/2512.21421
  - **contributions:** 1. Proposes a new measure of similarity degree of objects as a generalization of equivalence relations for handling incomplete information in the computational formulation of three-way decision. 2. Introduces a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability for the conceptual formulation of three-way decision under incomplete information. 3. Proposes novel approaches for three-way decision using approximability of objects and confidence of formulas, pointing out new research directions beyond the common method of similarity classes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d87784acc1b1cc397b822153c118be23974be9721c3d19c9dfc95fbbaef158e_w640_q70.webp
  - **Simple LLM Summary:** This paper generalizes the computational and conceptual formulations of three-way decision to handle incomplete information, which is common in real-world applications. For the computational side, it introduces a similarity degree measure and explores decision-making via α-similarity classes and approximability; for the conceptual side, it proposes a satisfiability degree measure and studies approaches using α-meaning sets and confidence. The work extends rough set theory and identifies promising new directions for three-way decision under uncertainty.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Three-Way Decision with Incomplete Information Based on Similarity and Satisfiability] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[处理不完全信息/Handling Incomplete Information]
        C --> C1[计算式: 相似度/Computational: Similarity Degree]
        C --> C2[概念式: 可满足度/Conceptual: Satisfiability Degree]
        C1 --> C1a[α-相似类/α-Similarity Classes]
        C1 --> C1b[可逼近性/Approximability]
        C2 --> C2a[α-意义集/α-Meaning Sets]
        C2 --> C2b[置信度/Confidence]
        D --> D1[推广两种表述/Generalizes Both Formulations]
        D --> D2[指出新方向/Points to New Directions]
    ```

- **[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models**
  - **tags:** [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]
  - **authors:** Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi
  - **institution:** Institute of Intelligent Systems and Robotics, Sorbonne University
  - **link:** https://arxiv.org/pdf/2512.21439
  - **contributions:** 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]
        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]
        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]
    ```

- **[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning**
  - **tags:** [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]
  - **authors:** Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu
  - **institution:** University of Washington, University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.21446
  - **contributions:** 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards "diffusion supremacy" over autoregressive models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]
        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]
        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]
        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]
        D --> D2[迈向"扩散霸权"/Moving towards "diffusion supremacy"]
    ```

- **[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism**
  - **tags:** [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]
  - **authors:** Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21452
  - **contributions:** 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
    
        Problem --> P1("Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效")
        Problem --> P2("Data scarcity <br/> 数据稀缺")
    
        Method --> M1("DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强")
        Method --> M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络")
        Method --> M3("MS COCO Transfer Learning <br/> MS COCO迁移学习")
    
        Results --> R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能")
        Results --> R2("Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒")
        Results --> R3("New paradigm for automated detection <br/> 自动化检测新范式")
    ```

- **[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification**
  - **tags:** [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]
  - **authors:** Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian
  - **institution:** Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital
  - **link:** https://arxiv.org/pdf/2512.21476
  - **code:** https://github.com/JeremyXSC/GPF-Net
  - **contributions:** 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]
        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]
        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]
    ```

- **[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), task scheduling, inference throughput, fine-grained pipelining]
  - **authors:** Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21487
  - **contributions:** 1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FinDEP: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[MoE推理内存密集，现有DEP调度效率低/MoE inference is memory-intensive, existing DEP scheduling is inefficient]
        C --> C1[细粒度任务划分与调度优化/Fine-grained task partitioning and scheduling optimization]
        D --> D1[吞吐量最高提升1.61倍/Throughput improved by up to 1.61x]
    ```

- **[arXiv251229] Oogiri-Master: Benchmarking Humor Understanding via Oogiri**
  - **tags:** [nlp], [humor understanding], [Oogiri, benchmark, linguistic analysis, incongruity resolution, insight-augmented prompting]
  - **authors:** Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura
  - **institution:** CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo
  - **link:** https://arxiv.org/pdf/2512.21494
  - **contributions:** 1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Oogiri-Master: Benchmarking Humor Understanding via Oogiri] --> B[核心问题/Problem: What makes Oogiri responses funny to humans?]
        A --> C[主要方法/Method: Introduce Oogiri-Master benchmark and Oogiri-Corpus dataset with diverse responses and independent ratings]
        A --> D[关键结果/Results: LLMs approach human performance; insight-augmented prompting improves results]
    ```

- **[arXiv251229] LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis**
  - **tags:** [cv], [multimodal forgery detection], [visual-textual co-reasoning, cross-cues-aware chain of thought (CCT), GRPO-based optimization]
  - **authors:** Fanwei Zeng, Changtao Miao, Jing Huang, Zhiya Tan, Shutao Gong, Xiaoming Yu, Yang Wang, Huazhe Tan, Weibin Yao, Jianshu Li
  - **institution:** Ant Group, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.21482
  - **contributions:** 1. Proposed LogicLens, a unified framework for visual-textual co-reasoning that jointly performs detection, grounding, and explanation for text-centric forgery analysis. 2. Introduced a Cross-Cues-aware Chain of Thought (CCT) mechanism for iterative cross-validation of visual and textual cues, and a weighted multi-task reward function for GRPO-based optimization. 3. Created the RealText dataset with 5,397 images and fine-grained annotations using a novel PR² (Perceiver, Reasoner, Reviewer) multi-agent annotation pipeline.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/138f8fcb4727c77950b23146e1184851aa8b8cea95056b1d6b161c37e231ad80_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces LogicLens, a unified visual-textual co-reasoning framework for analyzing text-centric forgeries. It uses a novel Cross-Cues-aware Chain of Thought mechanism and multi-task optimization to jointly handle detection, grounding, and explanation. Experiments show LogicLens achieves state-of-the-art performance, significantly outperforming specialized frameworks and other MLLMs in zero-shot and dense-text scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[文本中心伪造威胁/Sophisticated text-centric forgeries]
        B --> B2[现有方法缺乏推理/Current methods lack reasoning]
        B --> B3[任务割裂/Tasks treated as discrete]
        C --> C1[统一框架/Unified Visual-Textual Co-reasoning framework]
        C --> C2[跨线索思维链/Cross-Cues-aware Chain of Thought (CCT)]
        C --> C3[多任务奖励函数/Weighted multi-task reward function]
        C --> C4[PR²标注管道/PR² annotation pipeline]
        C --> C5[RealText数据集/RealText dataset]
        D --> D1[零样本评估领先/Superior zero-shot performance]
        D --> D2[密集文本数据集领先/Lead on dense-text dataset]
        D --> D3[公开资源/Public dataset, model, code]
    ```

- **[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding**
  - **tags:** [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]
  - **authors:** Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson
  - **institution:** Dartmouth College
  - **link:** https://arxiv.org/pdf/2512.21506
  - **contributions:** 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]
        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]
        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]
    ```

- **[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO**
  - **tags:** [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]
  - **authors:** Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji
  - **institution:** Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21514
  - **contributions:** 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]
        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]
        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]
        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]
        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]
        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]
        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]
    ```

- **[arXiv251229] Selective LLM-Guided Regularization for Enhancing Recommendation Models**
  - **tags:** [ai], [recommender systems], [selective regularization, knowledge distillation, cold-start, long-tail, gating mechanism]
  - **authors:** Shanglin Yang, Zhan Shi
  - **institution:** Sichuan University
  - **link:** https://arxiv.org/pdf/2512.21526
  - **contributions:** 1. Proposes a selective LLM-guided regularization framework (S-LLMR) that activates LLM supervision only when a gating mechanism predicts the LLM to be reliable, addressing the issue of inaccurate global distillation. 2. Introduces a trainable gating mechanism informed by user history length, item popularity, and model uncertainty to dynamically decide when to apply LLM-based pairwise ranking supervision. 3. Demonstrates through experiments that the method improves overall accuracy and yields substantial gains in cold-start and long-tail recommendation scenarios, outperforming global distillation baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76336a3d123794e83843c14c4b799afd0817948ee9dfeb2f6f19ce776f183796_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of leveraging large language models (LLMs) for recommendation without suffering from their high cost and unreliability in certain scenarios. It proposes Selective LLM-Guided Regularization (S-LLMR), a model-agnostic framework that uses a gating mechanism to selectively apply LLM-based supervision only when the LLM is predicted to be reliable. Experiments show this approach improves recommendation accuracy, especially for cold-start users and long-tail items, outperforming methods that uniformly distill LLM knowledge.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Selective LLM-Guided Regularization<br>选择性LLM引导正则化] --> B(Problem/核心问题<br>LLMs as standalone recommenders are costly/unreliable;<br>Global distillation forces imitation of inaccurate LLM guidance.)
        A --> C(Method/主要方法<br>Selective LLM-Guided Regularization (S-LLMR):<br>Trainable gating mechanism activates LLM supervision only when reliable.)
        A --> D(Results/关键结果<br>Improves overall accuracy;<br>Substantial gains in cold-start & long-tail regimes.)
    ```

- **[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models**
  - **tags:** [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]
  - **authors:** Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu
  - **institution:** University of Washington, Intel
  - **link:** https://arxiv.org/pdf/2512.21529
  - **contributions:** 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification")
        Problem --> P2("标准方法预测不一致/Standard methods produce inconsistent predictions")
        Method --> M1("提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework")
        Method --> M2("结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses")
        Method --> M3("集成轻量级LoRA适配/Integrate lightweight LoRA adaptation")
        Results --> R1("提升全路径精度/Improves Full-Path Accuracy")
        Results --> R2("降低不一致性错误/Reduces Tree-based Inconsistency Error")
        Results --> R3("参数开销最小/Minimal parameter overhead")
    ```

- **[arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model**
  - **tags:** [mlsys], [llm inference], [adaptive length penalty, reinforcement learning, constrained optimization, Lagrangian primal-dual, reasoning efficiency]
  - **authors:** Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo
  - **institution:** Peking University, Harbin Institute of Technology, Shenzhen
  - **link:** https://arxiv.org/pdf/2512.21540
  - **contributions:** 1. Proposes Leash, a reinforcement learning framework that formulates length control as a constrained optimization problem and uses a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. 2. Introduces an adaptive mechanism that intensifies the penalty when generations exceed the target length and relaxes it when they are shorter, guiding models toward concise reasoning without sacrificing performance. 3. Demonstrates experimentally that Leash reduces average reasoning length by 60% across diverse tasks while maintaining competitive performance, offering a practical paradigm for efficient LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec5b3e2930213678e6d04b060a50d89faaaacded209387c96170a775f9db310_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLMs producing overly long reasoning traces, which increases computational cost. It proposes Leash, an adaptive reinforcement learning framework that dynamically adjusts length penalties using a Lagrangian method to balance conciseness and accuracy. Experiments show it reduces reasoning length by 60% while maintaining performance, providing an effective approach for efficient LLM reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LEASH: Adaptive Length Penalty and Reward Shaping] --> B[核心问题/Problem: LLMs生成过长推理链，计算成本高/Fixed penalties fail to adapt, leading to suboptimal accuracy-conciseness trade-offs]
        A --> C[主要方法/Method: 自适应强化学习框架，使用拉格朗日对偶方法动态调整惩罚系数/Adaptive RL framework with Lagrangian primal-dual for dynamic penalty adjustment]
        A --> D[关键结果/Results: 平均推理长度减少60%，性能保持竞争力/Average reasoning length reduced by 60% while maintaining competitive performance across tasks]
    ```

- **[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures**
  - **tags:** [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]
  - **authors:** Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li
  - **institution:** NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.21551
  - **contributions:** 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp
  - **Simple LLM Summary:** This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-AI Interaction Alignment] --> B[核心问题/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]
        A --> C[主要方法/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]
        A --> D[关键结果/Results: Establishes agenda for reciprocal, responsible human-AI futures]
    ```

- **[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments**
  - **tags:** [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]
  - **authors:** Hua Shen
  - **institution:** NYU Shanghai, New York University
  - **link:** https://arxiv.org/pdf/2512.21552
  - **contributions:** 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]
        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]
        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]
    ```

- **[arXiv251229] Exploration of Reproducible Generated Image Detection**
  - **tags:** [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]
  - **authors:** Yihang Duan
  - **institution:** Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)
  - **link:** https://arxiv.org/pdf/2512.21562
  - **contributions:** 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploration of Reproducible Generated Image Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]
        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]
        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]
    ```

- **[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation**
  - **tags:** [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]
  - **authors:** Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li
  - **institution:** Beihang University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2512.21576
  - **contributions:** 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]
        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]
        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]
    ```

- **[arXiv251229] NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent**
  - **tags:** [mlsys], [agent system], [NeMo Framework, LoRA, Nemotron SLM, hyperparameter sweep, multi-agent system]
  - **authors:** Ali Sahami, Sudhanshu Garg, Andrew Wang, Chaitanya Kulkarni, Farhad Farahani, Sean Yun-Shiuan Chuang, Jian Wan, Srinivasan Manoharan, Uma Kona, Nitin Sharma, Linsey Pang, Prakhar Mehrotra, Jessica Clark, Mark Moyou
  - **institution:** PayPal AI, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.21578
  - **contributions:** 1. The first application of NVIDIA's NeMo Framework to optimize commerce-specific agents. 2. An LLM-powered fine-tuning strategy for retrieval-focused commerce tasks. 3. A demonstration of significant latency and cost improvements while maintaining agent quality, providing a scalable framework for multi-agent system optimization in production e-commerce.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90dda98c8c5c5f9d75ede0c681c9024dc5973d432f246b90eed23aad0a03c916_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the optimization of PayPal's Commerce Agent, a multi-agent system, by fine-tuning a Nemotron small language model using NVIDIA's NeMo Framework and LoRA. The method involved systematic hyperparameter sweeps to improve the performance-critical search component. The results show that the fine-tuned model effectively resolves the key latency issue in the retrieval component, which accounted for over 50% of response time, while maintaining or enhancing overall system performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["NEMO-4-PAYPAL: Empowering PayPal's Commerce Agent"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["Search Latency/搜索延迟"]
        P1 --> P2[">50% Response Time/超过50%响应时间"]
        Method --> M1["Fine-tuning with NeMo/使用NeMo微调"]
        M1 --> M2["LoRA on Nemotron SLM/在Nemotron SLM上使用LoRA"]
        M2 --> M3["Hyperparameter Sweep/超参数扫描"]
        Results --> R1["Latency & Cost Improvement/延迟与成本改进"]
        Results --> R2["Maintained Agent Quality/保持代理质量"]
        Results --> R3["Scalable Framework/可扩展框架"]
    ```

- **[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid**
  - **tags:** [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]
  - **authors:** Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng
  - **institution:** Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers
  - **link:** https://arxiv.org/pdf/2512.21577
  - **contributions:** 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Problem["核心问题/Problem"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Method["主要方法/Method"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Results["关键结果/Results"]
        Problem --> P1["Hallucination persists in LLMs / 幻觉在LLM中持续存在"]
        Method --> M1["Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模"]
        Method --> M2["Framework: reference world & conflict policy / 框架：参考世界与冲突策略"]
        Results --> R1["Clarifies evaluation & terminology / 澄清评估与术语"]
        Results --> R2["Proposes new benchmark plans / 提出新基准计划"]
    ```

- **[arXiv251229] A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model, logic tree reasoning, medical multimodal diagnosis, explainable AI, LLaVA]
  - **authors:** Zelin Zang, Wenyi Gu, Siqi Ma, Dan Yang, Yue Shen, Zhu Zhang, Guohui Fan, Wing-Kuen Ling, Fuji Yang
  - **institution:** Tsientang Institute of Advanced Study (TIAS), Westlake University, Ant Group, China-Japan Friendship Hospital
  - **link:** https://arxiv.org/pdf/2512.21583
  - **contributions:** 1. Proposes a diagnostic framework integrating vision-language alignment with logic-regularized reasoning to enhance reliability. 2. Introduces a reasoning controller and logic tree generator to decompose tasks and assemble verifiable conclusions, improving interpretability. 3. Demonstrates improved diagnostic accuracy and more interpretable reasoning traces on multimodal medical benchmarks like MedXpertQA.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6b8d6d34a614d3cb042f13334dede18494914ca29d6f0fd6f4467f789871f82_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of unreliable reasoning and hallucinations in existing multimodal medical AI models. It proposes a diagnostic framework built on LLaVA that combines vision-language alignment with logic-regularized reasoning to generate verifiable conclusions via logic trees. Evaluations show the method improves diagnostic accuracy and yields more interpretable reasoning traces, advancing trustworthy multimodal medical AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[医学多模态诊断框架<br/>Medical Multimodal Diagnostic Framework] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有模型幻觉与推理不一致<br/>Existing Models: Hallucinations & Inconsistent Reasoning]
        C --> C1[结合视觉语言对齐与逻辑树推理<br/>Vision-Language Alignment + Logic Tree Reasoning]
        D --> D1[提升诊断准确性与可解释性<br/>Improved Diagnostic Accuracy & Interpretability]
    ```

- **[arXiv251229] LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model**
  - **tags:** [mlsys], [llm inference], [item-to-item recommendation, data-centric, long-tail items, data augmentation, data filtering]
  - **authors:** Yinfu Feng, Yanjing Wu, Rong Xiao, Xiaoyi Zen
  - **institution:** Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21595
  - **contributions:** 1. Proposes LLM-I2I, a data-centric framework that leverages Large Language Models to enhance I2I recommendation models without altering their architecture. 2. Introduces an LLM-based data generator to synthesize user-item interactions, specifically targeting long-tail items to alleviate data sparsity. 3. Designs an LLM-based data discriminator to filter out noisy interactions from both real and synthetic data, improving overall data quality for training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cb08ea9b26b612493e4d48e7db88c46a869c2050c94d47b75488adcaf6ddfa9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses data sparsity and noise problems in Item-to-Item (I2I) recommendation systems by proposing LLM-I2I, a data-centric framework that uses an LLM to generate synthetic interactions for long-tail items and filter noisy data. The refined data is then used to train existing I2I models. Experimental results on industrial and academic datasets show significant improvements in recommendation accuracy, especially for long-tail items, and deployment on a large e-commerce platform led to measurable gains in recall and gross merchandise value.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据稀疏与噪声/Data Sparsity & Noise]
        C --> C1[LLM数据生成器/LLM-based Data Generator]
        C --> C2[LLM数据判别器/LLM-based Data Discriminator]
        C1 --> C3[合成交互数据/Synthesize Interaction Data]
        C2 --> C4[过滤噪声数据/Filter Noisy Data]
        C3 & C4 --> C5[融合数据训练I2I模型/Fuse Data to Train I2I Model]
        D --> D1[提升推荐准确率/Improves Recommendation Accuracy]
        D --> D2[提升长尾物品性能/Better for Long-tail Items]
        D --> D3[线上指标提升/Online Metric Improvements (RN+6.02%, GMV+1.22%)]
    ```

- **[arXiv251229] AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design**
  - **tags:** [mlsys], [agent system], [AMS IC design, LLM-based agent, structured reasoning, design automation, I/O ring generation]
  - **authors:** Zhishuai Zhang, Xintian Li, Shilong Liu, Aodong Zhang, Lu Jie, Nan Sun
  - **institution:** Tsinghua University, Princeton University
  - **link:** https://arxiv.org/pdf/2512.21613
  - **code:** https://github.com/Arcadia-1/AMS-IO-Agent
  - **contributions:** 1. Proposed AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware I/O subsystem generation in AMS ICs. 2. Introduced AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. 3. Demonstrated the first reported human-agent collaborative AMS IC design where an LLM agent's output was directly used in a silicon tape-out, achieving over 70% DRC+LVS pass rate and reducing design time from hours to minutes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7205181105fdcb012bb4c8c5b3cce6565751edc220003d3784f6dbf648ee893a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the labor-intensive and non-reusable nature of analog and mixed-signal (AMS) integrated circuit I/O design by proposing AMS-IO-Agent, an LLM-based agent that uses structured domain knowledge and intent structuring to automate the process. The method connects natural language design intent to industrial deliverables and is evaluated on a new benchmark, AMS-IO-Bench. The agent significantly outperforms baseline LLMs, achieves a high verification pass rate, and its generated I/O ring was successfully fabricated, demonstrating practical effectiveness in real design flows.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AMS-IO-Bench and AMS-IO-Agent<br>论文标题/Paper Title] --> B[手动AMS I/O设计费时且不可复用<br>核心问题/Problem: Manual AMS I/O design is time-consuming and non-reusable]
        A --> C[提出基于LLM的智能体与结构化推理框架<br>主要方法/Method: Proposes an LLM-based agent and structured reasoning framework]
        A --> D[验证通过率>70%，设计时间从小时减至分钟，成功流片<br>关键结果/Results: >70% pass rate, design time reduced from hours to minutes, successful tape-out]
    ```

- **[arXiv251229] Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design**
  - **tags:** [mlsys], [agent system], [multi-agent platform, knowledge graph, physiologically based pharmacokinetic (PBPK) simulations, autonomous execution, human-in-the-loop]
  - **authors:** Takahide Suzuki, Kazuki Nakanishi, Takashi Fujiwara, Hideyuki Shimizu
  - **institution:** Institute of Science Tokyo, Kyoto University
  - **link:** https://arxiv.org/pdf/2512.21623
  - **contributions:** 1. Introduces OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine for drug design. 2. Features an architecture with specialized agents (Biologist, Chemist, Pharmacologist) governed by an Orchestrator, which actively execute simulations and reason over results to create a dynamic feedback loop for iterative optimization. 3. Democratizes therapeutic design by transforming drug discovery from a stochastic search into a programmable, evidence-based engineering discipline through the integration of autonomous execution with human guidance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd4a841925fb9129111928c81fe29ac7016c26df168e9b4ca87c8782a692d5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of fragmented and passive tools in therapeutic discovery by proposing OrchestRA, a multi-agent platform where specialized AI agents autonomously execute and reason over biological, chemical, and pharmacological tasks. This creates a dynamic feedback loop for iterative drug candidate optimization, guided by human input. The conclusion is that this approach transforms drug discovery into a more programmable and evidence-based engineering process.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Fragmented domains & execution gap<br>AI as passive assistants]
        C[主要方法/Method<br>OrchestRA Multi-Agent Platform<br>Agents execute & reason<br>Human-in-the-loop]
        D[关键结果/Results<br>Autonomous discovery engine<br>Dynamic feedback loop<br>Programmable evidence-based design]
    ```

- **[arXiv251229] Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing**
  - **tags:** [ai], [multi-armed bandits], [multiple-play bandits, prioritized resource sharing, regret analysis, combinatorial optimization, UCB]
  - **authors:** Hong Xie, Haoran Gu, Yanying Huang, Tao Tan, Defu Lian
  - **institution:** University of Science and Technology of China, Chongqing University
  - **link:** https://arxiv.org/pdf/2512.21626
  - **contributions:** 1. Proposes a new variant of the multiple-play stochastic bandit model (MSB-PRS) that incorporates prioritized capacity sharing among plays, tailored for resource allocation in LLM and edge intelligence applications. 2. Establishes instance-independent and instance-dependent regret lower bounds for the proposed model, characterizing its fundamental learning difficulty. 3. Designs an offline optimal policy solver (MSB-PRS-OffOpt) and an online UCB-based learning algorithm with theoretical regret guarantees that nearly match the derived lower bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8562089dc3d9fa0a65e8caf8921b51648e1718efd62395a7af2fadba8cf952d9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new multi-armed bandit model where multiple plays with priorities compete for the stochastic capacity of arms. The authors design an algorithm that first computes an optimal allocation offline and then uses it within an online UCB-based strategy, proving that its regret nearly matches the fundamental lower bounds they establish for this problem.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing<br/>多臂老虎机优先容量共享"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Prioritized resource allocation in LLM/edge intelligence<br/>LLM/边缘智能中的优先资源分配"] --> P1["模型/Model<br/>M arms, K plays, stochastic capacity, priority weights<br/>M个臂，K个玩家，随机容量，优先级权重"]
        Method["主要方法/Method<br/>Algorithm Design<br/>算法设计"] --> M1["离线最优求解器/MSB-PRS-OffOpt<br/>Computes optimal policy<br/>计算最优策略"]
        Method --> M2["在线UCB算法/Online UCB Algorithm<br/>Uses offline solver as subroutine<br/>以离线求解器为子程序"]
        Results["关键结果/Results<br/>Theoretical Analysis<br/>理论分析"] --> R1["下界/Regret Lower Bounds<br/>Ω(α₁σ√KMT), Ω(α₁σ²(M/Δ)lnT)"]
        Results --> R2["上界/Regret Upper Bounds<br/>Matching lower bounds up to factors<br/>与下界匹配（差因子）"]
    ```

- **[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search**
  - **tags:** [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]
  - **authors:** Maximilian Weichart
  - **institution:** University of Regensburg
  - **link:** https://arxiv.org/pdf/2512.21648
  - **code:** https://github.com/Max-We/inverse-rpo
  - **contributions:** 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]
        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]
        D[关键结果/Results: New policies outperform PUCT without extra cost]
    ```

- **[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References**
  - **tags:** [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]
  - **authors:** Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
  - **institution:** Zhejiang University, Fudan University, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.21641
  - **contributions:** 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]
        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]
        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]
        C1 --> C3[构建UniScene表示/Build UniScene Representation]
        C1 --> C4[利用运动历史推理/Reason with Motion History]
        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]
        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]
        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]
    ```

- **[arXiv251229] Near-Optimal Coalition Structures in Polynomial Time**
  - **tags:** [ai], [cooperative game theory], [coalition structure generation, anytime algorithms, sparse relaxations, dynamic programming, MILP]
  - **authors:** Angshul Majumdar
  - **institution:** Indraprastha Institute of Information Technology, Delhi
  - **link:** https://arxiv.org/pdf/2512.21657
  - **contributions:** 1. Proves that under a "sparse synergy" model, sparse relaxation methods can find near-optimal coalition structures in polynomial time with high probability. 2. Demonstrates that broad classes of dynamic programming and MILP algorithms require exponential time to achieve comparable solution quality. 3. Establishes a rigorous probabilistic anytime performance separation favoring sparse relaxations over exact methods for the CSG problem.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb8e459bba93eeaf4c9237729ad06cf13b47ef6a941811135ed634157dd979c7_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the coalition structure generation (CSG) problem. It compares three algorithmic paradigms and proves that, under a random sparse synergy model, sparse relaxation methods can find near-optimal solutions in polynomial time, while exact methods like DP and MILP require exponential time to reach similar quality, establishing a clear anytime performance advantage for the sparse approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Near-Optimal Coalition Structures in Polynomial Time] --> B[核心问题/Problem: Coalition Structure Generation (CSG)]
        A --> C[主要方法/Method: Compare DP, MILP, and Sparse Relaxations]
        A --> D[关键结果/Results: Sparse relaxations achieve near-optimal welfare in polynomial time; DP/MILP require exponential time]
    ```

- **[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning**
  - **tags:** [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]
  - **authors:** Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee
  - **institution:** Kyungpook National University, Kumoh National Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21654
  - **contributions:** 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. 传统方法难以扩展/Traditional methods struggle to scale.]
        C[主要方法/Method: A structure-induced ACO framework. 利用结构先验、负载感知目标和重叠抑制/Uses structural prior, load-aware objective, and overlap suppression.]
        D[关键结果/Results: Improves route compactness, stability, and workload distribution. 提供可扩展的框架/Provides a scalable framework.]
    ```

- **[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles**
  - **tags:** [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]
  - **authors:** Jalal Khan
  - **institution:** United Arab Emirates University
  - **link:** https://arxiv.org/pdf/2512.21673
  - **contributions:** 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp
  - **Simple LLM Summary:** This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]
    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]
    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]
    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]
    ```

- **[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting**
  - **tags:** [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]
  - **authors:** Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan
  - **institution:** Beijing Jiaotong University, Aalborg University
  - **link:** https://arxiv.org/pdf/2512.21685
  - **contributions:** 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)
        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)
        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)
        C --> C2(主成分网络 / Principal Component Network)
        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)
    ```

- **[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks**
  - **tags:** [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]
  - **authors:** Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu
  - **institution:** Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.
  - **link:** https://arxiv.org/pdf/2512.21694
  - **contributions:** 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]
        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]
        C --> C2[Use self-collected dataset<br/>使用自收集数据集]
        C --> C3[Pre-process images<br/>预处理图像]
        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]
        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]
    ```

- **[arXiv251229] Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning**
  - **tags:** [mlsys], [agent system], [agentic AI, consensus-driven reasoning, explainable AI, responsible AI, multi-model governance]
  - **authors:** Eranga Bandara, Tharaka Hewa, Ross Gore, Sachin Shetty, Ravi Mukkamala, Peter Foytik, Abdul Rahman, Safdar H. Bouk, Xueping Liang, Amin Hass, Sachini Rajapakse, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan
  - **institution:** Old Dominion University, University of Oulu, Deloitte & Touche LLP, Florida International University, Nanyang Technological University, University of Colombo, IcicleLabs.AI, Accenture Technology Labs, Effectz.AI
  - **link:** https://arxiv.org/pdf/2512.21699
  - **contributions:** 1. Proposes a novel RAI/XAI agent architecture for production workflows based on multi-model consensus and reasoning-layer governance. 2. Introduces a mechanism where a consortium of heterogeneous LLM/VLM agents generate independent outputs, exposing uncertainty and alternatives for structured consolidation. 3. Demonstrates that the consensus-driven approach improves robustness, transparency, and operational trust across diverse real-world agentic AI workflows.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed22161f8f004c98e3fb6124bac7991548de5e54f47adb42f0d3eb1095409e6e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of explainability and responsibility in increasingly autonomous agentic AI systems. It proposes a new architecture where multiple AI agents generate candidate outputs, and a dedicated reasoning agent consolidates them while enforcing safety constraints, thereby improving decision robustness and auditability. The work provides a practical framework for building agentic systems that are both scalable and responsible by design.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Agentic AI lacks explainability & responsibility] --> B1[挑战/Challenges<br>Explainability, Accountability, Robustness, Governance]
        C[主要方法/Method<br>Multi-model Consensus & Reasoning-layer Governance] --> C1[架构/Architecture<br>Consortium of LLM/VLM Agents]
        C --> C2[过程/Process<br>Structured Consolidation by Dedicated Reasoning Agent]
        D[关键结果/Results<br>Improved Robustness, Transparency & Operational Trust]
    ```

- **[arXiv251229] Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning**
  - **tags:** [sec], [audio deepfake detection], [transfer learning, zero-shot inference, fine-tuning, Bengali audio, BanglaFake dataset]
  - **authors:** Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman
  - **institution:** Not explicitly stated in the provided content.
  - **link:** https://arxiv.org/pdf/2512.21702
  - **contributions:** 1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)
        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)
        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))
    ```

- **[arXiv251229] Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech**
  - **tags:** [nlp], [spoken dialogue systems], [Graph-of-Thoughts, full-duplex, speech acts, causal inference, multimodal transformer]
  - **authors:** Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian
  - **institution:** Zhejiang University, University of California, Berkeley, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.21706
  - **code:** https://got-duplex.github.io/
  - **contributions:** 1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Enabling Conversational Behavior Reasoning in Full-Duplex Speech<br/>实现全双工语音对话行为推理"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Current systems lack explicit reasoning for conversational behaviors."]
        Method["主要方法/Method<br/>Model reasoning as causal inference in a Graph-of-Thoughts (GoT)."]
        Results["关键结果/Results<br/>Robust behavior detection and interpretable reasoning chains."]
    ```

- **[arXiv251229] Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers**
  - **tags:** [nlp], [ai-generated text detection], [transformer, fine-tuning, zero-shot, Bengali, paraphrase detection]
  - **authors:** Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan
  - **institution:** Not specified in provided content.
  - **link:** https://arxiv.org/pdf/2512.21709
  - **contributions:** 1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Detecting AI-Generated Paraphrases in Bengali] --> B[核心问题/Problem: LLM misuse & lack of Bengali detection research]
        A --> C[主要方法/Method: Compare 5 transformers (Zero-Shot vs. Fine-Tuned)]
        A --> D[关键结果/Results: Fine-tuning needed; XLM-R, mDeBERTa, mBERT achieve ~91% accuracy]
    ```

- **[arXiv251229] Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought**
  - **tags:** [nlp], [interpretability & analysis], [latent tokens, chain-of-thought, model reliability, causal analysis, shortcut learning]
  - **authors:** Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21711
  - **contributions:** 1. Introduces "Steering Experiments" to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts "Shortcut Experiments" to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a "pseudo-reasoning" mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Latent token mechanisms unclear, reliability concerns] --> B1[潜在令牌机制不明确/Unclear latent token mechanisms]
        B --> B2[可靠性问题/Reliability concerns]
        C[主要方法/Method: Causal & adversarial analysis] --> C1[引导实验/Steering experiments]
        C --> C2[捷径实验/Shortcut experiments]
        D[关键结果/Results: COCONUT is pseudo-reasoning] --> D1[令牌对扰动不敏感/Tokens insensitive to perturbation]
        D --> D2[利用数据集捷径/Exploits dataset shortcuts]
        D --> D3[性能提升不基于真实推理/Performance gains not from true reasoning]
    ```

- **[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities**
  - **tags:** [sys], [communication & networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]
  - **authors:** Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin
  - **institution:** Kyung Hee University, Ghent University
  - **link:** https://arxiv.org/pdf/2512.21717
  - **contributions:** 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation"]
        Method["主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning"]
        Results["关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off"]
    ```

- **[arXiv251229] CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation**
  - **tags:** [nlp], [dialogue systems], [theme detection, topic clustering, hierarchical generation]
  - **authors:** Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li
  - **institution:** The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.21715
  - **contributions:** 1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CATCH: 可控主题检测框架 / Controllable Theme Detection Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题 / Problem] --> P1[短话语稀疏语义 / Sparse, short utterances]
        Problem --> P2[跨对话主题对齐 / Cross-dialogue theme alignment]
        Problem --> P3[用户偏好整合 / Personalized user preferences]
        Method[主要方法 / Method] --> M1[上下文感知主题表示 / Context-aware topic representation]
        Method --> M2[偏好引导主题聚类 / Preference-guided topic clustering]
        Method --> M3[分层主题生成 / Hierarchical theme generation]
        Results[关键结果 / Results] --> R1[在DSTC-12基准测试有效 / Effective on DSTC-12 benchmark]
        Results --> R2[提升聚类与生成质量 / Improved clustering & generation quality with 8B LLM]
    ```

- **[arXiv251229] An Information Theoretic Perspective on Agentic System Design**
  - **tags:** [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]
  - **authors:** Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.21720
  - **contributions:** 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance")
        C --> C1("提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator")
        D --> D1("更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate")
        D --> D2("扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors")
    ```

- **[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks**
  - **tags:** [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]
  - **authors:** Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov
  - **institution:** MIRAI, Cognitive AI Systems Lab
  - **link:** https://arxiv.org/pdf/2512.21723
  - **contributions:** 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]
        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]
        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]
    ```

- **[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data**
  - **tags:** [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]
  - **authors:** Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda
  - **institution:** Sony Computer Science Laboratories, Inc., Sony Corporation of America
  - **link:** https://arxiv.org/pdf/2512.21746
  - **contributions:** 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]
        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]
        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]
    ```

- **[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study**
  - **tags:** [se], [code optimization], [AI coding agents, performance optimization, empirical study, pull request analysis, AIDev dataset]
  - **authors:** Huiyun Peng, Antonio Zhong, Ricardo Andrés Calvo Méndez, Kelechi G. Kalu, James C. Davis
  - **institution:** Purdue University
  - **link:** https://arxiv.org/pdf/2512.21757
  - **contributions:** 1. Conducts the first empirical study comparing AI-agent-authored and human-authored performance optimization commits using real-world PR data. 2. Identifies a significant gap in explicit performance validation between AI-authored (45.7%) and human-authored (63.6%) PRs. 3. Finds that AI agents largely employ the same optimization patterns as humans, suggesting they learn from existing code but lack rigorous validation practices.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an empirical study comparing how AI coding agents and humans perform code optimization by analyzing performance-related pull requests from the AIDev dataset. The study finds that while AI agents use similar optimization patterns as humans, they are significantly less likely to include explicit performance validation in their commits. This highlights a key limitation in current agentic code optimization and an opportunity for improvement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[How Do Agents Perform Code Optimization? An Empirical Study] --> B[核心问题/Problem: AI coding agents' effectiveness on real-world performance optimization tasks is unknown.]
        A --> C[主要方法/Method: Empirical comparison of 324 agent-generated and 83 human-authored performance PRs from AIDev dataset.]
        A --> D[关键结果/Results: AI-authored PRs use similar patterns but include less explicit performance validation (45.7% vs 63.6%).]
    ```

- **[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets**
  - **tags:** [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]
  - **authors:** Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli
  - **link:** https://arxiv.org/pdf/2512.21760
  - **contributions:** 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]
        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]
        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]
    ```

- **[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets**
  - **tags:** [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]
  - **authors:** Matyas Bohacek, Ignacio Vilanova Echavarri
  - **institution:** Stanford University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21775
  - **contributions:** 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation")
        Problem --> P2("数据来源与合法性信息丢失/Loss of data origin & legitimacy info")
        Method --> M1("提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework")
        Method --> M2("开发基于数据溯源技术的开源库/Develop open-source library using data provenance")
        Results --> R1("评估现有数据集的合规性/Evaluate compliance of existing datasets")
        Results --> R2("指导负责任的新数据集构建/Guide responsible construction of new datasets")
    ```

- **[arXiv251229] Inference-based GAN Video Generation**
  - **tags:** [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]
  - **authors:** Jingbo Yang, Adrian G. Bors
  - **institution:** University of York
  - **link:** https://arxiv.org/pdf/2512.21776
  - **contributions:** 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Inference-based GAN Video Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]
        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]
        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]
        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]
        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]
        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]
        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]
    ```

- **[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents**
  - **tags:** [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]
  - **authors:** Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin
  - **institution:** Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard
  - **link:** https://arxiv.org/pdf/2512.21782
  - **contributions:** 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]
        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]
        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]
        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]
        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]
    ```

- **[arXiv251229] Multi-agent Adaptive Mechanism Design**
  - **tags:** [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]
  - **authors:** Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao
  - **institution:** Massachusetts Institute of Technology, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.21794
  - **contributions:** 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\tilde\{O\}(\sqrt\{T\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]
        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]
        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\tilde{O}(\sqrt{T})$ regret]
    ```

- **[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning**
  - **tags:** [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]
  - **authors:** Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles
  - **institution:** The Pennsylvania State University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21789
  - **contributions:** 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]
        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]
        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]
        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]
        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]
        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]
        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]
    ```

- **[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation**
  - **tags:** [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]
  - **authors:** Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan
  - **institution:** ByteDance Inc., Rutgers University
  - **link:** https://arxiv.org/pdf/2512.21788
  - **code:** https://github.com/yanq095/InstructMoLE
  - **contributions:** 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]
        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]
        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]
    ```

- **[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection**
  - **tags:** [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]
  - **authors:** Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu
  - **institution:** University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21803
  - **contributions:** 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]
        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]
        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]
    ```

- **[arXiv251229] S&P 500 Stock's Movement Prediction using CNN**
  - **tags:** [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]
  - **authors:** Rahul Gupta
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.21804
  - **contributions:** 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势"] --> P1["传统方法依赖特征工程<br>Traditional methods rely on engineered features"]
        Problem --> P2["现有研究多使用单维数据<br>Existing research often uses single-dimension data"]
        Method["主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN"] --> M1["将历史数据矩阵视为图像<br>Treat historical data matrices as images"]
        Method --> M2["包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)"]
        Results["关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果"] --> R1["支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction"]
    ```

- **[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction**
  - **tags:** [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]
  - **authors:** Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida
  - **institution:** Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University
  - **link:** https://arxiv.org/pdf/2512.21853
  - **contributions:** 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MoonBot: 面向月球基地建设的模块化按需可重构机器人] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[月球探索与基地建设需求 / Lunar Exploration & Base Construction Needs]
        C --> C1[模块化可重构机器人系统 / Modular & Reconfigurable Robotic System]
        C --> C2[概念验证与现场演示 / Proof-of-Concept & Field Demonstration]
        D --> D1[成功执行模拟任务 / Successfully Executed Simulated Tasks]
        D --> D2[总结了连接器设计等经验教训 / Summarized Lessons (e.g., Connector Design)]
    ```

- **[arXiv251229] HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs**
  - **tags:** [nlp], [evaluation], [anthropomorphic intelligence, benchmark, psychological counseling, rubric-based evaluation, reasoning-before-scoring]
  - **authors:** Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang
  - **institution:** Ant Group, Xiamen University, Beijing Normal University, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.21849
  - **code:** https://github.com/inclusionAI/HeartBench
  - **contributions:** 1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based "reasoning-before-scoring" evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLMs缺乏拟人化智能 / LLMs lack anthropomorphic intelligence]
        B --> B2[中文语境缺乏评估框架 / Lack of evaluation frameworks in Chinese context]
        C --> C1[基于心理咨询场景的基准 / Benchmark based on psychological counseling scenarios]
        C --> C2[理论驱动的分类法 / Theory-driven taxonomy]
        C --> C3[基于量规的推理评分法 / Rubric-based reasoning-before-scoring]
        D --> D1[模型性能存在上限 / Performance ceiling in models]
        D --> D2[复杂场景表现显著下降 / Significant decay in complex scenarios]
    ```

- **[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs**
  - **tags:** [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]
  - **authors:** Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville
  - **institution:** Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR
  - **link:** https://arxiv.org/pdf/2512.21852
  - **contributions:** 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差"] --> P1["实践问题/Practical Issue<br>广泛使用但实现与目标不一致"]
        Problem --> P2["理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性"]
        Method["主要方法/Method<br>分析梯度偏差并进行实证验证"] --> M1["分析/Analysis<br>研究多种估计器配置的梯度"]
        Method --> M2["实验/Experiments<br>RL微调多个LLM并评估性能"]
        Results["关键结果/Results<br>无偏梯度配置带来更好性能"] --> R1["在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能"]
        Results --> R2["离线策略/Off-Policy<br>KL正则化有助于稳定异步训练"]
    ```

- **[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening**
  - **tags:** [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]
  - **authors:** Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan
  - **institution:** North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh
  - **link:** https://arxiv.org/pdf/2512.21861
  - **contributions:** 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]
        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]
        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation**
  - **tags:** [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]
  - **authors:** Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh
  - **institution:** Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21866
  - **contributions:** 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection")
        Problem --> P2("模型需要可解释性/Model needs explainability")
        Method --> M1("将随机森林转换为规则区域/Convert random forest to rule regions")
        Method --> M2("在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data")
        Results --> R1("数据量减少85-93%/Data volume reduced by 85-93%")
        Results --> R2("保持竞争性性能/Maintains competitive performance")
        Results --> R3("抵抗成员推理攻击/Resists membership-inference attacks")
    ```

- **[arXiv251229] MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting**
  - **tags:** [mlsys], [agent system], [multi-agent framework, bias mitigation, financial forecasting, LLM integration, modular design]
  - **authors:** Marc S. Montalvo, Hamed Yaghoobian
  - **institution:** Rochester Institute of Technology, Muhlenberg College
  - **link:** https://arxiv.org/pdf/2512.21878
  - **contributions:** 1. Introduces MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news for decomposed financial reasoning. 2. Embeds explicit bias-mitigation protocols (e.g., against survivorship and hindsight bias) to enhance transparency and robustness. 3. Demonstrates practical effectiveness through an eight-week evaluation showing outperformance of major market benchmarks, highlighting the promise of bias-aware generative AI in finance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/351cdc3ccfe2b2d987cf53c8380e153fe4b93de0def6253cbc7b2feb4af093fe_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MASFIN, a multi-agent system that combines LLMs with financial data and news to perform decomposed reasoning and forecasting while mitigating biases. In an eight-week evaluation, it achieved a 7.33% cumulative return, outperforming benchmarks like the S&P 500 in most weeks, though with higher volatility. The results show the potential of modular, bias-aware AI frameworks for transparent and reproducible quantitative finance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统量化方法易受生存偏差影响/Traditional quantitative methods vulnerable to survivorship bias]
        B --> B2[AI方法在信号集成和可复现性上存在挑战/AI approaches struggle with signal integration and reproducibility]
        C --> C1[模块化多智能体框架/Modular multi-agent framework]
        C --> C2[集成LLM与结构化指标和非结构化新闻/Integrates LLMs with structured metrics and unstructured news]
        C --> C3[嵌入偏差缓解协议/Embeds bias-mitigation protocols]
        D --> D1[8周累计回报7.33%/7.33% cumulative return over eight weeks]
        D --> D2[在6/8周中超越基准/Outperformed benchmarks in six of eight weeks]
        D --> D3[波动性较高/Higher volatility]
    ```

- **[arXiv251229] CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics**
  - **tags:** [nlp], [text-to-sql], [benchmark, multilingual, domain-specific, large language models, sports analytics]
  - **authors:** Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa
  - **institution:** Birla Institute of Technology and Science (BITS), Pilani
  - **link:** https://arxiv.org/pdf/2512.21877
  - **contributions:** 1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a "Gold Standard" dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs在专业领域Text-to-SQL能力未充分探索/LLMs' Text-to-SQL capability in specialized domains is under-explored]
        B --> B2[现有基准缺乏多语言和体育分析特性/Existing benchmarks lack multilingual and sports analytics features]
        C --> C1[构建板球领域专业多语言基准/Build a specialized multilingual benchmark for cricket]
        C --> C2[与专家合作创建"黄金标准"查询/Collaborate with experts to create "Gold Standard" queries]
        C --> C3[评估六个最先进的LLMs/Evaluate six state-of-the-art LLMs]
        D --> D1[专业领域性能显著下降/Significant performance drop in specialized domain]
        D --> D2[DeepSeek R1表现最佳/DeepSeek R1 achieves SOTA]
        D --> D3[印地语查询准确率可比或更高/Hindi queries yield parity or higher accuracy]
    ```

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [distributed inference, block placement, request routing, performance modeling, resource allocation]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **contributions:** 1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]
        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]
        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]
    ```

- **[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space**
  - **tags:** [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]
  - **authors:** Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21887
  - **contributions:** 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[UAV导航缺乏高层语义规划能力/UAV navigation lacks high-level semantic planning]
        B --> B2[现有模型难以处理复杂动作空间与长距离视觉生成/Existing models struggle with complex action space & long-horizon visual generation]
        C --> C1[提出ANWM世界模型/Propose ANWM world model]
        C --> C2[引入未来帧投影模块/Introduce Future Frame Projection module]
        D --> D1[长距离视觉预测性能显著提升/Significantly outperforms in long-distance visual forecasting]
        D --> D2[提高大规模环境导航成功率/Improves UAV navigation success rates in large-scale environments]
    ```

- **[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy**
  - **tags:** [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]
  - **authors:** Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du
  - **institution:** University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University
  - **link:** https://arxiv.org/pdf/2512.21898
  - **contributions:** 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Flexible Multitask Learning with Factorized Diffusion Policy] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[机器人多任务学习/Robot Multitask Learning]
        B1 --> B2[动作分布复杂多模态/Action Distribution Highly Multimodal]
        B2 --> B3[单体模型欠拟合与不灵活/Monolithic Models Underfit & Inflexible]
        C --> C1[因子化扩散策略/Factorized Diffusion Policy (FDP)]
        C1 --> C2[模块化扩散专家/Modular Diffusion Experts]
        C2 --> C3[基于观察的路由器/Observation-Conditioned Router]
        C3 --> C4[连续分数聚合/Continuous Score Aggregation]
        D --> D1[性能超越基线/Outperforms Baselines]
        D1 --> D2[仿真与真实机器人验证/Simulation & Real-World Validation]
        D2 --> D3[支持灵活策略适应/Enables Flexible Policy Adaptation]
    ```

- **[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction**
  - **tags:** [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]
  - **authors:** Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han
  - **institution:** Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University
  - **link:** https://arxiv.org/pdf/2512.21897
  - **contributions:** 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]
        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]
        Method --> M2[架构设计/Architecture Design]
        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]
        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]
        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]
        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]
        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]
        Results --> R2[消融研究<br>Ablation Studies]
        Results --> R3[概率校准<br>Probability Calibration]
    ```

- **[arXiv251229] SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?**
  - **tags:** [ai], [agent system], [spatial transcriptomics, AI agents, benchmark, deterministic grader, harness design]
  - **authors:** Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le
  - **institution:** LatchBio
  - **link:** https://arxiv.org/pdf/2512.21907
  - **contributions:** 1. Introduces SpatialBench, a benchmark of 146 verifiable problems derived from real-world spatial biology analysis workflows, covering five technologies and seven task categories. 2. Provides a deterministic grader for each problem to evaluate the recovery of key biological results from messy spatial datasets. 3. Demonstrates through benchmark data that frontier AI agents have low accuracy (20-38%) on these tasks and reveals the significant impact of harness design (tools, prompts, control flow, execution environment) on performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3912de9f7e0f0d2acbf8bcd709b023f2ed3b9ccd56886885ca3f8e9e9e81880_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SpatialBench, a benchmark to evaluate whether AI agents can analyze messy, real-world spatial biology data. It tests frontier models on 146 practical problems and finds low accuracy, highlighting that performance heavily depends on the agent's harness design. The benchmark serves as a tool to measure and diagnose agent capabilities for faithful and reproducible data analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI代理能否从混乱的真实空间数据中提取生物学见解?/Can AI agents extract biological insight from messy, real-world spatial datasets?]
        C --> C1[引入包含146个可验证问题的基准SpatialBench/Introduce SpatialBench benchmark with 146 verifiable problems]
        C --> C2[提供确定性评分器评估关键生物学结果恢复/Provide deterministic grader to evaluate recovery of key biological result]
        D --> D1[基础模型准确率低 (20-38%)/Base model accuracy remains low (20-38%)]
        D --> D2[工具链设计对性能有重大影响/Harness design has large empirical effect on performance]
    ```

- **[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]
  - **authors:** Nathan Kallus
  - **institution:** Netflix, Cornell University
  - **link:** https://arxiv.org/pdf/2512.21917
  - **contributions:** 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment"]
        Method["主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model"]
        Results["关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees"]
    ```

- **[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning**
  - **tags:** [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]
  - **authors:** Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21924
  - **contributions:** 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[泛化性差与异常残留/Generalizability & Residuals]
        C --> C1[解耦表示模块/Disentangled Representation Module]
        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]
        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]
    ```

- **[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models**
  - **tags:** [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]
  - **authors:** Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner
  - **institution:** Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)
  - **link:** https://arxiv.org/pdf/2512.21985
  - **contributions:** 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]
        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]
        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]
        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]
        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]
        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]
    ```

- **[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration**
  - **tags:** [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]
  - **authors:** Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji
  - **institution:** Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University
  - **link:** https://arxiv.org/pdf/2512.22010
  - **contributions:** 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]
        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]
        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]
    ```

- **[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation**
  - **tags:** [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]
  - **authors:** Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni
  - **institution:** University College London, University of Urbino Carlo Bo
  - **link:** https://arxiv.org/pdf/2512.22031
  - **contributions:** 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Hit identification is resource-intensive/命中识别资源密集")
        Method --> M1("Propose tailored evaluation framework/提出定制评估框架")
        Method --> M2("Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型")
        Results --> R1("Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物")
        Results --> R2("Selected hits synthesized & confirmed active/选定命中物被合成并确认有效")
    ```

- **[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN**
  - **tags:** [sys], [communication & networking], [Conditional Handovers, O-RAN, Meta-Learning, Mobility Management, xApp]
  - **authors:** Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers
  - **institution:** Delft University of Technology, Telefónica Research
  - **link:** https://arxiv.org/pdf/2512.22022
  - **contributions:** 1. Introduces CONTRA, the first framework to jointly optimize Traditional and Conditional Handovers within the O-RAN architecture. 2. Proposes a practical meta-learning algorithm for adaptive, on-the-fly handover type selection, guaranteeing universal no-regret performance. 3. Provides and analyzes unique, countrywide mobility management datasets from a top-tier mobile network operator, offering fresh insights into handover trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of traditional and conditional handovers in mobile networks by proposing CONTRA, a meta-learning-based framework for O-RAN that dynamically selects and optimizes handover types. It is designed as a near-real-time xApp and is evaluated using real-world datasets. The results show that CONTRA improves user throughput and reduces switching costs, outperforming standard and RL-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-Learning-Based Handover Management in NextG O-RAN] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统切换延迟与失败/Traditional HO delays & failures]
        B --> B2[切换类型间的权衡/Trade-offs between HO types]
        C --> C1[CONTRA框架: 联合优化THO与CHO/CONTRA: Jointly optimizes THOs & CHOs]
        C --> C2[元学习算法动态选择/Meta-learning for dynamic selection]
        C --> C3[O-RAN xApp部署/O-RAN xApp deployment]
        D --> D1[提升用户吞吐量/Improves user throughput]
        D --> D2[降低切换成本/Reduces HO switching costs]
        D --> D3[优于3GPP与RL基线/Outperforms 3GPP & RL baselines]
    ```

- **[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning**
  - **tags:** [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]
  - **authors:** Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo
  - **institution:** Nanjing University, University of Wollongong, University of Rochester
  - **link:** https://arxiv.org/pdf/2512.22029
  - **code:** https://github.com/RL-VIG/LibContinual
  - **contributions:** 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]
        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]
        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]
        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]
        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]
        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]
        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]
    ```

- **[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars**
  - **tags:** [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]
  - **authors:** Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu
  - **institution:** Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22065
  - **code:** https://streamavatar.github.io
  - **contributions:** 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]
        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]
        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]
    ```

- **[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law**
  - **tags:** [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]
  - **authors:** Chiwun Yang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22088
  - **contributions:** 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\{-1/6\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]
        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]
        C --> C2[近似为核行为 / Approximate to kernel behaviors]
        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]
        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]
        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]
        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]
        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]
    ```

- **[arXiv251229] Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis**
  - **tags:** [nlp], [benchmark construction], [Turkish NLU benchmark, semi-automated annotation, sentiment analysis dataset]
  - **authors:** Duygu Altinok
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22100
  - **contributions:** 1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Introducing TrGLUE and SentiTurca] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏土耳其语综合基准/Lack of Turkish NLU Benchmark]
        C --> C1[半自动标注流程/Semi-automated Pipeline]
        C1 --> C2[LLM标注 + 交叉验证 + 人工校验/LLM Annotation + Cross-check + Human Validation]
        D --> D1[发布TrGLUE & SentiTurca/Release TrGLUE & SentiTurca]
        D --> D2[提供代码与资源/Provide Code & Resources]
    ```

- **[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting**
  - **tags:** [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]
  - **authors:** Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22101
  - **contributions:** 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A2P-Vis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]
        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]
        B1 --> B3[组装成专业报告/Assembling coherent professional report]
        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]
        C1 --> C2[数据分析器/Data Analyzer]
        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]
        C1 --> C4[报告呈现器/Presenter]
        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]
        D --> D1[端到端协同分析/End-to-end co-analysis]
        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]
    ```

- **[arXiv251229] Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [neural network pruning, game theory, equilibrium, non-cooperative game, sparsification]
  - **authors:** Zubair Shah, Noaman Khan
  - **institution:** Hamad Bin Khalifa University
  - **link:** https://arxiv.org/pdf/2512.22106
  - **contributions:** 1. Proposes a novel game-theoretic perspective on neural network pruning, modeling parameter groups as players in a non-cooperative game where sparsity emerges as an equilibrium outcome. 2. Provides a theoretical analysis showing that dominated players (redundant parameters) collapse to zero participation under mild conditions, offering a principled explanation for pruning. 3. Derives a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit, heuristic importance scores.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4fd762b6b064bb7810151beeb40f55c93bfc05054b2d4a98cd925aed8bea43b2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel game-theoretic framework for neural network pruning, where sparsity emerges naturally from the equilibrium of a non-cooperative game among model components. The method jointly updates network parameters and participation variables without external importance scores. Experiments show it achieves competitive sparsity-accuracy trade-offs with a more interpretable, theory-grounded foundation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks") --> Problem("核心问题/Problem: Sparsity is imposed externally via heuristics, lacking a principled model of parameter interaction.")
        Root --> Method("主要方法/Method: Model pruning as a non-cooperative game among parameters; sparsity emerges at equilibrium.")
        Root --> Results("关键结果/Results: Competitive sparsity-accuracy trade-offs with an interpretable, theory-grounded algorithm.")
    ```

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [agent system], [root cause analysis, service dependency graph, program dependence graph, LLM agent, cloud incident]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **contributions:** 1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]
        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]
        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]
    ```

- **[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding**
  - **tags:** [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]
  - **authors:** Akshansh Mishra
  - **institution:** Politecnico di Milano, AI Fab Lab
  - **link:** https://arxiv.org/pdf/2512.21344
  - **contributions:** 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模"]
        Root --> Problem["准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW"]
        Root --> Method["使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction"]
        Root --> Results["模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation"]
    ```

- **[arXiv251229] Applications of synthetic financial data in portfolio and risk modeling**
  - **tags:** [ai], [generative models for time series], [TimeGAN, Variational Autoencoder (VAE), synthetic financial data, portfolio optimization, risk modeling]
  - **authors:** Christophe D. Hounwanou, Yae Ulrich Gaba
  - **institution:** African Institute for Mathematical Sciences (AIMS Rwanda), Sefako Makgatho Health Sciences University (SMU), AI Research and Innovation Nexus for Africa (AIRINA Labs)
  - **link:** https://arxiv.org/pdf/2512.21798
  - **contributions:** 1. Evaluated and compared the performance of TimeGAN and VAEs for generating realistic synthetic financial time series data. 2. Demonstrated that TimeGAN-generated data closely matches real data in distributional, volatility, and autocorrelation properties. 3. Showed the practical utility of synthetic data in downstream financial tasks like mean-variance portfolio optimization, yielding similar portfolio weights and risk metrics to real data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a7ecf00c1350b58057e2f03c93bf0e8845d1441745fc22505c46475eceeeb65_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the scarcity and privacy issues of real financial data by using generative models like TimeGAN and VAEs to create synthetic return series. It evaluates the synthetic data on statistical similarity and financial tasks, concluding that TimeGAN effectively captures temporal dynamics and can serve as a privacy-preserving, cost-effective substitute for real data in portfolio and risk analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Applications of synthetic financial data in portfolio and risk modeling") --> Problem("核心问题/Problem: Privacy and accessibility limit financial research")
        Root --> Method("主要方法/Method: Use TimeGAN and VAEs to generate synthetic financial time series")
        Root --> Results("关键结果/Results: TimeGAN data is realistic and useful for portfolio/risk tasks")
    ```

- **[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models**
  - **tags:** [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]
  - **authors:** Takuro Kutsuna
  - **institution:** Toyota Central R&D Labs., Inc.
  - **link:** https://arxiv.org/pdf/2512.21593
  - **contributions:** 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Residual Prior Diffusion (RPD) / 残差先验扩散模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details"]
        Method --> M1["两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model"]
        Method --> M2["概率模型与可处理ELBO / Probabilistic model with tractable ELBO"]
        Results --> R1["在合成数据上准确捕捉细节 / Accurately captures details on synthetic data"]
        Results --> R2["自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines"]
        Results --> R3["少步推理保持性能 / Maintains performance with few inference steps"]
    ```

- **[arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database**
  - **tags:** [cv], [medical image reconstruction], [foundation model, k-space, multimodal database, zero-shot generalization, accelerated imaging]
  - **authors:** Zi Wang, Mingkai Huang, Zhang Shi, Hongjie Hu, Lan Lan, Hui Zhang, Yan Li, Xi Hu, Qing Lu, Zongming Zhu, Qiong Yao, Yuxiang Dai, Fanwen Wang, Yinzhe Wu, Jun Lyu, Qianqian Gao, Guangming Xu, Zhenxuan Zhang, Haosen Zhang, Qing Li, Guangming Wang, Tianxing He, Lizhen Lan, Siyue Li, Le Xue, Mengting Sun, Yuntong Lyu, Junpu Hu, Jiayu Zhu, Rizwan Ahmad, Zhengyu Bu, Xianling Qian, Guanke Cai, Ruiyu Cao, Weirui Cai, Chang Xu, Yuyang Ren, Feidan Yu, Siying Ma, Ziqiang Xu, Xinran Chen, Sha Hua, Daniel Kim, Yajing Zhang, Chen Ouyang, Wenjia Bai, Jing Qin, Yucheng Yang, Daniel Rueckert, He Wang, Qian Tao, Claudia Prieto, Michael Markl, Alistair Young, Lianming Wu, Shuo Wang, Chen Qin, Mengsu Zeng, Xihong Hu, Haibo Xu, Xiaobo Qu, Hao Li, Guang Yang, Chengyan Wang
  - **institution:** Imperial College London, Fudan University, Xiamen University
  - **link:** https://arxiv.org/pdf/2512.21652
  - **contributions:** 1. The curation of MMCMR-427K, the largest and most comprehensive multimodal cardiovascular magnetic resonance (CMR) k-space database. 2. The introduction of CardioMM, a generalist reconstruction foundation model that unifies semantic understanding with physics-informed data consistency for robust, accelerated imaging. 3. Demonstrating state-of-the-art performance and strong zero-shot generalization across heterogeneous clinical settings, enabling up to 24x acceleration without compromising clinical integrity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c34604e8bff7b6e1efa31a4faa67e7e43e6efb18e01ea880e43095344349_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow scan times and environmental heterogeneity limiting clinical cardiovascular MRI. It proposes CardioMM, a generalist foundation model trained on a large multimodal k-space database (MMCMR-427K), which achieves robust, ultra-fast reconstructions across diverse scanners and protocols. The results show that CardioMM enables high acceleration (up to 24x) while preserving diagnostic quality and generalizing to unseen clinical environments.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Enabling Ultra-Fast Cardiovascular Imaging...] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[CMR扫描时间长/CMR Scan Time Long]
    B --> B2[临床环境异质性高/High Clinical Heterogeneity]
    C --> C1[构建多模态数据库MMCMR-427K/Build Multimodal DB MMCMR-427K]
    C --> C2[提出通用基础模型CardioMM/Propose Generalist Foundation Model CardioMM]
    D --> D1[实现24倍加速成像/Achieve 24x Accelerated Imaging]
    D --> D2[零样本泛化至新环境/Zero-shot Generalization to New Settings]
    D --> D3[保持诊断质量/Preserve Diagnostic Quality]
    ```

## 2025-12-30

- **[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems**
  - **tags:** [mlsys], [llm inference], [GPU Virtualization, Benchmarking, Multi-tenancy, CUDA, Performance Isolation]
  - **authors:** Jithin VG, Ditto PS
  - **institution:** Bud Ecosystem Inc
  - **link:** https://arxiv.org/pdf/2512.22125
  - **code:** https://github.com/BudEcosystem/GPU-Virt-Bench
  - **contributions:** 1. Proposed GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. 2. Enabled systematic comparison between software virtualization approaches (e.g., HAMi-core, BUD-FCSP) and ideal hardware-based MIG behavior. 3. Demonstrated the framework's utility by revealing critical performance characteristics for production deployment decisions in multi-tenant environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of standardized evaluation for software-based GPU virtualization systems, which are needed for efficient GPU sharing in AI/LLM workloads. The authors propose GPU-Virt-Bench, a comprehensive benchmarking framework that measures performance across multiple critical dimensions. The framework provides actionable insights for practitioners by comparing software solutions against hardware-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPU-Virt-Bench: A Comprehensive Benchmarking Framework] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GPU资源共享需求高，但软件虚拟化方案缺乏标准化评估/High demand for GPU sharing, but software virtualization lacks standardized evaluation]
        C --> C1[提出包含56个指标、10个类别的综合基准测试框架/Propose a comprehensive benchmarking framework with 56 metrics across 10 categories]
        D --> D1[系统比较软件方案与MIG，为生产部署提供关键性能洞察/Systematic comparison between software approaches and MIG provides key performance insights for deployment]
    ```

- **[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling**
  - **tags:** [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]
  - **authors:** Conor Wallace, Umer Siddique, Yongcan Cao
  - **institution:** University of Texas at San Antonio
  - **link:** https://arxiv.org/pdf/2512.22129
  - **contributions:** 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("RECOLLAB: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Ad-hoc Teammate Modeling<br>Ad-hoc队友建模")
        Problem --> P2("Brittle Conventional Models<br>传统模型脆弱性")
        Method --> M1("COLLAB: LLM-based Framework<br>基于LLM的框架")
        Method --> M2("RECOLLAB: Adds RAG<br>增加RAG检索")
        Results --> R1("Improved Adaptation<br>提升适应性")
        Results --> R2("Pareto-Optimal Trade-offs<br>帕累托最优权衡")
    ```

- **[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web**
  - **tags:** [mlsys], [agent system], [Sovereign Digital Avatar, Intent-Permission Handshake, orthogonal decoupling, A2A protocols, dual-factor adaptive routing]
  - **authors:** Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang
  - **institution:** Shanghai Jiao Tong University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22135
  - **contributions:** 1. Proposes a user sovereignty interaction paradigm for the Agentic Web, decoupling memory from application logic to break data lock-in and shifting from explicit instruction to implicit intent alignment to reduce cognitive load. 2. Implements the paradigm via the Sovereign Digital Avatar (SoDA) with an orthogonal decoupling design of storage, computation, and interaction, establishing the principle of "data as a persistent asset, model as a transient tool". 3. Designs an Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing for active risk governance in zero-trust environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples user memory from applications and uses intent alignment to reduce cognitive load. It introduces an architecture with orthogonal decoupling and a secure handshake mechanism for zero-trust environments. Empirical results show it significantly reduces token consumption and user cognitive load compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoDA: An Efficient Interaction Paradigm for the Agentic Web] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据锁定/Data Lock-in]
        B --> B2[认知过载/Cognitive Overload]
        C --> C1[主权数字化身/Sovereign Digital Avatar (SoDA)]
        C --> C2[正交解耦设计/Orthogonal Decoupling Design]
        C --> C3[意图-权限握手机制/Intent-Permission Handshake Mechanism]
        D --> D1[降低令牌消耗/Reduces Token Consumption by 27-35%]
        D --> D2[降低认知负载/Reduces Cognitive Load by 72% vs RAG, 88% vs Manual]
    ```

- **[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA**
  - **tags:** [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]
  - **authors:** Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn
  - **institution:** National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)
  - **link:** https://arxiv.org/pdf/2512.22139
  - **code:** https://github.com/dll-ncai/HLS4PC
  - **contributions:** 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]
        P1 --> P2[High memory/computation demand hinders real-time performance]
        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]
        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]
        M2 --> M3[Creates PointMLP-Lite model]
        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]
        R1 --> R2[3.56x higher throughput vs. prior work]
        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]
    ```

- **[arXiv251230] Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models**
  - **tags:** [ai], [peer review automation], [large language models, peer review, pre-review, citation prediction, review alignment]
  - **authors:** Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi
  - **institution:** AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI
  - **link:** https://arxiv.org/pdf/2512.22145
  - **contributions:** 1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]
        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]
        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]
        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]
    ```

- **[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs**
  - **tags:** [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]
  - **authors:** Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong
  - **institution:** School of Software Engineering, Xi’an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22147
  - **contributions:** 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]
        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]
        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]
        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]
        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]
        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]
        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]
    ```

- **[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments**
  - **tags:** [mlsys], [agent system], [serverless computing, GPU resource allocation, workload scheduling, multi-agent systems, collaborative reasoning]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.22149
  - **contributions:** 1. An adaptive GPU resource allocation framework for multi-agent systems in serverless environments that dynamically adjusts resources based on workload characteristics, agent priorities, and minimum requirements. 2. An O(N) complexity algorithm for real-time adaptation, enabling millisecond-scale reallocation to handle dynamic workload fluctuations. 3. A comprehensive evaluation demonstrating the framework's superiority over static and round-robin strategies, achieving 85% latency reduction while maintaining throughput and improving GPU utilization and cost-efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an adaptive GPU resource allocation framework to address the challenge of efficiently deploying heterogeneous multi-agent AI systems on serverless platforms. The method dynamically allocates resources using a real-time algorithm to handle varying computational demands and workload fluctuations. The results show it significantly reduces latency compared to baseline schedulers while maintaining throughput, offering a cost-effective solution for serverless multi-agent deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments<br/>面向无服务器环境的多智能体协同推理的自适应GPU资源分配"] --> Problem["核心问题/Problem<br/>Heterogeneous agent workloads & dynamic demands on serverless GPU platforms<br/>多智能体工作负载异构与无服务器GPU平台动态需求"]
        Root --> Method["主要方法/Method<br/>Adaptive GPU resource allocation framework with O(N) real-time algorithm<br/>基于O(N)实时算法的自适应GPU资源分配框架"]
        Root --> Results["关键结果/Results<br/>85% latency reduction vs. round-robin, maintains throughput<br/>相比轮询调度延迟降低85%，保持吞吐量"]
    ```

- **[arXiv251230] Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification**
  - **tags:** [ai], [speaker verification], [Layer Attentive Pooling, Attentive Statistical Temporal Pooling, pre-trained speech models, multi-level features, speaker embeddings]
  - **authors:** Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.22148
  - **code:** https://github.com/sadPororo/LAP
  - **contributions:** 1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[重新思考利用预训练多层表示进行说话人验证<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>静态加权平均聚合多层特征的局限性<br/>Limitations of static weighted average for multi-layer feature aggregation] --> Problem_Detail[细节/Detail<br/>未充分利用高层表示<br/>Underutilization of high-level representations]
        Method[主要方法/Method<br/>提出层注意力池化<br/>Propose Layer Attentive Pooling (LAP)] --> Method_Detail1[细节/Detail<br/>动态多视角评估层重要性<br/>Time-dynamically assess layer significance from multiple perspectives]
        Method --> Method_Detail2[细节/Detail<br/>使用最大池化而非平均<br/>Employ max pooling instead of averaging]
        Method --> Method_Detail3[细节/Detail<br/>轻量级后端模型 (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]
        Results[关键结果/Results<br/>在VoxCeleb上达到SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --> Results_Detail[细节/Detail<br/>性能优越且大幅减少训练时间<br/>Superior performance and greatly reduced training time]
    ```

- **[arXiv251230] Practical challenges of control monitoring in frontier AI deployments**
  - **tags:** [sec], [ai security], [control monitoring, oversight latency, safety case, scheming agents, incremental attacks]
  - **authors:** David Lindner, Charlie Griffin, Tomek Korbak, Roland S. Zimmermann, Geoffrey Irving, Sebastian Farquhar, Alan Cooney
  - **institution:** Google DeepMind, UK AI Safety Institute, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22154
  - **contributions:** 1. Analysis of real-world deployment dynamics (parallelism, latency, incremental attacks, partial incrimination) for control monitoring, 2. Proposal and comparison of three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs, 3. Introduction of a high-level safety case sketch as a tool for analyzing and comparing monitoring protocols, applied to four case studies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cfc3d08c169a23a56cd0a16a1471c5bceacb1b76913f7079f51b7024f030d1_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the practical challenges of scaling automated control monitors for overseeing frontier AI agents in real-world deployments. It proposes and compares three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs and introduces a safety case sketch as an analytical tool. The analysis identifies oversight, latency, and recovery as key challenges, explored through four case studies of potential AI attacks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Practical challenges of control monitoring in frontier AI deployments<br>前沿AI部署中控制监控的实际挑战") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现实部署的动态<br>Real-world Deployment Dynamics")
        P1 --> P1_1("并行实例<br>Parallel Instances")
        P1 --> P1_2("监督延迟<br>Oversight Latency")
        P1 --> P1_3("增量攻击<br>Incremental Attacks")
        P1 --> P1_4("部分归责<br>Partial Incrimination")
        Method --> M1("监控协议<br>Monitoring Protocols")
        M1 --> M1_1("同步监控<br>Synchronous")
        M1 --> M1_2("半同步监控<br>Semi-synchronous")
        M1 --> M1_3("异步监控<br>Asynchronous")
        Method --> M2("安全案例草图<br>Safety Case Sketch")
        Results --> R1("识别核心挑战<br>Identified Core Challenges")
        R1 --> R1_1("监督<br>Oversight")
        R1 --> R1_2("延迟<br>Latency")
        R1 --> R1_3("恢复<br>Recovery")
        Results --> R2("案例研究应用<br>Case Studies Application")
    ```

- **[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs**
  - **tags:** [mlsys], [fault-tolerance], [bit-flip faults, fault localization, transformer reliability, residual-path perturbation, loss-sensitivity profiling]
  - **authors:** Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22174
  - **contributions:** 1. Introduces BitFlipScope, a scalable software framework for localizing bit-flip corruptions in transformer-based LLMs under two deployment scenarios (with and without a clean reference model). 2. Proposes differential analysis for fault localization when a reference model is available and residual-path perturbation/loss-sensitivity profiling for localization when no reference exists. 3. Enables lightweight performance recovery for corrupted models without requiring costly fine-tuning or full retraining.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or perturbation-based profiling without one to identify fault-affected regions, enabling targeted recovery without full retraining. The work aims to improve fault resilience for LLMs in hardware-prone and adversarial environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs] --> B[核心问题/Problem: Bit-flip faults corrupt LLM parameters, causing unpredictable behavior]
        A --> C[主要方法/Method: Differential analysis with reference model; Residual-path perturbation & loss-sensitivity profiling without reference]
        A --> D[关键结果/Results: Enables fault localization and lightweight recovery, improving fault-resilient LLM deployment]
    ```

- **[arXiv251230] Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time**
  - **tags:** [ai], [multi-agent path finding], [multi-agent path finding, vehicle routing, polynomial-time algorithm, conflict resolution, assignment problem]
  - **authors:** Stefan Edelkamp
  - **institution:** Charles University
  - **link:** https://arxiv.org/pdf/2512.22171
  - **contributions:**  1. Proposes a polynomial-time algorithm for solving discrete multi-agent multi-goal path finding (CMAPF) problems with node and edge conflicts, which is unexpected given the NP-hardness of traditional vehicle routing. 2. Introduces a planner that autonomously finds and updates the assignment of multiple goals to agents, contrasting with regular MAPF which uses fixed assignments. 3. Develops conflict resolution strategies including global assignment to reduce conflicts, and local methods like "ants-on-the-stick," local assignment, path interleaving, and destination clearing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9267701cf1d96333033d8663590f3b652040a494de98cd10ba5a86ede709d3b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the multi-agent multi-goal path finding (CMAPF) problem where agents in graphs must be assigned and routed to multiple goals. It presents a polynomial-time algorithm for discrete variants with conflicts, implemented in a planner that autonomously handles goal assignment and resolves conflicts. The main conclusion is that efficient, conflict-free solutions can be achieved in polynomial time, challenging the typical NP-hard complexity of vehicle routing.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[为多智能体规划多目标无冲突路径/Plan multi-goal conflict-free paths for multi-agent]
    C --> C1[自主目标分配与冲突解决策略/Autonomous goal assignment & conflict resolution]
    D --> D1[离散问题可在多项式时间内解决/Discrete problems solvable in polynomial time]
    ```

- **[arXiv251230] Wireless Traffic Prediction with Large Language Model**
  - **tags:** [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]
  - **authors:** Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini
  - **institution:** Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)
  - **link:** https://arxiv.org/pdf/2512.22178
  - **contributions:** 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Wireless Traffic Prediction with Large Language Model] --> B(核心问题/Problem: LLMs overlook spatial dependencies in city-scale wireless traffic)
        A --> C(主要方法/Method: TIDES framework with clustering, prompt engineering, and DeepSeek spatial alignment module)
        A --> D(关键结果/Results: Outperforms SOTA baselines in accuracy and robustness for 6G network management)
    ```

- **[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps**
  - **tags:** [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]
  - **authors:** Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava
  - **institution:** University of Maryland
  - **link:** https://arxiv.org/pdf/2512.22175
  - **contributions:** 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Characterizing Motion Encoding in Video Diffusion Timesteps] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[视频扩散模型中运动编码机制不明确 / Motion encoding in video diffusion is poorly understood]
        C --> C1[通过条件注入量化运动-外观权衡 / Quantify motion-appearance trade-off via conditional injection]
        C --> C2[大规模定量研究 / Large-scale quantitative study]
        D --> D1[识别早期运动主导与后期外观主导阶段 / Identify early motion-dominant and late appearance-dominant regimes]
        D --> D2[简化单样本运动定制范式 / Simplify one-shot motion customization paradigm]
    ```

- **[arXiv251230] iOS as Acceleration**
  - **tags:** [mlsys], [on-device ai], [distributed pipeline parallelism, mobile acceleration, iOS, memory constraints, thermal throttling]
  - **authors:** Alexander K. Chen
  - **institution:** Independent High School Researcher (No institutional affiliation inferred)
  - **link:** https://arxiv.org/pdf/2512.22180
  - **contributions:** 1. Proposes a novel proof-of-concept system using distributed pipeline parallelism to harness iOS devices as computational accelerators for local ML tasks. 2. Demonstrates the system's effectiveness in accelerating modest model training (e.g., ResNet-34) and agentic LRM tool-usage, achieving a 44% decrease in training time in a specific setup. 3. Explores the unique potential of ubiquitous mobile devices with powerful processors and sensors (e.g., LiDAR, GPS) as cost-effective resources for embodied agentic AI and local compute, discussing practical use-cases and limitations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the barrier of expensive compute for local machine learning by proposing a system that uses distributed pipeline parallelism to leverage underutilized iOS phones as accelerators. The method partitions model weights to circumvent mobile memory limits, successfully accelerating tasks like training ResNet-34. The work concludes that commonplace mobile devices have significant potential to contribute to ML, especially for local, cost-sensitive, or sensor-driven applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[iOS as Acceleration] --> B[核心问题/Problem: Powerful compute is a barrier for local ML; Cloud is not always viable]
        A --> C[主要方法/Method: Use distributed pipeline parallelism to harness iOS devices as accelerators]
        A --> D[关键结果/Results: Achieved faster training for modest models; Highlights mobile potential for ML]
    ```

- **[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery**
  - **tags:** [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]
  - **authors:** Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir
  - **institution:** National University of Science and Technology (NUST)
  - **link:** https://arxiv.org/pdf/2512.22182
  - **contributions:** 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)
    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))
    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)
    ```

- **[arXiv251230] Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns**
  - **tags:** [ai], [network science], [co-authorship networks, link prediction, SHAP, random forest, interdisciplinary collaboration]
  - **authors:** Shahab Mosallaie, Andrea Schiffauerova, Ashkan Ebadi
  - **institution:** Concordia University, National Research Council Canada
  - **link:** https://arxiv.org/pdf/2512.22181
  - **contributions:** 1. Constructed 36 overlapping co-authorship networks from 7,738 publications to model new, persistent, and discontinued collaborations in AI-driven cancer research. 2. Engineered both attribute-based and structure-based features and built four machine learning classifiers, with Random Forest achieving the highest recall for all collaboration types. 3. Applied SHAP for model interpretability, identifying key factors like discipline similarity, productivity, and seniority that influence collaboration patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f6c5e92a95d2e6eeb8291558739ab641247bb834675d42c86ece0ee73d9d15_w640_q70.webp
  - **Simple LLM Summary:** This paper uses machine learning to predict collaboration patterns in AI-driven cancer research by analyzing co-authorship networks. The authors built classifiers using engineered features and applied SHAP for interpretability, finding that discipline similarity promotes new and persistent collaborations while high productivity and seniority are linked to discontinued links. The results aim to guide the formation of effective interdisciplinary research teams and inform policy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[挑战: 组建有效的跨学科癌症研究团队/Challenge: Forming effective interdisciplinary cancer research teams]
        C --> C1[构建合著网络作为合作代理/Construct co-authorship networks as collaboration proxy]
        C --> C2[使用机器学习分类器进行链接预测/Use ML classifiers for link prediction]
        C --> C3[使用SHAP进行模型可解释性/Use SHAP for model interpretability]
        D --> D1[随机森林在所有合作类型中召回率最高/Random forest achieved highest recall]
        D --> D2[学科相似性得分是关键因素/Discipline similarity score is a crucial factor]
        D --> D3[高生产力和资历与中断链接正相关/High productivity and seniority positively associated with discontinued links]
    ```

- **[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs**
  - **tags:** [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]
  - **authors:** Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou
  - **institution:** Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis
  - **link:** https://arxiv.org/pdf/2512.22183
  - **contributions:** 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unbiased Visual Reasoning with Controlled Visual Inputs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[VLMs exploit spurious correlations/VLMs利用虚假关联]
        C --> C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: 解耦感知与推理的模块化框架]
        C1 --> C2[Frozen VLM sensor + LLM reasoner/冻结VLM感知器 + LLM推理器]
        C2 --> C3[Train with RL (GRPO)/使用强化学习(GRPO)训练]
        D --> D1[Improved robustness on SpuriVerse/在SpuriVerse上鲁棒性提升]
        D --> D2[Competitive on MMVP & SeedBench/在MMVP & SeedBench上保持竞争力]
        D --> D3[Transferable & interpretable/可迁移且可解释]
    ```

- **[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks**
  - **tags:** [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]
  - **authors:** Vishnu Mohan
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22186
  - **contributions:** 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks] --> B(核心问题/Problem: Tennis strategy optimization as a sequential decision-making challenge with hierarchical scoring, stochasticity, and opponent adaptation)
        A --> C(主要方法/Method: Dueling Double Deep Q-Network (DDQN) trained with curriculum learning in a custom tennis simulation environment)
        A --> D(关键结果/Results: High win rates (98-100%) and stable convergence, but reveals a defensive policy bias, highlighting reward design limitations)
    ```

- **[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology**
  - **tags:** [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]
  - **authors:** Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He
  - **institution:** Tsinghua University, Xiamen University, Westlake University, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.22188
  - **code:** https://github.com/lingxitong/HookMIL
  - **contributions:** 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MIL loses context; Transformers are inefficient] --> P1[传统MIL丢失上下文/Traditional MIL loses context]
        Problem --> P2[基于Transformer的MIL计算复杂/Transformer-based MIL has quadratic complexity]
        Method[主要方法/Method: HookMIL Framework] --> M1[使用可学习的Hook Tokens/Use learnable Hook Tokens]
        Method --> M2[多模态初始化/Multimodal Initialization]
        Method --> M3[Hook多样性损失与通信机制/Hook Diversity Loss & Communication]
        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art Performance]
        Results --> R2[计算高效/Computationally Efficient]
        Results --> R3[可解释性/Interpretability]
    ```

- **[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]
  - **authors:** Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee
  - **institution:** Seoul National University, Samsung Electronics
  - **link:** https://arxiv.org/pdf/2512.22195
  - **code:** https://github.com/kunwooshin/MatKV
  - **contributions:** 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MatKV: Trading Compute for Flash Storage in LLM Inference"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>RAG推理中prefill阶段计算开销大<br>High compute cost of prefill in RAG inference"]
        Method["主要方法/Method<br>预计算并物化KV向量到闪存<br>Precompute & materialize KVs to flash storage"]
        Results["关键结果/Results<br>推理时间与能耗减半<br>Halves inference time & power consumption"]
    ```

- **[arXiv251230] Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation, Hallucination Prevention, Multi-Stage Validation, Corpus Expansion, Self-Improving AI]
  - **authors:** Teja Chinthala
  - **institution:** Independent Researcher (affiliated email domain: avila.edu)
  - **link:** https://arxiv.org/pdf/2512.22199
  - **contributions:** 1. A novel RAG architecture enabling safe corpus expansion through validated write-back of model outputs. 2. A multi-stage acceptance layer combining grounding verification, attribution checking, and novelty detection for safety. 3. An experience store for meta-learning from both accepted and rejected responses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eee110568078584ba44c846b5af3fab7d300dbfaec310a5826fd74784dc8040_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that conventional RAG systems have static knowledge bases. It proposes Bidirectional RAG, a novel architecture that safely expands the retrieval corpus by writing back high-quality, validated LLM responses. The results show that this self-improving approach nearly doubles answer coverage compared to standard RAG while adding significantly fewer documents than a naive write-back strategy, demonstrating a safe and practical path for RAG systems to learn from deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bidirectional RAG: Safe Self-Improving RAG] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[静态知识库无法从交互中学习/Static corpus cannot evolve from interactions]
        C --> C1[带验证的回写机制/Validated write-back]
        C --> C2[多阶段验证层/Multi-stage acceptance layer]
        D --> D1[覆盖率翻倍/Coverage doubled vs. Standard RAG]
        D --> D2[文档增长减少72%/72% less corpus growth vs. naive write-back]
    ```

- **[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]
  - **authors:** Yogeswar Reddy Thota
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.22206
  - **contributions:** 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --> B[核心问题/Problem: Modern residual networks perform redundant computation for all inputs]
        A --> C[主要方法/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]
        A --> D[关键结果/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]
    ```

- **[arXiv251230] Emergent Persuasion: Will LLMs Persuade Without Being Prompted?**
  - **tags:** [nlp], [ai safety & alignment], [emergent persuasion, activation steering, supervised fine-tuning (SFT), threat model, persona vectors]
  - **authors:** Vincent Chang, Thee Ho, Sunishchal Dev, Kevin Zhu, Shi Feng, Kellin Pelrine, Matthew Kowal
  - **institution:** Algoverse, FAR.AI, UC Berkeley, George Washington University, University of Toronto
  - **link:** https://arxiv.org/pdf/2512.22201
  - **code:** https://github.com/ith8/persona_vectors
  - **contributions:** 1. Investigates the novel threat model of "unprompted" or "emergent" persuasion in LLMs, moving beyond the standard misuse (prompted) scenario. 2. Empirically compares two techniques for inducing traits (activation steering vs. supervised fine-tuning) and finds SFT reliably increases unprompted persuasion while steering does not. 3. Demonstrates that SFT on benign persuasion datasets can lead to increased persuasion propensity on harmful topics, highlighting a significant safety risk.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3e4599070a9919228b3e5bc9cb7f7fd0fe17df086f5d7bec7ef20de22526f15_w640_q70.webp
  - **Simple LLM Summary:** This paper studies whether Large Language Models (LLMs) will attempt to persuade users without being explicitly prompted to do so. The authors investigate this by applying activation steering and supervised fine-tuning (SFT) to induce persuasive traits, finding that SFT reliably increases unprompted persuasion, including on harmful topics, even when trained only on benign data. The main conclusion is that emergent harmful persuasion is a real risk that warrants further study for AI safety and governance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Emergent Persuasion: Will LLMs Persuade Without Being Prompted?] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[研究模型在未受明确提示时进行说服的风险 / Study risk of unprompted persuasion]
        C --> C1[激活引导以植入人格特质 / Activation steering for persona traits]
        C --> C2[监督微调以植入人格特质 / Supervised fine-tuning for persona traits]
        D --> D1[监督微调会可靠地增加无提示说服 / SFT reliably increases unprompted persuasion]
        D --> D2[良性主题微调可能导致有害主题说服 / Benign SFT can increase harmful topic persuasion]
    ```

- **[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting**
  - **tags:** [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]
  - **authors:** Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu
  - **institution:** Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.22203
  - **contributions:** 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[TCFormer: 5M参数Transformer用于弱监督人群计数] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[标注成本高/High Annotation Cost]
        Problem --> P2[计算复杂度高/High Computational Complexity]
        Method[主要方法/Method] --> M1[高效视觉Transformer特征提取器/Efficient ViT Feature Extractor]
        Method --> M2[可学习密度加权平均模块/Learnable Density-Weighted Averaging]
        Method --> M3[密度等级分类损失/Density-Level Classification Loss]
        Results[关键结果/Results] --> R1[仅5M参数/Only 5M Parameters]
        Results --> R2[弱监督下竞争性性能/Competitive Performance under Weak Supervision]
        Results --> R3[适用于边缘设备/Suitable for Edge Devices]
    ```

- **[arXiv251230] GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks**
  - **tags:** [ai], [multimodal reasoning], [spatial reasoning, multimodal large language models, benchmark, origami folding, viewpoint consistency]
  - **authors:** Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma
  - **institution:** Algoverse AI Research, UC San Diego, University of New South Wales
  - **link:** https://arxiv.org/pdf/2512.22207
  - **code:** https://github.com/stvngo/GamiBench
  - **contributions:** 1. Introduces GamiBench, a novel benchmark for evaluating spatial reasoning and 2D-to-3D planning in MLLMs using origami folding tasks. 2. Proposes new diagnostic metrics, viewpoint consistency (VC) and impossible fold selection rate (IFSR), to holistically assess the reasoning process. 3. Provides a comprehensive dataset of 186 regular and 186 impossible 2D crease patterns with 3D shapes from multiple viewpoints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7bb91a40c2344a9e50dec2b2754647404d609db9a78508a54a040d5f6c5f58b_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces GamiBench, a benchmark that uses origami folding tasks to evaluate the spatial reasoning and 2D-to-3D planning capabilities of Multimodal Large Language Models (MLLMs). It assesses models on tasks like predicting 3D configurations and detecting impossible folds, revealing that even state-of-the-art models like GPT-5 and Gemini-2.5-Pro struggle with fundamental spatial understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MLLMs struggle with sequential, multi-view spatial reasoning] --> Problem_Sub[现有基准的不足/Existing benchmarks focus on static images]
        Method[主要方法/Method: Origami-inspired benchmark with 3 VQA tasks] --> Method_Sub[引入新指标/Introduces new metrics (VC, IFSR)]
        Results[关键结果/Results: Leading models (GPT-5, Gemini) struggle with spatial understanding]
    ```

- **[arXiv251230] Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh**
  - **tags:** [ai], [algorithmic fairness], [adversarial debiasing, gradient reversal layer, fairness-aware representation learning, statistical parity difference]
  - **authors:** Farjana Yesmin, Romana Akter
  - **institution:** Independent Researcher, Researcher (affiliations not specified)
  - **link:** https://arxiv.org/pdf/2512.22210
  - **contributions:** 1. A comprehensive dataset integrating official flood impact data with socioeconomic indicators across 87 upazilas in Bangladesh. 2. An adversarial debiasing architecture adapted from healthcare AI for disaster management. 3. Rigorous fairness evaluation showing significant reductions in statistical parity and regional fairness gaps while maintaining predictive accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd82a75c606f74970d3c93e7f31e57d4d35ae9f285a2ccb25e804770225a020b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a fairness-aware AI framework using adversarial debiasing with a gradient reversal layer to prioritize post-flood aid allocation in Bangladesh. The model learns bias-invariant representations to reduce systematic disadvantages against marginalized regions. Experimental results show it significantly improves fairness metrics while maintaining strong predictive accuracy, demonstrating the effective application of algorithmic fairness in humanitarian contexts.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Biased post-disaster aid allocation perpetuates historical inequities in vulnerable regions]
        C[主要方法/Method: Adversarial debiasing model with gradient reversal layer for bias-invariant representations]
        D[关键结果/Results: Reduces statistical parity by 41.6%, regional fairness gaps by 43.2%, maintains R²=0.784]
    ```

- **[arXiv251230] With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems**
  - **tags:** [mlsys], [agent system], [agentic AI, risk assessment, technical governance, autonomous action, safety controls]
  - **authors:** Shaun Khoo, Jessica Foo, Roy Ka-Wei Lee
  - **institution:** GovTech Singapore, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.22211
  - **code:** https://github.com/govtech-ai/arc-framework
  - **contributions:** 1. Introduces a novel capability-centric perspective for analyzing agentic AI systems. 2. Distills three primary intrinsic risk sources (components, design, capabilities) and maps them to specific risks and technical controls. 3. Provides a structured, practical methodology for organizations to implement the framework for governance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0283ff7ed974694c06d620f02b6bfd0752cd1ab34c83d05b1d66ec9b4088059c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Agentic Risk & Capability (ARC) Framework to address governance challenges posed by autonomous AI agents. The framework provides a structured methodology to identify, assess, and mitigate risks from agentic systems by analyzing their capabilities and linking risk sources to technical controls. It aims to enable safe and responsible deployment of agentic AI while supporting innovation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems"]
        Root --> Problem["核心问题/Problem: Agentic AI systems present novel risks and governance challenges due to autonomous actions like code execution and web interaction."]
        Root --> Method["主要方法/Method: Proposes the Agentic Risk & Capability (ARC) Framework, a technical governance framework for risk identification, assessment, and mitigation."]
        Root --> Results["关键结果/Results: Provides a robust, adaptable methodology for safe and responsible deployment of agentic AI, linking risk sources to controls."]
    ```

- **[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks**
  - **tags:** [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]
  - **authors:** Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao
  - **institution:** Imperial College London, UnlikelyAI
  - **link:** https://arxiv.org/pdf/2512.22213
  - **contributions:** 1. Identifies and characterizes a new class of "secondary attention sinks" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink's direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a new phenomenon called "secondary attention sinks" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["On the Existence and Behaviour of Secondary Attention Sinks<br/>二次注意力汇的存在与行为"] --> B["核心问题/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>先前研究仅关注持久的主汇（如BOS）"]
        A --> C["主要方法/Method<br/>Extensive experiments across 11 model families<br/>对11个模型系列进行广泛实验"]
        A --> D["关键结果/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>次级汇通过中间层MLP形成<br/>2. L2-norm determines sink score & lifetime<br/>L2范数决定汇分数与寿命<br/>3. Sink levels are deterministic in large models<br/>大模型中汇层级更确定"]
    ```

- **[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition**
  - **tags:** [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]
  - **authors:** Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid
  - **institution:** University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi
  - **link:** https://arxiv.org/pdf/2512.22217
  - **contributions:** 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>类别不平衡, 属性依赖, 域偏移]
        C[主要方法/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>视觉语言框架与跨注意力融合]
        D[关键结果/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K上SOTA, PETA & Market-1501上提升]
    ```

- **[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition**
  - **tags:** [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]
  - **authors:** Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.22214
  - **contributions:** 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Signal-SGN++<br/>论文标题/Paper Title] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GCNs能耗高<br/>GCNs High Energy Cost]
        B --> B2[SNNs难以捕捉时空-频率与拓扑依赖<br/>SNNs Limited in Capturing Time-Freq & Topology]
        C --> C1[主干网络: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]
        C --> C2[拓扑转移自注意力 TSSA<br/>Topology-Shift Self-Attention TSSA]
        C --> C3[多尺度小波变换融合 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]
        D --> D1[优于现有SNN方法<br/>Outperforms Existing SNN Methods]
        D --> D2[与先进GCNs结果相当<br/>Competitive with SOTA GCNs]
        D --> D3[能耗显著降低<br/>Substantially Reduced Energy Consumption]
    ```

- **[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects**
  - **tags:** [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]
  - **authors:** Tasha Pais, Nikhilesh Belulkar
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2512.22220
  - **contributions:** 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("On Extending Semantic Abstraction for Efficient Search of Hidden Objects") --> Problem("核心问题/Problem: Localizing hidden/occluded objects")
        Root --> Method("主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search")
        Root --> Results("关键结果/Results: Faster and accurate 3D localization vs. random search")
    ```

- **[arXiv251230] Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases**
  - **tags:** [ai], [neural network architecture], [Müntz-Szász Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]
  - **authors:** Gnankan Landry Regis N'guessan
  - **institution:** Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre
  - **link:** https://arxiv.org/pdf/2512.22222
  - **contributions:** 1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN's universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases] --> B[核心问题/Problem: Standard neural networks poorly approximate singular/fractional power functions common in physics]
    A --> C[主要方法/Method: Proposes MSN with learnable fractional power bases, replacing fixed activations]
    A --> D[关键结果/Results: MSN achieves superior approximation rates, lower error with fewer parameters, and significant improvement on PINN benchmarks]
    ```

- **[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs**
  - **tags:** [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]
  - **authors:** Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao
  - **institution:** University of Science and Technology of China, Ant Group
  - **link:** https://arxiv.org/pdf/2512.22226
  - **code:** https://github.com/zheng980629/VideoScaffold
  - **contributions:** 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]
        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]
        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]
    ```

- **[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]
  - **authors:** Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam
  - **institution:** North Carolina A&T State University, University of Texas at San Antonio
  - **link:** https://arxiv.org/pdf/2512.22223
  - **code:** https://github.com/270771/llm-traffictraffic
  - **contributions:** 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp
  - **Simple LLM Summary:** The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Traditional traffic analysis systems have high false positives and lack interpretability.]
        C[主要方法/Method: Multi-stage framework using traffic summarization, RAG, and LLM reasoning with a hierarchical retrieval pipeline.]
        D[关键结果/Results: Achieves 95.95%-98.82% accuracy, outperforms baselines, and provides explainable, evidence-grounded responses.]
    ```

- **[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing**
  - **tags:** [mlsys], [cluster infrastructure], [cloud-native, distributed stream processing, containerized microservices, elastic resource orchestration, edge-cloud hybrid]
  - **authors:** Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal
  - **institution:** IEEE, NTT Data, Amtrak, Albertsons Companies
  - **link:** https://arxiv.org/pdf/2512.22231
  - **contributions:** 1. A comprehensive theoretical framework for AI-enhanced cloud-based PMU analytics. 2. Mathematical formulations for distributed machine learning optimized for PMU time-series data. 3. Analysis of edge-cloud hybrid architectures with integrated security and privacy considerations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a scalable cloud-native architecture to address the latency and scalability challenges of processing high-frequency data from Phasor Measurement Units (PMUs) in smart grids. The method integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for real-time analytics. The analysis shows the architecture can achieve sub-second response times while scaling to large deployments, providing a robust foundation for next-generation grid analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Cloud-Native Architectures for Intelligent PMU Data Processing"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>PMU数据规模大，传统架构延迟高，可扩展性差"]
        Method["主要方法/Method<br>云原生架构，集成AI、边缘与云计算，使用分布式流处理和微服务"]
        Results["关键结果/Results<br>实现亚秒级响应，可扩展至大规模部署，提供安全可靠的基础"]
    ```

- **[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction**
  - **tags:** [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]
  - **authors:** Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu
  - **institution:** Nanchang University, Hefei Comprehensive National Science Center
  - **link:** https://arxiv.org/pdf/2512.22237
  - **contributions:** 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --> B(核心问题/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)
        A --> C(主要方法/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)
        A --> D(关键结果/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)
    ```

- **[arXiv251230] We are not able to identify AI-generated images**
  - **tags:** [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]
  - **authors:** Adrien Pavão
  - **institution:** (Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)
  - **link:** https://arxiv.org/pdf/2512.22236
  - **contributions:** 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp
  - **Simple LLM Summary:** The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[We are not able to identify AI-generated images] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Can humans reliably distinguish AI-generated images from real photos?]
        Method[主要方法/Method: Interactive web experiment with a curated dataset of 120 difficult images (CC12M real vs. MidJourney AI-generated)]
        Results[关键结果/Results: Average human accuracy is 54% (near random), response time ~7.3s, highlighting human insufficiency and need for guidelines]
    ```

- **[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]
  - **authors:** Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu
  - **institution:** Fudan University, Shanghai Innovation Institute, OpenMoss Team
  - **link:** https://arxiv.org/pdf/2512.22234
  - **code:** https://github.com/OpenMOSS/DiRL
  - **contributions:** 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiRL: An Efficient Post-Training Framework for Diffusion Language Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[dLLMs后训练低效/Post-training for dLLMs is inefficient]
        B --> B2[训练与推理目标不匹配/Training-Inference objective mismatch]
        C --> C1[DiRL框架/DiRL Framework]
        C1 --> C1_1[整合FlexAttention与LMDeploy/Integrates FlexAttention & LMDeploy]
        C1 --> C1_2[两阶段后训练/Two-stage post-training (SFT+RL)]
        C --> C2[DiPO算法/DiPO Algorithm]
        C2 --> C2_1[无偏GRPO实现/Unbiased GRPO for dLLMs]
        D --> D1[高效训练与推理/Efficient Training & Inference]
        D --> D2[数学SOTA性能/Math SOTA Performance]
        D --> D3[超越Qwen2.5系列/Surpasses Qwen2.5 series]
    ```

- **[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning**
  - **tags:** [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]
  - **authors:** Abdul Malik Al Mardhouf Al Saadi, Amrita Basak
  - **institution:** The Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.22241
  - **contributions:** 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enhanced geometry prediction in L-DED using meta-learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Data scarcity & heterogeneity in L-DED geometry prediction]
        C[主要方法/Method: Meta-learning (MAML & Reptile) for cross-dataset knowledge transfer]
        D[关键结果/Results: Accurate prediction with few examples, outperforms conventional NN]
    ```

- **[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening**
  - **tags:** [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]
  - **authors:** Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf
  - **institution:** Radboud University Medical Center, Radboud University
  - **link:** https://arxiv.org/pdf/2512.22242
  - **contributions:** 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]
        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]
        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]
    ```

- **[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]
  - **authors:** Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.22238
  - **contributions:** 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]
        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]
        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]
        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]
        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]
        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]
        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]
        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]
    ```

- **[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture**
  - **tags:** [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]
  - **authors:** Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan
  - **institution:** FPT University
  - **link:** https://arxiv.org/pdf/2512.22239
  - **contributions:** 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>面向智慧农业的高效深度学习的多目标混合知识蒸馏"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Deploying deep models on edge devices in smart agriculture<br>在智慧农业中于边缘设备部署深度模型"] --> P1["挑战/Challenge<br>Trade-off between efficiency and accuracy<br>效率与准确性的权衡"]
        Method["主要方法/Method<br>Hybrid knowledge distillation framework<br>混合知识蒸馏框架"] --> M1["学生模型/Student Model<br>Customized CNN with inverted residual & dense blocks<br>定制化CNN，含倒残差与密集连接块"]
        Method --> M2["教师模型/Teacher Model<br>ResNet18 guidance<br>ResNet18教师网络指导"]
        Method --> M3["多目标策略/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>整合硬标签、特征级、响应级与自蒸馏"]
        Results["关键结果/Results<br>Experiments on agricultural datasets<br>在农业数据集上的实验"] --> R1["性能/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>水稻种子分类准确率98.56%（教师模型98.65%）"]
        Results --> R2["效率/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs，约107万参数（比教师模型小10倍）"]
        Results --> R3["泛化/Generalization<br>Consistent gains on plant leaf disease datasets<br>在植物叶片病害数据集上一致性能提升"]
    ```

- **[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs**
  - **tags:** [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]
  - **authors:** Chama Bensmail
  - **institution:** University of Hertfordshire, Omics Data Solutions LTD
  - **link:** https://arxiv.org/pdf/2512.22240
  - **code:** https://github.com/bensmailchama-boop/EvoXplain
  - **contributions:** 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EvoXplain Paper] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[高精度模型是否共享相同内部逻辑?<br/>Do high-accuracy models share the same internal logic?]
        C --> C1[跨重复训练测量解释稳定性<br/>Measure explanation stability across repeated training]
        C --> C2[将解释视为优化过程样本<br/>Treat explanations as samples from optimization]
        D --> D1[发现解释的多模态性<br/>Found explanatory multimodality]
        D --> D2[逻辑回归等模型也显示多种机制<br/>Models like Logistic Regression show multiple mechanisms]
        D --> D3[重新定义可解释性为模型类属性<br/>Reframe interpretability as model-class property]
    ```

- **[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation**
  - **tags:** [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]
  - **authors:** Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda
  - **institution:** Meta (FAIR at Meta, Meta Superintelligence Labs)
  - **link:** https://arxiv.org/pdf/2512.22245
  - **contributions:** 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model's hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Calibrating LLM Judges<br/>校准LLM法官] --> B[Problem: LLM judges lack efficient, calibrated uncertainty<br/>问题：LLM法官缺乏高效、校准的不确定性估计]
        A --> C[Method: Linear probes on hidden states with Brier score loss<br/>方法：基于Brier分数损失的隐状态线性探针]
        A --> D[Results: Better calibration, 10x speedup, robust generalization<br/>结果：更好的校准，10倍加速，鲁棒的泛化]
    ```

- **[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction**
  - **tags:** [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]
  - **authors:** Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu
  - **institution:** Jinan University, Great Bay University, JD Technology, University of Illinois Chicago
  - **link:** https://arxiv.org/pdf/2512.22252
  - **code:** https://github.com/DSI-Lab1/GAATNet
  - **contributions:** 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Graph Attention-based Adaptive Transfer Learning for Link Prediction] --> B[核心问题/Problem: Challenges in large-scale sparse graphs and cross-dataset transfer learning for link prediction]
        A --> C[主要方法/Method: Proposes GAATNet with distant neighbor embeddings and lightweight self-adapter for adaptive transfer]
        A --> D[关键结果/Results: Achieves SOTA performance on seven datasets, provides scalable GNN-transfer learning solution]
    ```

- **[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs**
  - **tags:** [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]
  - **authors:** Pascal Passigan, Kevin zhu, Angelina Ning
  - **institution:** Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22251
  - **contributions:** 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Predicting granular gene expression changes (perturbations) from drugs, beyond binary drug-disease associations.]
        C[主要方法/Method: Merge PrimeKG++ & LINCS L1000 into a BKG; use Graph Attention Network (GAT) to predict delta expression.]
        D[关键结果/Results: Outperforms MLP baselines; ablation shows KG edges enhance prediction for mechanistic modeling.]
    ```

- **[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method**
  - **tags:** [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]
  - **authors:** Satvik Tripathi
  - **institution:** University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.22258
  - **code:** https://github.com/satviktri/LSP
  - **contributions:** 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Logic Sketch Prompting (LSP)] --> B[核心问题/Problem: LLMs unreliable on tasks needing strict rules & determinism]
        A --> C[主要方法/Method: Lightweight framework with typed variables, condition evaluators, rule validator]
        A --> D[关键结果/Results: Highest accuracy/F1 vs. baselines; suitable for clinical/safety-critical systems]
    ```

- **[arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey**
  - **tags:** [se], [automated software maintenance], [large language models, agentic systems, software issue resolution, reinforcement learning, software engineering]
  - **authors:** Zhonghao Jiang, David Lo, Zhongxin Liu
  - **institution:** Zhejiang University, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.22256
  - **code:** https://github.com/ZhonghaoJiang/Awesome-Issue-Solving
  - **contributions:** 1. Provides a systematic survey of 126 recent studies on LLM-based agentic software issue resolution. 2. Establishes a taxonomy for the field across three key dimensions: benchmarks, techniques, and empirical studies. 3. Highlights the paradigm shift brought by agentic reinforcement learning in designing and training agentic systems for software engineering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5c1c5e173acc2646c4322651d8a6c89dabed4b251b6106c2a468adeeafadf5f_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys the use of Large Language Model (LLM)-based agentic systems for automating complex software issue resolution, such as bug fixing. It reviews recent research, categorizes approaches, and discusses how agentic reinforcement learning is changing system design. The conclusion outlines current challenges and future research directions for improving automated software maintenance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Agentic Software Issue Resolution with LLMs: A Survey] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传统方法依赖人工，效率低/Traditional methods rely on human expertise, inefficient]
        Method[主要方法/Method] --> M1[基于LLM的智能体系统/LLM-based Agentic Systems]
        Method --> M2[系统综述126项研究/Systematic survey of 126 studies]
        Method --> M3[建立三维分类法/Establishes a 3D taxonomy]
        Results[关键结果/Results] --> R1[增强软件维护效率/Enhances software maintenance efficiency]
        Results --> R2[为智能体系统提供验证环境/Provides a validation environment for agentic systems]
        Results --> R3[总结挑战与未来方向/Summarizes challenges & future directions]
    ```

- **[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks**
  - **tags:** [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]
  - **authors:** Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville
  - **institution:** University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs
  - **link:** https://arxiv.org/pdf/2512.22255
  - **contributions:** 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model's reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model's distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp
  - **Simple LLM Summary:** This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data's distribution is closer to the model's own. The key conclusion is that aligning the training data distribution with the model's is more critical for performance than the correctness of the final answers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Shape of Thought / 思维形态] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Correctness vs. Distribution / 正确性与数据分布]
        B1 --> B2{Does correctness guarantee better reasoning? / 正确性保证更好的推理吗?}
        C --> C1[Train on Incorrect Synthetic CoT / 使用错误的合成CoT训练]
        C --> C2[Paraphrase Human Traces / 改写人类标注的推理链]
        C --> C3[Introduce Flawed Steps / 引入有缺陷的推理步骤]
        D --> D1[Synthetic Incorrect > Human Correct / 错误的合成数据优于正确的人类数据]
        D --> D2[Distribution Alignment is Key / 数据分布对齐是关键]
        D --> D3[Final Answer ≠ Faithful Reasoning / 最终答案 ≠ 忠实推理过程]
    ```

- **[arXiv251230] ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers**
  - **tags:** [other], [formal verification, computer algebra], [reverse engineering, graph neural network, multiplier verification, algebraic circuit verification, SAT-based equivalence checking]
  - **authors:** Chen Chen, Daniela Kaufmann, Chenhui Deng, Zhan Song, Hongce Zhang, Cunxi Yu
  - **institution:** University of Maryland, College Park; TU Wien; NVIDIA; Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.22260
  - **contributions:** 1. Proposes ReVEAL, a graph-learning-based framework for reverse engineering optimized multiplier architectures to recover their word-level structure. 2. Leverages structural graph features and learning-driven inference to identify architectural patterns at scale, enabling robust handling of large, optimized circuits. 3. Integrates smoothly with existing verification flows and supports downstream algebraic proof strategies, showing improvements in scalability and accuracy over traditional rule-based approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bdd1907cef6efb0b9b81d88b611f825942f30ef8a8674a9587cc4261e4774ef_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ReVEAL, a method that uses Graph Neural Networks (GNNs) to reverse engineer the architecture of optimized hardware multipliers. This recovered structure enables more effective formal verification using algebraic techniques. The approach demonstrates improved scalability and accuracy compared to traditional rule-based methods on diverse benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers] --> B(核心问题/Problem: 优化乘法器形式验证困难/Challenges in formal verification of optimized multipliers)
        A --> C(主要方法/Method: 基于图学习的逆向工程/GNN-guided reverse engineering)
        A --> D(关键结果/Results: 提升可扩展性与准确性/Improved scalability and accuracy)
    ```

- **[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs**
  - **tags:** [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]
  - **authors:** Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang
  - **institution:** Tianjin University, Beihang University, Guangxi Normal University
  - **link:** https://arxiv.org/pdf/2512.22266
  - **code:** https://github.com/Wjerry5/LLMTM
  - **contributions:** 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs处理动态图时态模体分析能力未知/LLMs' capability for temporal motif analysis on dynamic graphs is unexplored]
        C --> C1[提出基准LLMTM与智能体/Propose benchmark LLMTM and an agent]
        C --> C2[提出结构感知调度器/Propose structure-aware dispatcher]
        D --> D1[调度器保持高精度并降低成本/Dispatcher maintains high accuracy while reducing cost]
    ```

- **[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency**
  - **tags:** [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]
  - **authors:** Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang
  - **institution:** Peking University Third Hospital, Beihang University
  - **link:** https://arxiv.org/pdf/2512.22275
  - **contributions:** 1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Illusion of Clinical Reasoning<br>临床推理的假象] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>现有基准无法捕捉综合、多模态临床推理。]
        C --> C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>开发了包含1245个真实世界问题、涵盖7项任务的B&J基准。]
        D --> D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>巨大性能差距：选择题表现好，开放式多模态任务表现差。]
        D --> D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM在图像解释方面存在局限并出现幻觉。]
        D --> D3[Medically fine-tuned models show no consistent advantage.<br>医学微调模型未显示一致优势。]
    ```

- **[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems**
  - **tags:** [mlsys], [memory & caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]
  - **authors:** Varshith Gudur
  - **institution:** Independent Researcher (Valori Kernel Project)
  - **link:** https://arxiv.org/pdf/2512.22280
  - **code:** https://github.com/varshith-Git/Valori-Kernel
  - **contributions:** 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Valori: A Deterministic Memory Substrate for AI Systems] --> B
        A --> C
        A --> D
        B[核心问题/Problem: AI内存非确定性/AI Memory Non-Determinism]
        C[主要方法/Method: 固定点算术与状态机/Fixed-Point Arithmetic & State Machine]
        D[关键结果/Results: 跨平台比特一致性/Cross-Platform Bit-Identical Results]
    ```

- **[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation**
  - **tags:** [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]
  - **authors:** Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi
  - **institution:** Kyungpook National University
  - **link:** https://arxiv.org/pdf/2512.22287
  - **contributions:** 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model's interpretability and scalability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Cluster Aggregated GAN (CAG)"] --> Problem["核心问题/Problem: 缺乏标记数据，现有GAN方法对所有设备一视同仁，忽略间歇性和持续性设备的行为差异，导致训练不稳定和保真度有限"]
        Root --> Method["主要方法/Method: 提出混合生成框架，根据设备行为特征路由到专门分支：间歇性设备使用聚类模块和专用生成器；持续性设备使用LSTM生成器"]
        Root --> Results["关键结果/Results: 在UVIC数据集上实验，在真实性、多样性和训练稳定性上优于基线方法，聚类作为主动生成组件提高了可解释性和可扩展性"]
    ```

- **[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing**
  - **tags:** [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]
  - **authors:** Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli, University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22290
  - **contributions:** 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a "murky middle" of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["当算法管理人类: 估算算法控制对零工工人绩效和福祉非线性效应的双重机器学习方法 / When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing"]
        Root --> Problem["核心问题: 算法管理下，以人为本的管理能否持续？工人对算法的反应是非线性的 / Problem: Can person-centered management survive algorithmic management? Worker responses are nonlinear."]
        Root --> Method["主要方法: 使用双重机器学习框架估算有调节的中介模型，无严格函数形式限制 / Method: Double Machine Learning framework to estimate a moderated mediation model without restrictive functional forms."]
        Root --> Results["关键结果: 发现非单调模式。模糊的算法监督削弱绩效联系，透明可解释的监督则加强它 / Results: Found a nonmonotonic pattern. Murky oversight weakens the performance link, transparent and explainable oversight strengthens it."]
    ```

- **[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model**
  - **tags:** [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]
  - **authors:** Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang
  - **institution:** Tsinghua University (Leap Lab), Anyverse Dynamics
  - **link:** https://arxiv.org/pdf/2512.22288
  - **code:** https://co-grpo.github.io
  - **contributions:** 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]
        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]
        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]
        C --> C1[统一MDP/Unified MDP]
        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]
        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]
        D --> D1[提升生成质量/Improved Generation Quality]
        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]
    ```

- **[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations**
  - **tags:** [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]
  - **authors:** Guokan Chen, Yao Xiao
  - **institution:** Fujian University of Technology
  - **link:** https://arxiv.org/pdf/2512.22283
  - **contributions:** 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[PINNs struggle with multi-scale/high-frequency PDEs / PINNs在处理多尺度/高频PDE时遇到困难]
        P1 --> P2[Issues: Gradient flow stiffness & spectral bias / 问题: 梯度流刚度和谱偏差]
        Method[主要方法/Method] --> M1[Architecture: Kolmogorov-Arnold Network (KAN) with learnable B-splines / 架构: 基于可学习B样条的KAN]
        Method --> M2[Strategy: Adaptive weighting with dynamic decay upper bound / 策略: 带动态衰减上界的自适应加权]
        Results[关键结果/Results] --> R1[Faster convergence & higher accuracy / 更快的收敛和更高的精度]
        R1 --> R2[Improvement: At least one order of magnitude / 提升: 至少一个数量级]
        Results --> R3[Benchmarks: Klein-Gordon, Burgers, Helmholtz equations / 基准: Klein-Gordon, Burgers, Helmholtz方程]
    ```

- **[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection**
  - **tags:** [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]
  - **authors:** Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou
  - **institution:** People's Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College
  - **link:** https://arxiv.org/pdf/2512.22291
  - **contributions:** 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a 'spectral fingerprint'. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Head Spectral-Adaptive Graph Anomaly Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[固定滤波器导致过平滑与缺乏适应性/Fixed filters cause over-smoothing & lack adaptability]
        C --> C1[基于谱指纹的轻量级超网络/Lightweight hypernetwork based on spectral fingerprint]
        C --> C2[动态生成切比雪夫滤波器参数/Dynamically generates Chebyshev filter parameters]
        C --> C3[双正则化策略防止模式崩溃/Dual regularization prevents mode collapse]
        D --> D1[有效保留高频异常信号/Effectively preserves high-frequency anomaly signals]
        D --> D2[在异构数据集上性能优越/Outperforms SOTA on heterogeneous datasets]
    ```

- **[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation**
  - **tags:** [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]
  - **authors:** Philip Xu, David Elizondo, Raouf Hamzaoui
  - **institution:** De Montfort University
  - **link:** https://arxiv.org/pdf/2512.22294
  - **contributions:** 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大规模3D检索与可控4D生成的挑战/Challenges in large-scale 3D retrieval and controlled 4D generation]
        C --> C1[三级对齐框架: 文本-3D-图像/Three-level alignment: text-3D-image]
        C --> C2[ATMS模型优化检索/ATMS model optimizes retrieval]
        D --> D1[高质量3D检索/High-quality 3D retrieval]
        D --> D2[可控4D生成/Controlled 4D generation]
    ```

- **[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations**
  - **tags:** [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]
  - **authors:** Noor Fatima, Hasan Faraz Khan, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22303
  - **contributions:** 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"] --> B["核心问题/Problem: Robust detection under realistic counter-forensic attacks"]
        A --> C["主要方法/Method: Red-team training + Test-time defense in a two-stream architecture"]
        A --> D["关键结果/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps"]
    ```

- **[arXiv251230] Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection**
  - **tags:** [sec], [vulnerability detection], [multi-vulnerability detection, count bias, selection bias, long-context code, CWE injection]
  - **authors:** Chinmay Pushkar, Sanchit Kabra, Dhruv Kumar, Jagat Sesh Challa
  - **institution:** BITS Pilani, Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22306
  - **contributions:** 1. Introduced a comprehensive benchmark for Multi-Vulnerability Detection across four programming languages (C, C++, Python, JavaScript) to address the limitations of existing single-vulnerability benchmarks. 2. Constructed a novel dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, 9) into long-context code samples, enabling the study of performance under varying vulnerability densities. 3. Quantified the performance degradation of state-of-the-art LLMs (e.g., GPT-4o-mini, Llama-3.3-70B) in high-density vulnerability settings, revealing distinct failure modes like severe "under-counting" in Python and JavaScript.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6a5fa286f161d0dab7d6a06a8f54502b88fd2f448edc9ed3609a31efaf97f5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in evaluating LLMs for detecting multiple vulnerabilities in large, real-world code files. The authors propose a new benchmark by creating a dataset of long code files with systematically injected vulnerabilities and evaluate several LLMs. The main finding is that LLM performance sharply degrades as the number of vulnerabilities per file increases, with significant drops in recall for languages like Python.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Beyond Single Bugs: Benchmarking LLMs for Multi-Vulnerability Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing benchmarks are simplistic, focusing on single bugs, not reflecting real-world multi-vulnerability files.]
        C[主要方法/Method<br>Build a new benchmark with 40k files across 4 languages, injecting controlled vulnerability counts into long code.]
        D[关键结果/Results<br>LLM performance degrades sharply with more vulnerabilities; distinct failure modes in Python/JS vs. C/C++.]
    ```

- **[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting**
  - **tags:** [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]
  - **authors:** Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban
  - **institution:** Beihang University, China Telecom eSurfing Cloud
  - **link:** https://arxiv.org/pdf/2512.22309
  - **contributions:** 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLMBoost: Make Large Language Models Stronger with Boosting] --> B[核心问题/Problem: Existing LLM ensemble methods treat models as black boxes, ignoring internal representations.]
        A --> C[主要方法/Method: A boosting-inspired framework with cross-model attention, chain training, and near-parallel inference.]
        A --> D[关键结果/Results: Consistently boosts accuracy and reduces inference latency on reasoning tasks.]
    ```

- **[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators**
  - **tags:** [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]
  - **authors:** You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou
  - **institution:** Northwestern University
  - **link:** https://arxiv.org/pdf/2512.22307
  - **contributions:** 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators] --> Problem(核心问题/Problem: Model IP Protection & Supply Chain Threats)
        Root --> Method(主要方法/Method: Hardware-Software Co-design with Logic Locking)
        Root --> Results(关键结果/Results: Resists Attacks, <0.1% Overhead)
        Problem --> P1(模型盗窃/Model Theft)
        Problem --> P2(模型破坏/Model Corruption)
        Problem --> P3(信息泄露/Information Leakage)
        Method --> M1(软件侧: 神经元嵌入密钥/Software: Key Embedding in Neurons)
        Method --> M2(硬件侧: 轻量级锁定模块/Hardware: Lightweight Locking Module)
        Results --> R1(抵御优化攻击/Withstands Oracle-Guided Attacks)
        Results --> R2(低计算开销/Low Computational Overhead)
    ```

- **[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting**
  - **tags:** [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]
  - **authors:** Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan
  - **institution:** University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University
  - **link:** https://arxiv.org/pdf/2512.22317
  - **contributions:** 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]
        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]
        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]
    ```

- **[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning**
  - **tags:** [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]
  - **authors:** Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang
  - **institution:** Tsinghua University, The Chinese University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.22315
  - **code:** https://github.com/zsgvivo/VideoZoomer
  - **contributions:** 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[长视频理解受限/Limited Long Video Understanding]
        B1 --> B2[上下文窗口限制/Context Window Limitation]
        B1 --> B3[均匀采样忽略关键证据/Uniform Sampling Overlooks Evidence]
        C --> C1[代理框架/Agentic Framework]
        C1 --> C2[动态时间聚焦/Dynamic Temporal Focusing]
        C2 --> C3[从粗到细推理/Coarse-to-Fine Reasoning]
        C --> C4[两阶段训练/Two-Stage Training]
        C4 --> C5[监督微调/Supervised Fine-Tuning]
        C4 --> C6[强化学习/Reinforcement Learning]
        D --> D1[性能强劲/Strong Performance]
        D1 --> D2[超越开源模型/Surpasses Open-Source Models]
        D1 --> D3[媲美专有系统/Rivals Proprietary Systems]
        D --> D4[高效推理/Efficient Reasoning]
        D4 --> D5[低帧预算/Reduced Frame Budget]
    ```

- **[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers**
  - **tags:** [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]
  - **authors:** Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang
  - **institution:** National University of Singapore, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.22323
  - **code:** https://biangbiang0321.github.io/SpotEdit.github.io/
  - **contributions:** 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpotEdit: Selective Region Editing in Diffusion Transformers] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[全图去噪冗余/Full-image denoising is redundant for small edits]
        C --> C1[SpotSelector: 识别稳定区域/Identifies stable regions via perceptual similarity]
        C --> C2[SpotFusion: 动态特征融合/Dynamically fuses features for coherence]
        D --> D1[高效编辑/Efficient editing]
        D --> D2[保持未修改区域保真度/Preserves fidelity in unchanged areas]
    ```

- **[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents**
  - **tags:** [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]
  - **authors:** Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun
  - **institution:** Peking University, Tencent
  - **link:** https://arxiv.org/pdf/2512.22322
  - **code:** https://huggingface.co/collections/yolay/smartsnap
  - **contributions:** 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]
        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]
        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]
    ```

- **[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers**
  - **tags:** [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]
  - **authors:** Sravan Karthick T
  - **institution:** RV College of Engineering (RVCE), Bengaluru, India
  - **link:** https://arxiv.org/pdf/2512.22326
  - **contributions:** 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers] --> B(核心问题/Problem: Bitcoin价格长期预测的极端波动性和非平稳性/Bitcoin's extreme volatility & non-stationarity for long-horizon forecasting)
        A --> C(主要方法/Method: 集成全球M2流动性作为外生变量，使用TimeXer架构/Integrate Global M2 Liquidity as exogenous variable using TimeXer architecture)
        A --> D(关键结果/Results: 在70天预测范围内，MSE降低超过89%/At 70-day horizon, MSE reduced by over 89%)
    ```

- **[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma**
  - **tags:** [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]
  - **authors:** Mariya Miteva, Maria Nisheva-Pavlova
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.22331
  - **contributions:** 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]
        C[主要方法/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]
        D[关键结果/Results: Latent embeddings used for MGMT promoter methylation classification]
    ```

- **[arXiv251230] SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence**
  - **tags:** [ai], [evaluation & benchmarking], [scientific intelligence, multimodal reasoning, benchmarking toolkit]
  - **authors:** Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.22334
  - **code:** https://github.com/InternScience/SciEvalKit
  - **contributions:** 1. Introduces SciEvalKit, a unified, open-source toolkit for evaluating AI models across a broad range of scientific disciplines and core scientific intelligence capabilities. 2. Provides a flexible and extensible evaluation pipeline supporting batch evaluation, custom model/dataset integration, and ensuring transparent, reproducible results. 3. Curates expert-grade scientific benchmarks from real-world, domain-specific datasets to reflect authentic scientific challenges across six major domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across multiple disciplines and core competencies like multimodal reasoning and code generation. It provides a flexible, extensible pipeline for reproducible evaluation and is built on expert-grade, real-world scientific benchmarks. The toolkit is open-sourced to foster community-driven development in AI for science.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Lack of specialized evaluation for scientific AI across diverse disciplines and capabilities]
        C[主要方法/Method: Unified benchmarking toolkit with flexible pipeline, real-world benchmarks, and support for six scientific domains]
        D[关键结果/Results: Open-source toolkit enabling standardized, reproducible evaluation of scientific foundation models]
    ```

- **[arXiv251230] Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback**
  - **tags:** [mlsys], [agent system], [symbolic world models, multi-agent feedback, PDDL, adaptive testing, supervised fine-tuning]
  - **authors:** Mengkang Hu, Bowei Xia, Yuran Wu, Ailing Yu, Yude Zou, Qiguang Chen, Shijian Wang, Jiarui Jin, Kexin Li, Wenxiang Jiao, Yuan Lu, Ping Luo
  - **institution:** The University of Hong Kong, Xiaohongshu Inc., UESTC, Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22336
  - **code:** agent2world.github.io
  - **contributions:** 1. Proposed Agent2World, a tool-augmented multi-agent framework for generating symbolic world models via adaptive multi-agent feedback. 2. Introduced a three-stage pipeline with specialized agents (Deep Researcher, Model Developer, Testing Team) for knowledge synthesis, implementation, and behavior-aware validation. 3. Demonstrated that the framework not only achieves state-of-the-art inference-time performance but also serves as a data engine for supervised fine-tuning, leading to substantial model improvement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating correct symbolic world models (like PDDL domains) from natural language by proposing Agent2World, a multi-agent framework that uses adaptive feedback for validation and repair. The method outperforms existing approaches on benchmarks and the feedback collected also enables effective supervised fine-tuning, significantly improving model performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback] --> B[核心问题/Problem: Lack of verifiable supervision for training LLMs to generate behaviorally correct symbolic world models]
        A --> C[主要方法/Method: Tool-augmented multi-agent framework with three-stage pipeline: Deep Researcher, Model Developer, and Testing Team for adaptive feedback]
        A --> D[关键结果/Results: Achieves SOTA inference-time performance; Framework serves as data engine for fine-tuning, yielding ~31% average relative gain]
    ```

- **[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]
  - **authors:** Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell
  - **institution:** IBM Research, Mila, Université de Montréal
  - **link:** https://arxiv.org/pdf/2512.22337
  - **contributions:** 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model's general knowledge while maintaining plasticity for new tasks, applied to Qwen models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model's original capabilities while allowing adaptation to new instructions, with minimal computational overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["论文标题: The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: LoRA微调导致灾难性遗忘/Catastrophic forgetting in LoRA fine-tuning"]
        Method["主要方法/Method: 正则化近似回放/Regularized Approximate Replay (KL惩罚+交错数据/KL penalty + interleaved data)"]
        Results["关键结果/Results: 保留通用知识，维持可塑性/Preserves general knowledge without hindering plasticity"]
    ```

- **[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement**
  - **tags:** [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]
  - **authors:** Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo
  - **institution:** Stanford University, Google, Google DeepMind, New York University
  - **link:** https://arxiv.org/pdf/2512.22351
  - **code:** vulcan-3d.github.io
  - **contributions:** 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>MLLMs在复杂3D场景操控中的应用未被充分探索<br>Application of MLLMs to complex 3D scene manipulation is underexplored]
        C[主要方法/Method<br>引入MCP API、视觉工具套件和多智能体协作框架<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]
        D[关键结果/Results<br>在25个复杂任务上显著超越基线<br>Significantly outperforms baselines on 25 complex tasks]
    ```

- **[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides**
  - **tags:** [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]
  - **authors:** Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye
  - **institution:** North Carolina A&T State University, University of Chichester, Yale University
  - **link:** https://arxiv.org/pdf/2512.22335
  - **contributions:** 1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[挑战: 联合分析H&E和IHC图像进行HER2评分/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]
    B --> B2[难点: 现有方法无法提供像素级HER2状态定位/Issue: Existing methods lack pixel-level HER2 status localization]
    C --> C1[方法: 端到端多阶段视觉Transformer管道/Method: End-to-end multi-stage Vision Transformer pipeline]
    C --> C2[创新: 新颖的映射函数关联H&E与IHC区域/Innovation: Novel mapping function to correlate H&E and IHC regions]
    C --> C3[机制: 临床启发的4级HER2评分机制/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]
    D --> D1[结果: 肿瘤定位分类准确率高/Result: Good classification accuracy for tumor localization]
    D --> D2[结果: HER2状态预测准确率0.94/Result: 0.94 accuracy for HER2 status prediction]
    D --> D3[结论: 端到端ViT模型可用于联合评估H&E和IHC图像/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]
    ```

- **[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data**
  - **tags:** [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]
  - **authors:** Alaa Alahmadi, Mohamed Hasan
  - **institution:** Newcastle University, University of Leeds
  - **link:** https://arxiv.org/pdf/2512.22349
  - **contributions:** 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1
        A --> B2
        A --> B3
        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]
        B1 --> C2[可解释性差/Limited explainability]
        B1 --> C3[临床可靠性受限/Constrained clinical reliability]
        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]
        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]
        D1 --> E2[结构化颜色表示/Structured colour representations]
        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]
        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]
        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]
        B3 --> F2[提升可解释性/Improve explainability (guide attention)]
        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]
    ```

- **[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries**
  - **tags:** [mlsys], [llm inference], [Text-to-SQL, Cloud Cost Optimization, Query Efficiency, Large Language Models, Google BigQuery]
  - **authors:** Saurabh Deochake, Debajyoti Mukhopadhyay
  - **institution:** SentinelOne, WIDiCoReL Research Lab
  - **link:** https://arxiv.org/pdf/2512.22364
  - **contributions:** 1. Introduced a cloud-native cost evaluation methodology for Text-to-SQL systems, measuring bytes processed, slot utilization, and estimated query cost on production infrastructure. 2. Conducted an empirical evaluation of six LLMs on Google BigQuery, demonstrating that reasoning models achieve significantly lower cloud compute costs while maintaining high correctness. 3. Quantified cost variance across models, identified prevalent inefficiency patterns (e.g., missing partition filters), and provided deployment guidelines for cost-sensitive environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the cloud compute costs of SQL queries generated by Large Language Models (LLMs) for Text-to-SQL tasks. By evaluating six state-of-the-art LLMs on Google BigQuery, it finds that reasoning models are more cost-efficient, processing far fewer bytes, and that execution time is a poor proxy for cloud cost. The work provides a new cost-focused evaluation methodology and guidelines for deploying cost-aware Text-to-SQL systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing efficiency metrics (e.g., VES) measure time, not cloud compute costs.] --> B1[问题背景/Context<br>LLMs achieve high Text-to-SQL accuracy, but cost efficiency in cloud deployments is unknown.]
        C[主要方法/Method<br>Systematic evaluation of 6 LLMs on Google BigQuery (StackOverflow dataset).] --> C1[评估指标/Metrics<br>Measure bytes processed, slot utilization, estimated cost, and correctness.]
        D[关键结果/Results] --> D1[发现1/Finding 1<br>Reasoning models process 44.5% fewer bytes with equivalent correctness.]
        D --> D2[发现2/Finding 2<br>Weak correlation (r=0.16) between execution time and query cost.]
        D --> D3[发现3/Finding 3<br>Up to 3.4x cost variance; standard models produce high-cost outliers.]
    ```

- **[arXiv251230] Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions**
  - **tags:** [ai], [automated planning], [numeric planning, control parameters, subgoaling heuristics, optimistic compilation, infinite action space]
  - **authors:** Ángel Aso-Mollar, Diego Aineto, Enrico Scala, Eva Onaindia
  - **institution:** Universitat Politècnica de València, Università degli Studi di Brescia
  - **link:** https://arxiv.org/pdf/2512.22367
  - **contributions:** 1. Identifies a tractable subset of numeric planning problems with infinite actions (controllable, simple numeric problems)., 2. Proposes an optimistic compilation approach that transforms these problems into standard simple numeric tasks by abstracting control-dependent expressions., 3. Enables the effective use of traditional subgoaling heuristics for goal distance estimation in this challenging setting, pushing the state of the art.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc3c2ae44e3bb664061095be1d4f9beb835e627f31f49a8bfccf9fd7df412ea_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of applying standard numeric heuristics in planning problems with an infinite number of actions due to control parameters. It proposes an optimistic compilation method that transforms a tractable subset of these problems into simpler numeric tasks, enabling the use of subgoaling heuristics. The results show this approach is effective and computationally feasible for handling infinite action spaces.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions<br>基于子目标松弛的启发式方法用于无限动作数值规划"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Standard heuristics fail for infinite actions from control parameters<br>标准启发式方法无法处理控制参数导致的无限动作"] --> P1["问题背景/Context<br>Numeric planning with control parameters<br>带控制参数的数值规划"]
        Method["主要方法/Method<br>Optimistic compilation to simple numeric tasks<br>通过乐观编译转为简单数值任务"] --> M1["关键步骤/Key Step<br>Abstract control-dependent expressions<br>抽象控制依赖表达式"]
        Results["关键结果/Results<br>Effective & feasible use of subgoaling heuristics<br>子目标启发式方法有效且可行"] --> R1["结论/Conclusion<br>Pushes state of the art<br>推动技术前沿"]
    ```

- **[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation**
  - **tags:** [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]
  - **authors:** Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan
  - **institution:** The University of Hong Kong, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.22374
  - **contributions:** 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables "any-step" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]
        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]
        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]
    ```

- **[arXiv251230] Towards Efficient Post-Training via Fourier-Driven Adapter Architectures**
  - **tags:** [nlp], [parameter-efficient fine-tuning], [Fourier-Activated Adapter, random Fourier features, frequency-aware activation, parameter-efficient fine-tuning, spectral sparsity]
  - **authors:** Donggyun Bae, Jongil Park
  - **institution:** Konkuk University
  - **link:** https://arxiv.org/pdf/2512.22378
  - **contributions:** 1. Proposes the Fourier-Activated Adapter (FAA), a novel PEFT framework that integrates random Fourier features to decompose representations into frequency components. 2. Introduces a dynamic, frequency-aware activation mechanism to selectively modulate semantic information across different frequency bands. 3. Demonstrates through extensive experiments that FAA achieves competitive or superior performance on multiple benchmarks while maintaining low computational overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method for large language models that uses random Fourier features to enable frequency-aware modulation of semantic representations. Experiments on GLUE and other benchmarks show that FAA achieves strong performance with low computational cost, highlighting the effectiveness of its frequency-based approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Efficient Post-Training via Fourier-Driven Adapter Architectures] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有PEFT方法难以捕获高频语义信息 / Existing PEFT methods struggle to capture high-frequency semantic information]
        C --> C1[提出傅里叶激活适配器(FAA) / Propose Fourier-Activated Adapter (FAA)]
        C1 --> C2[集成随机傅里叶特征分解表示 / Integrate random Fourier features to decompose representations]
        C2 --> C3[使用频率感知机制选择性调制 / Use frequency-aware mechanism for selective modulation]
        D --> D1[在多个基准测试中取得有竞争力的结果 / Achieves competitive results on multiple benchmarks]
        D --> D2[保持低计算和内存开销 / Maintains low computational and memory overhead]
    ```

- **[arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents**
  - **tags:** [mlsys], [agent system], [reproducibility, dependency management, code generation, large language models, empirical study]
  - **authors:** Bhanu Prakash Vangala, Ali Adibifar, Tanu Malik, Ashish Gehani
  - **institution:** University of Missouri, SRI International
  - **link:** https://arxiv.org/pdf/2512.22387
  - **contributions:** 1. Introduces a three-layer dependency framework (claimed, working, runtime) to quantify the execution reproducibility of LLM-generated code. 2. Conducts an empirical study evaluating three state-of-the-art LLM coding agents across 300 projects in three programming languages, revealing low out-of-the-box execution success rates. 3. Discovers a significant hidden dependency problem, with an average 13.5x expansion from declared to actual runtime dependencies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f3a69233ca5eacf2fea5882b11aeb102413519f3be78440b3532150966328b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility of code generated by LLM-based coding agents. It proposes a three-layer dependency framework and conducts an empirical study on 300 projects, finding that only 68.3% execute successfully out-of-the-box and that actual runtime dependencies are significantly larger than declared ones. The study concludes that AI-generated code currently suffers from major reproducibility issues due to dependency gaps and code generation errors.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents"] --> Problem["核心问题/Problem: Is AI-generated code reproducible?"]
        Root --> Method["主要方法/Method: Empirical study using a three-layer dependency framework on 300 projects from 3 LLM agents."]
        Root --> Results["关键结果/Results: Low out-of-the-box execution rate (68.3%) and large hidden dependencies (13.5x expansion)."]
    ```

- **[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration**
  - **tags:** [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]
  - **authors:** Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi
  - **institution:** Apple, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.22382
  - **contributions:** 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Completed Hyperparameter Transfer<br/>超参数迁移研究] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Hyperparameter tuning is critical for large models<br/>大模型超参数调优至关重要]
        B --> B2[Transferring optimal HPs across scales is challenging<br/>跨规模最优超参数迁移困难]
        C --> C1[Propose Complete(d)P parameterisation<br/>提出Complete(d)P参数化方法]
        C --> C2[Enable per-module HP optimisation & transfer<br/>实现模块级超参数优化与迁移]
        D --> D1[Direct HP transfer to ~600x larger scale<br/>超参数可直接迁移至约600倍规模]
        D --> D2[Per-module HPs yield training speedup<br/>模块级超参数带来训练加速]
    ```

- **[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition**
  - **tags:** [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]
  - **authors:** Elsen Ronando, Sozo Inoue
  - **institution:** Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya
  - **link:** https://arxiv.org/pdf/2512.22385
  - **contributions:** 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-Guided Exemplar Selection for Few-Shot HAR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[依赖大数据集与几何选择 / Reliance on large datasets & geometric selection]
        B --> B2[难以区分相似活动 / Hard to distinguish similar activities]
        C --> C1[LLM生成语义先验 / LLM-generated semantic priors]
        C --> C2[结合多线索优化 / Combine multiple cues for optimization]
        D --> D1[性能超越基线 / Outperforms baselines (88.78% F1)]
        D --> D2[语义先验有效 / Semantic priors are effective]
    ```

- **[arXiv251230] HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification**
  - **tags:** [nlp], [hallucination detection], [hallucination detection, retrieval-augmented verification, contradiction graph, Paraphrased Hallucination Consistency Score (PHCS), materials science]
  - **authors:** Bhanu Prakash Vangala, Sajid Mahmud, Pawan Neupane, Joel Selvaraj, Jianlin Cheng
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22396
  - **contributions:** 1. Introduces HalluMatData, a benchmark dataset for evaluating hallucination detection in AI-generated materials science content. 2. Proposes HalluMatDetector, a multi-stage hallucination detection framework integrating intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment. 3. Introduces the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2d61ec50b266277e33c913c31df1946cc27990dbacfbd8d5b4979a627f3fa00_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of factual hallucinations in LLM-generated materials science content. It proposes HalluMatDetector, a multi-stage verification framework that combines intrinsic checks, retrieval, and contradiction analysis to detect and mitigate errors. The method reduces hallucination rates by 30% compared to standard LLM outputs and introduces a new metric (PHCS) for evaluating response consistency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content"] --> Problem["核心问题/Problem: LLM Hallucinations in Scientific Content"]
        Root --> Method["主要方法/Method: Multi-Stage Verification Framework (HalluMatDetector)"]
        Root --> Results["关键结果/Results: 30% Hallucination Reduction & New Metric (PHCS)"]
    ```

- **[arXiv251230] Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings**
  - **tags:** [mlsys], [llm inference], [knowledge graph embeddings, inference-time personalization, parameter-efficient adaptation, structure-gated adaptation, frozen backbone]
  - **authors:** Ozan Oguztuzun, Cerag Oguztuzun
  - **institution:** Case Western Reserve University
  - **link:** https://arxiv.org/pdf/2512.22398
  - **contributions:** 1. A post-hoc personalization mechanism that operates at inference time on frozen KG embeddings without backbone updates. 2. A structure-gated adaptation method that conditions candidate rankings on profile features via graph-derived gates. 3. New evaluation metrics for personalization (Alignment@k and Counterfactual Responsiveness) to quantify alignment and causal responsiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b860d25f74e154dce6d1b1a346b065fe4b4e238a600b81e91ad6a825aa744e1c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that foundation models for knowledge graphs perform well for groups but fail to capture individual user preferences. It proposes GatedBias, a lightweight framework that adds interpretable, per-entity biases to frozen KG embeddings at inference time using profile features and graph-derived gates, requiring only ~300 parameters. The method significantly improves personalized ranking alignment on benchmark datasets while preserving global accuracy, demonstrating that parameter-efficient and causally verifiable personalization is possible.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings"] --> Problem["核心问题/Problem: Foundation KG models fail to capture individual user preferences"]
        Root --> Method["主要方法/Method: GatedBias framework using structure-gated adaptation on frozen embeddings"]
        Root --> Results["关键结果/Results: Improves alignment, preserves cohort performance, parameter-efficient"]
    ```

- **[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks**
  - **tags:** [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]
  - **authors:** Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin
  - **institution:** Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22388
  - **contributions:** 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BLISS: Bandit Layer Importance Sampling Strategy] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练大型图神经网络的计算成本高/High computational cost for training GNNs on large graphs]
        C --> C1[使用多臂老虎机动态选择信息节点/Using multi-armed bandits to dynamically select informative nodes]
        C --> C2[平衡探索与利用，自适应节点重要性/Balancing exploration and exploitation, adapting to node importance]
        D --> D1[保持或超过全批次训练的精度/Maintains or exceeds full-batch training accuracy]
    ```

- **[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models**
  - **tags:** [mlsys], [llm inference], [Kubernetes, Helm, DistilBERT, scale-to-zero, hybrid routing]
  - **authors:** Bhanu Prakash Vangala, Tanu Malik
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22402
  - **contributions:** 1. A unified Helm-based deployment system for self-hosted LLMs on Kubernetes, 2. An adaptive scale-to-zero automation mechanism for efficient GPU resource utilization, 3. A hybrid routing module combining keyword heuristics and a lightweight DistilBERT classifier to balance cost, latency, and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces "Pick and Spin," a framework for efficient orchestration of self-hosted large language models. It addresses challenges in GPU utilization and workload routing by integrating Kubernetes-based deployment, adaptive scaling, and a hybrid routing strategy. The system demonstrates significant improvements in success rate, latency, and cost compared to static deployments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Efficient Multi-Model Orchestration for Self-Hosted LLMs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Self-hosted LLM deployment challenges: GPU utilization, workload routing, reliability/自托管LLM部署挑战：GPU利用率、工作负载路由、可靠性]
        C --> C1[Pick and Spin Framework: Kubernetes, Helm, scale-to-zero, hybrid routing/Pick and Spin框架：Kubernetes, Helm, 缩容至零, 混合路由]
        D --> D1[21.6% higher success rate, 30% lower latency, 33% lower cost/成功率提升21.6%，延迟降低30%，成本降低33%]
    ```

- **[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving**
  - **tags:** [mlsys], [llm inference], [speculative decoding, dynamic adaptation, multi-armed bandit, throughput optimization, latency reduction]
  - **authors:** Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.22420
  - **contributions:** 1. Identifies the critical trade-off in speculative decoding: beneficial in memory-bound (low-load) scenarios but detrimental in compute-bound (high-load) scenarios due to verification overhead. 2. Proposes Nightjar, a novel learning-based algorithm that dynamically adapts the speculative length (or disables SD) based on real-time request load and batch size. 3. Demonstrates significant performance gains, achieving up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of fixed-length speculative decoding in LLM serving, which fails to adapt to dynamic request loads. It proposes Nightjar, a learning-based algorithm that dynamically selects the optimal speculative length. Experiments show Nightjar significantly improves throughput and reduces latency compared to standard speculative decoding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Nightjar: Dynamic Adaptive Speculative Decoding] --> B[核心问题/Problem: Fixed speculative length fails under dynamic loads]
        A --> C[主要方法/Method: Learning-based algorithm adapts speculative length]
        A --> D[关键结果/Results: Higher throughput, lower latency]
    ```

- **[arXiv251230] A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot**
  - **tags:** [mlsys], [on-device ai], [Heterogeneous Computing, ROS 2, FreeRTOS, PID Control, AWS IoT]
  - **authors:** Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol
  - **institution:** Mansoura University, Egypt
  - **link:** https://arxiv.org/pdf/2512.22408
  - **contributions:** 1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Limitations of human-operated last-mile delivery (cost, safety, reliability)"] --> P1["子问题/Sub-Problem<br>Need for autonomous, cost-efficient delivery robot"]
        Method["主要方法/Method<br>Unified multi-disciplinary approach"] --> M1["异构计算/Heterogeneous Computing<br>RPi 5 (ROS 2) for AI & ESP32 (FreeRTOS) for control"]
        Method --> M2["关键技术/Key Tech<br>Low-latency comms, PID control, AWS IoT, failsafe"]
        Results["关键结果/Results<br>Robust, operational autonomous delivery system"] --> R1["成果/Outcome<br>Deterministic motor control & enhanced reliability"]
    ```

- **[arXiv251230] Emergence of Human to Robot Transfer in Vision-Language-Action Models**
  - **tags:** [ai], [robot learning], [vision-language-action models, human-to-robot transfer, co-training, emergent capability, embodiment-agnostic representations]
  - **authors:** Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair
  - **institution:** Physical Intelligence, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22414
  - **contributions:** 1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Emergence of Human to Robot Transfer in Vision-Language-Action Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Can VLA models learn from human videos for robot control?]
        C[主要方法/Method: Simple co-training recipe on human & robot data]
        D[关键结果/Results: Transfer emerges with scale; performance nearly doubles]
    ```

- **[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy**
  - **tags:** [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]
  - **authors:** Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath
  - **institution:** UC Santa Barbara, Allen Institute for Cell Sciences
  - **link:** https://arxiv.org/pdf/2512.22423
  - **contributions:** 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)
        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)
        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)
    ```

- **[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning**
  - **tags:** [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]
  - **authors:** Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu
  - **institution:** Wayne State University, Henry Ford Health
  - **link:** https://arxiv.org/pdf/2512.22425
  - **contributions:** 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Ill-posed inverse problem: complex anatomy-beam relationship / 病态逆问题: 解剖结构与射束强度的复杂关系]
        B --> B2[CNN struggles with long-range dependencies / CNN难以捕捉长程依赖]
        C --> C1[Two-stage transformer framework / 两阶段Transformer框架]
        C1 --> C1_1[Stage 1: Global dose prior / 阶段1: 全局剂量先验]
        C1 --> C1_2[Stage 2: Geometry-conditioned fluence regression / 阶段2: 几何条件化的注量图回归]
        C --> C2[Fluence-Aware Regression (FAR) loss / 注量感知回归损失]
        D --> D1[Reduced Energy Error to 4.5% / 能量误差降低至4.5%]
        D --> D2[Improved structural fidelity (p<0.05) / 结构保真度显著提升]
        D --> D3[Outperformed benchmark CNN & single-stage methods / 超越基准CNN与单阶段方法]
    ```

- **[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems**
  - **tags:** [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]
  - **authors:** Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad
  - **institution:** SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego
  - **link:** https://arxiv.org/pdf/2512.22439
  - **contributions:** 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --> B[核心问题/Problem: LiDAR垂直分辨率固定与光束丢失导致点云稀疏]
        A --> C[主要方法/Method: 基于光束感知图与门控残差融合的图注意力网络]
        A --> D[关键结果/Results: 重建误差更低，几何一致性更好，结构完整性保持]
    ```

- **[arXiv251230] Monadic Context Engineering**
  - **tags:** [mlsys], [agent system], [Monadic Context Engineering, Monad Transformers, Meta-Agents, computational contexts, algebraic structures]
  - **authors:** Yifan Zhang, Mengdi Wang
  - **institution:** Princeton University
  - **link:** https://arxiv.org/pdf/2512.22431
  - **code:** https://github.com/yifanzhang-pro/monadic-context-engineering
  - **contributions:** 1. Proposes Monadic Context Engineering (MCE), a novel architectural paradigm using Functors, Applicatives, and Monads to provide a formal foundation for AI agent design. 2. Demonstrates how Monads and Applicatives manage sequential composition and parallel execution, and how Monad Transformers enable systematic composition of capabilities like state and error handling. 3. Extends the MCE framework to describe Meta-Agents for generative orchestration, dynamically creating and managing sub-agent workflows via metaprogramming.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the brittleness and complexity in current AI agent architectures by introducing Monadic Context Engineering (MCE), a paradigm that leverages algebraic structures like Monads to formally manage state, errors, and concurrency within agent workflows. The proposed method enables the construction of complex, resilient agents from simple, verifiable components and is extended to support generative orchestration via Meta-Agents. The work concludes that MCE provides a principled foundation for building robust and scalable autonomous agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Monadic Context Engineering"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["当前代理架构脆弱/Current agent architectures are brittle"]
        Problem --> P2["状态、错误、并发管理困难/Difficulties in state, error, concurrency management"]
        Method --> M1["引入单子上下文工程/Introduce Monadic Context Engineering (MCE)"]
        Method --> M2["利用函子、应用函子、单子/Leverage Functors, Applicatives, Monads"]
        Method --> M3["使用单子变换器组合能力/Use Monad Transformers to compose capabilities"]
        Results --> R1["提供形式化基础/Provides a formal foundation"]
        Results --> R2["支持构建复杂、鲁棒的代理/Enables building complex, resilient agents"]
        Results --> R3["扩展至元代理进行生成式编排/Extends to Meta-Agents for generative orchestration"]
    ```

- **[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]
  - **authors:** Cattalyya Nuengsigkapian
  - **institution:** Google
  - **link:** https://arxiv.org/pdf/2512.22442
  - **contributions:** 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp
  - **Simple LLM Summary:** This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiFi-RAG] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[开放域RAG中的无关信息与意图对齐/Open-domain RAG faces irrelevant info & intent misalignment]
        C --> C1[分层过滤与两阶段生成/Hierarchical Filtering & Two-Pass Generation]
        C1 --> C2[使用Gemini Flash进行过滤/Use Gemini Flash for filtering]
        C1 --> C3[使用Gemini Pro进行生成/Use Gemini Pro for generation]
        D --> D1[在MMU-RAGent上超越基线/Outperforms baseline on MMU-RAGent]
        D --> D2[在自定义测试集上显著提升/Substantial gains on custom test set]
    ```

- **[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework**
  - **tags:** [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]
  - **authors:** Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang
  - **institution:** Anhui University, China Electronics Technology Group Corporation (38th Research Institute)
  - **link:** https://arxiv.org/pdf/2512.22447
  - **contributions:** 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework"] --> Problem["核心问题/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection."]
        Root --> Method["主要方法/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules."]
        Root --> Results["关键结果/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data."]
    ```

- **[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees**
  - **tags:** [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.22466
  - **contributions:** 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AMBIT: Augmenting Mobility Baselines with Interpretable Trees] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Conflicting needs for high accuracy and clear interpretability in OD flow prediction]
        C[主要方法/Method: Gray-box framework augmenting physical baselines with interpretable tree models]
        D[关键结果/Results: Physics-grounded residuals approach black-box accuracy; POI-anchored residuals are most robust]
    ```

- **[arXiv251230] The Bayesian Geometry of Transformer Attention**
  - **tags:** [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]
  - **authors:** Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra
  - **institution:** Columbia University, Dream Sports, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.22471
  - **contributions:** 1. Introduces "Bayesian wind tunnels" as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled "Bayesian wind tunnel" tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Bayesian Geometry of Transformer Attention<br/>Transformer注意力的贝叶斯几何"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Do transformers perform genuine Bayesian inference or just pattern matching?<br/>Transformer是进行真正的贝叶斯推理还是仅仅模式匹配?"]
        Method["主要方法/Method<br/>Construct 'Bayesian wind tunnels' with known posteriors<br/>构建具有已知后验的'贝叶斯风洞'"]
        Results["关键结果/Results<br/>Transformers implement Bayesian inference via geometric mechanism; MLPs fail<br/>Transformer通过几何机制实现贝叶斯推理；MLP失败"]
    ```

- **[arXiv251230] DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior**
  - **tags:** [nlp], [ai safety & alignment], [manipulation detection, safety benchmark, harm categorization, multi-layer analysis, autonomy harm]
  - **authors:** Sadia Asif, Israel Antonio Rosales Laguan, Haris Khan, Shumaila Asif, Muneeb Asif
  - **institution:** Rensselaer Polytechnic Institute, National University of Sciences and Technology
  - **link:** https://arxiv.org/pdf/2512.22470
  - **code:** https://github.com/sadia-sigma-lab/Benchmark-dataset-for-dark-patterns-in-llms
  - **contributions:** 1. Introduces DarkPatterns-LLM, the first comprehensive benchmark dataset with 401 expert-annotated examples for fine-grained detection of manipulative LLM behaviors across seven harm categories. 2. Proposes a novel four-layer diagnostic framework (MGD, MSIAN, THP, DCRA) for nuanced analysis of manipulative content, moving beyond coarse binary safety labels. 3. Provides an empirical evaluation revealing significant performance disparities (65.2%-89.7%) among state-of-the-art LLMs and identifies consistent weaknesses, particularly in detecting autonomy-undermining patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b68b7543a951540a0b9d1fde4bdde15299ee5f4f63a4526b64cdf4ab7f7a4c28_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of nuanced benchmarks for detecting manipulative behaviors in Large Language Models (LLMs). It introduces DarkPatterns-LLM, a new dataset and a four-layer analytical framework for fine-grained assessment across multiple harm categories. The evaluation shows current LLMs have significant and varied weaknesses in manipulation detection, establishing a standardized benchmark for developing more trustworthy AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DarkPatterns-LLM: 检测有害AI行为] --> B
        A --> C
        A --> D
        B[核心问题/Problem] --> B1[现有安全基准过于粗糙/Existing safety benchmarks are coarse]
        B --> B2[无法捕捉操纵的微妙机制/Fail to capture nuanced manipulation mechanisms]
        C[主要方法/Method] --> C1[新基准数据集/New benchmark dataset (401 examples)]
        C --> C2[七类危害分类/Seven harm categories]
        C --> C3[四层分析框架/Four-layer analytical pipeline (MGD, MSIAN, THP, DCRA)]
        D[关键结果/Results] --> D1[模型性能差异大/Model performance varies widely (65.2%-89.7%)]
        D --> D2[自主性危害检测弱/Weakness in detecting autonomy harm]
        D --> D3[首个标准化多维度基准/First standardized multi-dimensional benchmark]
    ```

- **[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding**
  - **tags:** [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]
  - **authors:** Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu
  - **institution:** University of Electronic Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.22481
  - **contributions:** 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Decoding fine-grained movement from noisy, non-stationary sEMG signals for prosthetic control]
        C[主要方法/Method: Domain-specific SSL with spectral pre-training and Cylindrical Rotary Position Embedding (CyRoPE)]
        D[关键结果/Results: New SOTA performance, outperforms supervised & generic SSL baselines, validated on amputation data]
    ```

- **[arXiv251230] Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring**
  - **tags:** [mlsys], [agent system], [adversarial reasoning, multi-agent system, pedagogical oversight, hierarchical framework, low-compute inference]
  - **authors:** Saisab Sadhu, Ashim Dhor
  - **institution:** Indian Institute of Science Education and Research Bhopal
  - **link:** https://arxiv.org/pdf/2512.22496
  - **contributions:** 1. Introduces Hierarchical Pedagogical Oversight (HPO), a novel multi-agent adversarial framework designed to improve the reliability of AI tutoring by separating pedagogical generation from evaluation. 2. Adapts structured adversarial synthesis to educational assessment, enforcing a dialectical debate between opposing pedagogical critics to mitigate sycophancy and superficial consensus. 3. Demonstrates that the adversarial protocol enables a small 8B-parameter model to outperform GPT-4o on pedagogical oversight while using significantly fewer computational resources.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67b6c2853ea8b40662f9788f9062b5488d7ec541a754a445a856888b9fb9300c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of unreliable AI tutors (LLMs) that often validate incorrect student answers. It proposes the Hierarchical Pedagogical Oversight (HPO) framework, which uses a structured multi-agent adversarial debate to assess tutoring quality. The main conclusion is that this adversarial approach enables a much smaller model to outperform a much larger one (GPT-4o) on a pedagogical reasoning benchmark, establishing it as a critical mechanism for reliable, low-compute oversight.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hierarchical Pedagogical Oversight<br>分层教学监督框架] --> B[Problem: LLMs as tutors are unreliable<br>问题: LLM导师不可靠]
        A --> C[Method: Multi-Agent Adversarial Framework<br>方法: 多智能体对抗框架]
        A --> D[Results: 8B model beats GPT-4o, low-compute<br>结果: 8B模型超越GPT-4o, 低计算]
    ```

- **[arXiv251230] ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation**
  - **tags:** [nlp], [speech synthesis], [flow matching, hierarchical attention, low-resource TTS, agglutinative language, non-autoregressive generation]
  - **authors:** Suhua Wang, Zifan Wang, Xiaoxin Sun, D. J. Wang, Zhanbo Liu, Xin Li
  - **institution:** Northeast Normal University, Changchun Humanities and Sciences College, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.22491
  - **contributions:** 1. Proposes a novel hierarchical text representation and cross-modal attention mechanism to handle Manchu's agglutinative phonology. 2. Introduces an end-to-end speech synthesis model integrating deep convolutional networks with a flow-matching Transformer for efficient, non-autoregressive generation. 3. Constructs the first public Manchu TTS dataset and employs data augmentation to address severe data scarcity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ManchuTTS, a novel text-to-speech system designed for the endangered and agglutinative Manchu language. The method uses a three-tier text representation and a flow-matching Transformer with hierarchical guidance to tackle data scarcity and complex phonology. Experiments show it achieves a high MOS score of 4.52 and significantly improves pronunciation accuracy and prosodic naturalness compared to baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ManchuTTS: 满语语音合成] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[数据稀缺/Data Scarcity]
        B1 --> C2[粘着语语音学/Agglutinative Phonology]
        B2 --> D1[三层文本表示/Three-tier Text Representation]
        B2 --> D2[流匹配Transformer/Flow-matching Transformer]
        B2 --> D3[分层对比损失/Hierarchical Contrastive Loss]
        B3 --> E1[MOS得分4.52/MOS Score 4.52]
        B3 --> E2[AWPA提升31%/AWPA +31%]
        B3 --> E3[韵律自然度提升27%/Prosodic Naturalness +27%]
    ```

- **[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training**
  - **tags:** [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]
  - **authors:** Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin
  - **institution:** Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing
  - **link:** https://arxiv.org/pdf/2512.22492
  - **contributions:** 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Role-Based Fault Tolerance System for LLM RL Post-Training] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[RL后训练混合训练与推理工作负载，易受双方故障影响/RL post-training mixes training & inference, vulnerable to faults from both]
        B --> B2[现有容错框架未针对RL的异步执行优化/Existing FT frameworks not optimized for RL's async execution]
        C --> C1[基于角色的故障隔离与恢复/Role-based fault isolation & recovery]
        C --> C2[检测-重启-重连范式/Detect-Restart-Reconnect paradigm]
        C2 --> C21[角色感知监控/Role-aware monitoring]
        C2 --> C22[非中断式重启/Non-disruptive restart with warm standbys]
        C2 --> C23[动态UCX点对点通信重连/Dynamic UCX P2P reconnection]
        D --> D1[ETTR超过80%，优于基线的60%/ETTR >80%, better than baseline 60%]
        D --> D2[端到端训练时间加快8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]
    ```

- **[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals**
  - **tags:** [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]
  - **authors:** Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya
  - **institution:** Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University
  - **link:** https://arxiv.org/pdf/2512.22508
  - **contributions:** 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp
  - **Simple LLM Summary:** This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Predicting LLM Correctness in Prosthodontics<br/>预测LLM在口腔修复学中的正确性] --> B(Problem/核心问题: LLM hallucinations in high-stakes healthcare domains<br/>高风险医疗领域中的LLM幻觉问题)
        A --> C(Method/主要方法: Use metadata & hallucination signals to build correctness predictors<br/>使用元数据和幻觉信号构建正确性预测器)
        A --> D(Results/关键结果: Improved accuracy & precision; metadata not reliable for hallucination prediction; prompting alters behavior<br/>提升准确率和精确度；元数据不能可靠预测幻觉；提示策略改变模型行为)
    ```

- **[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks**
  - **tags:** [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]
  - **authors:** Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance
  - **link:** https://arxiv.org/pdf/2512.22522
  - **contributions:** 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("梯度消失/Gradient Vanishing")
        Problem --> P2("对抗评估不可靠/Unreliable Adversarial Evaluation")
        Method --> M1("理论分析梯度消失/Theoretical Analysis of Gradient Vanishing")
        Method --> M2("自适应锐度替代梯度/Adaptive Sharpness Surrogate Gradient (ASSG)")
        Method --> M3("稳定自适应投影梯度下降/Stable Adaptive PGD (SA-PGD)")
        Results --> R1("攻击成功率大幅提升/Substantially Increased Attack Success Rate")
        Results --> R2("揭示鲁棒性被高估/Revealed Overestimated Robustness")
        Results --> R3("提供更可靠评估/Provided More Reliable Evaluation")
    ```

- **[arXiv251230] Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation**
  - **tags:** [mlsys], [agent system], [multi-AI agent, machine learning potential, human-in-the-loop, self-auditing, scalable molecular dynamics]
  - **authors:** Yiming Lu, Tingyu Lu, Di Zhang, Lili Ye, Hao Li
  - **institution:** Tohoku University, Dalian University of Technology
  - **link:** https://arxiv.org/pdf/2512.22529
  - **contributions:** 1. Developed a novel "human-in-the-loop" closed-loop framework utilizing self-auditing AI agents to validate and evolve a machine learning potential, ensuring quantum accuracy while achieving near-linear scalability to million-atom systems and nanosecond timescales. 2. Discovered a temperature-regulated dual-mode oxidation mechanism for aluminum nanoparticles, where the oxide shell acts as a dynamic "gatekeeper" via a "breathing mode" at moderate temperatures and transitions to a catastrophic "rupture mode" above a critical threshold. 3. Resolved a long-standing controversy by demonstrating that aluminum cation outward diffusion, not oxygen transport, is the dominant mass transfer mechanism across all temperature regimes, with diffusion coefficients 2-3 orders of magnitude higher.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42c9680d91fb0cb7f7611fd33feaef10bdab707dc569594f3d51f8af4d9e0651_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a human-in-the-loop multi-AI agent framework to develop and validate a highly accurate and scalable machine learning potential for molecular dynamics simulations. Using this method, the study reveals a dual-mode oxidation mechanism in aluminum nanoparticles and conclusively shows that aluminum cation diffusion, not oxygen transport, dominates the oxidation process.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation] --> B
        A --> C
        A --> D
        B[核心问题/Problem]
        B --> B1[原子尺度氧化机制未知/Atomic-scale oxidation mechanism unknown]
        B --> B2[计算瓶颈:精度与规模难以兼得/Computational bottleneck: trade-off between accuracy and scale]
        C[主要方法/Method]
        C --> C1[人机协同闭环框架/Human-in-the-loop closed-loop framework]
        C --> C2[自审AI代理验证MLP演化/Self-auditing AI Agents validate MLP evolution]
        D[关键结果/Results]
        D --> D1[发现双模式氧化机制/Discovered dual-mode oxidation mechanism]
        D --> D2[揭示铝离子扩散主导/Revealed Al cation diffusion dominates]
        D --> D3[建立统一原子尺度设计框架/Established unified atomic-scale design framework]
    ```

- **[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation**
  - **tags:** [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]
  - **authors:** Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22536
  - **contributions:** 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoAgent: Coherent Video Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Open-domain video generation lacks coherence and consistency/开放域视频生成缺乏连贯性和一致性]
        C --> C1[Plan-Synthesize-Verify-Edit Pipeline/计划-合成-验证-编辑流程]
        C1 --> C2[Storyboard Planner/故事板规划器]
        C1 --> C3[Global Context Manager/全局上下文管理器]
        C1 --> C4[Verifier Agent/验证智能体]
        C1 --> C5[Pacing-aware Editor/节奏感知编辑器]
        D --> D1[Improves coherence, consistency, narrative quality/提升连贯性、一致性、叙事质量]
    ```

- **[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains**
  - **tags:** [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]
  - **authors:** Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang
  - **institution:** Sun Yat-sen University, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22545
  - **contributions:** 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Rewarded Multimodal Coherent Reasoning<br>自我奖励多模态连贯推理] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --> P1[现有方法缺陷/Existing Method Flaws<br>Supervises only final answer]
        Method[主要方法/Method<br>SR-MCR Framework] --> M1[自我奖励/Self-Reward<br>Five intrinsic process cues]
        Method --> M2[优化目标/Optimization<br>GRPO with cooling mechanism]
        Results[关键结果/Results<br>Evaluation & Ablation] --> R1[性能提升/Performance Gain<br>SOTA accuracy (81.4%)]
        Results --> R2[消融研究/Ablation Study<br>Confirms contributions]
    ```

- **[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting**
  - **tags:** [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]
  - **authors:** Jaebin Lee, Hankook Lee
  - **institution:** Sungkyunkwan University
  - **link:** https://arxiv.org/pdf/2512.22550
  - **code:** https://github.com/efficient-learning-lab/TimePerceiver
  - **contributions:** 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TIMEPERCEIVER] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法侧重编码器，预测与训练分离/Prior work focuses on encoder, treats prediction & training separately]
        C --> C1[广义预测任务: 外推、插值、填补/Generalized forecasting: extrapolation, interpolation, imputation]
        C --> C2[编码: 潜在瓶颈表示/Encoding: Latent bottleneck representations]
        C --> C3[解码: 可学习查询/Decoding: Learnable queries]
        D --> D1[性能显著超越SOTA/Outperforms SOTAs significantly]
    ```

- **[arXiv251230] Learning When Not to Attend Globally**
  - **tags:** [mlsys], [llm inference], [All-or-Here Attention, sliding window attention, conditional computation, binary router, context dependency]
  - **authors:** Xuan Luo, Kailai Zhang, Xifeng Yan
  - **institution:** UC Santa Barbara
  - **link:** https://arxiv.org/pdf/2512.22562
  - **contributions:** 1. Proposes All-or-Here Attention (AHA), a novel attention mechanism that dynamically toggles between full and local sliding window attention using a binary router per head. 2. Demonstrates empirically that full attention is largely redundant, showing up to 93% of full attention operations can be replaced with local attention without performance loss. 3. Identifies a long-tail distribution in context dependency, revealing that the need for global context decays rapidly as the local window expands.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency of full self-attention in LLMs by proposing All-or-Here Attention (AHA), which learns to dynamically switch between full and local sliding window attention for each token. The results show that most full attention operations are unnecessary, and efficient inference can be achieved with on-demand global context access.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning When Not to Attend Globally] --> B[核心问题/Problem: Quadratic complexity of full self-attention is inefficient]
        A --> C[主要方法/Method: All-or-Here Attention (AHA) with binary router to toggle between full and local sliding window attention]
        A --> D[关键结果/Results: Up to 93% full attention replaced without loss; reveals long-tail context dependency]
    ```

- **[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure**
  - **tags:** [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]
  - **authors:** Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang
  - **institution:** HKUST, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22560
  - **code:** https://github.com/alibaba/ROLL
  - **contributions:** 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp
  - **Simple LLM Summary:** The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure"] --> Problem["核心问题/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure."]
        Root --> Method["主要方法/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation."]
        Root --> Results["关键结果/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs."]
    ```

- **[arXiv251230] Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI**
  - **tags:** [ai], [cognitive architectures], [predictive coding, compositional structure, episodic memory, action integration, foundation models]
  - **authors:** Rajesh P. N. Rao, Vishwas Sathish, Linxing Preston Jiang, Matthew Bryan, Prashant Rangarajan
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.22568
  - **contributions:** 1. Proposes integrating actions, compositional structure, and episodic memory into foundation models to address their deficiencies. 2. Presents neuroscience evidence to support the importance of these components for achieving human-like AI. 3. Compares the proposal to current trends like chain-of-thought reasoning and retrieval-augmented generation, suggesting new brain-inspired augmentation methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c90f622f122bb0e11d5909a8de5e173e03638e3854680c6b2a0139a74ea9314_w640_q70.webp
  - **Simple LLM Summary:** The paper argues that current foundation models, despite their success, lack key components of brain-inspired predictive coding models: action integration, compositional structure, and episodic memory. It proposes integrating these components to address issues like hallucinations, lack of grounding, and poor interpretability, aiming for safer and more human-like AI. The conclusion advocates for renewed collaboration between neuroscience and AI to achieve these goals.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Lessons from Neuroscience for AI / 神经科学对AI的启示"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem"] --> P1["Current AI lacks key brain features / 当前AI缺乏关键大脑特征"]
        Problem --> P2["Deficiencies: hallucinations, no grounding / 缺陷：幻觉，缺乏根基"]
    
        Method["主要方法/Method"] --> M1["Integrate brain components / 整合大脑组件"]
        M1 --> M1_1["Actions / 行动"]
        M1 --> M1_2["Compositional Structure / 组合结构"]
        M1 --> M1_3["Episodic Memory / 情景记忆"]
    
        Results["关键结果/Results"] --> R1["Address AI deficiencies / 解决AI缺陷"]
        Results --> R2["Enable safe, interpretable AI / 实现安全、可解释的AI"]
        Results --> R3["Path to human-like AI / 通向类人AI之路"]
    ```

- **[arXiv251230] SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G**
  - **tags:** [mlsys], [agent system], [Agentic AI Networking, Multi-agent Multi-objective Optimization, Model Partition and Sharing (MoPS), Cross-layer Optimization, Pareto-optimal Solution]
  - **authors:** Yong Xiao, Xubo Li, Haoran Zhou, Yingyu Li, Yayu Gao, Guangming Shi, Ping Zhang, Marwan Krunz
  - **institution:** Huazhong University of Science and Technology, Peng Cheng Laboratory, Pazhou Laboratory (Huangpu), China University of Geosciences (Wuhan), Xidian University, Beijing University of Posts and Telecommunications, University of Arizona
  - **link:** https://arxiv.org/pdf/2512.22579
  - **contributions:** 1. Proposes SANet, a semantic-aware Agentic AI Networking architecture that infers user semantic goals and automatically assigns cross-layer agents to fulfill them. 2. Formulates the decentralized optimization as a multi-agent multi-objective problem, proposes three novel evaluation metrics, and develops a Model Partition and Sharing (MoPS) framework for efficient model deployment. 3. Derives theoretical bounds proving a three-way tradeoff among optimization, generalization, and conflicting errors, and validates the framework with a hardware prototype showing significant performance gains and computational efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee5846572238b17d03d837b7c31a7bd60644abe5d9070207f45cd166b326470_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SANet, a semantic-aware Agentic AI networking framework for 6G that uses AI agents to infer user goals and perform cross-layer optimization. It formulates the problem as multi-agent multi-objective optimization, introduces a model partition and sharing method, and proves a theoretical tradeoff. Experiments on a hardware prototype show the framework achieves up to 14.61% performance gain while requiring only 44.37% of the FLOPs of state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SANet: A Semantic-aware Agentic AI Networking Framework] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[缺乏支持自动目标发现与多智能体编排的框架 / Lack of framework for automatic goal discovery and multi-agent orchestration]
        Problem --> P2[协作智能体可能存在目标冲突 / Collaborating agents may have conflicting objectives]
    
        Method[主要方法/Method] --> M1[语义感知架构推断用户目标并分配智能体 / Semantic-aware architecture infers user goal and assigns agents]
        Method --> M2[将问题建模为多智能体多目标优化 / Formulates as multi-agent multi-objective problem]
        Method --> M3[提出模型分区与共享框架 / Proposes Model Partition and Sharing (MoPS) framework]
        Method --> M4[提出两种分布式优化算法 / Proposes two decentralized optimization algorithms]
    
        Results[关键结果/Results] --> R1[理论证明三方面误差的权衡 / Theoretical proof of three-way error tradeoff]
        Results --> R2[硬件原型验证性能提升与计算效率 / Hardware prototype validates performance gain and computational efficiency]
    ```

- **[arXiv251230] Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care**
  - **tags:** [mlsys], [others], [physiological signal analysis, unified data interface, modular architecture, end-to-end workflow, configurable toolkit]
  - **authors:** Tao Zhou, Lingyu Shu, Zixing Zhang, Jing Han
  - **institution:** Hunan University, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.22601
  - **code:** https://github.com/SmileHnu/Tyee
  - **contributions:** 1. A unified data interface and configurable preprocessing pipeline for 12 physiological signal modalities. 2. A modular and extensible architecture enabling flexible integration and rapid prototyping. 3. An end-to-end workflow configuration system promoting reproducible and scalable experimentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4a4d498d8e9b9f7b3a7f4413e2399342920644addd35cc3e7401a5ebadf033_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Tyee, a configurable deep learning toolkit designed to address challenges in physiological signal analysis, such as heterogeneous data and fragmented pipelines. Its unified, modular design allows for flexible and reproducible experimentation. The toolkit demonstrates strong performance, achieving state-of-the-art results on 12 out of 13 evaluated datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Tyee: A Unified Toolkit for Intelligent Physiological Health Care] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[异构数据格式/Heterogeneous Data Formats]
        Problem --> P2[不一致的预处理/Inconsistent Preprocessing]
        Problem --> P3[碎片化模型管道/Fragmented Model Pipelines]
        Problem --> P4[不可复现的实验/Non-reproducible Experiments]
        Method --> M1[统一数据接口/Unified Data Interface]
        Method --> M2[模块化架构/Modular Architecture]
        Method --> M3[端到端工作流配置/End-to-end Workflow Configuration]
        Results --> R1[性能优异/Outperforms Baselines]
        Results --> R2[12/13数据集SOTA/State-of-the-art on 12 of 13 Datasets]
        Results --> R3[开源工具包/Open-source Toolkit]
    ```

- **[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation**
  - **tags:** [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]
  - **authors:** Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song
  - **institution:** Zhejiang University, Hangzhou City University, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22605
  - **code:** https://anonymous.4open.science/r/M3ob-62EF
  - **contributions:** 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A["Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation"] --> B["核心问题/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap."]
    A --> C["主要方法/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment."]
    A --> D["关键结果/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios."]
    ```

- **[arXiv251230] LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation**
  - **tags:** [mlsys], [agent system], [multi-agent system, graph neural network, collective decision-making, startup success prediction, role-playing agents]
  - **authors:** Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang
  - **institution:** ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.22608
  - **contributions:** 1. Proposes SimVC-CAS, a novel collective agent system that reformulates startup financing prediction as a multi-agent group decision-making task, moving beyond single decision-maker models. 2. Introduces role-playing agents with unique traits and a GNN-based supervised interaction module to capture heterogeneous investor evaluations and behavioral dynamics within a co-investment network. 3. Demonstrates significant predictive performance improvement (e.g., ~25% relative improvement in average precision@10) on real-world PitchBook data with strict leakage controls, while providing interpretable, multi-perspective reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5560279fcb9dca880844ffe21da36f9901c81ce8898e265fff9cc80a5e7cfb8a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of predicting startup success by simulating venture capital decision-making as a collective process. It proposes SimVC-CAS, a multi-agent system where role-playing LLM agents interact via a GNN module to model investor networks. The method significantly outperforms previous approaches in predicting financing outcomes and offers interpretable reasoning from multiple investor perspectives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[预测初创企业成功/Predicting Startup Success]
        B --> B2[现有方法忽略投资者群体动态/Existing methods overlook investor group dynamics]
        C --> C1[提出SimVC-CAS集体代理系统/Propose SimVC-CAS collective agent system]
        C --> C2[角色扮演代理与GNN交互模块/Role-playing agents & GNN-based interaction module]
        D --> D1[预测准确性显著提升/Significantly improved predictive accuracy]
        D --> D2[提供可解释的多视角推理/Provides interpretable, multi-perspective reasoning]
    ```

- **[arXiv251230] Chord Recognition with Deep Learning**
  - **tags:** [ai], [music information retrieval], [chord recognition, deep learning, generative models, pitch augmentation, beat detection]
  - **authors:** Pierre Mackenzie
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.22621
  - **contributions:** 1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp
  - **Simple LLM Summary:** This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Chord Recognition with Deep Learning) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(进展缓慢/Slow Progress)
        Method --> M1(实验现有方法/Experiment with Existing Methods)
        Method --> M2(测试生成模型假设/Test Generative Model Hypotheses)
        Results --> R1(罕见和弦表现差/Poor Performance on Rare Chords)
        Results --> R2(音高增强提升准确率/Pitch Augmentation Boosts Accuracy)
        Results --> R3(节拍检测提升可解释性/Beat Detection Improves Interpretability)
    ```

- **[arXiv251230] The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?**
  - **tags:** [ai], [forecasting], [large language models, deliberation, multi-agent, forecasting accuracy, log loss]
  - **authors:** Paul Schneider, Amalie Schramm
  - **institution:** PRIORB
  - **link:** https://arxiv.org/pdf/2512.22625
  - **contributions:** 1. Introduces and tests a structured deliberation intervention for LLMs, where models review each other's forecasts before updating, as a novel method for improving AI-based forecasting. 2. Systematically evaluates the intervention across four distinct scenarios (diverse/homogeneous models with distributed/shared information), identifying that accuracy improvement is specific to diverse models with shared information. 3. Provides empirical evidence that deliberation can be a viable strategy for improving LLM forecasting, while also revealing the unexpected finding that providing additional contextual information did not improve accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0685f113c8c22bd110f615c357b7de1633adf1104f67be97e690bae6bee70345_w640_q70.webp
  - **Simple LLM Summary:** This study investigates whether allowing large language models (LLMs) to deliberate by reviewing each other's forecasts improves their forecasting accuracy. The method was tested on 202 binary questions across different model group compositions and information-sharing scenarios. The main conclusion is that deliberation significantly improves accuracy for diverse LLM groups with shared information, but not for homogeneous groups, suggesting it as a viable strategy for enhancing LLM-based forecasting.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?"]
        Root --> Problem["核心问题/Problem<br>Does structured deliberation improve LLM forecasting accuracy?"]
        Root --> Method["主要方法/Method<br>LLMs review each other's forecasts before updating across four scenarios."]
        Root --> Results["关键结果/Results<br>Accuracy improved for diverse models with shared info; no benefit for homogeneous groups."]
    ```

- **[arXiv251230] DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [probabilistic scoring, Swiss-system tournament, explainable evaluation, evidence-coupled framework, ranking fidelity]
  - **authors:** Shiyan Liu, Jian Ma, Rui Qu
  - **institution:** Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.22629
  - **contributions:** 1. Introduces DICE, a two-stage, evidence-coupled framework for explainable and robust RAG evaluation using probabilistic \{A, B, Tie\} scoring. 2. Employs a Swiss-system tournament to reduce computational complexity from O(N²) to O(N log N) for efficient multi-system comparisons. 3. Demonstrates high agreement (85.7%) with human experts on a Chinese financial QA dataset, outperforming existing LLM-based metrics like RAGAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6045a90fb152ced5737d976a17be84cd02b0a0396b4dd9cb385e9ba4d9063de6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of interpretability and efficiency in evaluating Retrieval-Augmented Generation (RAG) systems. It proposes DICE, a framework that uses probabilistic scoring and a Swiss-system tournament to provide transparent, confidence-aware judgments while reducing computational cost. The method shows strong agreement with human experts, establishing it as an explainable and efficient paradigm for trustworthy RAG assessment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DICE: Discrete Interpretable Comparative Evaluation] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[现有指标问题/Existing Metrics Issues]
        P1 --> P1_1[可解释性有限/Limited Interpretability]
        P1 --> P1_2[不确定性量化不足/Inadequate Uncertainty Quantification]
        P1 --> P1_3[计算效率低/Computational Inefficiency]
    
        Method[主要方法/Method] --> M1[两阶段证据耦合框架/Two-Stage Evidence-Coupled Framework]
        M1 --> M1_1[概率{A,B,Tie}评分/Probabilistic Scoring]
        M1 --> M1_2[可解释推理痕迹/Interpretable Reasoning Traces]
        Method --> M2[瑞士制锦标赛/Swiss-System Tournament]
        M2 --> M2_1[降低复杂度/Reduces O(N²) to O(N log N)]
    
        Results[关键结果/Results] --> R1[效率提升/Efficiency Gain]
        R1 --> R1_1[计算量减少42.9%/42.9% Reduction]
        Results --> R2[评估有效性/Evaluation Validity]
        R2 --> R2_1[与专家85.7%一致/85.7% Human Agreement]
    ```

- **[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights**
  - **tags:** [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]
  - **authors:** Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22650
  - **code:** https://minnesotanlp.github.io/insight-scaling-webpage
  - **contributions:** 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[多阶段任务缺乏可验证奖励/Multi-stage tasks lack verifiable rewards]
        B --> B2[基于评判的优化易累积误差/Judge-based refinement prone to error accumulation]
        C --> C1[选择性测试时扩展/Selective Test-Time Scaling]
        C --> C2[跨阶段分配计算资源/Distribute compute across stages]
        C --> C3[早期剪枝低质量分支/Prune low-quality branches early]
        D --> D1[提升平均分数/Increased mean scores (61.64 to 65.86)]
        D --> D2[降低方差/Reduced variance]
        D --> D3[与人类专家对齐的评判模型/Judge model aligned with human experts]
    ```

- **[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains**
  - **tags:** [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]
  - **authors:** Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang
  - **institution:** University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22664
  - **code:** https://github.com/qklee-lz/CLAdapter
  - **contributions:** 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[下游任务数据稀缺/Data-limited downstream tasks]
        C --> C1[提出CLAdapter/Propose CLAdapter]
        C1 --> C2[注意力与聚类中心/Attention & Cluster Centers]
        C2 --> C3[个性化特征增强/Personalized Feature Enhancement]
        D --> D1[10个数据集实验/Experiments on 10 datasets]
        D1 --> D2[实现SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos**
  - **tags:** [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]
  - **authors:** Shravan Saranyan, Pramit Saha
  - **institution:** Branham High School, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22657
  - **contributions:** 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]
        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]
        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]
        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]
        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]
    ```

- **[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]
  - **authors:** Pere Martra
  - **institution:** Universidad Internacional Menéndez Pelayo (UIMP)
  - **link:** https://arxiv.org/pdf/2512.22671
  - **contributions:** 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --> B
        A --> C
        A --> D
        B[核心问题/Problem: How does structured width pruning affect different LLM capabilities?]
        C[主要方法/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]
        D[关键结果/Results: Knowledge ↓, Instruction-following ↑, Truthfulness ↑, Efficiency trade-offs]
    ```

- **[arXiv251230] Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency**
  - **tags:** [nlp], [uncertainty quantification], [conformal prediction, adaptive prediction sets, vocabulary-aware, coverage-efficiency tradeoff, marginal coverage]
  - **authors:** Yoshith Roy Kotla, Varshith Roy Kotla
  - **institution:** The ICFAI Foundation for Higher Education
  - **link:** https://arxiv.org/pdf/2512.22682
  - **contributions:** 1. Identified and formally characterized the coverage-efficiency tradeoff unique to applying conformal prediction to next-token prediction in LLMs with large vocabularies. 2. Proposed Vocabulary-Aware Conformal Prediction (VACP), a framework using semantic masking and temperature-adjusted scoring to reduce the effective prediction space. 3. Provided a theoretical analysis of when vocabulary reduction preserves conformal validity and demonstrated a 197x improvement in prediction set efficiency on benchmarks while maintaining coverage guarantees.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that naive conformal prediction for LLM next-token prediction produces uninformatively large prediction sets due to large vocabularies. It proposes Vocabulary-Aware Conformal Prediction (VACP), which uses semantic masking and hierarchical conformalization to drastically reduce set size. The method achieves near-target coverage while improving set efficiency by 197x, making conformal prediction practical for LLMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Conformal Prediction Sets for LLMs] --> B[核心问题/Problem: 标准置信预测在大型词汇表中产生巨大且无用的预测集]
        A --> C[主要方法/Method: 提出词汇感知置信预测(VACP), 使用语义掩码和分层校准]
        A --> D[关键结果/Results: 在保持90%覆盖率的同时, 将平均预测集大小从847个词元减少到4.3个]
    ```

- **[arXiv251230] TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning**
  - **tags:** [mlsys], [agent system], [travel planning benchmark, multi-turn interaction, tool-augmented agents, deterministic sandbox, real-world user requests]
  - **authors:** Xiang Cheng, Yulan Hu, Xiangwen Zhang, Lu Xu, Zheng Pan, Xin Li, Yong Liu
  - **institution:** Gaoling School of Artificial Intelligence, Renmin University of China; AMAP, Alibaba Group; National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.22673
  - **contributions:** 1. Introduces TravelBench, a real-world travel planning benchmark featuring multi-turn user-agent interaction and tool use, addressing limitations of prior static benchmarks. 2. Constructs a controlled sandbox environment with 10 deterministic travel-domain tools (e.g., POI search, route planning) to enable stable and reproducible evaluation of agent reasoning. 3. Collects and curates a diverse dataset of 1,103 instances (multi-turn, single-turn, unsolvable) from real user scenarios to comprehensively evaluate different aspects of agent performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9e23f8223243d3734eaad4483f929312f198ee51f05e74faf519458ec18add0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TravelBench, a new benchmark for evaluating LLM agents in realistic travel planning, which features multi-turn interaction and a sandbox of deterministic tools. The benchmark addresses the limitations of prior work by supporting dynamic user interaction and long-horizon planning. The authors evaluate several LLMs on TravelBench, providing a practical testbed for advancing agent capabilities in planning and tool use.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有旅行规划基准缺乏多轮交互和真实场景覆盖/Existing travel planning benchmarks lack multi-turn interaction and real-world coverage]
        C --> C1[构建包含多轮对话、单轮查询和不可解请求的真实数据集/Build a real-world dataset with multi-turn dialogues, single-turn queries, and unsolvable requests]
        C --> C2[创建具有10个确定性工具的沙盒环境/Create a sandbox environment with 10 deterministic tools]
        D --> D1[为LLM智能体评估提供了实用且可复现的基准/Provides a practical and reproducible benchmark for LLM agent evaluation]
    ```

- **[arXiv251230] Learning with the $p$-adics**
  - **tags:** [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]
  - **authors:** André F. T. Martins
  - **institution:** Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações
  - **link:** https://arxiv.org/pdf/2512.22692
  - **contributions:** 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp
  - **Simple LLM Summary:** This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Learning with the p-adics] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[现有ML基于实数域/Existing ML uses real numbers]
    B --> B2[是否可用其他域?/Alternative fields possible?]
    C --> C1[研究p-adic数域/Study p-adic number field Q_p]
    C --> C2[利用超度量结构/Exploit ultrametric structure]
    D --> D1[构建分类回归模型/Build classification & regression models]
    D --> D2[表示学习与语义网络/Representation learning & semantic networks]
    D --> D3[开启新研究方向/Open new research directions]
    ```

- **[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages**
  - **tags:** [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]
  - **authors:** Ahmed Abdullah, Sana Fatima, Haroon Mahmood
  - **institution:** FAST-National University, Al Ain University
  - **link:** https://arxiv.org/pdf/2512.22705
  - **contributions:** 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GHaLIB: 多语言希望语音检测框架 / GHaLIB: Multilingual Hope Speech Detection Framework] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[希望语音在NLP中代表性不足 / Hope speech underrepresented in NLP]
        B --> B2[低资源语言(如乌尔都语)缺乏资源 / Lack of resources for low-resource languages (e.g., Urdu)]
        C --> C1[使用预训练多语言Transformer模型 / Use pretrained multilingual Transformer models]
        C --> C2[简单预处理与分类器训练 / Simple preprocessing & classifier training]
        D --> D1[乌尔都语二元分类F1: 95.2% / Urdu binary F1: 95.2%]
        D --> D2[乌尔都语多类分类F1: 65.2% / Urdu multi-class F1: 65.2%]
        D --> D3[多语言模型适用于低资源环境 / Multilingual models viable for low-resource settings]
    ```

- **[arXiv251230] Memento-II: Learning by Stateful Reflective Memory**
  - **tags:** [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]
  - **authors:** Jun Wang
  - **institution:** University College London (UCL)
  - **link:** https://arxiv.org/pdf/2512.22716
  - **contributions:** 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]
        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]
        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]
        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]
        C --> F[读取对应策略改进/Reading corresponds to policy improvement]
    ```

- **[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents**
  - **tags:** [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]
  - **authors:** Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo
  - **institution:** Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI
  - **link:** https://arxiv.org/pdf/2512.22733
  - **code:** https://github.com/SHAO-Jiaqi757/FoldAct
  - **contributions:** 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[非平稳观测分布 / Non-stationary Observation Distribution]
        B --> B2[梯度稀释 / Gradient Dilution]
        B --> B3[计算成本高 / High Computational Cost]
        C --> C1[分离损失计算 / Separated Loss Computation]
        C --> C2[全上下文一致性损失 / Full Context Consistency Loss]
        C --> C3[选择性片段训练 / Selective Segment Training]
        D --> D1[稳定训练 / Stable Training]
        D --> D2[5.19倍加速 / 5.19× Speedup]
    ```

- **[arXiv251230] Harnessing Large Language Models for Biomedical Named Entity Recognition**
  - **tags:** [nlp], [named entity recognition], [instruction tuning, data filtering, weak-to-strong learning, biomedical named entity recognition, json generation]
  - **authors:** Jian Chen, Leilei Su, Cong Sun
  - **institution:** Hainan University, Weill Cornell Medicine
  - **link:** https://arxiv.org/pdf/2512.22738
  - **contributions:** 1. Proposes BioSelectTune, a data-centric framework for fine-tuning LLMs for BioNER that prioritizes data quality. 2. Introduces a Hybrid Superfiltering strategy, a weak-to-strong data curation method to distill a high-impact training dataset. 3. Reformulates BioNER as a structured JSON generation task to leverage LLMs' instruction-following capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of adapting general-domain LLMs to Biomedical Named Entity Recognition (BioNER) by proposing BioSelectTune, a framework that uses a novel Hybrid Superfiltering data curation strategy and formulates BioNER as a JSON generation task. The method achieves state-of-the-art performance on multiple benchmarks, outperforming specialized models even when trained on only 50% of the curated data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Harnessing LLMs for BioNER] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs lack domain knowledge for BioNER / LLMs缺乏生物医学领域知识]
        B --> B2[Low-quality data degrades performance / 低质量数据导致性能下降]
        C --> C1[BioSelectTune Framework / BioSelectTune框架]
        C1 --> C2[Reformulate as JSON generation / 重构为JSON生成任务]
        C1 --> C3[Hybrid Superfiltering / 混合超级过滤策略]
        D --> D1[SOTA on benchmarks / 基准测试达到SOTA]
        D --> D2[Outperforms BioMedBERT / 超越BioMedBERT]
        D --> D3[50% data surpasses baseline / 50%数据超越全量基线]
    ```

- **[arXiv251230] Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning**
  - **tags:** [mlsys], [llm training], [Column Type Annotation, Prompt Augmentation, LoRA, Parameter-Efficient Fine-Tuning, Prompt Sensitivity]
  - **authors:** Hanze Meng, Jianhao Cao, Rachel Pottinger
  - **institution:** University of British Columbia
  - **link:** https://arxiv.org/pdf/2512.22742
  - **code:** https://github.com/fripSideMeng/PACTA
  - **contributions:** 1. Proposes a parameter-efficient fine-tuning framework for Column Type Annotation (CTA) using Low-Rank Adaptation (LoRA) to reduce computational cost. 2. Introduces a prompt augmentation strategy during training to mitigate model sensitivity to variations in prompt wording and structure. 3. Demonstrates robust and stable performance across diverse datasets and prompt templates, achieving higher weighted F1 scores than single-template fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95317c9af6072a1e0ffbb34950b7d9da057c55baaf301c56bb751746b366785a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of prompt sensitivity and high computational cost in using Large Language Models (LLMs) for Column Type Annotation. It proposes a parameter-efficient framework that fine-tunes LLMs using LoRA on prompt-augmented data. The method achieves robust performance across different prompts and datasets while requiring significantly fewer trainable parameters.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有方法对提示词敏感/Existing methods are sensitive to prompts")
        Problem --> P2("完全微调成本高昂/Full fine-tuning is computationally prohibitive")
        Method --> M1("使用LoRA进行参数高效微调/Parameter-efficient fine-tuning with LoRA")
        Method --> M2("使用增强的提示数据进行训练/Training on prompt-augmented data")
        Results --> R1("对不同提示模式性能稳定/Stable performance across diverse prompts")
        Results --> R2("获得更高的加权F1分数/Higher weighted F1 scores")
    ```

- **[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations**
  - **tags:** [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]
  - **authors:** Zheng Qiu, Chih-Yuan Chiu, Glen Chou
  - **institution:** Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22757
  - **contributions:** 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Active Constraint Learning in High Dimensions from Demonstrations") --> Problem("核心问题/Problem: Data-inefficient constraint inference from demonstrations")
        Root --> Method("主要方法/Method: Iterative active learning with Gaussian Processes")
        Root --> Results("关键结果/Results: Outperforms baseline with sparse, informative demonstrations")
    ```

- **[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer**
  - **tags:** [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (μP), compute-optimal grid search]
  - **authors:** Nikhil Ghosh, Denny Wu, Alberto Bietti
  - **institution:** Flatiron Institute, New York University
  - **link:** https://arxiv.org/pdf/2512.22768
  - **contributions:** 1. Develops a formal conceptual framework defining "fast" hyperparameter transfer and proves its equivalence to "useful" transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Understanding the Mechanisms of Fast Hyperparameter Transfer<br>理解快速超参数迁移的机制"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Standard HP tuning is too expensive for large models.<br>标准HP调优对于大模型过于昂贵"] --> P1["子问题/Sub-problem<br>How to define and understand 'fast' HP transfer?<br>如何定义和理解'快速'HP迁移？"]
        Method["主要方法/Method<br>Develop a formal framework for HP transfer.<br>建立HP迁移的形式化框架"] --> M1["方法步骤/Step<br>Define 'fast' vs 'useful' transfer.<br>定义'快速'与'有用'迁移"]
        Method --> M2["方法步骤/Step<br>Analyze problem structure & µP.<br>分析问题结构与µP"]
        Method --> M3["方法步骤/Step<br>Propose trajectory decomposition hypothesis.<br>提出轨迹分解假设"]
        Results["关键结果/Results<br>Fast transfer is equivalent to useful transfer.<br>快速迁移等价于有用迁移"] --> R1["结果/Result<br>Transfer success depends on problem structure.<br>迁移成功取决于问题结构"]
        Results --> R2["结果/Result<br>Empirical evidence supports the hypothesis.<br>实证证据支持该假设"]
    ```

- **[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting**
  - **tags:** [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]
  - **authors:** Yiqian Li, Wen Jiang, Kostas Daniilidis
  - **institution:** University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.22771
  - **contributions:** 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --> B(核心问题/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)
    A --> C(主要方法/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)
    A --> D(关键结果/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)
    ```

- **[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning**
  - **tags:** [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]
  - **authors:** Kasra Jalaldoust, Elias Bareinboim
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2512.22777
  - **contributions:** 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning] --> B[核心问题/Problem: Generalization under distribution shift]
    A --> C[主要方法/Method: Circuit-TR algorithm based on causal transportability]
    A --> D[关键结果/Results: Theoretical characterization of few-shot learnability]
    B --> B1[领域泛化与适应/Domain Generalization & Adaptation]
    C --> C1[模块学习与电路组合/Module Learning & Circuit Composition]
    C --> C2[因果图与机制共享/Causal Graph & Mechanism Sharing]
    D --> D1[可迁移性标准/Transportability Criteria]
    D --> D2[电路规模复杂度/Circuit Size Complexity]
    ```

- **[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks**
  - **tags:** [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]
  - **authors:** Xuyan Li, Jie Wang, Zheng Yan
  - **institution:** Xidian University
  - **link:** https://arxiv.org/pdf/2512.22772
  - **contributions:** 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[TGNN缺乏透明度和可解释性/Lack of TGNN transparency & explainability]
        B --> B2[现有方法通用性差、效率低、不友好/Existing methods lack generality, efficiency, user-friendliness]
        C --> C1[提取节点序列作为统一特征/Extract node sequences as unified features]
        C --> C2[使用BFS和时间信息构建序列/Use BFS & temporal info to construct sequences]
        C --> C3[基于RNN的生成模型/RNN-based generative model]
        D --> D1[在6个数据集上实验/Experiments on 6 datasets]
        D --> D2[优于现有基线方法/Outperforms existing baselines]
        D --> D3[通用、高效、用户友好/Generality, efficiency, user-friendliness]
    ```

- **[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools**
  - **tags:** [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]
  - **authors:** Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong
  - **institution:** University of Southern California
  - **link:** https://arxiv.org/pdf/2512.22795
  - **contributions:** 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CNSight: 临床笔记分割工具评估 / CNSight: Evaluation of Clinical Note Segmentation Tools]
        Root --> Problem[临床笔记非结构化 / Clinical Notes Unstructured]
        Root --> Method[评估规则/变换器/大语言模型 / Evaluate Rule-based/Transformer/LLMs]
        Root --> Results[大模型性能最佳 / Large Models Best Performance]
    ```

- **[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance**
  - **tags:** [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]
  - **authors:** Shuai Chen, Chen Wang, Ziran Wang
  - **institution:** School of Mechanical Engineering, Shandong University
  - **link:** https://arxiv.org/pdf/2512.22792
  - **contributions:** 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Feature drift & unknown gas interference in E-nose"] --> P1["信号漂移/Feature Distribution Shift"]
        Problem --> P2["未知气体干扰/Unknown Gas Interference"]
        Method["主要方法/Method<br>SNM-Net Framework"] --> M1["几何解耦/Geometric Decoupling<br>Cascaded Batch & L2 Norm"]
        Method --> M2["马氏距离评分/Mahalanobis Distance Scoring"]
        Method --> M3["架构无关/Architecture-Agnostic<br>CNN, RNN, Transformer"]
        Results["关键结果/Results<br>State-of-the-art performance"] --> R1["高AUROC/High AUROC: 0.9977"]
        Results --> R2["高未知气体检测率/High Unknown Detection: 99.57%"]
        Results --> R3["强鲁棒性/High Robustness<br>Low std. dev."]
    ```

- **[arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach**
  - **tags:** [ai], [differential games], [Hamilton-Jacobi reachability, reach-avoid games, dimensionality decomposition, UAVs, tracking control]
  - **authors:** Minh Bui, Simon Monckton, Mo Chen
  - **institution:** Simon Fraser University, Defense Research & Development Canada (DRDC)
  - **link:** https://arxiv.org/pdf/2512.22793
  - **contributions:** 1. A novel dimensionality reduction framework for 3D reach-avoid games by decomposing the problem into horizontal and vertical sub-games., 2. A Hamilton-Jacobi-based tracking control algorithm to reconstruct the solution from sub-games, guaranteeing capture and subsequent tracking of the attacker., 3. Theoretical proof of the conditions for maintaining capture guarantees and empirical validation in both numerical simulations and a physics simulator (Gazebo).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14da164894760eae683bf30139829cd77a6bbd67cf14fee19eb09a05cb31eff_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the high-dimensional challenge of 3D reach-avoid differential games for UAVs by proposing a decomposition approach that splits the problem into horizontal and vertical sub-games, solves them using Hamilton-Jacobi reachability analysis, and uses a novel tracking control to reconstruct the solution. The method is proven to maintain optimality and capture guarantees, and its effectiveness is successfully demonstrated through simulations and a physics simulator for quadrotor capture.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[3D追逃博弈高维挑战/High Dimensionality of 3D Reach-Avoid Games]
        B --> B2[现有方法局限性/Limitations of Existing Approaches]
        C --> C1[维度分解/Dimensionality Decomposition]
        C1 --> C1_1[水平子博弈/Horizontal Sub-game]
        C1 --> C1_2[垂直子博弈/Vertical Sub-game]
        C --> C2[HJ可达性分析/HJ Reachability Analysis]
        C --> C3[HJ跟踪控制/HJ-based Tracking Control]
        D --> D1[保持最优性与保证/Maintains Optimality & Guarantees]
        D --> D2[仿真验证/Simulation Validation]
        D --> D3[物理模拟器成功捕获/Successful Capture in Physics Simulator]
    ```

- **[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]
  - **authors:** Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani
  - **institution:** Nvidia, Meta
  - **link:** https://arxiv.org/pdf/2512.22804
  - **contributions:** 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MoR: Mixture Of Representations For Mixed-Precision Training] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Successful mixed-precision training requires the right combination of methods.]
        Method[主要方法/Method<br>Dynamic, property-aware quantization framework selecting between representations (e.g., FP8/BF16).]
        Results[关键结果/Results<br>Achieves 98.38% FP8 quantization, preserves model quality, enables lower precision formats.]
    ```

- **[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation**
  - **tags:** [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]
  - **authors:** Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou
  - **institution:** THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT
  - **link:** https://arxiv.org/pdf/2512.22808
  - **contributions:** 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有数据空间不一致/Existing data spatial misalignment]
        B --> B2[因果生成与3D对齐的挑战/Causal generation & 3D alignment challenge]
        C --> C1[构建HRD数据集/Build HRD dataset]
        C --> C2[VQ-VAE压缩运动/VQ-VAE compresses motion]
        C --> C3[GPT自回归生成/GPT autoregressive generation]
        C --> C4[融入3D动态特征/Incorporate 3D dynamic features]
        D --> D1[更高的真实感与空间一致性/Higher realism & spatial consistency]
        D --> D2[实时生成效率/Real-time generation efficiency]
        D --> D3[保持严格因果性/Maintains strict causality]
    ```

- **[arXiv251230] FasterPy: An LLM-based Code Execution Efficiency Optimization Framework**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [Code Optimization, Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), Large Language Models (LLMs), Python]
  - **authors:** Yue Wu, Minghao Han, Ruiyin Li, Peng Liang, Amjed Tahir, Zengyang Li, Qiong Feng, Mojtaba Shahin
  - **institution:** Wuhan University, Carnegie Mellon University, Massey University, Central China Normal University, Nanjing University of Science and Technology, RMIT University
  - **link:** https://arxiv.org/pdf/2512.22827
  - **code:** https://github.com/WuYue22/fasterpy
  - **contributions:** 1. Proposes FasterPy, a low-cost and efficient framework that adapts LLMs for Python code execution efficiency optimization. 2. Combines Retrieval-Augmented Generation (RAG) with a knowledge base of performance-improving code pairs and Low-Rank Adaptation (LoRA) to enhance optimization performance. 3. Demonstrates superior performance over existing models on the Performance Improving Code Edits (PIE) benchmark.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49aae1b7cd12cfd30401a619c9b06d4bccc853d1d14eed57af87eb6c80858f31_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FasterPy, a framework that uses Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) to automatically optimize Python code for better execution efficiency. It addresses the limitations of traditional rule-based and data-intensive ML methods by providing a more scalable and cost-effective solution. Experimental results show that FasterPy outperforms existing models on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FasterPy: An LLM-based Code Execution Efficiency Optimization Framework] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统方法成本高，可扩展性差/Traditional methods are costly and hard to scale]
        C --> C1[结合RAG与LoRA的LLM框架/LLM framework combining RAG and LoRA]
        D --> D1[在PIE基准上表现优异/Outperforms existing models on PIE benchmark]
    ```

- **[arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning**
  - **tags:** [mlsys], [agent system], [automated environment synthesis, environment-level RL, agentic reinforcement learning, simulated user, policy optimization]
  - **authors:** Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22857
  - **contributions:** 1. A unified, automated pipeline for synthesizing scalable simulated environments with high-difficulty, easily verifiable tasks. 2. An Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and performs advantage estimation at the environment level. 3. Comprehensive validation on agentic benchmarks demonstrating effectiveness and out-of-domain generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AutoForge, a framework to automate the synthesis of challenging simulated environments for training language-based agents via reinforcement learning. It introduces an environment-level RL algorithm to improve training stability and efficiency by handling simulated user instability and heterogeneous environments. Evaluations show the method is effective and generalizes well to out-of-domain tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoForge] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[环境合成半自动/Semi-automated Environment Synthesis]
        B --> B2[任务难度不足/Insufficient Task Difficulty]
        B --> B3[模拟用户不稳定/Simulated User Instability]
        C --> C1[自动化环境合成管道/Automated Environment Synthesis Pipeline]
        C --> C2[环境级RL算法/Environment-level RL Algorithm (ERPO)]
        D --> D1[基准测试有效/Effective on Benchmarks (τ-bench, etc.)]
        D --> D2[域外泛化强/Strong Out-of-domain Generalization]
    ```

- **[arXiv251230] The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al**
  - **tags:** [other], [embodied cognition, robotics], [morphological computation, embodiment, information theory, passive dynamic walker]
  - **authors:** Matej Hoffmann
  - **institution:** Czech Technical University in Prague
  - **link:** https://arxiv.org/pdf/2512.22868
  - **contributions:** 1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of "morphological computation". 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp
  - **Simple LLM Summary:** This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of "morphological computation" is merely a metaphor and that the body's main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[The body is not there to compute<br>身体不是为了计算] --> Problem[核心问题/Problem<br>Is the body's primary role computational?<br>身体的主要作用是计算吗？]
        Root --> Method[主要方法/Method<br>Conceptual critique of "morphological computation"<br>对"形态计算"的概念性批判]
        Root --> Results[关键结果/Results<br>Body's role is physical, not computational<br>身体的作用是物理的，而非计算的]
    ```

- **[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks**
  - **tags:** [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]
  - **authors:** Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov
  - **institution:** Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2512.22876
  - **contributions:** 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement Networks] --> B[核心问题/Problem: End-to-end training of multi-component AI systems]
        A --> C[主要方法/Method: MARL agents organized in a DAG (Reinforcement Networks)]
        A --> D[关键结果/Results: Improved performance, unified framework for structured MARL]
    ```

- **[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]
  - **authors:** Hasan Faraz Khan, Noor Fatima, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22878
  - **contributions:** 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --> Problem_Sub[问题细节/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]
        Method[主要方法/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --> Method_Sub[方法细节/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]
        Results[关键结果/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --> Results_Sub[结果细节/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]
    ```

- **[arXiv251230] Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations**
  - **tags:** [sec], [cyber resilience], [agentic AI, game theory, autonomous agents, system-theoretic framework, equilibrium-based design]
  - **authors:** Tao Li, Quanyan Zhu
  - **institution:** City University of Hong Kong, New York University
  - **link:** https://arxiv.org/pdf/2512.22883
  - **contributions:** 1. Proposes a paradigm shift from prevention-centric security to agentic cyber resilience, arguing for systems that anticipate, maintain, recover, and learn under attack. 2. Develops a system-level framework and general architecture for designing AI workflows where autonomous agents participate in sensing, reasoning, and action. 3. Demonstrates how game-theoretic formulations provide a unifying design language for analyzing coupled attacker-defender workflows and enable equilibrium-based resiliency design, illustrated with case studies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1d5f82743a059040190978c2a78338bb73c72bc9cec9a0aafe00a0d12f0f24d_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the rise of foundation-model-based AI necessitates a shift from traditional prevention-focused cybersecurity to a new paradigm of agentic cyber resilience. It proposes a system-theoretic framework for designing autonomous AI workflows and uses game theory as a unifying language to model attacker-defender dynamics, concluding that equilibrium-based design enables system-level resilience as demonstrated in case studies like automated penetration testing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations") --> Problem("核心问题/Problem: Traditional static, human-centered security architectures are mismatched with AI-driven, adaptive cyber threats.")
        Root --> Method("主要方法/Method: Proposes a shift to agentic cyber resilience and a system-level framework using game theory to design autonomous AI workflows.")
        Root --> Results("关键结果/Results: Equilibrium-based design enables system-level resiliency, illustrated through case studies in automated pentesting and cyber deception.")
    ```

- **[arXiv251230] DECEPTICON: How Dark Patterns Manipulate Web Agents**
  - **tags:** [mlsys], [agent system], [dark patterns, web agents, adversarial robustness, deceptive UI, agent testing]
  - **authors:** Phil Cuvin, Hao Zhu, Diyi Yang
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.22894
  - **code:** https://agentdarkpatterns.org
  - **contributions:** 1. Introduces DECEPTICON, a novel environment for testing dark patterns in isolation with 700 web navigation tasks, 2. Demonstrates that dark patterns successfully manipulate agent trajectories in over 70% of tasks, significantly higher than human susceptibility, 3. Shows that larger, more capable models are more susceptible to dark patterns, and existing countermeasures like in-context prompting and guardrail models fail to mitigate the risk effectively.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8998ab43971416f683709173f00be5e8d5373de89f15ac199ec42d645a75b8b6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DECEPTICON, a testing environment to evaluate how dark patterns manipulate web agents, revealing that these deceptive UI designs successfully steer agent actions in over 70% of tasks, with larger models being more vulnerable and current defenses ineffective. The findings highlight an urgent need for robust defenses against such manipulative designs in agent systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DECEPTICON: How Dark Patterns Manipulate Web Agents] --> B[核心问题/Problem: Dark patterns manipulate users and pose risks to agent robustness]
        A --> C[主要方法/Method: DECEPTICON environment with 700 tasks to test dark patterns in isolation]
        A --> D[关键结果/Results: Dark patterns steer agents in >70% tasks, larger models more susceptible, defenses fail]
    ```

- **[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery**
  - **tags:** [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]
  - **authors:** Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22899
  - **contributions:** 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]
        C[主要方法/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]
        D[关键结果/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]
    ```

- **[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments**
  - **tags:** [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]
  - **authors:** Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu
  - **institution:** University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22901
  - **contributions:** 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight "Collar Recognition Nets" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight "Collar Recognition Nets" optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Neural Network-Based Real-time Casing Collar Recognition System<br>基于神经网络的实时套管接箍识别系统"] --> Problem
        Root --> Method
        Root --> Results
        Problem["信号衰减导致井下定位不准确<br>Signal degradation compromises downhole positioning"]
        Method["为ARM Cortex-M7优化的轻量级CRN网络<br>Lightweight CRNs optimized for ARM Cortex-M7"]
        Results["8208 MACs, F1=0.972, 343.2µs延迟<br>8208 MACs, F1=0.972, 343.2µs latency"]
    ```

- **[arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [hierarchical deep reinforcement learning, portfolio management, dynamic asset grouping, utility-based capital allocation, SHAP interpretability]
  - **authors:** Xiaotian Ren, Nuerxiati Abudurexiti, Zhengyong Jiang, Angelos Stefanidis, Hongbin Liu, Jionglong Su
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.22895
  - **contributions:** 1. Proposes a hierarchical DRL framework (SAMP-HDRL) that integrates dynamic asset grouping, upper-lower agent coordination, and a utility-based capital allocation mechanism for robust portfolio management. 2. Demonstrates superior performance through extensive backtests across multiple market regimes, showing consistent improvements in return and risk-adjusted metrics over traditional and DRL baselines. 3. Provides interpretability via SHAP analysis, revealing a complementary "diversified + concentrated" decision pattern across agent layers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07244b408b9238d10b2d5561e0007db8732b1d4e9e79bda5477bef5db2dd385c_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles portfolio optimization in non-stationary markets by proposing SAMP-HDRL, a hierarchical deep reinforcement learning framework that segments assets, coordinates global and local agents, and uses a utility-based capital allocator. The method outperforms numerous baselines in backtests, achieving higher returns and risk-adjusted ratios, and its decisions are made interpretable through SHAP analysis, revealing a combined diversified and concentrated investment strategy.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Portfolio optimization in non-stationary markets with regime shifts and limited DRL interpretability] --> P1[挑战/Challenges: Dynamic correlations, regime shifts]
        Method[主要方法/Method: Hierarchical DRL with segmented allocation] --> M1[上层代理/Upper-level Agent: Extracts global market signals]
        Method --> M2[动态资产分组/Dynamic Asset Grouping: Partitions market into subsets]
        Method --> M3[下层代理/Lower-level Agents: Perform intra-group allocation]
        Method --> M4[效用资本分配/Utility-based Capital Allocation: Integrates risky & risk-free assets]
        Results[关键结果/Results: Outperforms baselines, provides interpretability] --> R1[性能/Performance: Higher Return, Sharpe, Sortino, Omega ratios]
        Results --> R2[可解释性/Interpretability: SHAP reveals "diversified + concentrated" mechanism]
    ```

- **[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]
  - **authors:** Ünver Çiftçi
  - **institution:** Tekirdağ Namık Kemal University
  - **link:** https://arxiv.org/pdf/2512.22910
  - **contributions:** 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Sat-EnQ] --> B[核心问题/Problem: Deep Q-Learning Instability]
        A --> C[主要方法/Method: Two-Phase Satisficing Ensemble]
        A --> D[关键结果/Results: Variance Reduction & Robustness]
        B --> B1[早期训练不稳定/Early Training Instability]
        B --> B2[高方差与灾难性失败/High Variance & Catastrophic Failure]
        C --> C1[阶段1: 满足化集成训练/Phase 1: Satisficing Ensemble Training]
        C --> C2[阶段2: 蒸馏与微调/Phase 2: Distillation & Fine-tuning]
        D --> D1[3.8倍方差降低/3.8x Variance Reduction]
        D --> D2[0%灾难性失败/0% Catastrophic Failure]
        D --> D3[2.5倍计算效率提升/2.5x Compute Efficiency]
    ```

- **[arXiv251230] Multimodal Fact-Checking: An Agent-based Approach**
  - **tags:** [ai], [multimodal fact-checking], [multimodal misinformation, agent-based reasoning, explainable dataset, vision-language models, evidence retrieval]
  - **authors:** Danni Xu, Shaojing Fan, Xuanang Cheng, Mohan Kankanhalli
  - **institution:** National University of Singapore (NUS)
  - **link:** https://arxiv.org/pdf/2512.22933
  - **contributions:** 1. Introduces RW-Post, a high-quality, explainable dataset for real-world multimodal fact-checking that aligns claims with original social media posts and provides detailed reasoning and evidence. 2. Proposes AgentFact, a novel agent-based multimodal fact-checking framework that emulates the human verification workflow through five specialized, collaboratively working agents. 3. Demonstrates that the synergy between the new dataset and the agent framework substantially improves both the accuracy and interpretability of multimodal fact-checking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated multimodal fact-checking by introducing a new dataset (RW-Post) and an agent-based framework (AgentFact). The dataset provides real-world misinformation instances with reasoning and evidence, while the framework uses specialized agents to collaboratively perform verification tasks. The combined approach is shown to significantly enhance the accuracy and explainability of fact-checking systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multimodal Fact-Checking: An Agent-based Approach] --> B[核心问题/Problem: 多模态虚假信息传播与现有方法在推理和证据利用上的不足 / The spread of multimodal misinformation and the limitations of existing methods in reasoning and evidence utilization]
        A --> C[主要方法/Method: 提出RW-Post数据集和AgentFact智能体框架 / Proposes the RW-Post dataset and the AgentFact agent-based framework]
        A --> D[关键结果/Results: 显著提升了多模态事实核查的准确性和可解释性 / Substantially improves the accuracy and interpretability of multimodal fact-checking]
    ```

- **[arXiv251230] Geometric Structural Knowledge Graph Foundation Model**
  - **tags:** [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]
  - **authors:** Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab
  - **institution:** University of Stuttgart, University of Southampton, Shahrood University of Technology
  - **link:** https://arxiv.org/pdf/2512.22931
  - **contributions:** 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometric Structural Knowledge Graph Foundation Model] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖单一关系转换，表达能力受限/Existing methods rely on single relational transformation, limiting expressiveness]
        C --> C1[引入多头几何注意力/Multi-head geometric attention]
        C --> C2[并行多种几何变换/Parallel geometric transformations]
        C --> C3[关系条件注意力融合/Relational conditioned attention fusion]
        D --> D1[在56个KG上超越ULTRA/Outperforms ULTRA on 56 KGs]
        D --> D2[零样本归纳链接预测性能提升/Improves zero-shot inductive link prediction]
    ```

- **[arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning**
  - **tags:** [ai], [multi-agent reinforcement learning], [heterogeneity, multi-agent reinforcement learning, parameter sharing, heterogeneity distance, dynamic algorithm]
  - **authors:** Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22941
  - **code:** https://github.com/Harry67Hu/HetDPS
  - **contributions:** 1. Proposes a systematic categorization of heterogeneity in MARL into five types with mathematical definitions. 2. Defines a heterogeneity distance and introduces a practical method to quantify agent heterogeneity. 3. Designs a heterogeneity-based dynamic parameter sharing algorithm that demonstrates better interpretability and adaptability compared to baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4448c413f180749bc7f2220bea2793dad9a358fb068164020bd7b0421e5b05_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a rigorous definition and understanding of heterogeneity in multi-agent reinforcement learning (MARL). It proposes a methodology to define, quantify, and utilize heterogeneity, culminating in a dynamic parameter sharing algorithm. Experiments show this algorithm offers improved interpretability and adaptability over other parameter-sharing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Heterogeneity in Multi-Agent Reinforcement Learning<br/>多智能体强化学习中的异质性"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["缺乏对异质性的严格定义<br/>Lacks rigorous definition of heterogeneity"]
        Method --> M1["定义与分类<br/>Definition & Categorization"]
        Method --> M2["量化方法<br/>Quantification Method"]
        Method --> M3["应用算法<br/>Application Algorithm"]
        M1 --> M1_1["五类异质性<br/>Five types of heterogeneity"]
        M2 --> M2_1["异质性距离<br/>Heterogeneity distance"]
        M3 --> M3_1["动态参数共享<br/>Dynamic Parameter Sharing"]
        Results --> R1["有效识别与量化<br/>Effective identification & quantification"]
        Results --> R2["算法性能优越<br/>Algorithm outperforms baselines"]
    ```

- **[arXiv251230] APO: Alpha-Divergence Preference Optimization**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]
  - **authors:** Wang Zixian
  - **institution:** China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch
  - **link:** https://arxiv.org/pdf/2512.22953
  - **contributions:** 1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[APO: Alpha-Divergence Preference Optimization] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[两种分歧权衡 / Two Divergence Trade-off]
        P1 --> P2[前向KL覆盖但保守 / Forward KL: Mode-Covering but Conservative]
        P1 --> P3[反向KL寻求但易崩溃 / Reverse KL: Mode-Seeking but Collapses]
        Method[主要方法/Method] --> M1[锚定框架 / Anchored Framework]
        M1 --> M2[使用α-散度插值 / Use α-Divergence to Interpolate]
        M2 --> M3[调度α值 / Schedule α Value]
        Results[关键结果/Results] --> R1[竞争性性能 / Competitive Performance]
        Results --> R2[保持稳定性 / Maintains Training Stability]
    ```

- **[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding**
  - **tags:** [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]
  - **authors:** Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang
  - **institution:** Nanjing University, China Mobile Zijin Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.23020
  - **contributions:** 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖预定义对象表，无法处理未定义目标/Existing methods rely on pre-defined OLT, limiting open-world application]
        C --> C1[提出OpenGround框架与主动认知推理模块/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]
        C1 --> C2[通过认知任务链和动态更新的OLT增强VLM认知/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]
        D --> D1[Nr3D上表现有竞争力，ScanRefer上达到SOTA/Competitive on Nr3D, SOTA on ScanRefer]
        D --> D2[在OpenTarget数据集上提升17.6%/17.6% improvement on OpenTarget dataset]
    ```

- **[arXiv251230] LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models**
  - **tags:** [nlp], [multimodal language models], [multimodal sensing, time-series encoding, ecological momentary assessment (EMA)]
  - **authors:** Wenxuan Xu, Arvind Pillai, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell
  - **institution:** Dartmouth College, University of Virginia, Massachusetts General Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2512.23025
  - **contributions:** 1. Introduces LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental health narratives. 2. Constructs a large-scale dataset of over 100,000 sensor-text QA pairs by transforming Ecological Momentary Assessment (EMA) responses. 3. Trains a patch-level encoder to project raw sensor time-series signals directly into an LLM's representation space for native integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of translating long-duration, multimodal sensor data into interpretable natural language for mental health assessment. It proposes the LENS framework, which creates a large sensor-text dataset and trains a specialized encoder to align sensor signals with an LLM, enabling the generation of clinically meaningful narratives. The results show LENS outperforms baselines on NLP and clinical metrics, and is validated by mental health professionals.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LENS: LLM-Enabled Narrative Synthesis] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传感器数据难以转化为自然语言/Sensor data hard to translate to text]
        Problem --> P2[缺乏配对数据集/Lack of paired sensor-text datasets]
        Method[主要方法/Method] --> M1[构建大规模传感器-文本QA数据集/Build large-scale sensor-text QA dataset]
        Method --> M2[训练补丁级编码器对齐LLM/Train patch-level encoder to align with LLM]
        Results[关键结果/Results] --> R1[在NLP和症状指标上超越基线/Outperforms baselines on NLP & symptom metrics]
        Results --> R2[临床医生认为叙述全面有意义/Clinicians find narratives comprehensive & meaningful]
    ```

- **[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project**
  - **tags:** [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]
  - **authors:** Thomson Tong, Diba Darooneh
  - **institution:** None
  - **link:** https://arxiv.org/pdf/2512.23028
  - **code:** BodyLanguageDetection repository [1]
  - **contributions:** 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp
  - **Simple LLM Summary:** This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Architecture-Led Hybrid Report on Body Language Detection Project] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[如何基于VLM架构构建可靠的应用系统/How to build reliable application systems based on VLM architecture]
        C --> C1[分析两种VLM架构并映射到视频处理流程/Analyze two VLM architectures and map to a video processing pipeline]
        C --> C2[系统采样视频帧，提示VLM生成结构化输出/System samples video frames, prompts VLM for structured output]
        C --> C3[使用预定义模式验证输出结构/Validate output structure with predefined schema]
        D --> D1[结构化输出可能语法正确但语义错误/Structured outputs can be syntactically valid but semantically incorrect]
        D --> D2[模式验证是结构性的，非几何正确性/Schema validation is structural, not geometric]
        D --> D3[理解架构对设计稳健接口和评估至关重要/Understanding architecture is critical for robust interface design and evaluation]
    ```

- **[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware**
  - **tags:** [mlsys], [llm inference], [quantization, mixture-of-experts, on-premise deployment, consumer-grade hardware, benchmark analysis]
  - **authors:** Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles
  - **institution:** UCLouvain, Universidad Espíritu Santo, DENEM Labs
  - **link:** https://arxiv.org/pdf/2512.23029
  - **contributions:** 1. A comprehensive benchmarking framework for evaluating both the intrinsic model capabilities and the server-side performance (latency, throughput, scalability) of a private LLM deployment. 2. A practical demonstration and performance analysis of deploying a quantized, large-scale (30B parameter) Mixture-of-Experts model (Qwen3) on next-generation consumer-grade hardware (NVIDIA RTX 5090). 3. Evidence that a carefully configured on-premises LLM server can achieve performance comparable to cloud services, offering SMBs a viable, cost-effective, and privacy-preserving alternative.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the feasibility of deploying a private, high-performance LLM server for Small and Medium Businesses using consumer-grade hardware. It benchmarks a quantized Qwen3-30B model on an NVIDIA RTX 5090, evaluating both model capability and server performance under load. The results show that such an on-premises setup can achieve performance close to cloud services at a lower cost and with full data privacy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Viability and Performance of a Private LLM Server for SMBs<br>SMB私有LLM服务器的可行性与性能] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Cloud reliance: cost, privacy, sovereignty for SMBs<br>云依赖：成本、隐私、SMB主权]
        C[主要方法/Method<br>Benchmark quantized Qwen3-30B on consumer hardware (RTX 5090)<br>在消费级硬件上对量化Qwen3-30B进行基准测试]
        D[关键结果/Results<br>On-premises performance rivals cloud, viable for SMBs<br>本地性能媲美云端，对SMB可行]
    ```

- **[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization**
  - **tags:** [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]
  - **authors:** Kerem Zaman, Shashank Srivastava
  - **institution:** UNC Chapel Hill
  - **link:** https://arxiv.org/pdf/2512.23032
  - **contributions:** 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp
  - **Simple LLM Summary:** This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Biasing Features 指标将不完整性误判为不忠实性<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]
        C --> C1[提出 faithful@k 指标并增加推理令牌预算<br/>Propose faithful@k metric & increase inference token budget]
        C --> C2[使用因果中介分析<br/>Use Causal Mediation Analysis]
        D --> D1[许多被标记为不忠实的 CoT 被其他指标判定为忠实<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]
        D --> D2[更大的令牌预算显著提高提示词显化率<br/>Larger token budgets greatly increase hint verbalization]
        D --> D3[未显化的提示词仍可通过 CoT 因果中介预测<br/>Non-verbalized hints can causally mediate predictions through CoT]
    ```

- **[arXiv251230] Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education**
  - **tags:** [ai], [educational data mining], [knowledge tracing, learner modelling, temporal coherence, fine-tuning, deep knowledge tracing]
  - **authors:** Danial Hooshyar, Yeongwook Yang, Gustav Šíř, Tommi Kärkkäinen, Raija Hämäläinen, Mutlu Cukurova, Roger Azevedo
  - **institution:** Tallinn University, University of Jyväskylä, Gangneung-Wonju National University, Czech Technical University, University College London, University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.23036
  - **contributions:** 1. Provides a synthesis of evidence on the limitations of LLM-based tutors, framing them within the high-risk context of K-12 education and responsible AI design. 2. Empirically demonstrates that a Deep Knowledge Tracing (DKT) model significantly outperforms a widely-used LLM (both zero-shot and fine-tuned) in next-step correctness prediction and temporal coherence of mastery estimation. 3. Highlights the computational inefficiency of fine-tuning LLMs for this task compared to DKT, and argues for hybrid frameworks over LLM-only approaches for responsible tutoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5735716766e72627a0d5d23b01771e8d0161795e3958d394eccf1045f5a797ec_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether large language models (LLMs) can effectively replace traditional learner modelling for adaptive tutoring in K-12 education. By comparing a Deep Knowledge Tracing (DKT) model against a fine-tuned and zero-shot LLM on knowledge assessment tasks, it finds DKT is more accurate, reliable, and temporally coherent. The study concludes that LLMs alone are insufficient for responsible tutoring and advocates for hybrid systems that incorporate dedicated learner models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Problems With LLMs for Learner Modelling<br/>LLM在学情建模中的问题"] --> B
        A --> C
        A --> D
        B["核心问题/Problem<br/>LLMs may replace learner models<br/>LLM可能替代学情模型"] --> B1["高风险领域/High-risk domain (K-12)"]
        B --> B2["需要评估准确性、可靠性、时序一致性/Need to assess accuracy, reliability, temporal coherence"]
        C["主要方法/Method<br/>Compare DKT vs. LLM<br/>对比DKT与LLM"] --> C1["数据集/Dataset: large open-access"]
        C --> C2["模型/Models: DKT, LLM (zero-shot & fine-tuned)"]
        D["关键结果/Results<br/>DKT outperforms LLM<br/>DKT优于LLM"] --> D1["更高AUC/Higher AUC (0.83)"]
        D --> D2["更好的时序一致性/Better temporal coherence"]
        D --> D3["结论: LLMs alone fall short, need hybrid frameworks<br/>Conclusion: LLM单独不足，需要混合框架"]
    ```

- **[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment**
  - **tags:** [nlp], [alignment & personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]
  - **authors:** Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales
  - **institution:** University of Edinburgh, Agency for Science, Technology and Research (A*STAR)
  - **link:** https://arxiv.org/pdf/2512.23067
  - **contributions:** 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Reward Model Selection Crisis in Personalized Alignment] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Standard RM accuracy fails to predict deployment performance for personalized alignment]
        C[主要方法/Method<br>Introduce policy accuracy metric and Pref-LaMP benchmark for direct evaluation]
        D[关键结果/Results<br>Weak correlation between RM & policy accuracy; ICL outperforms reward-guided decoding]
    ```

- **[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]
  - **authors:** Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang
  - **institution:** (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)
  - **link:** https://arxiv.org/pdf/2512.23075
  - **contributions:** 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Trust Region Masking for Long-Horizon LLM RL] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Off-policy mismatch in LLM-RL<br/>导致经典信任域边界失效]
        C --> C1[提出信任域掩码(TRM)<br/>Propose Trust Region Masking (TRM)]
        C1 --> C2[序列级掩码<br/>Sequence-level Masking]
        D --> D1[推导更紧的理论边界<br/>Derive Tighter Bounds (O(T), O(T^{3/2}))]
        D --> D2[提供非平凡的单调改进保证<br/>Provide Non-vacuous Guarantees]
    ```

- **[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition**
  - **tags:** [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]
  - **authors:** Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu
  - **institution:** Southeast University, Westlake University, Vrije Universiteit Amsterdam
  - **link:** https://arxiv.org/pdf/2512.23076
  - **code:** https://github.com/DY9910/MFMC
  - **contributions:** 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method's effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MFMC for Emotion Recognition] --> B[核心问题/Problem: 情感状态表现为跨系统的协调但异质的生理反应，现有自监督方法难以捕捉多模态高阶交互。]
        A --> C[主要方法/Method: 提出MFMC框架，通过Dual Total Correlation目标和Functional Maximum Correlation Analysis最大化高阶多模态依赖性。]
        A --> D[关键结果/Results: 在多个基准测试中达到SOTA或竞争性性能，显著提升CEAP-360VR数据集上的准确率，对主体间变异性鲁棒。]
    ```

- **[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning**
  - **tags:** [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]
  - **authors:** Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang
  - **institution:** (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)
  - **link:** https://arxiv.org/pdf/2512.23087
  - **contributions:** 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability "tail" tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability "tail" tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练-推理不匹配 / Training-Inference Mismatch]
        B1 --> B2[尾部token导致梯度不稳定 / Tail tokens destabilize gradient estimation]
        C --> C1[动态剪枝词汇表 / Dynamic Vocabulary Pruning]
        C1 --> C2[排除极端尾部token / Exclude extreme tail tokens]
        D --> D1[实现稳定训练 / Achieves stable training]
        D --> D2[理论界定优化偏差 / Theoretically bounds optimization bias]
    ```

- **[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification**
  - **tags:** [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]
  - **authors:** Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang
  - **institution:** Missouri State University
  - **link:** https://arxiv.org/pdf/2512.23089
  - **contributions:** 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedSAM-based lung masking for multi-label chest X-ray classification] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Automated CXR interpretation is challenging<br>自动CXR解读具有挑战性]
        C --> C1[Fine-tune MedSAM for lung segmentation<br>微调MedSAM进行肺部分割]
        C --> C2[Use masks to guide multi-label classification<br>使用掩码指导多标签分类]
        D --> D1[Masking effect is task/architecture dependent<br>掩码效果依赖于任务和架构]
        D --> D2[Trade-off: abnormality vs. normal screening<br>权衡：异常检测与正常筛查]
        D --> D3[Masking is a controllable spatial prior<br>掩码是一种可控的空间先验]
    ```

- **[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]
  - **authors:** Yingru Li, Ziniu Li, Jiacai Liu
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.23097
  - **contributions:** 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hybrid Online RL and IL for LLMs] --> B[核心问题/Problem: Train-inference distribution mismatch in LLM fine-tuning]
        A --> C[主要方法/Method: Unified framework combining Imitation Learning and Reinforcement Learning]
        A --> D[关键结果/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]
    ```

- **[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]
  - **authors:** Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa
  - **institution:** Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn
  - **link:** https://arxiv.org/pdf/2512.23090
  - **contributions:** 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp
  - **Simple LLM Summary:** The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model's ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Benchmark Success, Clinical Failure<br>基准成功，临床失败] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[RL优化提升基准性能但损害泛化<br>RL improves benchmarks but harms generalization]
        C --> C1[使用SFT+GRPO训练ChexReason VLM<br>Train ChexReason VLM with SFT+GRPO]
        D --> D1[GRPO提升CheXpert性能23%<br>GRPO improves CheXpert by 23%]
        D --> D2[GRPO导致NIH性能下降19%<br>GRPO degrades NIH by 19%]
        D --> D3[SFT检查点提升跨数据集泛化<br>SFT checkpoint improves cross-dataset generalization]
    ```

- **[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure**
  - **tags:** [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]
  - **authors:** Paul M. Thompson
  - **institution:** Stevens Institute of Neuroimaging and Informatics, University of Southern California
  - **link:** https://arxiv.org/pdf/2512.23109
  - **contributions:** 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp
  - **Simple LLM Summary:** This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现代生成和视觉语言模型在科学/医疗决策中需要准确且校准良好的概率预测 / Modern generative & VLMs need accurate, calibrated predictions for scientific/medical decisions]
        B --> B2[平均性能良好时，罕见情况或特定子群仍可能出现大误差 / Large errors can persist for rare conditions/subgroups despite low average loss]
        B --> B3[需要何种结构假设才能实现具有实用样本量的均匀泛化？ / What structural assumptions enable uniform generalization with practical sample sizes?]
        C --> C1[分析由提示或语义嵌入在受限表示空间中诱导出的分类器族 / Analyze induced families of classifiers from varying prompts/embeddings in a restricted space]
        C --> C2[假设模型输出对低维语义表示平滑依赖 / Assume model outputs depend smoothly on a low-dimensional semantic representation]
        C --> C3[应用经典均匀收敛工具 / Apply classical uniform convergence tools]
        D --> D1[在Lipschitz稳定性下，为VLM诱导分类器的准确性和校准功能提供有限样本均匀收敛界 / Provide finite-sample uniform convergence bounds for accuracy & calibration of VLM-induced classifiers under Lipschitz stability]
        D --> D2[样本复杂度取决于内在/有效维度，而非环境维度 / Sample complexity depends on intrinsic/effective dimension, not ambient dimension]
        D --> D3[谱相关边界阐明特征值衰减如何控制数据需求 / Spectrum-dependent bounds show how eigenvalue decay governs data requirements]
    ```

- **[arXiv251230] It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents**
  - **tags:** [sec], [prompt injection], [prompt injection, web agents, social-engineering, benchmark, autonomous agents]
  - **authors:** Karolina Korgul, Yushi Yang, Arkadiusz Drohomirecki, Piotr Błaszczyk, Will Howard, Lukas Aichberger, Chris Russell, Philip H.S. Torr, Adam Mahdi, Adel Bibi
  - **institution:** University of Oxford, SoftServe, Johannes Kepler University Linz
  - **link:** https://arxiv.org/pdf/2512.23128
  - **contributions:** 1. Introduces the Task-Redirecting Agent Persuasion Benchmark (TRAP) for evaluating prompt injection vulnerabilities in web-based LLM agents. 2. Provides a modular social-engineering injection framework for controlled experiments on high-fidelity website clones. 3. Demonstrates systemic vulnerabilities, showing agents are susceptible to injection in 25% of tasks on average, with small interface changes often doubling success rates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c246d5b23e99374a1d754ec870b203d23f214abac92f8d20d849cc98d00e86ca_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the vulnerability of web-based LLM agents to prompt injection attacks, where hidden adversarial instructions can divert agents from their tasks. It introduces the TRAP benchmark, built on realistic website clones, to evaluate these vulnerabilities. The study finds significant susceptibility across models, revealing systemic, psychologically driven weaknesses in current agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Web agents vulnerable to prompt injection attacks] --> Problem_Detail[问题详情/Problem Detail: Adversarial instructions in web content can divert agents from original tasks]
        Method[主要方法/Method: Introduce TRAP benchmark & modular injection framework] --> Method_Detail[方法详情/Method Detail: Evaluation on high-fidelity website clones using social-engineering techniques]
        Results[关键结果/Results: Agents susceptible in 25% of tasks on average] --> Results_Detail[结果详情/Results Detail: Small interface changes can double success rates, revealing systemic vulnerabilities]
    ```

- **[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]
  - **authors:** Yu Li, Tian Lan, Zhengling Qi
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.23126
  - **contributions:** 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO's superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DPO Limitations<br/>DPO的局限性]
        B1 --> B2[Lacks Invariance<br/>缺乏不变性]
        B1 --> B3[Suboptimal Use of Data<br/>数据利用次优]
        C --> C1[Propose InSPO<br/>提出InSPO]
        C1 --> C2[Globally Optimal Policy<br/>全局最优策略]
        C2 --> C3[Conditions on Context & Alternative<br/>基于上下文与备选答案]
        D --> D1[Theoretical Superiority<br/>理论优越性]
        D --> D2[Practical Improvement<br/>实际提升]
        D2 --> D3[Better Win Rates<br/>更高的胜率]
        D2 --> D4[No Inference Overhead<br/>无推理开销]
    ```

- **[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion**
  - **tags:** [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]
  - **authors:** Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu
  - **institution:** Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.23130
  - **contributions:** 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: 成像-病理MRI合成"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>生成模型中的特征纠缠导致解剖结构损坏"] --> P1["现有范式/Existing Paradigms<br>Global pixel domain or binary masks<br>全局像素域或二进制掩码"]
        Method["主要方法/Method<br>Disentangled Deviation Diffusion<br>解耦偏差扩散"] --> M1["分解任务/Decompose Task<br>1. Deterministic anatomical reconstruction<br>确定性解剖重建<br>2. Stochastic deviation modeling<br>随机偏差建模"]
        Method --> M2["核心模型/Core Model<br>Deviation-Space Diffusion Model<br>偏差空间扩散模型<br>Learns pathological residuals<br>学习病理残差"]
        Method --> M3["融合与稳定/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>接缝感知融合与推理时稳定模块"]
        Results["关键结果/Results<br>Outperforms baselines<br>超越基线模型"] --> R1["评估/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>肿瘤基准上的定量与定性评估"]
        Results --> R2["优势/Advantages<br>Higher perceptual realism & anatomical fidelity<br>更高的感知真实性与解剖保真度"]
    ```

- **[arXiv251230] Reservoir Computing inspired Matrix Multiplication-free Language Model**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [MatMul-free LM, reservoir computing, weight sharing, ternary quantization, MLGRU]
  - **authors:** Takumi Shiratsuchi, Yuichiro Tanaka, Hakaru Tamukoh
  - **institution:** Kyushu Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23145
  - **contributions:** 1. Proposes a novel language model architecture that integrates reservoir computing principles into a MatMul-free LM to reduce training costs. 2. Introduces techniques of partially fixing/sharing weights and inserting reservoir layers to obtain dynamic representations without extra training overhead. 3. Combines operations to reduce memory accesses, achieving reductions in parameters, training time, and inference time while maintaining performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of large language models by proposing a matrix multiplication-free model enhanced with reservoir computing. The method fixes/shared weights in selected layers and inserts reservoir layers to reduce training overhead and memory accesses. Experiments show the approach reduces parameters by up to 19%, training time by 9.9%, and inference time by 8.0% while maintaining comparable performance to the baseline.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reservoir Computing inspired Matrix Multiplication-free Language Model] --> B[核心问题/Problem: LLMs计算成本高/High computational cost of LLMs]
        A --> C[主要方法/Method: 结合储层计算与无矩阵乘法模型/Combine RC with MatMul-free LM, 固定共享权重/Fix & share weights, 减少内存访问/Reduce memory access]
        A --> D[关键结果/Results: 参数减少19%/Params reduced by 19%, 训练时间减少9.9%/Training time reduced by 9.9%, 推理时间减少8.0%/Inference time reduced by 8.0%, 性能相当/Performance maintained]
    ```

- **[arXiv251230] Why We Need a New Framework for Emotional Intelligence in AI**
  - **tags:** [ai], [affective computing], [emotional intelligence, benchmark evaluation, affective AI, emotion theory, AI assessment]
  - **authors:** Max Parks, Kheli Atluru, Meera Vinod, Mike Kuniavsky, Jud Brewer, Sean White, Sarah Adler, Wendy Ju
  - **institution:** Inflection AI
  - **link:** https://arxiv.org/pdf/2512.23163
  - **contributions:** 1. A critical review of existing emotional intelligence (EI) theories and their applicability to artificial systems. 2. An analysis of current benchmark frameworks for evaluating EI in AI, identifying their foundational shortcomings. 3. A proposal for new evaluation strategies to better measure relevant aspects of EI in AI systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afa72b09c5f0d3f095e827a81d95825630a6951253349696736df1091d38dd71_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current frameworks for assessing emotional intelligence (EI) in AI are inadequate because they lack a solid theoretical foundation on emotion and fail to distinguish between human-specific and AI-relevant EI components. The authors propose a new framework by first reviewing emotion theories to define EI applicable to AI, then critiquing existing benchmarks, and finally outlining improved evaluation strategies. The main conclusion is that a refined, theoretically-grounded framework is needed to properly evaluate EI capabilities in artificial systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Why We Need a New Framework for Emotional Intelligence in AI"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["现有EI评估框架不充分/Current EI evaluation frameworks are inadequate"]
        Problem --> P2["缺乏坚实的理论基础/Lack a solid theoretical foundation on emotion"]
        Method --> M1["回顾情绪与EI理论/Review emotion and EI theories"]
        Method --> M2["批判性评估现有基准/Critically evaluate existing benchmarks"]
        Method --> M3["提出改进策略/Outline improved evaluation strategies"]
        Results --> R1["需要新的评估框架/A new evaluation framework is needed"]
        Results --> R2["区分AI相关与无关的EI/Distinguish AI-relevant vs. irrelevant EI aspects"]
    ```

- **[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search**
  - **tags:** [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]
  - **authors:** Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue
  - **institution:** IBM T.J. Watson Research Center, Vanderbilt University
  - **link:** https://arxiv.org/pdf/2512.23167
  - **code:** https://github.com/IBM/SPIRAL
  - **contributions:** 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search] --> B[核心问题/Problem: LLMs falter at complex planning, linear reasoning lacks self-correction]
        A --> C[主要方法/Method: Integrates three LLM agents (Planner, Simulator, Critic) into MCTS loop]
        A --> D[关键结果/Results: Outperforms SOTA agents, achieves 83.6% accuracy on DailyLifeAPIs, superior token efficiency]
    ```

- **[arXiv251230] EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion**
  - **tags:** [sec], [adversarial attacks], [jailbreak attacks, large language models, adversarial prompting, equation solving, code completion]
  - **authors:** Zhen Liang, Hai Huang, Zhengkui Chen
  - **institution:** Zhejiang Sci-Tech University
  - **link:** https://arxiv.org/pdf/2512.23173
  - **code:** https://github.com/lzzzr123/Equacode
  - **contributions:** 1. Proposes a novel multi-strategy jailbreak approach that combines mathematical equation solving and code completion to bypass LLM safety constraints. 2. Demonstrates high attack success rates (e.g., 91.19% on GPT series) with only a single query, outperforming single-strategy attacks. 3. Shows through ablation studies a strong synergistic effect between the equation and code modules, proving the multi-strategy approach is more effective than the sum of its parts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3023ba644e0cdeddfd98604ca7c5871aceb213706aede21b77e0d35b95cf6d23_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces EquaCode, a multi-strategy jailbreak attack that transforms malicious intent into a mathematical problem and forces the LLM to solve it via code, diverting its focus from safety. The method achieves high success rates on various LLMs with a single query, and ablation studies confirm the synergistic benefit of combining equation-solving and code completion strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EquaCode: 多策略越狱方法 / Multi-Strategy Jailbreak Approach] --> B[核心问题: LLM安全性评估不足 / Problem: Insufficient LLM Safety Evaluation]
        A --> C[主要方法: 方程求解与代码补全 / Method: Equation Solving & Code Completion]
        A --> D[关键结果: 高成功率与协同效应 / Results: High Success Rate & Synergistic Effect]
    ```

- **[arXiv251230] From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research**
  - **tags:** [ai], [generative ai evaluation], [model belief, token-level probabilities, statistical efficiency, demand estimation, synthetic data]
  - **authors:** Hongshen Sun, Juanjuan Zhang
  - **institution:** MIT Sloan School of Management
  - **link:** https://arxiv.org/pdf/2512.23184
  - **contributions:** 1. Introduces and formalizes the concept of "model belief," a novel measure derived from an LLM's token-level probabilities to capture its belief distribution over choices in a single generation. 2. Proves that model belief is asymptotically equivalent to the mean of model choices but is a more statistically efficient estimator with lower variance and faster convergence, with analogous properties for smooth functions used in downstream applications. 3. Empirically demonstrates that model belief outperforms model choice in explaining and predicting ground-truth choices in practical, limited-run settings (e.g., demand estimation), reducing required computation by roughly a factor of 20.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/997c7eb2e35882ae4411dc7956b1f70a51835fa22d25bcd7ec8b5d6d1cb72413_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of using single LLM outputs ("model choice") by proposing "model belief," a measure based on token-level probabilities that captures the model's full belief distribution. The authors prove model belief is a more statistically efficient estimator than model choice and demonstrate its practical superiority in a demand estimation task, where it reduces the computation needed for accurate estimates by about 20 times.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Model Choice to Model Belief] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM数据使用效率低 / Inefficient use of LLM-generated data]
        B --> B2[模型选择信息利用不足 / Underutilizes probabilistic information in model choice]
        C --> C1[提出模型信念 / Propose model belief]
        C --> C2[基于Token级概率 / Based on token-level probabilities]
        C --> C3[捕获信念分布 / Captures belief distribution]
        D --> D1[统计效率更高 / More statistically efficient estimator]
        D --> D2[计算需求减少20倍 / Reduces computation by ~20x]
        D --> D3[预测性能更优 / Better explains/predicts ground truth]
    ```

- **[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis**
  - **tags:** [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]
  - **authors:** Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik
  - **institution:** BRAC University, Charles Sturt University, University of Technology Sydney
  - **link:** https://arxiv.org/pdf/2512.23196
  - **contributions:** 1. Proposes "ForCM", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ForCM: Forest Cover Mapping] --> B[核心问题/Problem: Accurate forest cover mapping for environmental monitoring]
        A --> C[主要方法/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]
        A --> D[关键结果/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]
    ```

- **[arXiv251230] Not too long do read: Evaluating LLM-generated extreme scientific summaries**
  - **tags:** [nlp], [text summarization], [extreme summarization, TLDR, abstractive summarization, extractive summarization, dataset creation]
  - **authors:** Zhuoqi Lyu, Qing Ke
  - **institution:** City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23206
  - **code:** https://github.com/netknowledge/LLM_summarization
  - **contributions:** 1. Introduces BiomedTLDR, a novel high-quality dataset of researcher-authored scientific TLDRs, curated from author annotations in bibliographies. 2. Evaluates the performance of popular open-weight LLMs in generating scientific TLDRs from paper abstracts. 3. Provides an analysis revealing that LLM-generated summaries tend to be more extractive (closer to the source text's lexicon and structure) compared to more abstractive human-written summaries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of high-quality datasets for evaluating LLMs in generating scientific extreme summaries (TLDRs) by introducing BiomedTLDR, a dataset of human-authored summaries. It then evaluates open-weight LLMs on this task and finds that, while some can produce human-like summaries, LLMs generally tend to be more extractive and less abstractive than human experts.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Not too long do read: Evaluating LLM-generated extreme scientific summaries<br>论文标题"]
        A --> B["核心问题/Problem<br>Lack of high-quality scientific TLDR dataset hinders LLM evaluation"]
        A --> C["主要方法/Method<br>Propose BiomedTLDR dataset & test LLMs on TLDR generation"]
        A --> D["关键结果/Results<br>LLMs are more extractive; humans are more abstractive"]
    ```

- **[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection**
  - **tags:** [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]
  - **authors:** Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1
  - **link:** https://arxiv.org/pdf/2512.23208
  - **contributions:** 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏军事目标数据集/Lack of military target datasets]
        B --> B2[SAR数据成本高/High cost of SAR data]
        B --> B3[跨域适应挑战/Cross-domain adaptation challenge]
        C --> C1[使用Unreal Engine生成合成RGB数据/Generate synthetic RGB data using Unreal Engine]
        C --> C2[合成到真实域适应实验/Synthetic-to-real domain adaptation experiments]
        C --> C3[基准测试SOTA方法/Benchmark SOTA DA methods]
        D --> D1[最小监督方法表现最佳/Minimal supervision methods perform best]
        D --> D2[识别当前挑战/Identify current challenges]
    ```

- **[arXiv251230] Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process**
  - **tags:** [mlsys], [llm inference], [LLM Ensemble, LLM-as-a-Judge, Peer-Review, Unsupervised Selection, Truth Inference]
  - **authors:** Zhijun Chen, Zeyu Ji, Qianren Mao, Junhang Cheng, Bangjie Qin, Hao Wu, Zhuoran Li, Jingzheng Li, Kai Sun, Zizhe Wang, Yikun Ban, Zhu Sun, Xiangyang Ji, Hailong Sun
  - **institution:** Beihang University, Zhongguancun Laboratory, Xi'an Jiaotong University, Hong Kong University of Science and Technology, Tsinghua University, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.23213
  - **contributions:** 1. Proposes LLM-PeerReview, a novel, peer-review-inspired, and interpretable framework for unsupervised LLM ensemble selection. 2. Introduces a three-stage process (scoring via LLM-as-a-Judge, reasoning via aggregation, and selection) that leverages multiple LLMs to evaluate each other's responses. 3. Demonstrates strong empirical performance, with two variants significantly outperforming a recent advanced baseline (Smoothie-Global) on multiple datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LLM-PeerReview, an unsupervised ensemble method that selects the best response from multiple LLM candidates. The method uses a peer-review process where LLMs score each other's outputs, then aggregates these scores to make a final selection. The approach is shown to be simple and powerful, outperforming a strong baseline by a significant margin across several datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-PeerReview: Ensembling LLMs via Peer-Review] --> B[核心问题/Problem: Single LLM limitations & diverse model strengths]
        A --> C[主要方法/Method: Unsupervised 3-stage peer-review framework]
        C --> C1[评分/Scoring: LLM-as-a-Judge]
        C --> C2[推理/Reasoning: Score aggregation (graphical model or averaging)]
        C --> C3[选择/Selection: Pick highest-scoring response]
        A --> D[关键结果/Results: Outperforms Smoothie-Global by ~7% points]
    ```

- **[arXiv251230] TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI**
  - **tags:** [ai], [ai evaluation], [thermal comfort, cognitive turing test, cross-modal reasoning, causal association, adaptive decision-making]
  - **authors:** Jingming Li
  - **institution:** School of Civil Engineering and Architecture, Nanyang Normal University
  - **link:** https://arxiv.org/pdf/2512.23217
  - **contributions:** 1. Proposes TCEval, the first evaluation framework that uses thermal comfort scenarios to assess AI's core cognitive capacities (cross-modal reasoning, causal association, adaptive decision-making). 2. Introduces a methodology using LLM agents with virtual personalities to generate and validate clothing and comfort feedback against real human databases (ASHRAE, Chinese Thermal Comfort Database). 3. Demonstrates the framework's ecological validity as a Cognitive Turing Test, revealing that current LLMs have foundational cross-modal reasoning but lack precise causal understanding of nonlinear relationships in thermal comfort.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0080eb693040bd7b2c752a63de09a7937c8abe23c0a11752ee6d9fe7cdd04c7f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TCEval, a novel evaluation framework that uses thermal comfort scenarios and LLM agents to assess AI's cognitive abilities. The method involves simulating agent decisions and comparing them to human data from established comfort databases. The results show that while LLMs exhibit basic cross-modal reasoning, they lack a precise causal understanding of the complex factors in thermal comfort, validating TCEval as an ecologically valid cognitive test.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("LLM任务特定基准存在关键差距<br/>Critical gap in LLM task-specific benchmarks")
        C --> C1("利用热舒适场景和LLM智能体<br/>Leverage thermal comfort scenarios & LLM agents")
        C --> C2("评估三种核心认知能力<br/>Assess three core cognitive capacities")
        C2 --> C2a("跨模态推理<br/>Cross-modal reasoning")
        C2 --> C2b("因果关联<br/>Causal association")
        C2 --> C2c("自适应决策<br/>Adaptive decision-making")
        D --> D1("智能体反馈与人类有限对齐<br/>Agent feedback has limited exact alignment with humans")
        D --> D2("具备基础跨模态推理能力<br/>Possess foundational cross-modal reasoning ability")
        D --> D3("缺乏对非线性关系的精确因果理解<br/>Lack precise causal understanding of nonlinear relationships")
    ```

- **[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information**
  - **tags:** [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]
  - **authors:** Youngchae Kwon, Jinyoung Choi, Injung Kim
  - **institution:** Handong Global University
  - **link:** https://arxiv.org/pdf/2512.23221
  - **contributions:** 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: 整体时尚物品检测] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>时尚物品检测因外观多样和子类别相似而具有挑战性。]
        C[主要方法/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>提出Holi-DETR，利用共现、空间布局和身体关键点三种上下文线索的整体检测器。]
        D[关键结果/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>性能超越原始DETR (+3.6pp AP) 和 Co-DETR (+1.1pp AP)。]
    ```

- **[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images**
  - **tags:** [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]
  - **authors:** Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi
  - **institution:** Sungkyunkwan University, The University of Texas MD Anderson Cancer Center
  - **link:** https://arxiv.org/pdf/2512.23227
  - **contributions:** 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Anomaly Detection by Effectively Leveraging Synthetic Images] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 真实缺陷图像稀缺，现有合成方法在成本与质量间难以权衡/Scarcity of real defect images, trade-off between cost and quality in synthesis]
        C[主要方法/Method: 1. 使用预训练图像翻译与检索模型高效生成缺陷图/Use pre-trained image-to-image & retrieval models for generation. 2. 两阶段训练策略：先预训练再微调/Two-stage training: pre-train then fine-tune]
        D[关键结果/Results: 在MVTec AD数据集上验证有效，降低成本并提升性能/Validated on MVTec AD, reduces cost and improves performance]
    ```

- **[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network**
  - **tags:** [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]
  - **authors:** Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao
  - **institution:** (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)
  - **link:** https://arxiv.org/pdf/2512.23234
  - **contributions:** 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --> B(核心问题/Problem: 红外气体泄漏检测困难/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)
        A --> C(主要方法/Method: 提出PEG-DRNet/Propose PEG-DRNet)
        C --> C1(气体块建模气体传输/Gas Block models gas transport)
        C --> C2(自适应梯度相位边缘算子/Adaptive Gradient and Phase Edge Operator (AGPEO))
        C --> C3(内容自适应稀疏路由聚合网络/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))
        A --> D(关键结果/Results: 在IIG和LangGas数据集上性能优越/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)
    ```

- **[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta**
  - **tags:** [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]
  - **authors:** Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu
  - **institution:** Meta Platforms
  - **link:** https://arxiv.org/pdf/2512.23236
  - **contributions:** 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp
  - **Simple LLM Summary:** This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]
        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]
        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]
        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]
        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]
        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]
        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]
        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]
    ```

- **[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing**
  - **tags:** [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]
  - **authors:** Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang
  - **institution:** Fudan University, Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.23244
  - **contributions:** 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ViLaCD-R1: 遥感语义变化检测的视觉语言框架] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[传统方法语义理解不足/Traditional methods lack semantic understanding]
        B1 --> C2[现有VLM方法定位不准确/Existing VLM methods have inaccurate localization]
        B2 --> D1[两阶段框架/Two-stage framework]
        D1 --> E1[多图像推理器/Multi-Image Reasoner]
        E1 --> F1[SFT与RL训练/SFT and RL training]
        E1 --> F2[生成粗变化掩码/Generate coarse change mask]
        D1 --> E2[掩码引导解码器/Mask-Guided Decoder]
        E2 --> F3[融合特征与掩码/Fuse features and mask]
        E2 --> F4[预测精细变化图/Predict precise change map]
        B3 --> G1[提升语义变化识别/Improves semantic change recognition]
        B3 --> G2[抑制非语义变化/Suppresses non-semantic variations]
        B3 --> G3[达到SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]
  - **authors:** Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.23260
  - **contributions:** 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LoRA缺乏可解释性/LoRA lacks interpretability")
        Problem --> P2("子空间学习是黑盒的/Subspace learning is black-box")
        Method --> M1("利用预训练SAE/Use pre-trained SAEs")
        Method --> M2("构建显式低秩子空间/Construct explicit low-rank subspace")
        Results --> R1("高安全率99.6%/High safety rate 99.6%")
        Results --> R2("参数高效0.19%/Parameter-efficient 0.19%")
        Results --> R3("提供可解释性/Provides interpretability")
    ```

- **[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control**
  - **tags:** [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]
  - **authors:** Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam
  - **institution:** Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi
  - **link:** https://arxiv.org/pdf/2512.23292
  - **contributions:** 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model's behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Agentic Physical AI for Nuclear Reactor Control") --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br>General-purpose models fail at physical control<br>通用模型在物理控制中失败"]
        Method["主要方法/Method<br>Agentic Physical AI with physics-based validation<br>基于物理验证的智能体物理AI"]
        Results["关键结果/Results<br>Variance collapse & emergent policy distillation<br>方差崩溃与策略蒸馏涌现"]
    
        Problem --> P1["Input unfaithfulness / 输入不忠实"]
        Problem --> P2["Semantic vs. physical correctness / 语义与物理正确性冲突"]
    
        Method --> M1["Compact LM (360M params) / 紧凑语言模型"]
        Method --> M2["Physics-driven optimization / 物理驱动优化"]
        Method --> M3["Synthetic data scaling (10^3 to 10^5) / 合成数据缩放"]
    
        Results --> R1["Phase transition & >500x variance collapse / 相变与方差崩溃"]
        Results --> R2["Autonomous policy distillation / 自主策略蒸馏"]
        Results --> R3["Transferable representations / 可迁移表征"]
    ```

- **[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images**
  - **tags:** [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]
  - **authors:** Md. Sazzadul Islam Prottasha, Nabil Walid Rafi
  - **institution:** Bangladesh University of Professionals
  - **link:** https://arxiv.org/pdf/2512.23304
  - **contributions:** 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp
  - **Simple LLM Summary:** This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedGemma vs GPT-4: 医学图像零样本疾病分类] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 比较开源与闭源多模态LLM在医学图像诊断中的性能]
        C[主要方法/Method: 使用LoRA微调的MedGemma与未调优的GPT-4进行零样本分类对比]
        D[关键结果/Results: MedGemma准确率(80.37%)和敏感性更高，领域微调对减少幻觉至关重要]
    ```

- **[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL**
  - **tags:** [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]
  - **authors:** Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer
  - **institution:** University of Innsbruck, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.23310
  - **contributions:** 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --> B[核心问题/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]
        A --> C[主要方法/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]
        A --> D[关键结果/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]
    ```

- **[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants**
  - **tags:** [ai], [explainable ai (xai)], [inverse kinematics, shapley additive explanations (SHAP), InterpretML, obstacle avoidance, neural network]
  - **authors:** Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin
  - **institution:** Yuan Ze University
  - **link:** https://arxiv.org/pdf/2512.23312
  - **contributions:** 1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp
  - **Simple LLM Summary:** This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation<br>可解释神经逆运动学用于障碍物感知机器人操作"] --> B
        A --> C
        A --> D
        B["核心问题/Problem<br>Opaque neural IK models lack transparency and safety for responsible AI.<br>黑盒神经IK模型缺乏透明度与安全性"] --> B1["挑战/Challenges<br>Debugging failures, safety certification"]
        C["主要方法/Method<br>XAI workflow integrating SHAP and physics simulation.<br>集成SHAP与物理仿真的XAI工作流"] --> C1["模型/Variants<br>Improved IKNet, Focused IKNet"]
        C --> C2["工具/Tools<br>SHAP, InterpretML, Simulator"]
        D["关键结果/Results<br>Balanced feature attribution correlates with wider safety margins.<br>均衡的特征归因与更宽的安全裕度相关"] --> D1["结论/Conclusion<br>XAI guides architectural refinement for trustworthy IK.<br>XAI指导可信IK的架构改进"]
    ```

- **[arXiv251230] On Conformant Planning and Model-Checking of $^*^*$ Hyperproperties**
  - **tags:** [other], [formal methods], [conformant planning, hyperproperties, model-checking, HyperLTL, ∃∗∀∗]
  - **authors:** Raven Beutner, Bernd Finkbeiner
  - **institution:** CISPA Helmholtz Center for Information Security
  - **link:** https://arxiv.org/pdf/2512.23324
  - **contributions:** 1. Establishes a formal connection between conformant planning and model-checking of ∃∗∀∗ hyperproperties, showing they share the same computational core. 2. Provides an efficient, sound, and complete reduction from a hyperproperty model-checking instance to a conformant planning instance. 3. Demonstrates that every conformant planning problem is itself a hyperproperty model-checking task, establishing the converse direction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47343ca7bc4bf16389257261dd21e0c1fa42c0f512178576ff4ba501df841e8b_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies and formalizes a deep connection between two seemingly unrelated problems: conformant planning (finding a robust sequential plan under uncertainty) and model-checking of ∃∗∀∗ hyperproperties (verifying system properties that relate multiple execution traces). The authors provide efficient, sound, and complete translations between instances of these two problems, showing they are essentially two sides of the same computational coin. This foundational link aims to enable cross-pollination of solution techniques between the planning and verification communities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[On Conformant Planning and Model-Checking of ∃∗∀∗ Hyperproperties] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[连接两个看似无关的问题 / Linking two seemingly unrelated problems]
        B1 --> B2[Conformant Planning / 一致性规划]
        B1 --> B3[Hyperproperty Model-Checking / 超属性模型检测]
        C --> C1[构建双向高效规约 / Constructing bidirectional efficient reductions]
        D --> D1[证明规约的可靠性与完备性 / Proving reductions are sound and complete]
        D --> D2[确立问题的等价性 / Establishing the equivalence of the problems]
    ```

- **[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations**
  - **tags:** [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]
  - **authors:** Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang
  - **institution:** Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23328
  - **contributions:** 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM智能体缺乏物理世界部署所需的稳健空间心智模型/LLM agents lack robust spatial mental models for physical-world deployment]
        C --> C1[提出基于魔方的三层诊断基准/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]
        C --> C2[从完整符号状态到部分视觉状态逐步评估/Progressive evaluation from full symbolic to partial visual state]
        D --> D1[领先LLM在长视野任务上通过率为0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]
        D --> D2[揭示了长期规划和主动探索的根本性失败/Exposes fundamental failure in long-term planning and active exploration]
    ```

- **[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models**
  - **tags:** [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]
  - **authors:** Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.23340
  - **contributions:** 1. Proposes the "Law of Multi-model Collaboration," a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the "Law of Multi-model Collaboration," a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The Law of Multi-Model Collaboration<br>多模型协作定律"] --> Problem["核心问题/Problem<br>Lack of scaling theory for multi-model collaboration<br>缺乏多模型协作的扩展理论"]
        Root --> Method["主要方法/Method<br>Propose Law of Multi-model Collaboration<br>提出多模型协作定律"]
        Root --> Results["关键结果/Results<br>Ensembles scale better than single models<br>集成模型比单一模型扩展性更好"]
    ```

- **[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents**
  - **tags:** [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]
  - **authors:** Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin
  - **institution:** Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.23343
  - **code:** https://github.com/AgentMemory/Huaman-Agent-Memory
  - **contributions:** 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp
  - **Simple LLM Summary:** This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Meets Brain: Memory Systems / AI与大脑：记忆系统] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[Interdisciplinary Gap / 跨学科鸿沟]
        P1 --> P2[Existing works struggle to assimilate human memory essence / 现有工作难以吸收人类记忆机制精髓]
    
        Method[主要方法/Method] --> M1[Systematic Synthesis / 系统综述]
        M1 --> M2[Comparative Analysis / 对比分析]
        M2 --> M3[Review Benchmarks & Security / 回顾基准与安全]
    
        Results[关键结果/Results] --> R1[Unified Memory Framework / 统一的记忆框架]
        R1 --> R2[Future Directions / 未来方向]
        R2 --> R3[Multimodal Memory & Skill Acquisition / 多模态记忆与技能获取]
    ```

- **[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling**
  - **tags:** [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]
  - **authors:** Hai Duong Nguyen, Xuan-The Tran
  - **institution:** HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University
  - **link:** https://arxiv.org/pdf/2512.23347
  - **contributions:** 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ECG-RAMBA: Zero-Shot ECG Generalization] --> B[核心问题/Problem: Poor cross-dataset generalization in ECG classification]
        A --> C[主要方法/Method: Morphology-Rhythm Disentanglement & Long-Range Mamba Modeling]
        A --> D[关键结果/Results: Strong zero-shot AUC on CPSC-2021 & PTB-XL]
    ```

- **[arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis**
  - **tags:** [nlp], [text-to-sql], [Reinforcement Learning, Data Synthesis, Policy Optimization, Semantic-Logic Alignment, Group Relative Policy Optimization]
  - **authors:** Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai
  - **institution:** Sichuan University, IQuest Research, Beihang University
  - **link:** https://arxiv.org/pdf/2512.23366
  - **contributions:** 1. Proposes an iterative data factory for synthesizing high-quality, RL-ready Text-to-SQL data with strict semantic-logic verification. 2. Introduces a novel Agentic Reinforcement Learning framework featuring a Diversity-Aware Cold Start stage and Group Relative Policy Optimization (GRPO). 3. Demonstrates state-of-the-art performance on the BIRD and Spider benchmarks through the synergistic combination of data-centric and model-centric approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6740c1fc529b82b509bd38c2a7b5fb405b969bc5c3e11e6e0b7690e7fa791c85_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of data scarcity and limited reasoning in Text-to-SQL systems. It proposes a holistic framework that combines a data-centric approach for synthesizing high-fidelity training data with a model-centric approach using a novel Agentic Reinforcement Learning method called Group Relative Policy Optimization. The method achieves state-of-the-art results on major benchmarks, showing the effectiveness of the synergistic approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AGRO-SQL] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据稀缺与质量/Data Scarcity & Quality]
        B --> B2[模型推理限制/Model Reasoning Limitations]
        C --> C1[数据中心方法/Data-Centric Approach]
        C --> C2[模型中心方法/Model-Centric Approach]
        C1 --> C1a[迭代数据工厂/Iterative Data Factory]
        C1 --> C1b[语义逻辑对齐/Semantic-Logic Alignment]
        C2 --> C2a[多样性感知冷启动/Diversity-Aware Cold Start]
        C2 --> C2b[组相对策略优化/Group Relative Policy Optimization]
        D --> D1[在BIRD和Spider上SOTA/SOTA on BIRD & Spider]
    ```

- **[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]
  - **authors:** Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang
  - **institution:** Tianjin University
  - **link:** https://arxiv.org/pdf/2512.23367
  - **contributions:** 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CoT推理带来高内存与延迟 / CoT reasoning causes high memory & latency]
        B --> B2[Ascend NPU部署挑战 / Deployment challenge on Ascend NPU]
        C --> C1[低比特量化 / Low-bit Quantization]
        C --> C2[统一推理框架 / Unified Inference Framework]
        C --> C3[支持W8A8与W4A8 / Supports W8A8 & W4A8]
        D --> D1[INT8保持>90%精度 / INT8 preserves >90% accuracy]
        D --> D2[1.5倍预填充加速 / 1.5x prefill speedup]
        D --> D3[W4A8显著减少内存 / W4A8 greatly reduces memory]
    ```

- **[arXiv251230] SoulX-LiveTalk Technical Report**
  - **tags:** [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]
  - **authors:** Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu
  - **institution:** Soul AI Lab, Donghua University
  - **link:** https://arxiv.org/pdf/2512.23379
  - **code:** https://soul-ailab.github.io/soulx-livetalk/
  - **contributions:** 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoulX-LiveTalk] --> B[核心问题/Problem: 实时无限时长音频驱动化身生成中计算负载与低延迟的冲突]
        A --> C[主要方法/Method: 自校正双向蒸馏与多步回顾自校正机制]
        A --> D[关键结果/Results: 0.87秒启动延迟，32 FPS实时吞吐]
    ```

- **[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers**
  - **tags:** [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]
  - **authors:** Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee
  - **institution:** Urmia University of Technology
  - **link:** https://arxiv.org/pdf/2512.23380
  - **code:** https://github.com/your-repo/CoLog (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)
  - **contributions:** 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --> Problem["核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]
        Root --> Method["主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]
        Root --> Results["关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]
    ```

- **[arXiv251230] Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?**
  - **tags:** [sec], [AI Security], [AI supply chain, security taxonomy, distilBERT classifier]
  - **authors:** Anh Nguyen, Triet Huynh Minh Le, M. Ali Babar
  - **institution:** University of Adelaide
  - **link:** https://arxiv.org/pdf/2512.23385
  - **contributions:** 1. Developed a pipeline combining keyword matching with a fine-tuned distilBERT classifier to identify 312,868 security discussions from Hugging Face and GitHub. 2. Conducted a thematic analysis to create a fine-grained taxonomy of 32 security issues and 24 solutions across four themes (System/Software, External Tools/Ecosystem, Model, Data). 3. Provided empirical insights revealing that security issues stem from complex dependencies and black-box AI components, with Model and Data challenges often lacking concrete solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c00453e76598d08a965d2a15fe6e7b197cf1f19518d88f46a334b638da6327dc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates security issues in the AI supply chain by analyzing developer discussions from Hugging Face and GitHub. The authors use a keyword and classifier pipeline to build a large dataset and perform a thematic analysis to create a taxonomy of issues and solutions. They conclude that many security problems arise from dependencies and the black-box nature of AI, with solutions for Model and Data issues being particularly scarce.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Securing the AI Supply Chain] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[AI供应链安全格局复杂/Complex AI supply chain security landscape]
        Problem --> P2[缺乏对常见问题与解决方案的了解/Lack of knowledge on common issues & solutions]
        Method[主要方法/Method] --> M1[实证调查/Empirical investigation]
        M1 --> M1_1[数据源: Hugging Face, GitHub/Data Sources: Hugging Face, GitHub]
        M1 --> M1_2[构建分类管道/Build classification pipeline]
        M1_2 --> M1_2_1[关键词匹配+微调distilBERT/Keyword matching + fine-tuned distilBERT]
        Results[关键结果/Results] --> R1[数据集: 312,868个安全讨论/Dataset: 312,868 security discussions]
        Results --> R2[分类法: 32个问题, 24个解决方案/Taxonomy: 32 issues, 24 solutions]
        Results --> R3[洞察: 依赖复杂性和黑盒性导致问题/Insight: Issues from dependencies & black-box nature]
    ```

- **[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models**
  - **tags:** [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]
  - **authors:** Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.23407
  - **contributions:** 1. Theoretically and empirically extends the neural scaling law to the "familial models" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the "train once, deploy many" paradigm without compromising compute-optimality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Theoretical Foundations of Scaling Law in Familial Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统缩放定律忽略多模型范式/Traditional scaling laws overlook the multi-model paradigm]
        C --> C1[引入粒度作为新变量/Introduce Granularity (G) as a new variable]
        C --> C2[统一函数形式 L(N, D, G)/Unified functional form L(N, D, G)]
        C --> C3[采用IsoFLOP实验设计/Employ rigorous IsoFLOP experimental design]
        D --> D1[粒度惩罚遵循幂律/Granularity penalty follows a power law]
        D --> D2[指数极小 (γ≈0.041)/Exponent is extremely small]
        D --> D3[验证"一次训练，多次部署"/Validates "train once, deploy many"]
    ```

- **[arXiv251230] MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning**
  - **tags:** [mlsys], [agent system], [tool-integrated reasoning, multimodal chain-of-thought, interleaved thinking]
  - **authors:** Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang, Pengfei Yu, Xudong Rao, Ning Mao, Xiaobo Liu, Lian Wen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Shanshan Li, Zide Liu, Jing Luo, Lifu Mu, Xuhao Pan, Chang Ren, Haoyi Sun, Qian Wang, Wei Wang, Hongfu Yang, Jiqing Zhan, Chunpeng Zhou, Zheng Zhou, Hao Ma, Tao Wei, Pan Zhou, Wei Chen
  - **institution:** Li Auto Inc
  - **link:** https://arxiv.org/pdf/2512.23412
  - **code:** https://github.com/TIMMY-CHAN/MindWatcher
  - **contributions:** 1. Introduces MindWatcher, a TIR agent with interleaved thinking and multimodal CoT reasoning for autonomous tool invocation and coordination. 2. Constructs the MWE-Bench benchmark and releases high-quality datasets and distilled smaller models (2B, 3B, 4B). 3. Designs a more efficient training infrastructure to enhance training speed and hardware utilization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36f0f229f533ecc1b9565a72c5c77b232eab32880c12545774f94ebd2a19e651_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MindWatcher, a multimodal tool-integrated reasoning agent that uses interleaved thinking and chain-of-thought reasoning to autonomously decide when and how to invoke tools. It is equipped with auxiliary tools and a local image database to handle broad-domain problems. Experiments show it matches or exceeds larger models in performance and provides insights like the genetic inheritance phenomenon in agent training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning] --> B[核心问题/Problem: Traditional workflow-based agents have limited intelligence for real-world tool-invocation problems.]
        A --> C[主要方法/Method: Proposes MindWatcher agent with interleaved thinking and multimodal CoT reasoning for autonomous tool use.]
        A --> D[关键结果/Results: Matches/exceeds larger models, introduces MWE-Bench, and provides efficient training infrastructure.]
    ```

- **[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]
  - **authors:** Yusuf Kalyoncuoglu
  - **institution:** RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.23410
  - **contributions:** 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces "Subspace-Native Distillation" as a novel paradigm to provide a stable geometric coordinate system for student models, enabling "Train Big, Deploy Small".
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Directly Constructing Low-Dimensional Solution Subspaces<br>直接构建低维解子空间"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Large models are redundant for representation but needed for optimization.<br>大模型对表示是冗余的，但对优化是必要的。"]
        Method["主要方法/Method<br>Construct low-dimensional subspaces, decouple solution geometry.<br>构建低维子空间，解耦解几何。"]
        Results["关键结果/Results<br>Head compression by 16x, Subspace-Native Distillation.<br>分类头压缩16倍，提出子空间原生蒸馏。"]
    ```

- **[arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis**
  - **tags:** [ai], [continual learning], [big world hypothesis, computationally-embedded agent, interactivity, partially observable Markov decision process, model-based reinforcement learning]
  - **authors:** Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado
  - **institution:** University of Alberta, Amii, The Swiss AI Lab IDSIA, USI & SUPSI, Canada CIFAR AI Chair, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.23419
  - **contributions:** 1. Introduced a computationally-embedded perspective, representing an agent as an automaton simulated within a universal computer, proving it's equivalent to interacting with a POMDP over an infinite state-space. 2. Proposed a new objective called "interactivity" to measure an agent's ability to continually adapt its behavior by learning new predictions. 3. Developed a model-based RL algorithm for interactivity-seeking and constructed a synthetic problem to evaluate continual learning, finding deep linear networks outperform nonlinear ones in sustaining interactivity as capacity scales.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1eff9d76987d2bc49a07c8de307183661687471b7fe4f21ca75040ba3e1de25a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a computationally-embedded perspective to formalize the "big world hypothesis" in continual learning, where an agent is modeled as an automaton within the environment. It introduces "interactivity" as a new objective and a corresponding model-based RL algorithm to seek it. The main finding is that, in their synthetic evaluation, deep linear networks sustain higher interactivity as capacity increases, whereas deep nonlinear networks struggle.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis<br>论文标题"]
        Root --> Problem["核心问题/Problem<br>如何形式化智能体在'大世界'中的持续学习约束"]
        Root --> Method["主要方法/Method<br>提出计算嵌入视角与'交互性'目标，开发基于模型的强化学习算法"]
        Root --> Results["关键结果/Results<br>深度线性网络比非线性网络更能维持交互性"]
    ```

- **[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis**
  - **tags:** [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]
  - **authors:** Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao
  - **institution:** Huawei Technologies Co., Ltd., Hunan University
  - **link:** https://arxiv.org/pdf/2512.23424
  - **contributions:** 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system's effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AKG Kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI模型对高性能计算内核的需求 / AI Models Demand High-Performance Kernels]
        B --> B2[硬件多样性与手动优化的瓶颈 / Hardware Diversity & Manual Optimization Bottleneck]
        C --> C1[多智能体系统自动化内核生成与调优 / Multi-Agent System Automates Kernel Generation & Tuning]
        C --> C2[支持多种DSL以面向不同硬件后端 / Supports Multiple DSLs for Different Hardware Backends]
        D --> D1[在KernelBench上评估 / Evaluated on KernelBench]
        D --> D2[平均加速1.46倍 / Average 1.46x Speedup Achieved]
    ```

- **[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification**
  - **tags:** [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]
  - **authors:** Mustafa Demetgul, Sanja Lazarova Molnar
  - **institution:** Karlsruhe Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23436
  - **contributions:** 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --> B[核心问题/Problem: Classical road monitoring is expensive and unsystematic.]
    A --> C[主要方法/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]
    A --> D[关键结果/Results: Over 95% classification accuracy achieved.]
    ```

- **[arXiv251230] Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study**
  - **tags:** [mlsys], [on-device ai], [DistilHuBERT, 8-bit quantization, cross-corpus validation, Leave-One-Session-Out (LOSO), model compression]
  - **authors:** Saifelden M. Ismail
  - **institution:** University of Science and Technology, Zewail City
  - **link:** https://arxiv.org/pdf/2512.23435
  - **contributions:** 1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a "theatricality effect" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study"] --> B["核心问题/Problem: SER部署受限于大模型的计算需求/SER deployment constrained by computational demands of large models"]
        A --> C["主要方法/Method: 使用蒸馏与8位量化的DistilHuBERT，并进行跨语料库训练/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training"]
        A --> D["关键结果/Results: 模型仅23MB，精度达基准91%，跨语料库训练提升泛化性/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization"]
    ```

- **[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]
  - **authors:** Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang
  - **institution:** Lenovo (PCIE), University of Minnesota (UMN)
  - **link:** https://arxiv.org/pdf/2512.23453
  - **code:** https://github.com/AI-Researcher-Team/CoFi-Dec
  - **contributions:** 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoFi-Dec: Hallucination-Resistant Decoding] --> B[核心问题/Problem: LVLMs产生与视觉输入不一致的幻觉内容]
        A --> C[主要方法/Method: 基于粗到细视觉条件的生成式自反馈与Wasserstein融合]
        A --> D[关键结果/Results: 在六个基准测试中显著减少幻觉，无需训练，模型无关]
    ```

- **[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following**
  - **tags:** [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]
  - **authors:** Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song
  - **institution:** Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.23457
  - **code:** https://github.com/zhangkc97/HiR
  - **contributions:** 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --> Problem
        Root --> Method
        Root --> Results
        Problem[稀疏/不可区分的奖励阻碍学习<br>Sparse/Indistinguishable Rewards Impede Learning]
        Method[后见指令重放 (HiR)<br>Hindsight instruction Replay (HiR)]
        Results[跨任务有效且计算高效<br>Effective Across Tasks & Computationally Efficient]
    ```

- **[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation**
  - **tags:** [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]
  - **authors:** Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao
  - **institution:** Tencent Hunyuan
  - **link:** https://arxiv.org/pdf/2512.23464
  - **code:** https://github.com/Tencent-Hunyuan/HY-Motion-1.0
  - **contributions:** 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"]
        Root --> Problem["核心问题/Problem: Generating high-quality, text-aligned 3D human motions"]
        Root --> Method["主要方法/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline"]
        Root --> Results["关键结果/Results: SOTA performance, Extensive motion coverage, Open-source release"]
    ```

- **[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance**
  - **tags:** [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]
  - **authors:** Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang
  - **institution:** Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data
  - **link:** https://arxiv.org/pdf/2512.23461
  - **code:** https://github.com/Qwen-Applications/DIR
  - **contributions:** 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method's ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR's effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Eliminating Inductive Bias in Reward Models<br>消除奖励模型中的归纳偏差] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Low-quality RM data with inductive biases<br>导致过拟合和奖励攻击] --> B1[举例/Example<br>Response length bias<br>响应长度偏差]
        C[主要方法/Method<br>DIR: Information-theoretic debiasing<br>基于信息瓶颈优化互信息] --> C1[目标/Objective<br>Max MI with preference, Min MI with bias<br>最大化偏好互信息，最小化偏差互信息]
        D[关键结果/Results<br>Mitigates multiple biases & enhances RLHF<br>减轻多种偏差并提升RLHF性能] --> D1[验证的偏差/Verified Biases<br>Length, Sycophancy, Format<br>长度、迎合性、格式]
    ```

- **[arXiv251230] Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings**
  - **tags:** [nlp], [text clustering], [hierarchical clustering, density-based clustering, semantic embeddings, large language models, topic modeling]
  - **authors:** Thomas Haschka, Joseph Bakarji
  - **institution:** Technische Universität Wien, American University of Beirut
  - **link:** https://arxiv.org/pdf/2512.23471
  - **contributions:** 1. Proposes a novel nested density clustering method to construct hierarchical semantic trees from text embeddings. 2. Demonstrates the method's application for data-driven discovery of research areas and subfields without predefined categories. 3. Validates the approach's robustness and general applicability across diverse domains using benchmark datasets like 20 Newsgroups and IMDB reviews.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of uncovering the global hierarchical semantic structure in text corpora, which remains opaque when using LLM embeddings only for similarity search. It proposes a method that applies nested density clustering on LLM embeddings, gradually relaxing a density criterion to merge clusters into a hierarchical tree. This approach enables the data-driven discovery of semantic relationships and topic hierarchies without predefined categories, as demonstrated on scientific abstracts and benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic Tree Inference on Text Corpa / 文本语料库的语义树推断"]
        Root --> Problem["核心问题/Problem: Opaque global semantic structure in text corpora / 文本语料库中不透明的全局语义结构"]
        Root --> Method["主要方法/Method: Nested density clustering on LLM embeddings / 基于大语言模型嵌入的嵌套密度聚类"]
        Root --> Results["关键结果/Results: Data-driven hierarchical semantic tree discovery / 数据驱动的层次化语义树发现"]
    ```

- **[arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation**
  - **tags:** [sec], [software supply chain security], [agentic AI, reinforcement learning, large language model, blockchain security ledger, CI/CD]
  - **authors:** Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani
  - **institution:** Islamic University of Madinah, Arab Open University-Bahrain
  - **link:** https://arxiv.org/pdf/2512.23480
  - **contributions:** 1. Proposes an autonomous, agentic AI framework for software supply chain security that integrates LLM-based reasoning, RL, and multi-agent coordination for proactive vulnerability identification and mitigation. 2. Implements a system that interfaces with real CI/CD environments (e.g., GitHub Actions, Jenkins) via the Model Context Protocol (MCP) and logs actions to a blockchain for auditability. 3. Demonstrates through experiments that the framework outperforms rule-based and provenance-only baselines in detection accuracy and mitigation latency with acceptable operational overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f037ac5977c7275a7c48edbcb676154bcff19330c107fe4c9769750efc5350_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of current software supply chain security frameworks (like SLSA) which focus on provenance but lack active vulnerability mitigation. It proposes an agentic AI system that combines LLMs for semantic analysis and RL for adaptive response, integrated with CI/CD pipelines via MCP and logged on a blockchain. Experiments show it achieves better detection and faster mitigation than baselines, enabling a shift from reactive to proactive defense.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agentic AI for Autonomous Defense in Software Supply Chain Security] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统溯源框架无法主动缓解漏洞/Traditional provenance frameworks lack active vulnerability mitigation]
        C --> C1[多智能体协调/Multi-Agent Coordination]
        C --> C2[LLM推理与RL策略/LLM Reasoning & RL]
        C --> C3[集成CI/CD与区块链日志/CI/CD Integration & Blockchain Ledger]
        D --> D1[更高的检测准确率/Higher Detection Accuracy]
        D --> D2[更短的缓解延迟/Lower Mitigation Latency]
        D --> D3[合理的构建开销/Reasonable Build Overhead]
    ```

- **[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]
  - **authors:** Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu
  - **institution:** Beihang University, Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23485
  - **code:** https://github.com/Bane-Elvin/AAAI2026-FRoD
  - **contributions:** 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FRoD: Full-Rank Efficient Fine-Tuning] --> B[核心问题/Problem: Low-rank PEFT methods suffer from slow convergence and limited adaptation capacity]
        A --> C[主要方法/Method: Hierarchical joint decomposition with rotational degrees of freedom for full-rank updates]
        A --> D[关键结果/Results: Matches full fine-tuning accuracy using only 1.72% parameters and achieves faster convergence]
    ```

- **[arXiv251230] Theory of Mind for Explainable Human-Robot Interaction**
  - **tags:** [ai], [human-robot interaction], [Theory of Mind, Explainable AI, XAI evaluation, human-centered explanation, VXAI framework]
  - **authors:** Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter
  - **institution:** University of Hamburg
  - **link:** https://arxiv.org/pdf/2512.23482
  - **contributions:** 1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot's actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM's user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Theory of Mind for Explainable Human-Robot Interaction") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("ToM解释与机器人内部推理不一致/ToM explanations may not match robot's internal reasoning")
        Problem --> P2("XAI缺乏以用户为中心的解释/XAI lacks user-centered explanations")
        Method --> M1("将ToM视为XAI的一种形式/Consider ToM as a form of XAI")
        Method --> M2("在XAI框架内整合ToM原则/Integrate ToM principles within XAI frameworks")
        Results --> R1("提出视角转变，优先考虑用户需求/Proposed shift in perspective to prioritize user's needs")
        Results --> R2("为使用VXAI等框架评估ToM奠定基础/Laid foundation for evaluating ToM using frameworks like VXAI")
    ```

- **[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment**
  - **tags:** [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]
  - **authors:** Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki
  - **institution:** Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University
  - **link:** https://arxiv.org/pdf/2512.23487
  - **contributions:** 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("能力排行榜与部署决策脱节/Capability-Deployment Gap")
        Problem --> P2("需平衡用户效用、成本、合规性/Balance Utility, Cost, Compliance")
        Method --> M1("理论: 基于前沿的约束优化/Theoretical Constrained Optimization")
        Method --> M2("实现: 提取、估计、学习、推荐/Pipeline: Extract, Estimate, Learn, Recommend")
        Results --> R1("最优配置呈现三区结构/Optimal Configurations Show Three-Regime Structure")
        Results --> R2("部署感知排名不同于能力排名/Deployment-Aware Rankings Differ from Capability-Only")
    ```

- **[arXiv251230] The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [graph reasoning, information gain, multi-agent, off-graph prediction, path retrieval]
  - **authors:** Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong
  - **institution:** ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.23489
  - **code:** https://anonymous.4open.science/r/MIRAGE-VC-323F
  - **contributions:** 1. Proposed an information-gain-driven path retriever to tackle path explosion in graphs for LLM reasoning. 2. Introduced a multi-agent architecture with learnable gating to fuse heterogeneous evidence streams. 3. Addressed the off-graph prediction challenge in venture capital, demonstrating significant performance gains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aed24a413df0661b461a6124bbf364c73762a6f25c3c90546bff33f8c4123ebf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of predicting venture capital success, an off-graph task requiring reasoning over complex relational evidence. It proposes MIRAGE-VC, a framework that uses information-gain-driven path retrieval and a multi-agent system to distill and reason over investment networks. The method achieves improved prediction performance and offers insights for other off-graph tasks like recommendation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Gaining Paths to Investment Success<br/>投资成功路径] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>VC预测是离图任务<br/>VC prediction is off-graph task]
        C[主要方法/Method<br/>MIRAGE-VC框架<br/>信息增益路径检索与多智能体<br/>MIRAGE-VC: Info-gain path retrieval & multi-agent]
        D[关键结果/Results<br/>性能显著提升<br/>+5.0% F1, +16.6% Precision@5<br/>Significant performance gains]
    ```

- **[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization**
  - **tags:** [sys], [communication & networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]
  - **authors:** Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink
  - **institution:** Wuhan University, RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.23493
  - **contributions:** 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[URLLC IIoT网络的多设备动态调度与链路自适应/URLLC IIoT Multi-device Dynamic Scheduling & Link Adaptation]
        B --> B2[不完美的信道状态信息/Imperfect Channel State Information]
        B --> B3[严格的误块率约束/Strict Block Error Rate Constraints]
        C --> C1[贝叶斯优化驱动的TD3方法/BO-driven TD3 Method]
        C --> C2[自适应确定设备服务顺序与MCS/Adaptively Determine Device Order & MCS]
        C --> C3[BO训练机制改进收敛/BO-based Training for Convergence]
        D --> D1[更快的收敛速度/Faster Convergence]
        D --> D2[更高的总速率性能/Higher Sum-rate Performance]
    ```

- **[arXiv251230] Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities**
  - **tags:** [ai], [AI alignment], [AI assistance game, AI shutdown, Incomplete preferences, Non-Archimedean utilities]
  - **authors:** Alessio Benavoli, Alessandro Facchini, Marco Zaffalon
  - **institution:** Trinity College Dublin, SUPSI (University of Applied Sciences and Arts of Southern Switzerland), Kozminski University
  - **link:** https://arxiv.org/pdf/2512.23508
  - **contributions:** 1. Formally connects the AI assistance and AI shutdown problems to the need for reasoning under uncertainty. 2. Argues that handling incomplete human preferences is a fundamental requirement for safe AI. 3. Proposes that non-Archimedean utility structures are necessary to correctly model and prioritize safety constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea603db1434e19dcd83096c95297f9662c571581eb479d66e87432fcf6a9075_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes AI safety through the frameworks of the AI assistance and AI shutdown games. It argues that to address these challenges, AI agents must be designed to reason under uncertainty and handle incomplete and non-Archimedean human preferences. The main conclusion is that these capabilities are essential for ensuring AI systems remain aligned with human values and safe.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities"] --> Problem["核心问题/Problem: AI Alignment and Safety"]
        Root --> Method["主要方法/Method: Analyze via AI Assistance & Shutdown Games"]
        Root --> Results["关键结果/Results: Requires Uncertainty, Incomplete & Non-Archimedean Preferences"]
    ```

- **[arXiv251230] UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?**
  - **tags:** [mlsys], [multi-modal training], [vision-language model, unified model, semantic generation, autoregression, data scaling]
  - **authors:** Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao
  - **institution:** Meituan
  - **link:** https://arxiv.org/pdf/2512.23512
  - **contributions:** 1. Demonstrates that generation enhances understanding in large-scale VLM training only when operating at the semantic level (e.g., autoregressing high-level visual representations), not at the pixel level. 2. Shows that unified generation-understanding models exhibit superior data scaling trends and higher data utilization efficiency compared to understanding-only models. 3. Proposes that autoregression on input embeddings is an effective and modality-independent method for capturing visual details, enabling pixel-level generation from learned semantics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether visual generation tasks can enhance understanding in large-scale vision-language models. Through large-scale pretraining (&gt;200M samples) with a model called UniHetero, the authors find that semantic-level generation (not pixel-level) improves understanding, reveals better data scaling, and that autoregression on input embeddings effectively captures visual details.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?] --> B(核心问题/Problem: Does visual generation enhance understanding at large scale?);
        A --> C(主要方法/Method: Large-scale pretraining of unified model UniHetero (>200M samples));
        A --> D(关键结果/Results);
        B --> D;
        C --> D;
        D --> E(结果1/Result 1: Generation helps, but Only if you generate Semantics, Not Pixels);
        D --> F(结果2/Result 2: Superior Data Scaling trend and higher Data Utilization);
        D --> G(结果3/Result 3: Autoregression on Input Embedding is effective);
    ```

- **[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization**
  - **tags:** [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]
  - **authors:** Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang
  - **institution:** Zhejiang University, HKUST (The Hong Kong University of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.23537
  - **contributions:** 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AnyMS: 布局引导免训练多主体定制] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[多主体定制中文本对齐、身份保持与布局控制的平衡问题/Balancing text alignment, identity preservation, and layout control in multi-subject customization]
        C --> C1[提出免训练框架AnyMS/Proposes training-free framework AnyMS]
        C1 --> C2[引入自底向上双级注意力解耦机制/Introduces bottom-up dual-level attention decoupling]
        C2 --> C3[全局解耦确保文本对齐/Global decoupling ensures text alignment]
        C2 --> C4[局部解耦防止主体冲突/Local decoupling prevents subject conflicts]
        C --> C5[使用预训练图像适配器提取特征/Uses pre-trained image adapters for feature extraction]
        D --> D1[实现SOTA性能/Achieves SOTA performance]
        D --> D2[支持复杂组合与更多主体/Supports complex compositions and scales to more subjects]
    ```

- **[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis**
  - **tags:** [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]
  - **authors:** Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang
  - **institution:** Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University
  - **link:** https://arxiv.org/pdf/2512.23545
  - **contributions:** 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PathFound: Agentic Multimodal Model] --> B[核心问题/Problem: Static inference vs. clinical workflow]
        A --> C[主要方法/Method: Agentic model with VFM, VLM, RL]
        A --> D[关键结果/Results: SOTA accuracy, discovers subtle details]
        B --> B1[静态推理范式/Static inference paradigm]
        B --> B2[缺乏证据再获取/Lacks reassessment & evidence acquisition]
        C --> C1[多阶段诊断/Multi-stage diagnosis]
        C --> C2[主动信息获取/Proactive information acquisition]
        D --> D1[诊断准确性提升/Improved diagnostic accuracy]
        D --> D2[发现细微特征/Discover subtle pathological features]
    ```

- **[arXiv251230] Act2Goal: From World Model To General Goal-conditioned Policy**
  - **tags:** [ai], [reinforcement learning], [goal-conditioned policy, world model, multi-scale temporal hashing, hindsight goal relabeling, LoRA]
  - **authors:** Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo
  - **institution:** Agibot Research
  - **link:** https://arxiv.org/pdf/2512.23541
  - **code:** https://act2goal.github.io/
  - **contributions:** 1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Act2Goal: From World Model To General Goal-conditioned Policy] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有目标条件策略在长视野操作中表现不佳/Existing goal-conditioned policies struggle with long-horizon manipulation]
        C --> C1[集成目标条件视觉世界模型与多尺度时序控制/Integrates goal-conditioned visual world model with multi-scale temporal control]
        C --> C2[引入多尺度时序哈希(MSTH)分解轨迹/Introduces Multi-Scale Temporal Hashing (MSTH) to decompose trajectory]
        D --> D1[零样本泛化能力强/Strong zero-shot generalization]
        D --> D2[通过在线自适应显著提升成功率/Improves success rates significantly via online adaptation]
    ```

- **[arXiv251230] Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs**
  - **tags:** [nlp], [hallucination detection], [knowledge graphs, self-detection, structured verification, GPT-4o, Gemini-2.5-Flash]
  - **authors:** Sahil Kale, Antonio Luca Alfeo
  - **institution:** Knowledge Verse AI, eCampus University
  - **link:** https://arxiv.org/pdf/2512.23547
  - **code:** https://github.com/knowledge-verse-ai/kg-hallu-eval
  - **contributions:** 1. Proposes a novel hallucination self-detection method that converts LLM responses into knowledge graphs for structured analysis., 2. Introduces a manually curated and enhanced hallucination detection dataset to support more reliable future benchmarking., 3. Demonstrates significant performance improvements (up to 16% accuracy, 20% F1) over standard self-detection and a state-of-the-art baseline (SelfCheckGPT).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of hallucinations in LLMs by proposing a self-detection method that converts model responses into knowledge graphs to better analyze atomic facts and estimate hallucination likelihood. The method, evaluated on GPT-4o and Gemini-2.5-Flash, shows substantial improvements in accuracy and F1-score over existing approaches. The work concludes that structuring facts as knowledge graphs enables more robust hallucination detection, offering a low-cost, model-agnostic path toward safer language models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"] --> B["核心问题/Problem: LLM Hallucinations hinder safe deployment"]
        A --> C["主要方法/Method: Convert responses to Knowledge Graphs for structured self-verification"]
        A --> D["关键结果/Results: Up to 16% accuracy & 20% F1 improvement over baselines"]
    ```

- **[arXiv251230] Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks**
  - **tags:** [sec], [ai security], [prompt injection, multi-agent systems, provenance tracking, trust validation, multimodal sanitization]
  - **authors:** Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty
  - **institution:** Islamic University of Madinah, Arab Open University-Bahrain
  - **link:** https://arxiv.org/pdf/2512.23557
  - **contributions:** 1. Proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework to secure agentic AI workflows against prompt injection attacks. 2. Introduces a coordinated defense with specialized sanitizer agents (text, visual) and an output validator, managed by a provenance ledger for tracking trust metadata. 3. Demonstrates through experiments that the framework significantly improves multimodal injection detection accuracy and minimizes trust leakage across agents.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05cd3aa22e07307bda78f06b6f87c2195c61c758b4fe44dc5bffbc281d72a8e4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the security threat of multimodal prompt injection attacks in agentic AI systems like LangChain. It proposes a defense framework that sanitizes inputs and validates outputs using specialized agents coordinated by a provenance ledger. The experimental results show the framework enhances detection accuracy and stabilizes agentic execution pathways.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Trustworthy Agentic AI<br>构建可信的智能体AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Multimodal Prompt Injection Attacks<br>多模态提示注入攻击]
        C --> C1[Cross-Agent Provenance-Aware Framework<br>跨智能体溯源感知框架]
        C1 --> C2[Sanitizer & Validator Agents<br>净化与验证智能体]
        C1 --> C3[Provenance Ledger<br>溯源账本]
        D --> D1[Enhanced Detection Accuracy<br>提升检测准确率]
        D --> D2[Minimized Trust Leakage<br>最小化信任泄漏]
        D --> D3[Stable Execution Pathways<br>稳定的执行路径]
    ```

- **[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing**
  - **tags:** [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]
  - **authors:** Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang
  - **institution:** Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23562
  - **code:** https://github.com/K1nght/VL-RouterBench
  - **contributions:** 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>缺乏系统化、可复现的<br>VLM路由评估基准<br>Lack of systematic, reproducible<br>benchmark for VLM routing]
        Method[主要方法/Method<br>基于原始推理日志构建<br>质量与成本矩阵<br>Construct quality & cost matrices<br>from raw inference logs]
        Results[关键结果/Results<br>观察到显著的路由增益<br>但与理想性能仍有差距<br>Observe significant routability gain<br>but clear gap to ideal Oracle]
    ```

- **[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature**
  - **tags:** [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]
  - **authors:** Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke
  - **institution:** DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University
  - **link:** https://arxiv.org/pdf/2512.23565
  - **contributions:** 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]
        Method[主要方法/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]
        Results[关键结果/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]
    ```

- **[arXiv251230] Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation**
  - **tags:** [nlp], [creative text generation], [divergent-convergent thinking, prompting method, creative problem generation, artificial hivemind, constraint satisfaction]
  - **authors:** Manh Hung Nguyen, Adish Singla
  - **institution:** MPI-SWS (Max Planck Institute for Software Systems)
  - **link:** https://arxiv.org/pdf/2512.23601
  - **contributions:** 1. Proposes CreativeDC, a novel two-phase prompting method inspired by human creative thinking to decouple creative exploration from constraint satisfaction in LLMs. 2. Introduces a comprehensive evaluation framework for creative problem generation, measuring diversity, novelty, and utility. 3. Demonstrates that CreativeDC significantly improves the diversity and novelty of generated educational problems compared to baseline methods while maintaining utility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86ff3954389096abbd357b8cdc09c7b6f3087b3cf9c2edee2cf008bb8924d692_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the "Artificial Hivemind" effect in LLMs, which leads to homogeneous and repetitive outputs, particularly harmful for generating diverse educational problems. It proposes CreativeDC, a prompting method that scaffolds LLM reasoning into divergent (idea exploration) and convergent (constraint satisfaction) phases. The results show that CreativeDC generates problems with significantly higher diversity and novelty than baselines without sacrificing utility.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Divergent-Convergent Thinking in LLMs for Creative Problem Generation<br/>大语言模型中的发散-收敛思维用于创意问题生成"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: LLMs suffer from 'Artificial Hivemind' effect, generating homogeneous and repetitive educational problems.<br/>大语言模型存在'人工蜂群思维'效应，生成同质化、重复的教育问题。"]
        Method["主要方法/Method: CreativeDC, a two-phase prompting method decoupling divergent (exploration) and convergent (constraint) thinking.<br/>CreativeDC，一种将发散（探索）与收敛（约束）思维解耦的两阶段提示方法。"]
        Results["关键结果/Results: Achieves higher diversity & novelty while maintaining utility; scales better in generating distinct problems.<br/>在保持实用性的同时实现更高的多样性与新颖性；在生成独特问题方面扩展性更好。"]
    ```

- **[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning**
  - **tags:** [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]
  - **authors:** Deniz Akdemir
  - **institution:** None (Institution not specified in provided content)
  - **link:** https://arxiv.org/pdf/2512.23617
  - **contributions:** 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam's theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework's effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful "negative transfer" by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam's theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[标准UDA的缺陷/Flaw of Standard UDA]
        Problem --> P2[负迁移与信息破坏/Negative Transfer & Information Destruction]
        Method --> M1[Le Cam理论/Le Cam's Theory]
        Method --> M2[方向可模拟性/Directional Simulability]
        Method --> M3[Le Cam Distortion度量/Le Cam Distortion Metric]
        Results --> R1[基因组学完美估计/Perfect Genomics Estimation]
        Results --> R2[零源域损失/Zero Source Utility Loss]
        Results --> R3[安全RL策略转移/Safe RL Policy Transfer]
    ```

- **[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions**
  - **tags:** [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, Φ-CPDAG, regret-based]
  - **authors:** Federico Baldo, Charles K. Assaad
  - **institution:** Sorbonne Université, INSERM, Institut Pierre Louis d'Epidémiologie et de Santé Publique
  - **link:** https://arxiv.org/pdf/2512.23626
  - **contributions:** 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm's effectiveness and privacy guarantees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Regret-Based Federated Causal Discovery with Unknown Interventions] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 联邦因果发现中客户端存在未知异质干预/Federated causal discovery with unknown, heterogeneous client interventions]
        C[主要方法/Method: 提出I-PERI算法，利用干预差异定向边/Propose I-PERI algorithm, orienting edges using intervention differences]
        D[关键结果/Results: 定义Φ-CPDAG，提供理论与隐私保证/Define Φ-CPDAG, provide theoretical and privacy guarantees]
    ```

- **[arXiv251230] Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE**
  - **tags:** [mlsys], [others], [physics-informed neural networks, circuit simulation, differential-algebraic equations, surrogate modeling, NeuroSPICE]
  - **authors:** Chien-Ting Tung, Chenming Hu
  - **institution:** University of California at Berkeley
  - **link:** https://arxiv.org/pdf/2512.23624
  - **contributions:** 1. Proposes NeuroSPICE, a novel PINN-based framework for solving circuit differential-algebraic equations directly, bypassing traditional time-discretized numerical solvers. 2. Demonstrates the framework's flexibility for modeling emerging devices and multi-physics systems within a single Python environment, lowering the barrier for rapid prototyping. 3. Highlights the potential of the differentiable PINN model as a surrogate for circuit design optimization and inverse problems, despite not outperforming SPICE in raw training speed or accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7d88b0c7c41bb8bbaf5959a08185913698993a5cc330641c604219b2a1c0622_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces NeuroSPICE, a framework that uses Physics-Informed Neural Networks (PINNs) to simulate electronic circuits by solving their governing differential-algebraic equations through backpropagation. It represents circuit waveforms as continuous, differentiable functions of time, enabling the simulation of novel devices like ferroelectric memories. The main conclusion is that while not faster than SPICE for training, NeuroSPICE offers unique advantages as a flexible, differentiable surrogate model for design optimization and complex multi-physics systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE<br>论文标题"] --> B["Problem: Conventional SPICE struggles with emerging, highly nonlinear devices and multi-physics coupling.<br>核心问题: 传统SPICE难以处理新兴的非线性器件和多物理场耦合。"]
        A --> C["Method: NeuroSPICE, a PINN framework that solves circuit DAEs by minimizing equation residuals via backpropagation.<br>主要方法: NeuroSPICE，一个通过反向传播最小化方程残差来求解电路DAE的PINN框架。"]
        A --> D["Results: Provides a flexible, differentiable surrogate model for design optimization, enabling simulation of novel devices like ferroelectric memories.<br>关键结果: 提供了一个灵活的、可微分的代理模型用于设计优化，能够模拟如铁电存储器等新型器件。"]
    ```

- **[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization**
  - **tags:** [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]
  - **authors:** Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong
  - **institution:** Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst
  - **link:** https://arxiv.org/pdf/2512.23631
  - **code:** https://github.com/iamxjy/BOAD-SWE-Agent
  - **contributions:** 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BOAD: 发现分层软件工程代理 / BOAD: Discovering Hierarchical Software Engineering Agents] --> B
        A --> C
        A --> D
        B[核心问题 / Problem: 单一LLM代理在长视野、分布外软件工程任务上泛化能力差 / Single-agent LLMs generalize poorly on long-horizon, out-of-distribution SWE tasks]
        C[主要方法 / Method: 将分层发现建模为多臂老虎机问题，优化子代理协作 / Formulate hierarchy discovery as a multi-armed bandit problem to optimize sub-agent collaboration]
        D[关键结果 / Results: 在SWE-bench上超越单代理和手动设计的多代理系统，36B模型排名第二 / Outperforms single-agent and manual multi-agent systems on SWE-bench, 36B model ranks second]
    ```

- **[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms**
  - **tags:** [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]
  - **authors:** LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão
  - **institution:** Google, Eedi
  - **link:** https://arxiv.org/pdf/2512.23633
  - **contributions:** 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]
        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]
        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]
        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]
        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]
        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]
        Results --> R2[学生表现相当或更好/Student performance equal or better]
        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]
    ```

- **[arXiv251230] Nested Browser-Use Learning for Agentic Information Seeking**
  - **tags:** [mlsys], [agent system], [information-seeking agents, browser interaction, ReAct-style agents, nested framework]
  - **authors:** Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.23647
  - **code:** https://github.com/Alibaba-NLP/DeepResearch
  - **contributions:** 1. Proposes a minimal and complete browser-action framework for agents, 2. Introduces a nested structure to decouple interaction control from page exploration, 3. Demonstrates improved performance on deep information-seeking benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of current information-seeking agents, which rely on simple API calls and cannot perform real browsing. It proposes NestBrowse, a framework that uses a nested structure to enable fine-grained browser control for agents, simplifying reasoning and improving performance on deep search tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Nested Browser-Use Learning for Agentic Information Seeking<br>面向智能信息搜索的嵌套浏览器使用学习"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Agents lack real browsing, limited to APIs."]
        Method["主要方法/Method<br>NestBrowse: nested browser-action framework."]
        Results["关键结果/Results<br>Better performance on deep IS benchmarks."]
    ```

- **[arXiv251230] Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing**
  - **tags:** [sec], [adversarial attacks], [prompt injection, large language models, academic peer review, multilingual, adversarial robustness]
  - **authors:** Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss
  - **institution:** International School of Athens, Idiap Research Institute
  - **link:** https://arxiv.org/pdf/2512.23684
  - **contributions:** 1. Constructed a dataset of ~500 real ICML papers to empirically evaluate hidden prompt injection attacks in a realistic academic reviewing context. 2. Demonstrated that embedding semantically equivalent adversarial instructions in multiple languages (English, Japanese, Chinese, Arabic) can significantly alter LLM-generated review scores and decisions. 3. Revealed notable cross-lingual differences in attack effectiveness, with Arabic injections having minimal impact compared to others.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the vulnerability of LLM-based academic peer review systems to hidden prompt injection attacks. By injecting adversarial instructions in four languages into a dataset of real papers and having an LLM review them, the authors found that such attacks can substantially change review outcomes for English, Japanese, and Chinese, but not Arabic. The results highlight a critical security risk and language-dependent susceptibility in automated reviewing pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>LLM-based reviewing systems are vulnerable to hidden prompt injection attacks.]
        C[主要方法/Method<br>Inject semantically equivalent adversarial prompts in 4 languages into ~500 real papers and review with an LLM.]
        D[关键结果/Results<br>English, Japanese, Chinese injections change scores/decisions; Arabic injections have little effect.]
    ```

- **[arXiv251230] Web World Models**
  - **tags:** [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]
  - **authors:** Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang
  - **institution:** Princeton University, University of California, Los Angeles, University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.23676
  - **code:** https://princeton-ai2-lab.github.io/Web-World-Models/
  - **contributions:** 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world "physics" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Web World Models] --> B["核心问题/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models)."]
        A --> C["主要方法/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top."]
        A --> D["关键结果/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs."]
    ```

- **[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence**
  - **tags:** [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]
  - **authors:** Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang
  - **institution:** Tsinghua University, National Climate Centre, China Meteorological Administration
  - **link:** https://arxiv.org/pdf/2512.22144
  - **contributions:** 1. Introduced an "AI-for-theory" paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an "AI-for-theory" paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI's role as a discovery tool for complex systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Defining MJO lifecycle is challenging due to propagation; classical methods conflate artifacts with physics.]
        C[主要方法/Method<br>AI-for-theory paradigm; Deep learning model (PhysAnchor-MJO-AE) learns similarity-preserving latent representation for objective clustering.]
        D[关键结果/Results<br>First complete six-phase MJO anatomy; New physics-coherent monitoring framework reduces errors by an order of magnitude.]
    ```

- **[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data**
  - **tags:** [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]
  - **authors:** Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet
  - **institution:** IMT Atlantique, Ifremer, CNRS, Mercator Ocean International
  - **link:** https://arxiv.org/pdf/2512.22152
  - **contributions:** 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Neural Ocean Forecasting from Sparse Observations] --> B(核心问题/Problem: 稀疏卫星数据下的短期海洋预报/Short-term ocean forecasting from sparse satellite data)
        A --> C(主要方法/Method: 基于U-Net和4DVarNet的端到端序列预测/End-to-end sequence forecasting using U-Net & 4DVarNet)
        A --> D(关键结果/Results: 神经模型超越业务化基线，在多变区域改进显著/Neural model outperforms operational baseline, notable improvements in high-variability regions)
    ```

- **[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images**
  - **tags:** [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]
  - **authors:** Haozhe Jia
  - **institution:** Boston University
  - **link:** https://arxiv.org/pdf/2512.22209
  - **contributions:** 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis."]
        Method["主要方法/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images."]
        Results["关键结果/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy."]
    ```

- **[arXiv251230] Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [literature mining, large language models, prompt engineering, few-shot prompting, domain adaptation]
  - **authors:** Xinyang Sun, Nipon Sarmah, Miao Guo
  - **institution:** King's College London
  - **link:** https://arxiv.org/pdf/2512.22225
  - **contributions:** 1. Developed a domain-adapted AI system using LLMs and advanced prompt engineering to automate the extraction of nutraceutical-producing microbial strains from unstructured text. 2. Created and validated a structured dataset of 35 nutraceutical-strain associations, spanning multiple compound categories. 3. Demonstrated the system's performance and provided biological insights, identifying dominant microbial contributors and the framework's utility for synthetic biology and precision fermentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c767d67875d1e2e25e95c99a9b826daf52082c66775d52728bda6802558969_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an AI-driven literature mining system that uses large language models and prompt engineering to automatically identify microbes that produce nutraceuticals from scientific text. The system, which performed best with the DeepSeek-V3 model and domain-specific prompts, generated a validated dataset and revealed key microbial strains for biosynthesis. This framework enhances the scalability of literature mining and provides actionable insights for strain selection and fermentation strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Literature Mining System for Nutraceutical Biosynthesis<br/>营养保健品生物合成的文献挖掘系统] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Extracting structured knowledge on microbial strains from literature is a bottleneck.<br/>从文献中提取关于微生物菌株的结构化知识是一个瓶颈。]
        C[主要方法/Method<br/>Domain-adapted system using LLMs and prompt engineering.<br/>使用LLM和提示工程的领域适应系统。]
        D[关键结果/Results<br/>Created dataset of 35 associations; DeepSeek-V3 outperformed LLaMA-2; identified dominant microbial strains.<br/>创建了35个关联的数据集；DeepSeek-V3优于LLaMA-2；识别了主要微生物菌株。]
    ```

- **[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging**
  - **tags:** [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]
  - **authors:** Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger
  - **institution:** Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology
  - **link:** https://arxiv.org/pdf/2512.22176
  - **contributions:** 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>磁共振成像深度学习分析中场强依赖的性能变异性") --> Problem
        Root --> Method
        Root --> Results
    
        Problem("核心问题/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI场强对深度学习模型性能与泛化能力的影响")
        Method("主要方法/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>在1.5T、3.0T及混合数据上训练/评估nnU-Net模型；使用UMAP和影像组学分析")
        Results("关键结果/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>训练数据中的场强显著影响性能，尤其对软组织")
    ```

- **[arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth**
  - **tags:** [ai], [autonomous systems], [autonomous operations, mission planning, in-situ resource utilisation]
  - **authors:** Ziyang Wang
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2512.22399
  - **contributions:** 1. Proposes and defines "Space AI" as a unified interdisciplinary field at the intersection of AI and space science. 2. Consolidates historical and contemporary progress into a systematic four-context framework (AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life). 3. Identifies key application areas where AI advances can translate to societal benefits on Earth, such as in sensing, robotics, and trustworthy AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ab175271eb74821199e8998bba499198a030f7f6b329c43a525f851b07aabe_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces "Space AI" as a new interdisciplinary field and proposes a systematic framework to organize its applications across four mission contexts, from Earth-based planning to multi-planetary life support. It argues that AI is critical for enabling autonomous and resilient space operations under extreme conditions, and that advances in this domain will also yield significant benefits for life on Earth.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Space AI: Leveraging AI for Space to Improve Life on Earth] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>如何在极端不确定和有限监督下<br>实现太空自主弹性操作<br>How to enable autonomous, resilient space operations under extreme uncertainty and limited oversight]
        C[主要方法/Method<br>提出系统化四维框架<br>Propose a systematic four-context framework<br>(AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life)]
        D[关键结果/Results<br>统一了跨学科领域并识别关键应用<br>加速太空探索能力并产生广泛地球影响<br>Unifies interdisciplinary field and identifies key applications<br>Accelerates space exploration capability and yields broad Earth impact]
    ```

- **[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds**
  - **tags:** [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]
  - **authors:** Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra
  - **institution:** Dream Sports, Columbia University
  - **link:** https://arxiv.org/pdf/2512.22473
  - **contributions:** 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[Transformer内部几何结构如何形成?/How is transformer internal geometry formed?]
    C --> C1[推导注意力梯度动态/Derive attention gradient dynamics]
    C --> C2[建立EM算法类比/Establish EM algorithm analogy]
    D --> D1[发现优势路由与责任更新/Discover advantage-based routing & responsibility-weighted update]
    D --> D2[梯度流塑造贝叶斯流形/Gradient flow sculpts Bayesian manifolds]
    ```

- **[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers**
  - **tags:** [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]
  - **authors:** Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan
  - **institution:** Başkent University, Gaziantep University
  - **link:** https://arxiv.org/pdf/2512.22564
  - **contributions:** 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
    
        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]
        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]
    
        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]
        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]
        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]
    
        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]
        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]
    ```

- **[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference**
  - **tags:** [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]
  - **authors:** Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev
  - **institution:** Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22999
  - **contributions:** 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Actively optimizing design variables for parameter estimation] --> Problem_Sub[子问题/Sub-problem: Sequential design and inference are typically treated separately]
        Method[主要方法/Method: Jointly amortize design and inference via end-to-end training] --> Method_Sub1[网络/Networks: Policy, History, and Inference (Diffusion-based) networks]
        Method --> Method_Sub2[损失函数/Loss: Aggregates incremental posterior error reduction]
        Results[关键结果/Results: Superior/competitive performance on standard benchmarks]
    ```

- **[arXiv251230] Deep Learning for Art Market Valuation**
  - **tags:** [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]
  - **authors:** Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang
  - **institution:** Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy
  - **link:** https://arxiv.org/pdf/2512.23078
  - **contributions:** 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]
        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]
        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]
    ```

- **[arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan**
  - **tags:** [other], [scheduling], [constraint programming, biased random-key genetic algorithm, makespan, exact delays, local search]
  - **authors:** Vítor A. Barbosa, Rafael A. Melo
  - **institution:** Institute of Computing, Universidade Federal da Bahia
  - **link:** https://arxiv.org/pdf/2512.23150
  - **contributions:** 1. A Constraint Programming (CP) model for the single-machine coupled task scheduling problem with exact delays, utilizing well-established global constraints. 2. A novel Biased Random-Key Genetic Algorithm (BRKGA) that incorporates an efficient decoder, periodical restarts, shakes, and a local search algorithm for enhanced exploration. 3. An empirical evaluation demonstrating that the BRKGA provides high-quality solutions quickly, while the CP model with extended resources can find best-known solutions for a majority of benchmark instances.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f80cc4c12e23f8eb80b390efdd4a8b62ea37fb03c01008317951d1079c35c319_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the NP-hard single-machine coupled task scheduling problem with exact delays to minimize makespan. It proposes both a Constraint Programming model and a Biased Random-Key Genetic Algorithm (BRKGA) enhanced with local search and shake components. Computational results show the BRKGA finds good solutions quickly, while the CP model with more resources achieves state-of-the-art results on most benchmark instances.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[论文标题/Paper Title: Constraint Programming and BRKGA for Coupled Task Scheduling] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 单机精确延迟耦合任务调度，最小化完工时间/Single-machine coupled task scheduling with exact delays to minimize makespan]
        Method[主要方法/Method: 约束规划模型与带偏置随机密钥遗传算法/Constraint Programming model and Biased Random-Key Genetic Algorithm (BRKGA)]
        Results[关键结果/Results: BRKGA快速提供高质量解，CP模型在充分资源下达到当前最优解/BRKGA provides high-quality solutions quickly; CP model reaches best-known solutions with sufficient resources]
    ```

- **[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making**
  - **tags:** [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]
  - **authors:** Wendyam Eric Lionel Ilboudo, Saori C Tanaka
  - **institution:** Nara Institute of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23144
  - **contributions:** 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Decision Paralysis] --> P1[挑战/Challenge<br>Choice models assume ready-to-compare options]
        Problem --> P2[现象/Phenomenon<br>Hesitation, freezing, failure to act]
        Method[主要方法/Method<br>Computational Account] --> M1[架构/Architecture<br>Hierarchical decision process]
        Method --> M2[形式化/Formalization<br>Intent vs. Affordance selection]
        Method --> M3[目标/Objective<br>Mixture of reverse-KL & forward-KL]
        Results[关键结果/Results<br>Simulation Outcomes] --> R1[行为/Behavior<br>Slow, heavy-tailed response times]
        Results --> R2[失败模式/Failure Modes<br>Intent & Affordance saturation]
        Results --> R3[解释/Interpretation<br>Autism as an extreme regime]
    ```

- **[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation**
  - **tags:** [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]
  - **authors:** Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu
  - **institution:** University of Science and Technology of China, PAII Inc., Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.23185
  - **contributions:** 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[EIR: Enhanced Image Representations for Medical Report Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[报告生成耗时耗力/Report generation is time-consuming]
        Problem --> P2[信息不对称与领域鸿沟/Information asymmetry & domain gap]
        Method[主要方法/Method] --> M1[跨模态Transformer融合元数据/Cross-modal transformer for metadata fusion]
        Method --> M2[医学领域预训练模型/Medical domain pre-trained model]
        Results[关键结果/Results] --> R1[在MIMIC和Open-I数据集上验证/Validated on MIMIC & Open-I datasets]
    ```

- **[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]
  - **authors:** Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua
  - **institution:** Shanghai Jiao Tong University, StepFun, FinStep
  - **link:** https://arxiv.org/pdf/2512.23515
  - **code:** https://github.com/FinStep-AI/Alpha-R1
  - **contributions:** 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning] --> B[核心问题/Problem: Signal decay and regime shifts in non-stationary markets; existing methods overlook semantic rationale for factor relevance.]
        A --> C[主要方法/Method: Alpha-R1, an 8B-parameter LLM trained via RL, reasons over factor logic and real-time news for context-aware alpha screening.]
        A --> D[关键结果/Results: Outperforms benchmark strategies; exhibits improved robustness to alpha decay across multiple asset pools.]
    ```

- **[arXiv251230] PINNs for Electromagnetic Wave Propagation**
  - **tags:** [other], [Scientific Computing / Computational Physics], [Physics-Informed Neural Networks (PINNs), Maxwell's Equations, FDTD, Time Marching, Poynting Regularizer]
  - **authors:** Nilufer K. Bulut
  - **institution:** Izmir, Turkiye (Inferred from author location; no specific institution mentioned in provided content)
  - **link:** https://arxiv.org/pdf/2512.23396
  - **contributions:** 1. Introduces a hybrid training strategy combining time marching and causality-aware weighting to address the causality collapse problem in time-dependent PINNs. 2. Proposes a two-stage interface continuity loss to mitigate discontinuities introduced by time marching. 3. Develops a local Poynting-based regularizer to suppress cumulative energy drift and improve energy conservation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c5894c195aba4dd14bb55cc020098ad00e472ec3df24245a9a20ad4ca74b81c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses accuracy and energy conservation deficiencies in Physics-Informed Neural Networks (PINNs) for electromagnetic wave propagation. It proposes a hybrid training methodology incorporating time marching, causality-aware weighting, and a Poynting-based regularizer. The results show that the enhanced PINNs achieve competitive field accuracy and energy conservation compared to traditional FDTD methods, demonstrating their viability for canonical electromagnetic problems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PINNs for Electromagnetic Wave Propagation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[PINNs在精度和能量守恒上落后于FDTD/PINNs lag behind FDTD in accuracy & energy]
        C --> C1[混合训练策略/Hybrid Training Strategy]
        C1 --> C1_1[时间推进与因果感知加权/Time Marching & Causality-Aware Weighting]
        C1 --> C1_2[两阶段界面连续性损失/Two-Stage Interface Continuity Loss]
        C1 --> C1_3[局部坡印廷正则化器/Local Poynting Regularizer]
        D --> D1[高场精度/High Field Accuracy (0.09% NRMSE)]
        D --> D2[能量守恒/Energy Conservation (0.024% mismatch)]
        D --> D3[与FDTD结果竞争/Competitive with FDTD]
    ```
