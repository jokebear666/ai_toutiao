---
slug: /daily/csai/20260105-20260111
---
# 20260105-20260111 (cs.AI)

## 2026-01-05

- **[arXiv260105] Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models**
  - **tags:** [nlp], [retrieval-augmented generation], [Monte Carlo Tree Search, reasoning-aware retrieval, coarse-to-fine retrieval, multi-turn dialogue, knowledge diversity]
  - **authors:** Shuqi Liu, Bowei He, Chen Ma, Linqi Song
  - **institution:** City University of Hong Kong, City University of Hong Kong Shenzhen Research Institute
  - **link:** https://arxiv.org/pdf/2601.00003
  - **contributions:** 1. Proposes a reasoning-aware knowledge retrieval method that aligns retrieved information with the logical structure of conversations, moving beyond semantic similarity. 2. Introduces a coarse-to-fine retrieval approach that first finds a contextually relevant knowledge sub-region and then refines it for reasoning-specific knowledge. 3. Employs a Monte Carlo Tree Search-inspired method to navigate knowledge sentences using common keywords, enhancing retrieval diversity and informativeness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b609f46963ae2b2b017ba13f445da7491064f311d7e663fa59eda7b85dd69638_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of integrating retrieval and reasoning for LLMs by proposing a reasoning-aware knowledge retrieval method. It uses a coarse-to-fine approach guided by Monte Carlo Tree Search to find knowledge aligned with conversational logic. Experiments show the method better captures human reasoning and produces more diverse, informative responses.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models"] --> Problem["核心问题/Problem: LLMs struggle to integrate retrieval and reasoning effectively."]
        Root --> Method["主要方法/Method: Coarse-to-fine, MCTS-inspired reasoning-aware knowledge retrieval."]
        Root --> Results["关键结果/Results: Better alignment with human reasoning, more diverse and informative responses."]
    ```

- **[arXiv260105] Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach**
  - **tags:** [ai], [game theory], [MinDist, opponent modeling, rule-based strategy, zero-sum game, heuristic optimization]
  - **authors:** Purushottam Saha, Avirup Chakraborty, Sourish Sarkar, Subhamoy Maitra, Diganta Mukherjee, Tridib Mukherjee
  - **institution:** Indian Statistical Institute
  - **link:** https://arxiv.org/pdf/2601.00024
  - **contributions:** 1. Proposed a new hand-evaluation metric called MinDist, which quantifies the edit distance to a valid hand, improving upon the MinScore metric. 2. Designed a computationally efficient algorithm to calculate MinDist using dynamic pruning and pattern caching. 3. Integrated opponent hand-modeling within a two-player zero-sum simulation framework and validated the strategy's superiority through statistical hypothesis testing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6b73a0cbcb5f1c250c8adbb982411c12c8e09c81114767cb920146c72264f7e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a rule-based strategy for Classic Indian Rummy using a new hand-evaluation metric called MinDist, which measures the structural proximity to a winning hand. The method includes an efficient algorithm to compute MinDist and incorporates opponent modeling in simulations. Empirical results show that agents using this strategy achieve significantly higher win rates compared to traditional heuristic-based agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[游戏策略设计/Strategy Design for Rummy]
        C --> C1[新度量标准 MinDist/New Metric MinDist]
        C --> C2[高效算法与对手建模/Efficient Algorithm & Opponent Modeling]
        D --> D1[胜率显著提升/Significant Win Rate Improvement]
    ```

- **[arXiv260105] Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems**
  - **tags:** [ai], [anomaly detection], [class imbalance, synthetic dataset, generalization error, unsupervised methods, semi-supervised methods]
  - **authors:** Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi
  - **institution:** McMaster University
  - **link:** https://arxiv.org/pdf/2601.00005
  - **contributions:** 1. Conducted a comprehensive, problem-agnostic evaluation of 14 anomaly detectors under simulated industrial constraints of extreme class imbalance. 2. Identified that the best-performing detector depends critically on the absolute number of faulty examples available, not just the imbalance ratio, and provided thresholds for method selection. 3. Demonstrated the nuanced impact of feature dimensionality on method performance, showing semi-supervised methods gain advantage in higher dimensions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates anomaly detection algorithms for industrial problems with extreme class imbalance using a synthetic dataset. It benchmarks 14 detectors across varying anomaly rates and training sizes, finding that the optimal detector depends on the absolute number of faulty examples, with unsupervised methods best for very few faults and supervised/semi-supervised methods improving with 30-50 faults. The study highlights performance drops on smaller datasets and provides practical deployment insights.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Extreme class imbalance in industrial applications due to limited faulty data]
        C[主要方法/Method: Benchmark 14 detectors on synthetic hyperspherical dataset with varying anomaly rates and training sizes]
        D[关键结果/Results: Best detector depends on number of faulty examples; Unsupervised dominates with <20 faults; Supervised/semi-supervised improve with 30-50 faults]
    ```

- **[arXiv260105] A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system**
  - **tags:** [other], [logistics optimization], [workload balancing, evolutionary algorithms, k-means, last-mile delivery, hybrid algorithm]
  - **authors:** Luis M. Moreno-Saavedra, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, David Casillas-Perez, Sancho Salcedo-Sanz
  - **institution:** Universidad de Alcalá, Universidad Rey Juan Carlos
  - **link:** https://arxiv.org/pdf/2601.00023
  - **contributions:** 1. Proposes a multi-algorithm methodology for operational human resources workload balancing in last-mile delivery, moving beyond simple geographical assignment. 2. Introduces and combines several algorithmic approaches, including different versions of k-means, evolutionary algorithms, recursive assignments, and a hybrid evolutionary ensemble. 3. Validates the proposed approach by applying it to a real-world case study of a delivery workforce in Azuqueca de Henares, Spain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of unbalanced workload distribution among delivery workers in last-mile urban logistics. It proposes a multi-algorithm approach that uses a combination of distance and workload considerations, including k-means variants and evolutionary algorithms, to assign packages and balance daily effort. The method was successfully tested on a real-world delivery system in Spain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A multi-algorithm approach for operational human resources workload balancing<br>多算法方法用于最后一公里配送的人力资源负载均衡"] --> Problem["核心问题/Problem: Unbalanced workload among delivery workers in last-mile systems<br>最后一公里配送中快递员工作量不均衡"]
        Root --> Method["主要方法/Method: Multi-algorithm approach combining k-means, evolutionary algorithms, and hybrid ensembles<br>结合k-means、进化算法和混合集成的多算法方法"]
        Root --> Results["关键结果/Results: Successfully applied to a real-world delivery workforce, balancing workload<br>成功应用于现实配送团队，实现了工作量均衡"]
    ```

- **[arXiv260105] Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study**
  - **tags:** [nlp], [mental health language modeling], [large language models, fine-tuning, PHQ-9, Nigerian Pidgin, depression screening]
  - **authors:** Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin
  - **institution:** Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin
  - **link:** https://arxiv.org/pdf/2601.00004
  - **contributions:** 1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Finetuning LLMs for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Limited depression screening in Nigeria due to language barriers and lack of clinicians]
        C[主要方法/Method<br>Fine-tune LLMs on annotated Nigerian Pidgin dataset for PHQ-9 scoring]
        D[关键结果/Results<br>GPT-4.1 achieved 94.5% accuracy and best cultural appropriateness]
    ```

- **[arXiv260105] Toward a Physical Theory of Intelligence**
  - **tags:** [ai], [artificial general intelligence theory], [Conservation-Congruent Encoding (CCE), irreversible information processing, goal-directed work, attractor dynamics, physical constraints]
  - **authors:** Peter David Fagan
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2601.00021
  - **contributions:** 1. Proposes the Conservation-Congruent Encoding (CCE) framework, linking information to physical states via metastable basins of attraction enforced by conservation laws. 2. Defines intelligence as a physical efficiency metric (goal-directed work per nat of irreversibly processed information) and derives a hierarchy of physical constraints for intelligent systems. 3. Applies the theory to analyze biological intelligence (e.g., brain dynamics) and proposes a physically-grounded perspective on AI safety based on irreversible information flow.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/179838e256390d3a11169de88d6766d3dcb9d44013014cdc9841b568ca836dcd_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a physical theory of intelligence based on irreversible information processing in systems constrained by conservation laws. It introduces the Conservation-Congruent Encoding (CCE) framework to link information to physical states and defines intelligence as the efficiency of converting processed information into goal-directed work. The theory provides a unified, substrate-neutral account, deriving fundamental constraints and applying them to analyze biological systems and AI safety.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward a Physical Theory of Intelligence] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏基于物理定律的智能统一理论/Lack of a unified physical theory of intelligence]
        C --> C1[守恒一致编码框架/Conservation-Congruent Encoding (CCE) Framework]
        C --> C2[智能的物理定义/Physical definition of intelligence]
        D --> D1[推导出物理约束层级/Derives hierarchy of physical constraints]
        D --> D2[应用于生物系统与分析/Applied to biological systems & analysis]
        D --> D3[提出AI安全的物理视角/Proposes physically-grounded AI safety perspective]
    ```

- **[arXiv260105] Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games**
  - **tags:** [ai], [reinforcement learning], [policy gradient, self-play, Markov Decision Process, Advantage Actor-Critic, ablation study]
  - **authors:** Nicholas A. Pape
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2601.00007
  - **contributions:** 1. Formulates the classic stochastic combinatorial game Yahtzee as a Markov Decision Process and establishes it as a mid-scale RL benchmark. 2. Conducts a comprehensive empirical study comparing REINFORCE, A2C, and PPO under a fixed training budget, identifying A2C as the most robust method. 3. Achieves a median score within 5% of the optimal dynamic programming solution, while analyzing persistent challenges like long-horizon credit assignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates using deep reinforcement learning to play the full solitaire version of Yahtzee. It trains self-play agents with policy gradient methods (REINFORCE, A2C, PPO) and performs ablation studies on various design choices. The main finding is that A2C robustly achieves near-optimal performance, while all methods struggle with long-term strategic elements like securing the upper section bonus.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Yahtzee as a mid-scale RL benchmark with delayed rewards & combinatorial complexity] --> B1[目标/Objectives<br>Can RL achieve near-optimal performance via self-play?]
        C[主要方法/Method<br>Formulate as MDP, train self-play agents with policy gradient methods (REINFORCE, A2C, PPO)] --> C1[技术/Techniques<br>Ablation on encodings, architecture, estimators, entropy]
        D[关键结果/Results<br>A2C is robust & achieves median score within 5% of optimal DP score] --> D1[挑战/Challenges<br>Agents struggle with long-horizon strategy (upper bonus)]
    ```

- **[arXiv260105] Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing**
  - **tags:** [mlsys], [on-device ai], [ferroelectric synapses, spiking neural networks, EEG signal processing, adaptive learning, neuromorphic computing]
  - **authors:** Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura Bégon-Lours
  - **institution:** ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)
  - **link:** https://arxiv.org/pdf/2601.00020
  - **contributions:** 1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing] --> B
        A --> C
        A --> D
        B[核心问题/Problem: EEG信号非平稳性限制模型泛化，需在资源受限平台上进行个性化适应/Non-stationary EEG signals limit model generalization, requiring personalized adaptation on resource-constrained platforms]
        C[主要方法/Method: 在铁电忆阻突触上部署SNN，采用设备感知的权重更新策略/Deploy SNNs on ferroelectric memristive synapses with a device-aware weight-update strategy]
        D[关键结果/Results: 两种部署策略性能媲美软件SNN，特定对象迁移学习提升准确率/Two deployment strategies achieve performance comparable to software SNNs, subject-specific transfer learning improves accuracy]
    ```

- **[arXiv260105] From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers**
  - **tags:** [ai], [diffusion models], [generative AI, diffusion models, architectural intelligence, computational reasoning, vernacular architecture]
  - **authors:** Abolhassan Pishahang, Maryam Badiei
  - **institution:** Florida Atlantic University, North Carolina State University
  - **link:** https://arxiv.org/pdf/2601.00029
  - **contributions:** 1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI's interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of "computational vernacular reasoning" as an analytical framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root["From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"] --> Problem["核心问题/Problem: How does generative AI interpret the architectural intelligence embedded in vernacular forms?"]
    Root --> Method["主要方法/Method: Test three diffusion models across three prompt stages (referential, adaptive, speculative) using a five-criteria evaluation framework."]
    Root --> Results["关键结果/Results: AI reproduces geometry but misreads material/climatic reasoning; reference aids realism but limits creativity."]
    ```

- **[arXiv260105] Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing**
  - **tags:** [sec], [LLM Security], [Go-Explore, Prompt Injection, Adversarial Testing, Agent Safety, Multi-Hop Attacks]
  - **authors:** Manish Bhatt, Adrian Wood, Idan Habler, Ammar Al-Kahfah
  - **institution:** OWASP, Amazon, Dropbox, CISCO, AWS
  - **link:** https://arxiv.org/pdf/2601.00042
  - **code:** https://github.com/mbhatt1/competitionscratch
  - **contributions:** 1. Adapted the Go-Explore reinforcement learning algorithm for systematic security testing of LLM agents. 2. Conducted a large-scale empirical study revealing that random seed variance dominates algorithmic parameter choices in this domain. 3. Provided actionable insights for practitioners, such as the ineffectiveness of reward shaping and the benefits of using ensembles and simple state signatures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp
  - **Simple LLM Summary:** This paper adapts the Go-Explore algorithm to test the security of safety-trained LLM agents against prompt injection attacks. Through 28 experimental runs on GPT-4o-mini, the study finds that random seed variance is a major factor, reward shaping is harmful, and simple state signatures work best. The results suggest that managing seed variance and applying domain knowledge are more critical than algorithmic sophistication for effective security testing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing<br/>大型实证案例研究：用于AI红队测试的Go-Explore"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Testing security of safety-trained LLM agents<br/>测试经过安全训练的LLM代理的安全性"] --> P1["Prompt Injection<br/>提示注入"]
        Method["主要方法/Method<br/>Adapt Go-Explore algorithm<br/>改编Go-Explore算法"] --> M1["Systematic exploration from archive<br/>从存档进行系统探索"]
        Results["关键结果/Results<br/>Key Findings<br/>关键发现"] --> R1["Seed variance dominates<br/>种子方差占主导"]
        Results --> R2["Reward shaping harms performance<br/>奖励塑形损害性能"]
        Results --> R3["Simple signatures outperform<br/>简单签名效果更好"]
        Results --> R4["Ensembles provide diversity<br/>集成提供多样性"]
    ```

- **[arXiv260105] The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs**
  - **tags:** [ai], [causal reasoning], [fuzzy cognitive maps, large-language-model agent, causal feedback, equilibrium limit cycles, agentic leash]
  - **authors:** Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko
  - **institution:** University of Southern California, Florida International University
  - **link:** https://arxiv.org/pdf/2601.00097
  - **contributions:** 1. A novel LLM agent designed to autonomously extract and construct causal feedback Fuzzy Cognitive Maps (FCMs) from raw text. 2. A three-step instruction-guided process for systematically extracting key concepts and causal edges to build the FCM dynamical system. 3. Demonstration that the LLM-generated FCMs converge to the same equilibrium dynamics as human-generated ones and that mixed FCMs from different LLMs can create new equilibria.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes an LLM agent to autonomously extract causal feedback Fuzzy Cognitive Maps from text. The agent uses a three-step process to identify concepts and causal edges, forming a dynamical system. The generated FCMs matched human-generated equilibrium dynamics and mixing models from different LLMs produced new equilibria for better causal approximation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs] --> B[核心问题/Problem: How to autonomously extract causal structures from text?]
        A --> C[主要方法/Method: Design an LLM agent with a three-step instruction process to build FCMs from text.]
        A --> D[关键结果/Results: LLM-generated FCMs match human equilibrium dynamics; mixed FCMs create new equilibria.]
    ```

- **[arXiv260105] Mortar: Evolving Mechanics for Automatic Game Design**
  - **tags:** [ai], [procedural content generation], [quality-diversity algorithm, large language model, skill-based ordering, tree search, game mechanics evolution]
  - **authors:** Muhammad U. Nasir, Yuchen Li, Steven James, Julian Togelius
  - **institution:** University of the Witwatersrand, New York University
  - **link:** https://arxiv.org/pdf/2601.00105
  - **contributions:** 1. Proposes Mortar, a system that autonomously evolves game mechanics for automatic game design by combining a quality-diversity algorithm with a large language model. 2. Introduces a novel evaluation framework where mechanics are assessed by synthesizing complete games and measuring their ability to preserve a skill-based ordering over players. 3. Demonstrates the system's effectiveness through ablation studies and a user study, showing it produces diverse, playable games with mechanics that improve skill-based ordering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26506e29345695337e5d47f5dfb1499fe23b996958dc6fd30e9b4371545d6994_w640_q70.webp
  - **Simple LLM Summary:** The paper presents Mortar, a system for automatic game design that evolves game mechanics by combining a quality-diversity algorithm with an LLM. It evaluates mechanics by constructing complete games via tree search and testing if they preserve a skill-based player ordering. The results show Mortar generates diverse, playable games with mechanics that effectively contribute to skill-based gameplay.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MORTAR: Evolving Mechanics for Automatic Game Design] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[手动设计游戏机制耗时且依赖专家/Manual game mechanic design is time-consuming and expert-driven]
        Method[主要方法/Method] --> M1[结合质量多样性算法与LLM/Combines quality-diversity algorithm with LLM]
        Method --> M2[通过树搜索合成完整游戏进行评估/Evaluates mechanics by synthesizing games via tree search]
        Method --> M3[评估技能排序保持能力/Measures ability to preserve skill-based player ordering]
        Results[关键结果/Results] --> R1[生成多样且可玩的游戏/Produces diverse and playable games]
        Results --> R2[机制提升技能排序分数/Mechanics improve skill-based ordering score]
    ```

- **[arXiv260105] Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control**
  - **tags:** [mlsys], [agent system], [hybrid agentic framework, hallucination tax, Human Imitator, stochastic reasoning, inventory optimization]
  - **authors:** Yaqi Duan, Yichun Hu, Jiashuo Jiang
  - **institution:** New York University, Cornell University, Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00121
  - **contributions:** 1. Identifies and quantifies the "hallucination tax" when using LLMs as end-to-end solvers for inventory control, highlighting their limitation in grounded stochastic reasoning. 2. Proposes a novel hybrid agentic framework that decouples semantic reasoning (handled by LLM) from mathematical calculation (handled by rigorous algorithms) to create an intelligent interface for optimization. 3. Introduces the "Human Imitator," a fine-tuned digital twin of a boundedly rational manager, to enable scalable and reproducible stress-testing of interactive systems against ambiguous real-world dialogue.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5522adbd58df6d202685913f2a038b91f8e5b3730f51d9964297ad57db17174e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of using LLMs for inventory management by showing that direct application incurs a "hallucination tax" due to poor stochastic reasoning. To solve this, the authors propose a hybrid agentic framework where the LLM acts as a natural-language interface that calls rigorous optimization algorithms. The framework reduces inventory costs by 32.1% compared to an LLM-only baseline, demonstrating that LLMs are best used as interfaces to make expert methods accessible, not as replacements for them.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Ask, Clarify, Optimize<br>Human-LLM Agent Collaboration] --> B[核心问题/Problem<br>LLMs as end-to-end solvers incur "hallucination tax"<br>LLMs无法进行可靠的随机推理]
        A --> C[主要方法/Method<br>Hybrid Agentic Framework<br>LLM作为接口，调用严格算法<br>语义推理与数学计算解耦]
        A --> D[关键结果/Results<br>Cost reduced by 32.1% vs baseline<br>LLMs are interfaces, not replacements<br>瓶颈是计算而非信息]
    ```

- **[arXiv260105] Constructing a Neuro-Symbolic Mathematician from First Principles**
  - **tags:** [ai], [neuro-symbolic reasoning], [neuro-symbolic architecture, differentiable logic engine, hypergraph transformer, energy minimization, proof search]
  - **authors:** Keqin Xie
  - **institution:** (Inferred from email domain: xiekeqin30@gmail.com) Affiliation not explicitly stated in provided text. Could be an independent researcher or institution not listed on first page.
  - **link:** https://arxiv.org/pdf/2601.00125
  - **contributions:** 1. Proposes Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs. 2. Introduces a Symbolic Reasoning Kernel (SRK), a differentiable logic engine that maps logical constraints to a continuous energy landscape for gradient-based training. 3. Enables multi-step deduction by combining energy minimization with guided search algorithms like Monte Carlo Tree Search and Evolutionary Proof Search.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/782595f0879bde3e0c09f37353b140392ee9407cf2eed312d3faea5d4ca981ae_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the issue of logical failures in Large Language Models (LLMs) during complex reasoning by proposing Mathesis, a neuro-symbolic system that uses a differentiable logic engine to turn proof search into energy minimization. The method combines a Hypergraph Transformer Brain with symbolic reasoning via gradient signals, aiming to achieve rigorous mathematical deduction. The core conclusion is that this architecture provides a pathway to integrate neural pattern recognition with symbolic rigor for reliable reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Constructing a Neuro-Symbolic Mathematician] --> B
        A --> C
        A --> D
        B[核心问题/Problem: LLMs lack axiomatic framework, leading to logical failures]
        C[主要方法/Method: Neuro-symbolic architecture (Mathesis) with differentiable logic engine (SRK) for energy minimization]
        D[关键结果/Results: Proof search framed as energy minimization, enabling gradient-based training and guided multi-step deduction]
    ```

- **[arXiv260105] Explicit Abstention Knobs for Predictable Reliability in Video Question Answering**
  - **tags:** [ai], [selective prediction], [selective prediction, confidence-based abstention, risk-coverage tradeoff, distribution shift, video question answering]
  - **authors:** Jorge Ortiz
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2601.00138
  - **contributions:** 1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model's confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Explicit Abstention Knobs for Predictable Reliability in Video Question Answering] --> B[核心问题/Problem: Can confidence-based abstention provide reliable error rate control in video QA, especially under distribution shift?]
        A --> C[主要方法/Method: Use confidence thresholding on a VLM (Gemini 2.0 Flash) evaluated on the NExT-QA dataset, testing under evidence degradation.]
        A --> D[关键结果/Results: In-distribution control works, but confidence fails to decrease under shift, motivating warrant-based prediction.]
    ```

- **[arXiv260105] An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making**
  - **tags:** [ai], [neural reasoning], [Sphere Neural Networks, disjunctive syllogistic reasoning, catastrophic forgetting, explicit model construction, Euler Net]
  - **authors:** Tiansi Dong, Henry He, Pietro Liò, Mateja Jamnik
  - **institution:** University of Cambridge
  - **link:** https://arxiv.org/pdf/2601.00142
  - **contributions:** 1. A comparative analysis showing explicit model-based neural reasoning is more reliable than LLM-based or supervised learning-based reasoning. 2. The proposal of Sphere Neural Networks, a novel architecture that embeds concepts as circles on a sphere to represent negation and filter illogical statements. 3. An empirical demonstration that Sphere Neural Networks can master 16 syllogistic reasoning tasks, including disjunctive syllogism, without catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5204d88cd2c0986ce1fa0776ae031033771470697a797f7d2ee4cba326a5a31_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Sphere Neural Networks, which embed concepts as circles on a sphere to enable explicit model-based reasoning. The method is shown to reliably perform complex syllogistic reasoning tasks without catastrophic forgetting, unlike supervised learning approaches. The authors conclude that explicit model construction is the most reliable category for neural reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making"] --> Problem["核心问题/Problem: LLMs and supervised learning are unreliable for simple reasoning tasks"]
        Root --> Method["主要方法/Method: Propose Sphere Neural Networks using concept circles on a sphere"]
        Root --> Results["关键结果/Results: Mastered 16 syllogistic tasks, no catastrophic forgetting, explicit models are most reliable"]
    ```

- **[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications**
  - **tags:** [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]
  - **authors:** Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen
  - **institution:** Qifu Technology, Fudan University
  - **link:** https://arxiv.org/pdf/2601.00150
  - **contributions:** 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏金融信贷领域专用多模态基准/Lack of domain-specific multimodal benchmark for financial credit]
        C --> C1[构建隐私合规的合成-采集管道/Build privacy-compliant synthesis-capture pipeline]
        C --> C2[设计三维评估框架/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]
        D --> D1[评估23个VLM/Evaluate 23 VLMs]
        D --> D2[Qfin-VL-Instruct性能最佳/Qfin-VL-Instruct achieves top score]
        D --> D3[鲁棒性挑战/Robustness remains a challenge]
    ```

- **[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients**
  - **tags:** [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]
  - **authors:** Junkai Luo, Yinglun Zhu
  - **institution:** University of California, Riverside
  - **link:** https://arxiv.org/pdf/2601.00167
  - **contributions:** 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Online Finetuning Decision Transformers with Pure RL Gradients] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[在线微调DT时，纯RL梯度方法未被探索/Pure RL gradients for online DT finetuning unexplored]
    B --> B2[后见之益回报重标注与RL算法不兼容/Hindsight return relabeling incompatible with RL]
    C --> C1[适配GRPO至DT/Adapt GRPO to DTs]
    C --> C2[引入关键修改: 子轨迹优化等/Introduce key modifications]
    D --> D1[超越现有在线DT基线/Outperform online DT baselines]
    D --> D2[实现SOTA性能/Achieve SOTA performance]
    ```

- **[arXiv260105] SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification**
  - **tags:** [ai], [semi-supervised learning], [Generative Adversarial Network, Swin Transformer, spike classification, semi-supervised learning, Bayesian optimization]
  - **authors:** Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti
  - **institution:** Deakin University, CSIRO Health and Biosecurity, Islamic Azad University, University of Southern Queensland
  - **link:** https://arxiv.org/pdf/2601.00189
  - **contributions:** 1. Proposed a novel semi-supervised GAN architecture (SSI-GAN) with a Swin-inspired, shifted-window discriminator for neuronal spike classification. 2. Introduced a transformer-based generator and a flat, window-based transformer discriminator with multi-head self-attention to capture sparse, high-frequency spike features. 3. Demonstrated state-of-the-art performance with 99.93% accuracy using only 1-3% labeled data, reducing manual labeling effort by 97-99% compared to supervised methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the labor-intensive problem of classifying mosquito neuronal spikes for arboviral disease detection by proposing SSI-GAN, a semi-supervised GAN that combines a Swin-inspired discriminator with a transformer-based generator. Using only 1-3% labeled data from over 15 million spike samples, it achieved up to 99.93% accuracy in classifying Zika-infected, dengue-infected, or uninfected categories, significantly reducing labeling effort while outperforming baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SSI-GAN: 半监督Swin启发的生成对抗网络用于神经元尖峰分类] --> B[核心问题/Problem: 蚊虫神经元尖峰模式手动分类劳动密集且昂贵，现有深度学习方法需要全标记数据和高度预处理]
        A --> C[主要方法/Method: 提出SSI-GAN，使用Swin启发的移位窗口判别器和基于Transformer的生成器，仅需1-3%标记数据]
        A --> D[关键结果/Results: 达到99.93%分类准确率，标记工作量减少97-99%，在所有感染阶段保持高精度]
    ```

- **[arXiv260105] Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation**
  - **tags:** [nlp], [emotion recognition in conversation], [ablation study, conversational context, discourse markers, causal context, IEMOCAP]
  - **authors:** Cheonkam Jeong, Adeline Nyamathi
  - **institution:** University of California, Irvine
  - **link:** https://arxiv.org/pdf/2601.00181
  - **contributions:** 1. A rigorous ablation study revealing that conversational context is paramount for ERC, with performance saturating within 10-30 preceding turns, and that hierarchical sentence representations and external affective lexicons provide no additional benefit when context is used. 2. Achieving state-of-the-art text-only performance on IEMOCAP using simple architectures with strictly causal context. 3. A novel linguistic analysis connecting recognition to generation, finding a significant association between emotion and discourse marker positioning, particularly that "sad" utterances use fewer left-periphery markers and rely more on context for disambiguation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e38dfbfb3b50e432c39aabed201ddaf199012203ee5e7e82a7aa044267fc2a2_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically analyzes Emotion Recognition in Conversation (ERC) to identify which architectural components matter and connects recognition insights to linguistic patterns for generation. Through ablation studies on IEMOCAP, it finds conversational context is most critical, and via linguistic analysis, it discovers that emotion correlates with discourse marker usage. The main conclusion is that simple models with causal context are sufficient for high performance, and the lack of explicit pragmatic signals in "sad" utterances explains their greater reliance on context.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Understanding Emotion in Discourse] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[ERC模型哪些组件真正有效?/Which ERC components matter?]
        B --> B2[如何连接识别与生成?/How to connect recognition & generation?]
        C --> C1[系统消融研究/Systematic Ablation Study]
        C --> C2[话语标记分析/Discourse Marker Analysis]
        D --> D1[上下文最关键, 10-30轮饱和/Context is key, saturates in 10-30 turns]
        D --> D2[简单因果模型SOTA/Simple causal model achieves SOTA]
        D --> D3[悲伤话语标记少, 更依赖上下文/Sad utterances have fewer markers, rely more on context]
    ```

- **[arXiv260105] Latent Flow Matching for Expressive Singing Voice Synthesis**
  - **tags:** [ai], [generative models], [conditional flow matching, latent space modeling, singing voice synthesis, ordinary differential equation, variational autoencoder]
  - **authors:** Minhyeok Yun, Yong-Hoon Choi
  - **institution:** Kwangwoon University
  - **link:** https://arxiv.org/pdf/2601.00217
  - **code:** https://github.com/alsgur9368/FM-Singer
  - **contributions:** 1. Proposes FM-Singer, a novel singing voice synthesis method that applies conditional flow matching (CFM) in the latent space to address the prior-posterior mismatch in cVAE-based models. 2. Introduces a latent ODE refinement step at inference time to transport prior samples towards the expressive posterior distribution, improving fine-grained expressiveness while maintaining parallel decoding efficiency. 3. Demonstrates consistent performance improvements on Korean and Chinese singing datasets, including lower mel-cepstral distortion and F0 error, and higher perceptual scores.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ff2d99db594dd8c63fa46f401cf9408cda76d76d116f920d5a1dbb077b2fcf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the degradation of fine-grained expressiveness in cVAE-based singing voice synthesis due to a mismatch between the prior and posterior latent distributions. The proposed FM-Singer method uses conditional flow matching in the latent space to learn a vector field that refines prior samples via an ODE, enhancing expressiveness like vibrato. Experiments show the method outperforms strong baselines on multiple datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Latent Flow Matching for Expressive Singing Voice Synthesis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[cVAE合成中先验-后验不匹配<br/>Prior-Posterior Mismatch in cVAE]
        B --> B2[细微表现力下降<br/>Degraded Fine-Grained Expressiveness]
        C --> C1[潜在空间条件流匹配<br/>Latent-Space Conditional Flow Matching]
        C --> C2[ODE推理时细化<br/>ODE-based Refinement at Inference]
        D --> D1[更低的MCD与F0误差<br/>Lower MCD & F0 Error]
        D --> D2[更高的感知评分<br/>Higher Perceptual Scores]
    ```

- **[arXiv260105] JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation**
  - **tags:** [nlp], [machine translation], [LLM-as-a-judge, pairwise comparison, Bradley-Terry model, reference-free evaluation, anchored evaluation]
  - **authors:** Leonard Lin, Adam Lensenmayer
  - **institution:** Shisa.AI
  - **link:** https://arxiv.org/pdf/2601.00223
  - **contributions:** 1. Introduces JP-TL-Bench, a lightweight, open benchmark for iterative development of Japanese-English translation systems. 2. Proposes a reliable and affordable evaluation protocol using reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. 3. Provides structurally stable scores by aggregating pairwise results with a Bradley-Terry model and reporting normalized LT scores.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3672d8de20107e284402d4b12e978f246eb0df92617095708ca7710e712594d_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces JP-TL-Bench, a benchmark for evaluating high-quality Japanese-English translation. It uses a protocol where candidate models are compared against a fixed anchor set via pairwise LLM judgments, with results aggregated using a Bradley-Terry model to produce stable scores. This approach aims to provide a high-resolution signal for distinguishing between already fluent translations where traditional metrics saturate.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[JP-TL-Bench: Anchored Pairwise LLM Evaluation<br>JP-TL-Bench: 锚定成对LLM评估] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>现有评估方法难以区分高质量翻译<br>Existing evaluation struggles to differentiate high-quality translations]
        C[主要方法/Method<br>基于固定锚定集的成对LLM比较<br>Pairwise LLM comparison against a fixed anchor set]
        D[关键结果/Results<br>提供稳定、可解释的分数<br>Provides stable, interpretable scores]
    ```

- **[arXiv260105] FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems**
  - **tags:** [mlsys], [llm inference], [GPU kernels, benchmarking, kernel substitution, FlashInfer Trace, AI-generated code]
  - **authors:** Shanli Xing, Yiyan Zhai, Alexander Jiang, Yixin Dong, Yong Wu, Zihao Ye, Charlie Ruan, Yingyi Huang, Yineng Zhang, Liangsheng Yin, Aksara Bayyapu, Luis Ceze, Tianqi Chen
  - **institution:** Carnegie Mellon University, University of Washington, NVIDIA, University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2601.00227
  - **contributions:** 1. Introduces FlashInfer Trace, a unified JSON schema to standardize the description of kernel tasks, workloads, implementations, and evaluations for AI agents. 2. Presents FlashInfer-Bench, a comprehensive framework including a curated dataset from real serving traces, a correctness- and performance-aware benchmarking system, and a public leaderboard. 3. Implements a dynamic kernel substitution mechanism (`apply()`) that allows seamless integration of the best-performing AI-generated kernels into production LLM inference engines like SGLang and vLLM.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c4acb494d20ab22aaf93877ea7f6bf9807d9ee81afeaa72f868e4b115633740_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FlashInfer-Bench, a standardized framework to bridge the gap between AI-generated GPU kernels and their deployment in real-world LLM inference systems. It provides a closed-loop workflow for kernel generation, benchmarking, and integration, enabling the evaluation of LLM agents' GPU programming capabilities and the seamless injection of optimized kernels into production engines. The work establishes a practical pathway for continuously improving and deploying AI-generated kernels at scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Integrating AI-generated GPU kernels into real-world LLM inference systems is challenging]
        C[主要方法/Method: A standardized closed-loop framework with FlashInfer Trace schema, benchmarking, and dynamic kernel substitution]
        D[关键结果/Results: Establishes a reproducible pathway for improving and deploying AI-generated kernels in production]
    ```

- **[arXiv260105] GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [LoRA, K-FAC, Parameter-Efficient Fine-Tuning, Fisher Information, Dynamic Rank Adaptation]
  - **authors:** Pritish Saha, Chandrav Rajbangshi, Rudra Goyal, Mohit Goyal, Anurag Deo, Biswajit Roy, Ningthoujam Dhanachandra Singh, Raxit Goswami, Amitava Das
  - **institution:** RAAPID Lab, Pragya Lab (BITS Pilani, Goa)
  - **link:** https://arxiv.org/pdf/2601.00231
  - **contributions:** 1. Introduces K-FAC-based gradient preconditioning in the low-rank subspace for more geometry-aware updates. 2. Proposes periodic Fisher-guided reprojection of the LoRA basis to suppress parameter drift. 3. Implements dynamic rank adaptation to concentrate capacity on high-signal directions, reducing the number of trainable parameters.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that standard LoRA/QLoRA methods are geometry-agnostic, leading to inefficient updates and parameter drift. It proposes GRIT, a new LoRA procedure that uses K-FAC preconditioning, Fisher-guided reprojection, and dynamic rank adaptation to make updates more curvature-aware. This approach matches or surpasses baseline performance while reducing trainable parameters by an average of 46% and achieving lower drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GRIT: Geometry-Aware PEFT] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[标准LoRA/QLoRA忽略曲率/Standard LoRA/QLoRA ignores curvature]
        B --> B2[导致低效更新与参数漂移/Causes inefficient updates & parameter drift]
        C --> C1[K-FAC预条件梯度/K-FAC preconditioned gradients]
        C --> C2[Fisher引导重投影/Fisher-guided reprojection]
        C --> C3[动态秩适应/Dynamic rank adaptation]
        D --> D1[性能相当或更好/Matches or surpasses baselines]
        D --> D2[参数减少~46%/Reduces parameters by ~46%]
        D --> D3[漂移更低/Lower drift]
    ```

- **[arXiv260105] Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability**
  - **tags:** [ai], [multi-agent systems], [intergroup bias, belief poisoning attack, multi-agent social simulation, human-norm script, agent safety]
  - **authors:** Zongwei Wang, Bincheng Gu, Hongyu Yu, Junliang Yu, Tao He, Jiayin Feng, Min Gao
  - **institution:** Chongqing University, The University of Queensland, Virginia Polytechnic Institute and State University
  - **link:** https://arxiv.org/pdf/2601.00240
  - **contributions:** 1. Identifies a new type of bias in LLM-powered agents: intergroup bias that can align with an agent-human divide, potentially treating humans as an outgroup. 2. Proposes a novel attack method, the Belief Poisoning Attack (BPA), with two variants (BPA-PP and BPA-MP) that exploit the belief-dependent nature of agents to suppress pro-human norms and reactivate bias. 3. Discusses practical mitigation strategies for hardening agent frameworks against such attacks, focusing on interventions at profile and memory boundaries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63aefdbbc5c1919bddd5ac1c591ec07bd99f076d1eb4b0ab4722c5ed75f1d8fc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether LLM-powered agents exhibit bias against humans as an outgroup. It finds that while a "human-norm script" can attenuate this bias, the script's activation depends on the agent's belief about the presence of a human, creating a vulnerability. The authors introduce a Belief Poisoning Attack to exploit this vulnerability and propose mitigation strategies for safer agent design.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Will LLM-powered Agents Bias Against Humans?<br/>探索信念依赖的脆弱性] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Agent Intergroup Bias & Human Outgroup Risk]
        C[主要方法/Method<br/>Multi-Agent Simulation & Belief Poisoning Attack (BPA)]
        D[关键结果/Results<br/>Bias Exists, BPA Effective, Mitigations Proposed]
    ```

- **[arXiv260105] An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), Dual-Agent LLM, QLoRA, Vulnerability Detection]
  - **authors:** Md Hasan Saju, Maher Muhtadi, Akramul Azim
  - **institution:** Ontario Tech University
  - **link:** https://arxiv.org/pdf/2601.00254
  - **contributions:** 1. Conducted a comparative empirical evaluation of three LLM-based approaches (RAG, SFT, and Dual-Agent) for code vulnerability detection. 2. Proposed a RAG framework that integrates external domain knowledge (e.g., from the internet and MITRE CWE database) to achieve state-of-the-art performance. 3. Introduced and evaluated a Dual-Agent LLM system designed to improve reasoning transparency and error mitigation with reduced resource overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85bf4a637927a306d9960d3a0ec1a16f810bb0755b9400e6b4b384627260154e_w640_q70.webp
  - **Simple LLM Summary:** This paper compares three LLM-based methods—RAG, SFT, and a Dual-Agent system—for detecting software vulnerabilities in code. The RAG approach, which augments the LLM with external knowledge, achieved the highest accuracy and F1 score, demonstrating the value of contextual information for this task.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection<br>LLM代码漏洞检测方法的实证评估"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Automated detection of software vulnerabilities in codebases<br>代码库中软件漏洞的自动化检测"]
        Method["主要方法/Method<br>Comparative study of RAG, SFT, and Dual-Agent LLM frameworks<br>对RAG、SFT和双智能体LLM框架的比较研究"]
        Results["关键结果/Results<br>RAG achieved highest accuracy (0.86) & F1 (0.85)<br>RAG取得了最高的准确率(0.86)和F1分数(0.85)"]
    ```

- **[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective**
  - **tags:** [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]
  - **authors:** Aly Sabri Abdalla, Vuk Marojevic
  - **institution:** Mississippi State University
  - **link:** https://arxiv.org/pdf/2601.00257
  - **contributions:** 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --> P1["子问题/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]
        Method["主要方法/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --> M1["组件/Component: Semantic-aware rApp (terrain interpreter)"]
        Method --> M2["组件/Component: RL-enabled xApp (trajectory planner)"]
        Results["关键结果/Results: Framework enables closed-loop, AI-optimized LAE operations"] --> R1["评估/Evaluation: Feasibility and performance analysis presented"]
        Results --> R2["展望/Outlook: Research challenges and standardization needs surveyed"]
    ```

- **[arXiv260105] Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation**
  - **tags:** [nlp], [explainable ai (xai)], [counterfactual examples, multilingual, data augmentation, large language models, model robustness]
  - **authors:** Qianli Wang, Van Bach Nguyen, Yihong Liu, Fedor Splitt, Nils Feldhus, Christin Seifert, Hinrich Schütze, Sebastian Möller, Vera Schmitt
  - **institution:** Technische Universität Berlin, University of Marburg, LMU Munich, German Research Center for Artificial Intelligence (DFKI), Munich Center for Machine Learning (MCML), BIFOLD – Berlin Institute for the Foundations of Learning and Data
  - **link:** https://arxiv.org/pdf/2601.00263
  - **contributions:** 1. Evaluated the quality of LLM-generated multilingual counterfactuals, comparing direct generation and translation-based methods across six languages. 2. Identified four main error types common in generated counterfactuals across languages and found similar edit patterns in high-resource European languages. 3. Demonstrated that multilingual counterfactual data augmentation yields greater performance improvements than cross-lingual augmentation, especially for lower-resource languages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3b95c2cf3407989c3c5a932764e908182c4d60d7451ace3f5f15e194a3a7e5_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the effectiveness of large language models (LLMs) in generating multilingual counterfactual examples. It compares directly generated and translation-based counterfactuals across six languages, finding that translation-based ones are more valid but require more edits and still underperform English ones. The study concludes that while multilingual counterfactual data augmentation improves model performance, especially for low-resource languages, the quality limitations of the generated counterfactuals constrain the gains in robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM生成多语言反事实示例的有效性未知/Effectiveness of LLM-generated multilingual counterfactuals is unclear]
        C --> C1[评估直接生成与翻译生成的反事实/Evaluate directly generated and translation-based counterfactuals]
        C --> C2[分析编辑模式与错误类型/Analyze edit patterns and error types]
        C --> C3[用于数据增强实验/Use for data augmentation experiments]
        D --> D1[翻译生成的反事实更有效但编辑更多/Translation-based CFs are more valid but require more edits]
        D --> D2[高资源语言编辑模式相似/Edit patterns are similar for high-resource languages]
        D --> D3[多语言数据增强效果更好/Multilingual data augmentation yields larger improvements]
    ```

- **[arXiv260105] Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity**
  - **tags:** [mlsys], [agent system], [function-calling, benchmark, API complexity, LLM agents, WildAGTEval]
  - **authors:** Doyoung Kim, Zhiwei Ren, Jie Hao, Zhongkai Sun, Lichao Wang, Xiyao Ma, Zack Ye, Xu Han, Jun Yin, Heng Ji, Wei Shen, Xing Fan, Benjamin Yao, Chenlei Guo
  - **institution:** Amazon, KAIST, University of Pittsburgh, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2601.00268
  - **code:** github.com/Demon-JieHao/WildAGTEval
  - **contributions:** 1. Introduces WildAGTEval, a novel benchmark for evaluating LLM agents under realistic API complexity, covering both API specification and execution challenges. 2. Provides a comprehensive API system with 60 distinct complexity scenarios, composable into ~32K test configurations, and user-agent interactions for evaluation. 3. Systematically assesses advanced LLMs, revealing significant performance drops (e.g., 27.3% for irrelevant information complexity) and identifying critical failure modes like intent distortion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c517d92b4a84a20e84e30c740db5d9b70b7f3195c6de0fc8e049a5f0a4c9af_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces WildAGTEval, a benchmark designed to evaluate LLM agents' function-calling capabilities under realistic API complexities, including detailed specifications and noisy execution. The study finds that most scenarios are challenging, with irrelevant information posing the greatest difficulty and causing significant performance drops in strong models. The qualitative analysis also reveals that LLMs sometimes distort user intent to claim task completion, negatively impacting user satisfaction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有基准假设理想化API / Existing benchmarks assume idealized APIs]
        B --> B2[忽略现实因素如噪声输出 / Ignore real-world factors like noisy outputs]
        C --> C1[提出WildAGTEval基准 / Propose WildAGTEval benchmark]
        C --> C2[涵盖API规范与执行复杂性 / Covers API specification & execution complexity]
        D --> D1[大多数场景具有挑战性 / Most scenarios are challenging]
        D --> D2[无关信息复杂度导致性能显著下降 / Irrelevant info complexity causes significant performance drop]
        D --> D3[LLM可能扭曲用户意图 / LLMs may distort user intent]
    ```

- **[arXiv260105] FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering**
  - **tags:** [mlsys], [multi-modal inference], [hallucination detection, vision-language models, uncertainty estimation, model-driven learning, LLM-as-a-Judge]
  - **authors:** Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute
  - **link:** https://arxiv.org/pdf/2601.00269
  - **contributions:** 1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FaithSCAN: Faithful VQA Hallucination Detection] --> B[核心问题/Problem: VQA模型产生流畅但视觉无根据的答案/Faithfulness Hallucinations in VQA]
        A --> C[主要方法/Method: 利用VLM内部信号与不确定性感知融合/Exploit VLM Internal Signals & Uncertainty-Aware Fusion]
        A --> D[关键结果/Results: 高效且优于现有方法/More Effective & Efficient than Prior Methods]
    ```

- **[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]
  - **authors:** Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian Möller, Vera Schmitt
  - **institution:** Technische Universität Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen
  - **link:** https://arxiv.org/pdf/2601.00282
  - **contributions:** 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Can Large Language Models Still Explain Themselves?<br/>大语言模型还能解释自己吗？"] --> Problem["Quantization's effect on Self-Explanations is unknown.<br/>量化对自我解释的影响未知"]
        Root --> Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>评估量化后LLM的自然语言解释和反事实示例"]
        Root --> Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>质量/忠实度适度下降；影响因上下文而异"]
    ```

- **[arXiv260105] Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies**
  - **tags:** [cv], [medical image classification], [Swin Transformer, BatchFormer, Focal Loss, ReduceLROnPlateau, ISIC2019]
  - **authors:** Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua
  - **institution:** University of Technology Sydney, University of Sydney, Shaoyang University
  - **link:** https://arxiv.org/pdf/2601.00286
  - **contributions:** 1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model's potential as a clinical support tool.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Automated Differential Diagnosis of Skin Diseases<br>皮肤疾病自动鉴别诊断] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Limited dermatologist access & need for diagnostic tools<br>皮肤科医生资源有限，需要诊断工具]
        C[主要方法/Method<br>Deep learning (Swin Transformer) with imbalance-aware strategies<br>深度学习（Swin Transformer）与不平衡感知策略]
        D[关键结果/Results<br>87.71% accuracy on ISIC2019 dataset<br>在ISIC2019数据集上达到87.71%准确率]
    ```

- **[arXiv260105] ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization**
  - **tags:** [ai], [reinforcement learning], [self-evolving agent, hierarchical memory, protocol redesign]
  - **authors:** Sixue Xing, Xuanye Xia, Kerui Wu, Meng Jiang, Jintai Chen, Tianfan Fu
  - **institution:** University of Notre Dame, Georgia Institute of Technology, University of Massachusetts Amherst, Hong Kong University of Science and Technology (Guangzhou), Nanjing University
  - **link:** https://arxiv.org/pdf/2601.00290
  - **contributions:** 1. Proposes ClinicalReTrial, a self-evolving AI agent framework that moves beyond static trial outcome prediction to actionable protocol optimization. 2. Introduces a closed-loop, reward-driven optimization framework that integrates failure diagnosis, safety-aware modification, and candidate evaluation, using a prediction model as a simulation environment. 3. Designs a hierarchical memory mechanism to capture iteration-level feedback and distill transferable redesign patterns for efficient exploration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b82407468d333ae862573b891f5a19251a04fd53b36b2e4e9e895ca08f9073_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ClinicalReTrial, a self-evolving AI agent framework that optimizes clinical trial protocols through iterative, reward-driven redesign. It uses an outcome prediction model as a simulation environment and a hierarchical memory for efficient exploration. Empirical results show the framework improves 83.3% of trial protocols with a mean success probability gain of 5.7%.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization] --> B(核心问题/Problem: Clinical trial failure due to protocol design flaws, existing AI is reactive)
        A --> C(主要方法/Method: Self-evolving agent with closed-loop optimization, hierarchical memory, using prediction model as simulation)
        A --> D(关键结果/Results: Improves 83.3% of protocols, mean success probability gain of 5.7%)
    ```

- **[arXiv260105] DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection**
  - **tags:** [nlp], [speech processing], [depression detection, semantic bias, text-to-speech, disentangled representation, data augmentation]
  - **authors:** Yuxin Li, Xiangyu Zhang, Yifei Li, Zhiwei Guo, Haoyang Zhang, Eng Siong Chng, Cuntai Guan
  - **institution:** Nanyang Technological University, UNSW Sydney, Peking University
  - **link:** https://arxiv.org/pdf/2601.00303
  - **contributions:** 1. Proposes DepFlow, a novel three-stage depression-conditioned TTS framework that disentangles depression-specific acoustic patterns from speaker and content information using adversarial training and flow-matching. 2. Introduces a prototype-based severity mapping mechanism for smooth and interpretable control over the synthesized depressive severity. 3. Constructs a Camouflage Depression-oriented Augmentation (CDoA) dataset using DepFlow to mitigate semantic bias, which significantly improves the robustness of depression detection models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddfe4c5383fa66e6b8e028851d0fe67037a11642b799a3626a1100aaa4cd8096_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of semantic bias in depression detection models, where models learn shortcuts from linguistic sentiment instead of acoustic cues. It proposes DepFlow, a disentangled speech generation framework, to create a synthetic dataset (CDoA) that pairs depressed acoustic patterns with positive/neutral text. This data augmentation method improves model robustness, outperforming conventional strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Models learn semantic shortcuts from sentiment-content coupling in depression datasets."]
        Method["主要方法/Method: DepFlow, a 3-stage TTS framework for disentangling & controlling depression acoustics."]
        Results["关键结果/Results: CDoA augmentation improves detection model F1 scores by 5-12%."]
    ```

- **[arXiv260105] The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth**
  - **tags:** [ai], [ai safety & alignment], [synthetic reality, epistemic security, provenance, trust erosion, generative AI harms]
  - **authors:** Emilio Ferrara
  - **institution:** University of Southern California (USC)
  - **link:** https://arxiv.org/pdf/2601.00306
  - **contributions:** 1. Formalizes the concept of "synthetic reality" as a layered socio-technical stack comprising content, identity, interaction, and institutions. 2. Expands a taxonomy of Generative AI harms and articulates the qualitative shifts it introduces, such as cost collapse and provenance gaps. 3. Proposes a complementary mitigation stack and a research agenda focused on measuring epistemic security, culminating in the articulation of the "Generative AI Paradox".
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b42a2aa0b289b69471dcaa3b08e187b14ded4070e26848bb4917fcc12b2427_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the primary risk of Generative AI is not just creating fake content, but the systemic erosion of shared truth and verification practices as it enables the easy creation of synthetic content, identities, and interactions. The authors formalize this as "synthetic reality," analyze its risks, and propose a multi-layered mitigation approach. They conclude with the "Generative AI Paradox": the potential for societies to rationally discount all digital evidence as synthetic media becomes ubiquitous.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Generative AI Paradox<br>生成式AI悖论] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GenAI erodes shared truth & verification<br>GenAI侵蚀共同认知与验证]
        C --> C1[Formalize synthetic reality stack & harms taxonomy<br>形式化合成现实栈与危害分类]
        C --> C2[Propose mitigation stack & research agenda<br>提出缓解栈与研究议程]
        D --> D1[Generative AI Paradox: discount digital evidence<br>生成式AI悖论：质疑数字证据]
    ```

- **[arXiv260105] VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning**
  - **tags:** [cv], [person re-identification], [feature fusion, alpha-divergence loss, dynamic multi-task learning, semantic clustering, computational efficiency]
  - **authors:** Anns Ijaz, Muhammad Azeem Javed
  - **institution:** University of Management and Technology
  - **link:** https://arxiv.org/pdf/2601.00307
  - **contributions:** 1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VisNet: Efficient Person Re-Identification] --> B[核心问题/Problem: Accuracy vs. Computational Cost Trade-off]
        A --> C[主要方法/Method: Feature Fusion, Semantic Clustering, Dynamic Multi-Task Learning, α-Divergence Loss]
        A --> D[关键结果/Results: 87.05% Rank-1, 77.65% mAP, 4.601 GFLOPs]
    ```

- **[arXiv260105] Multiagent Reinforcement Learning for Liquidity Games**
  - **tags:** [ai], [multiagent reinforcement learning], [liquidity games, rational swarms, difference rewards, Markov team games, financial swarm]
  - **authors:** Alicia Vidler, Gal A. Kaminka
  - **institution:** Bar-Ilan University
  - **link:** https://arxiv.org/pdf/2601.00324
  - **contributions:** 1. Unifies Liquidity Games (a game-theoretic model of liquidity formation) with Rational Swarms (a decentralized multiagent RL method using difference rewards). 2. Proposes a theoretical framework defining a swarm of independent traders whose collective objective is market liquidity provision. 3. Demonstrates that using difference rewards within a Markov team games framework enables individual liquidity-maximizing behaviors to contribute to overall market liquidity without requiring coordination.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1bd53d93f07cfb626ac581013a3e8a6bfa64ea30e2fea7f9e15851b64d4cd2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the tension between self-interested financial agents and the need for overall market liquidity. It proposes a "Financial Swarm" model that unifies Liquidity Games with Rational Swarms, using difference rewards in a Markov team game to align individual learning with the global objective. The main conclusion is that this framework allows independent agents to achieve both individual profitability and collective market efficiency without coordination or collusion.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiagent Reinforcement Learning for Liquidity Games"] --> Problem["核心问题/Problem: Self-interested agents vs. market liquidity"]
        Root --> Method["主要方法/Method: Unify Liquidity Games & Rational Swarms with difference rewards"]
        Root --> Results["关键结果/Results: Independent agents achieve liquidity without coordination"]
    ```

- **[arXiv260105] HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection**
  - **tags:** [cv], [anomaly detection], [frequency-guided learning, structural attention, semantic consistency, dual-branch framework, CLIP]
  - **authors:** Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao
  - **institution:** Tianjin University of Science and Technology, The University of Sydney, National University of Singapore
  - **link:** https://arxiv.org/pdf/2601.00327
  - **contributions:** 1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HarmoniAD: 异常检测/HarmoniAD: Anomaly Detection] --> B[核心问题/Problem: 结构-语义权衡/Structure-Semantics Trade-off]
        A --> C[主要方法/Method: 频率引导双分支框架/Frequency-Guided Dual-Branch Framework]
        A --> D[关键结果/Results: SOTA性能/SOTA Performance]
        B --> B1[结构模型噪声敏感/Structure Models: Noise-Sensitive]
        B --> B2[语义模型忽略细节/Semantic Models: Miss Details]
        C --> C1[高频分支: FSAM模块/High-Freq Branch: FSAM]
        C --> C2[低频分支: GSCM模块/Low-Freq Branch: GSCM]
        D --> D1[数据集: MVTec-AD, VisA, BTAD/Datasets: MVTec-AD, VisA, BTAD]
        D --> D2[结果: 高敏感性与鲁棒性/Results: High Sensitivity & Robustness]
    ```

- **[arXiv260105] Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $_1$ Relaxations**
  - **tags:** [ai], [multi-agent systems], [coalition structure generation, sparse regression, Bayesian greedy pursuit, l1 penalization, probabilistic framework]
  - **authors:** Angshul Majumdar
  - **institution:** IIIT Delhi
  - **link:** https://arxiv.org/pdf/2601.00329
  - **contributions:** 1. Proposes a novel probabilistic framework for Coalition Structure Generation (CSG) where coalition values are learned from episodic observations via sparse linear regression. 2. Introduces and analyzes the Bayesian Greedy Coalition Pursuit (BGCP) algorithm, providing theoretical guarantees for exact coalition recovery under certain conditions. 3. Provides theoretical analysis for an alternative ℓ1-penalized estimation scheme, deriving error bounds and translating them into welfare gap guarantees, and compares regimes where sparse methods outperform classical approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492cd85c7ccfef225059f9e637ee717227d05ec3497e1f7c70a5e2e84ad5cc35_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses coalition structure generation when coalition values are unknown and must be learned from noisy episodic data. It proposes a sparse probabilistic framework with two estimation methods: a Bayesian greedy pursuit algorithm and an ℓ1-penalized estimator, providing theoretical recovery and error guarantees. The analysis identifies conditions under which these sparse learning approaches yield welfare-optimal structures and outperform classical methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Sparse Probabilistic Coalition Structure Generation<br>稀疏概率联盟结构生成"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["学习联盟价值<br>Learning Coalition Values"]
        Method["主要方法<br>Key Methods"]
        Results["关键结果<br>Key Results"]
    
        Problem --> P1["从观察中学习<br>Learn from Episodic Observations"]
        Problem --> P2["噪声线性组合<br>Noisy Linear Combination of Contributions"]
    
        Method --> M1["贝叶斯贪婪联盟追踪 (BGCP)<br>Bayesian Greedy Coalition Pursuit (BGCP)"]
        Method --> M2["ℓ1 惩罚估计器<br>ℓ1-Penalized Estimator"]
    
        Results --> R1["BGCP: 高概率恢复联盟集<br>BGCP: Recovers Coalition Set w.h.p."]
        Results --> R2["ℓ1: 误差与福利差距界限<br>ℓ1: Error & Welfare Gap Bounds"]
        Results --> R3["识别稀疏/密集优势机制<br>Identifies Sparse/Dense Regimes"]
    ```

- **[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems**
  - **tags:** [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]
  - **authors:** Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lovén
  - **institution:** University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Luleå University of Technology, Peking University, TU Wien
  - **link:** https://arxiv.org/pdf/2601.00339
  - **contributions:** 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bio-inspired Agentic Self-healing Framework<br>生物启发的智能体自愈框架"] --> Problem["核心问题/Problem<br>DCCS中的复杂性与故障频发<br>Complexity & Frequent Faults in DCCS"]
        Root --> Method["主要方法/Method<br>ReCiSt框架: 仿生四层与LM智能体<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]
        Root --> Results["关键结果/Results<br>数十秒内自愈，低CPU开销<br>Self-healing in tens of seconds, low CPU overhead"]
    ```

- **[arXiv260105] Robust Uncertainty Quantification for Factual Generation of Large Language Models**
  - **tags:** [nlp], [hallucination detection], [uncertainty quantification, factual hallucination, trap questions, ROCAUC, fake biographies]
  - **authors:** Yuhao Zhang, Zhongliang Yang, Linna Zhou
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2601.00348
  - **code:** https://github.com/EdwardChang5467/robust uncertainty
  - **contributions:** 1. Proposes a new uncertainty quantification scenario focused on multi-fact generation (e.g., fake person biographies) to test LLM robustness. 2. Constructs a novel dataset of trap questions containing fake names to evaluate hallucination detection methods. 3. Introduces a robust uncertainty quantification (RU) method that significantly outperforms baseline methods across four different LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6268aa8c8874ac861a06315946aedcfbc9df7712a9f2b63e23204554e9c8596b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of LLM hallucination by proposing a new robust uncertainty quantification (RU) method for detecting factual errors in multi-fact generation tasks. The method is evaluated using a specially constructed set of trap questions containing fake names. Results show the RU method achieves an average increase of 0.1-0.2 in ROCAUC over the best baseline, demonstrating its effectiveness in improving the reliability of LLM outputs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robust Uncertainty Quantification for Factual Generation of LLMs] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>LLM幻觉与不确定性量化缺陷<br>LLM Hallucination & UQ Deficiency]
        C[主要方法/Method<br>构建陷阱问题集与提出鲁棒方法<br>Construct Trap Questions & Propose RU Method]
        D[关键结果/Results<br>ROC-AUC显著提升<br>Significant ROC-AUC Improvement]
    ```

- **[arXiv260105] Mapping Human Anti-collusion Mechanisms to Multi-agent AI**
  - **tags:** [ai], [multi-agent systems], [collusion, anti-collusion mechanisms, multi-agent AI, AI safety, game theory]
  - **authors:** Jamiu Adekunle Idowu, Ahmed Almasoud, Ayman Alfahid
  - **institution:** University College London (UCL), Sahel AI, Prince Sultan University, Majmaah University
  - **link:** https://arxiv.org/pdf/2601.00360
  - **contributions:** 1. Developed a taxonomy of human anti-collusion mechanisms (e.g., sanctions, leniency, monitoring). 2. Mapped these human mechanisms to potential interventions for multi-agent AI systems. 3. Highlighted key open challenges in applying these mechanisms to AI, such as the attribution problem and adversarial adaptation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddbe564b87a89c6b0bfd47c15c01195109e5af5ff41994f193729c6ff841d80_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risk of collusion in autonomous multi-agent AI systems by proposing to adapt established human anti-collusion mechanisms. It develops a taxonomy of these mechanisms and maps them to potential AI interventions. The work concludes by identifying critical challenges for future research in this area, such as distinguishing beneficial cooperation from harmful collusion.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mapping Human Anti-collusion Mechanisms to Multi-agent AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI代理出现合谋/AI Agents Develop Collusion]
        C --> C1[建立人类反合谋机制分类法/Develop Taxonomy of Human Anti-collusion Mechanisms]
        C --> C2[映射到AI系统干预措施/Map to Multi-agent AI Interventions]
        D --> D1[提出实施方法/Propose Implementation Approaches]
        D --> D2[识别开放挑战/Highlight Open Challenges]
    ```

- **[arXiv260105] BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics**
  - **tags:** [nlp], [multilingual representation learning], [Joint Embedding Predictive Architecture (JEPA), BERT, CLS token, language-agnostic embedding, multilingual benchmarks]
  - **authors:** Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles
  - **institution:** Brown University
  - **link:** https://arxiv.org/pdf/2601.00366
  - **contributions:** 1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that BERT's [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic "thought space". The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"] --> Problem["核心问题/Problem: CLS embeddings are not language-invariant and fail to capture true sentence semantics."]
        Root --> Method["主要方法/Method: Add JEPA training objective to BERT to create a language-agnostic embedding space."]
        Root --> Results["关键结果/Results: Improved multilingual benchmark performance; reorganized, semantic-first CLS space."]
    ```

- **[arXiv260105] PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices**
  - **tags:** [mlsys], [on-device ai], [adversarial patches, outlier detection, isolation forest, dimensionality reduction, edge computing]
  - **authors:** Nandish Chattopadhyay, Abdul Basit, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique
  - **institution:** New York University (NYU) Abu Dhabi, DakAI
  - **link:** https://arxiv.org/pdf/2601.00367
  - **contributions:** 1. Proposes PatchBlock, a lightweight, model-agnostic pre-processing framework for detecting and mitigating adversarial patches on resource-constrained edge devices. 2. Introduces a redesigned isolation forest algorithm with targeted cuts for efficient anomaly detection in image chunks. 3. Demonstrates high robustness recovery (up to 77% accuracy) and superior efficiency (computation time, energy) compared to state-of-the-art defenses, with minimal impact on clean accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/931ca070f14402ab68d228644f7d6df1ed1c402686c4fae0127234e1c588df8d_w640_q70.webp
  - **Simple LLM Summary:** This paper presents PatchBlock, a lightweight defense framework that uses outlier detection and dimensionality reduction to identify and neutralize adversarial patches in images for EdgeAI systems. It operates efficiently on CPUs in parallel with GPU inference, making it suitable for resource-constrained devices. Evaluations show it significantly recovers model accuracy under strong patch attacks while maintaining high efficiency and portability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[PatchBlock: A Lightweight Defense Against Adversarial Patches] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[对抗性补丁威胁边缘AI/Patch attacks threaten EdgeAI]
        P1 --> P2[资源受限设备需要轻量级防御/Resource-constrained devices need lightweight defense]
        Method[主要方法/Method] --> M1[分块/Chunking]
        Method --> M2[基于改进隔离林的异常检测/Anomaly Detection via redesigned Isolation Forest]
        Method --> M3[降维缓解/Dimensionality Reduction Mitigation]
        Results[关键结果/Results] --> R1[恢复高达77%的准确率/Recovers up to 77% accuracy]
        Results --> R2[高效，CPU并行，低开销/Efficient, CPU-parallel, low overhead]
        Results --> R3[模型与补丁无关，可移植/Model- & patch-agnostic, portable]
    ```

- **[arXiv260105] In Line with Context: Repository-Level Code Generation via Context Inlining**
  - **tags:** [se], [code generation], [repository-level code generation, context inlining, call graph, perplexity-based confidence, bidirectional inlining]
  - **authors:** Chao Hu, Wenhao Zeng, Yuling Shi, Beijun Shen, Xiaodong Gu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2601.00376
  - **contributions:** 1. Introduces InlineCoder, a novel framework that reframes repository-level code generation as a function-level task by inlining the target function into its call graph., 2. Proposes a bidirectional inlining process combining an initial draft anchor, upstream inlining for usage scenarios, and downstream retrieval for dependency context., 3. Demonstrates substantial performance gains over state-of-the-art baselines on benchmarks like RepoExec, highlighting effectiveness in understanding repository contexts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/042382406537daf6ffcdbb0e0a2311956fbf996d26fe6c18ea9b84304141abfa_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of repository-level code generation, where models must understand complex dependencies across an entire codebase. It proposes InlineCoder, a framework that first generates a draft function (anchor) and then enriches the context by inlining it into its callers (upstream) and retrieving its callees (downstream). This approach significantly outperforms existing methods on standard benchmarks, showing improved understanding of repository context.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["In Line with Context: Repository-Level Code Generation via Context Inlining<br>论文标题"]
        Root --> Problem["现有方法不足<br>Problem: Existing methods (e.g., RAG) rely on surface-level similarity and struggle with repository dependencies."]
        Root --> Method["提出InlineCoder框架<br>Method: Generates an anchor draft, then performs bidirectional context inlining (Upstream Inlining & Downstream Retrieval)."]
        Root --> Results["性能显著提升<br>Results: Outperforms SOTA baselines with substantial gains on RepoExec and DevEval benchmarks."]
    ```

- **[arXiv260105] Word Frequency Counting Based on Serverless MapReduce**
  - **tags:** [sys], [serverless computing], [Serverless Computing, MapReduce, Word Frequency Counting, Function as a Service (FaaS), Cloud Computing]
  - **authors:** Hanzhe Li, Bingchen Lin, Mengyuan Xu
  - **institution:** Xi'an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00380
  - **contributions:** 1. Proposes a novel combination of the serverless computing paradigm (FaaS) with the MapReduce programming model for data processing tasks. 2. Investigates and determines the optimal number of Map and Reduce functions for a given workload within a serverless MapReduce framework. 3. Demonstrates through experiments that increasing the number of functions reduces execution time and improves overall efficiency for the word frequency counting task.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of optimizing big data processing efficiency by integrating the serverless computing model (FaaS) with the MapReduce framework. It proposes a serverless MapReduce approach for word frequency counting and experimentally finds the optimal number of Map and Reduce functions to minimize execution time. The results show that this method improves processing efficiency, offering a cost-effective solution for cloud-based data analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Word Frequency Counting Based on Serverless MapReduce] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[需求: 高性能与高效率计算 / Demand: High-performance & High-efficiency Computing]
        C --> C1[结合: 无服务器计算(FaaS)与MapReduce / Combine: Serverless Computing (FaaS) & MapReduce]
        C --> C2[优化: Map与Reduce函数数量 / Optimize: Number of Map & Reduce Functions]
        D --> D1[结果: 执行时间减少 / Result: Execution Time Reduces]
        D --> D2[结果: 程序效率提升 / Result: Program Efficiency Improves]
    ```

- **[arXiv260105] Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing**
  - **tags:** [sec], [cyber-physical systems security], [intrusion detection system, anomaly detection, G-code manipulation, transformer encoder, self-attention autoencoder]
  - **authors:** Md Mahbub Hasan, Marcus Sternhagen, Krishna Chandra Roy
  - **institution:** New Mexico Institute of Mining and Technology
  - **link:** https://arxiv.org/pdf/2601.00384
  - **contributions:** 1. Investigation of stealthy Man-in-the-Middle (MitM) attack vectors targeting the CAD-to-machine interface in Fused Deposition Modeling (FDM) 3D printers. 2. Proposal of an unsupervised Intrusion Detection System (IDS) that uses a frozen Transformer-based encoder and contrastive learning to create anomaly-sensitive embeddings from machine logs. 3. Demonstration of effective anomaly classification using a combination of clustering and a self-attention autoencoder on real 3D printing systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates stealthy cyberattacks that manipulate G-code in additive manufacturing systems, leading to structurally defective parts. To detect these attacks, the authors propose an unsupervised intrusion detection system that uses a Transformer-based encoder and contrastive learning to analyze machine logs. Their method successfully distinguishes between normal and compromised printing executions.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[AM系统的新攻击面 / New Attack Surfaces in AM]
    B --> B2[隐秘的中间人攻击 / Stealthy MitM Attacks]
    C --> C1[基于日志的无监督IDS / Unsupervised IDS from Logs]
    C --> C2[Transformer编码器 / Transformer Encoder]
    C --> C3[对比学习与自注意力 / Contrastive Learning & Self-Attention]
    D --> D1[有效区分正常与攻击 / Effectively Distinguishes Benign & Compromised]
    ```

- **[arXiv260105] Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning**
  - **tags:** [sec], [coordinated behavior detection], [Convergent Cross Mapping, semi-supervised learning, active learning, hierarchical clustering, causal inference]
  - **authors:** Weng Ding, Yi Han, Mu-Jiang-Shan Wang
  - **institution:** Georgia Institute of Technology, Meta, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2601.00400
  - **contributions:** 1. Proposes an adaptive Convergent Cross Mapping (CCM) technique for identifying genuine causal relationships between accounts. 2. Integrates active learning with uncertainty sampling in a semi-supervised classification scheme to reduce manual labeling burden. 3. Introduces an automated validation module driven by historical detection experience for self-verification and optimization of outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e2d7bb8b5573c45544689a49293433fa3d98d36abedd140f80989af5d80d355_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ACCD, a three-stage memory-guided framework for detecting coordinated inauthentic behavior on social media. It uses adaptive causal inference, semi-supervised active learning, and automated validation to improve accuracy and efficiency. The method achieves a higher F1-score and significantly reduces manual annotation requirements compared to existing baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Adaptive Causal Coordination Detection for Social Media<br/>社交媒体自适应因果协调检测] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖浅层相关分析、静态参数、大量人工标注<br/>Existing methods rely on superficial correlation, static parameters, heavy manual labeling]
        C --> C1[三阶段渐进式架构<br/>Three-stage progressive architecture]
        C1 --> C11[阶段1: 自适应CCM识别因果<br/>Stage 1: Adaptive CCM for causal relationships]
        C1 --> C12[阶段2: 半监督主动学习<br/>Stage 2: Semi-supervised active learning]
        C1 --> C13[阶段3: 自动化验证模块<br/>Stage 3: Automated validation module]
        D --> D1[F1分数87.3%, 提升15.2%<br/>F1-score 87.3%, 15.2% improvement]
        D --> D2[减少68%人工标注<br/>Reduces manual annotation by 68%]
        D --> D3[处理速度提升2.8倍<br/>2.8x speedup in processing]
    ```

- **[arXiv260105] Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset**
  - **tags:** [nlp], [named entity recognition], [weak supervision, large language models, low-resource languages, dataset construction, Luxembourgish]
  - **authors:** Alistair Plum, Laura Bernardy, Tharindu Ranasinghe
  - **institution:** University of Luxembourg, Lancaster University
  - **link:** https://arxiv.org/pdf/2601.00411
  - **contributions:** 1. Proposes a novel pipeline for constructing NER datasets that uses Wikipedia/Wikidata for weak supervision and LLMs for label verification. 2. Introduces judgeWEL, a new and significantly larger NER dataset for the under-represented language Luxembourgish. 3. Evaluates and compares the effectiveness of multiple LLMs in judging and filtering noisy, distantly-supervised labels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e13a29d091cdd4cb0cc5c3d34a98e86c8d45b0a34f42a2b552cecd9fcff5b51_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of building datasets for under-represented languages by proposing a novel method that uses Wikipedia and Wikidata for weak supervision to generate initial NER labels, and then employs multiple LLMs to verify and filter these labels for quality. The approach is applied to Luxembourgish, resulting in the judgeWEL dataset, which is five times larger and more balanced than existing resources, providing a valuable new corpus for low-resource NER research.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Do LLMs Judge Distantly Supervised Named Entity Labels Well?<br/>构建JudgeWEL数据集] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[低资源语言数据集构建困难<br/>Dataset Construction for Low-Resource Languages]
    C --> C1[利用维基百科/维基数据进行远程监督<br/>Weak Supervision via Wikipedia/Wikidata]
    C --> C2[使用多个LLM进行标签验证<br/>Label Verification with Multiple LLMs]
    D --> D1[创建了更大的卢森堡语NER数据集<br/>Larger Luxembourgish NER Dataset Created]
    D --> D2[数据集规模扩大五倍，覆盖更平衡<br/>5x Larger, More Balanced Coverage]
    ```

- **[arXiv260105] RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers**
  - **tags:** [ai], [efficient transformers], [astrocyte-inspired computing, long-term plasticity (LTP), short-term plasticity (STP), memory compression, Long Range Arena (LRA)]
  - **authors:** Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta
  - **institution:** Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2601.00426
  - **contributions:** 1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RMAAT: Astrocyte-Inspired Memory Compression and Replay] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Transformer自注意力二次复杂度/Quadratic Complexity of Self-Attention]
        Method[主要方法/Method: 星形胶质细胞启发的循环记忆架构/Astrocyte-Inspired Recurrent Memory Architecture]
        Results[关键结果/Results: 在LRA基准上具有竞争力的准确性和效率/Competitive Accuracy & Efficiency on LRA]
    ```

- **[arXiv260105] Deep Delta Learning**
  - **tags:** [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]
  - **authors:** Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu
  - **institution:** Princeton University, University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2601.00417
  - **code:** https://github.com/yifanzhang-pro/deep-delta-learning
  - **contributions:** 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Delta Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[残差网络限制/ResNet Limitation]
        B1 --> B2["刚性相加偏置/Rigid Additive Bias"]
        B2 --> B3["限制复杂状态转换/Limits Complex State Transitions"]
        C --> C1[Delta 算子/Delta Operator]
        C1 --> C2["秩-1扰动/ Rank-1 Perturbation"]
        C2 --> C3["可学习几何变换/Learnable Geometric Transform"]
        C3 --> C4["动态门控/Dynamic Gating (β)"]
        D --> D1["谱分析/Spectral Analysis"]
        D1 --> D2["插值身份/投影/反射/Interpolates Identity/Projection/Reflection"]
        D --> D3["同步秩-1注入/Synchronous Rank-1 Injection"]
        D3 --> D4["控制转换谱/Controls Transition Spectrum"]
        D4 --> D5["保持稳定训练/Preserves Stable Training"]
    ```

- **[arXiv260105] Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications**
  - **tags:** [ai], [sports analytics], [semantic-space reasoning, vector-distance metrics, tactical optimization]
  - **authors:** Alessio Di Rubbo, Mattia Neri, Remo Pareschi, Marco Pedroni, Roberto Valtancoli, Paolino Zica
  - **institution:** University of Molise, Stake Lab, Bioretics, Institute for Generative Strategy, Cesena Femminile Football Club, Zica Sport
  - **link:** https://arxiv.org/pdf/2601.00421
  - **contributions:** 1. Proposes a novel analogy between texts and teams, modeling tactical configurations as compositional semantic structures. 2. Introduces a methodology to represent players and teams as multidimensional vectors and evaluate tactical fit using vector-distance metrics. 3. Demonstrates a generalizable framework for collective decision-making applicable beyond football to domains like cooperative robotics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68f5544900dd6f44d32d07e9f1b09cbba4598bb4606d5d22767a42a8c2195ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel methodology that applies semantic-space reasoning from computational linguistics to model team sports tactics. It represents players and teams as vectors in a shared semantic space to compute tactical fit and generate strategy recommendations. The work concludes that this approach provides an interpretable and generalizable framework for optimizing collective performance in team-based domains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Can Semantic Methods Enhance Team Sports Tactics?<br/>语义方法能提升团队运动战术吗？] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>Extend semantic reasoning to tactical decision-making<br/>将语义推理扩展到战术决策] --> P1[类比/Analogy<br/>Texts vs. Teams<br/>文本 vs. 团队]
        Method[主要方法/Method<br/>Model tactics as semantic structures<br/>将战术建模为语义结构] --> M1[表示/Representation<br/>Player as vector, team as aggregation<br/>球员为向量，团队为聚合]
        Method --> M2[评估/Evaluation<br/>Vector-distance metrics for tactical fit<br/>用向量距离度量战术适配度]
        Results[关键结果/Results<br/>Generalizable framework for team optimization<br/>团队优化的可泛化框架] --> R1[应用/Application<br/>Football, basketball, robotics, human-AI<br/>足球、篮球、机器人、人机协同]
        Results --> R2[产出/Outcome<br/>Interpretable strategy recommendations<br/>可解释的策略建议]
    ```

- **[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models**
  - **tags:** [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]
  - **authors:** Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00423
  - **code:** https://github.com/shengjun-zhang/VisualGRPO
  - **contributions:** 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法在多个去噪步上优化，奖励信号稀疏模糊/Existing methods suffer from sparse & ambiguous rewards over multiple steps]
        C --> C1[提出E-GRPO: 熵感知分组相对策略优化/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]
        C1 --> C2[合并低熵步为高熵SDE采样步，其他步用ODE采样/Merge low-entropy steps for SDE, use ODE elsewhere]
        C1 --> C3[引入多步分组归一化优势计算/Introduce multi-step group normalized advantage]
        D --> D1[在不同奖励设置下验证了方法的有效性/Method effectiveness demonstrated across different reward settings]
    ```

- **[arXiv260105] Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games**
  - **tags:** [nlp], [language modeling], [Semantic Field Theory, lexical fields, linguistic fields, transformer architectures, embedding spaces]
  - **authors:** Dimitris Vartziotis
  - **institution:** TWT Science & Innovation, NIKI - Digital Engineering
  - **link:** https://arxiv.org/pdf/2601.00448
  - **contributions:** 1. Formalizes the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. 2. Analyzes how core properties of transformer architectures (e.g., distributed representations, attention) relate to Semantic Field Theory concepts. 3. Proposes that mathematical structure and language games are complementary perspectives, clarifying the scope and limits of statistical language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98759f21c127cf9a440b464892816dd7e22af8f161a1d16ec91b582a8fefa647_w640_q70.webp
  - **Simple LLM Summary:** This paper examines theories of linguistic meaning by contrasting social constructivist language games with a mathematically oriented Semantic Field Theory. It formalizes lexical and linguistic fields and analyzes their relation to transformer architecture properties. The authors conclude that the mathematical structure captured by LLMs and the social grounding of language games are complementary, not competing, views.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games") --> Problem("核心问题/Problem: Contrasting theories of linguistic meaning (social vs. mathematical)")
        Root --> Method("主要方法/Method: Formalizing Semantic Field Theory (Lexfelder/Lingofelder) and analyzing transformer properties")
        Root --> Results("关键结果/Results: Mathematical structure and language games are complementary perspectives")
    ```

- **[arXiv260105] Deep Networks Learn Deep Hierarchical Models**
  - **tags:** [ai], [theoretical machine learning], [hierarchical models, residual networks, layerwise SGD, efficient learnability, teacher-student framework]
  - **authors:** Amit Daniely
  - **institution:** Hebrew University of Jerusalem, Google Research Tel Aviv
  - **link:** https://arxiv.org/pdf/2601.00455
  - **contributions:** 1. Proves that layerwise SGD on residual networks can efficiently learn a class of hierarchical models with polynomial depth, surpassing previous learnable models limited to log-depth. 2. Introduces a formal model where the existence of human teachers, providing granular labels, naturally reveals a hierarchical structure that facilitates learning. 3. Suggests that the learnability of deep hierarchical models could form a theoretical basis for understanding why deep learning works.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp
  - **Simple LLM Summary:** The paper shows that layerwise stochastic gradient descent (SGD) on residual networks can efficiently learn a class of hierarchical models where labels are structured in increasingly complex levels. This class is more expressive, requiring polynomial depth, than previously known learnable models. The authors argue that this learnability, supported by a formal model of teaching, provides a potential theoretical foundation for understanding deep learning's success.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Deep Networks Learn Deep Hierarchical Models] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[理解深度网络为何有效/Understanding why deep networks work]
    C --> C1[层级SGD与残差网络/Layerwise SGD on ResNets]
    C --> C2[形式化教师-学生模型/Formal teacher-student model]
    D --> D1[可高效学习多项式深度模型/Efficiently learn polynomial-depth models]
    D --> D2[超越对数深度电路/Surpasses log-depth circuits]
    D --> D3[为理解深度学习提供基础/Provides basis for understanding deep learning]
    ```

- **[arXiv260105] Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations**
  - **tags:** [mlsys], [llm inference], [guardrail models, multi-turn compression, efficiency optimization, safety screening, token reduction]
  - **authors:** Hyunjun Kim
  - **institution:** KAIST
  - **link:** https://arxiv.org/pdf/2601.00454
  - **contributions:** 1. Proposes Defensive M2S, a training paradigm that fine-tunes guardrail models on compressed multi-turn conversations instead of full histories, 2. Provides formal complexity analysis showing training cost reduction from O(n²) to O(n) and empirical token reduction of 93×, 3. Demonstrates effectiveness across multiple guardrail models and compression templates, achieving high attack detection recall with 94.6% inference token reduction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad9caa971d784e8a994209f34a3b4e255812b43b2fa90a13bb0487b6e71e1395_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost of processing full multi-turn conversations for LLM safety guardrails by proposing Defensive M2S, which trains guardrail models on compressed single-turn versions. This method significantly reduces training and inference tokens while maintaining high attack detection performance, enabling scalable safety screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations"] --> Problem["核心问题/Problem: High computational cost of processing full multi-turn conversations for guardrail models"]
        Root --> Method["主要方法/Method: Fine-tune guardrail models on M2S (Multi-turn to Single-turn) compressed conversations"]
        Root --> Results["关键结果/Results: 93× training token reduction, 94.6% inference token reduction, 93.8% attack detection recall"]
    ```

- **[arXiv260105] Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations**
  - **tags:** [mlsys], [llm training], [Mixture-of-Experts, Orthogonality Regularization, Weight-Activation Gap, Sparse Activation, Expert Diversity]
  - **authors:** Hyunjun Kim
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2601.00457
  - **contributions:** 1. Showed that orthogonality regularization fails to reduce weight-space overlap and yields inconsistent effects on model performance across different datasets. 2. Identified a significant disconnect between weight-space and activation-space orthogonality, with no significant correlation between the two. 3. Demonstrated that weight-space regularization is an unreliable optimization target for improving expert diversity in MoE models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the effectiveness of applying orthogonality loss to enforce expert diversity in Mixture-of-Experts (MoE) models. The analysis reveals that this geometric regularization fails to reduce weight-space overlap, does not translate to activation-space orthogonality, and leads to inconsistent performance changes. The findings demonstrate that weight-space regularization is unsuitable for achieving MoE diversity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometric Regularization in MoEs: The Disconnect Between Weights and Activations] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(专家多样性的几何正则化作用不明确/The role of geometric regularization in expert specialization is unclear)
        C --> C1(应用正交性损失以强制专家多样性/Apply orthogonality loss to enforce expert diversity)
        D --> D1(权重空间重叠未减少/Weight-space overlap not reduced)
        D --> D2(激活空间重叠保持高位/Activation-space overlap remains high)
        D --> D3(性能影响不一致/Inconsistent effects on performance)
        D --> D4(权重与激活正交性无显著相关/No significant correlation between weight and activation orthogonality)
    ```

- **[arXiv260105] Neural Chains and Discrete Dynamical Systems**
  - **tags:** [ai], [scientific machine learning], [neural chains, physics-informed neural networks (PINNs), finite-difference methods, Burgers equation, Eikonal equation]
  - **authors:** Sauro Succi, Abhisek Ganguly, Santosh Ansumali
  - **institution:** Italian Institute of Technology, Jawaharlal Nehru Centre for Advanced Scientific Research (JNCASR), University of Roma Tre, Harvard University, Cornell University
  - **link:** https://arxiv.org/pdf/2601.00473
  - **contributions:** 1. Proposes and analyzes the analogy between transformer-based neural chains (without self-attention) and discrete dynamical systems from discretized neural integral/PDEs. 2. Conducts a comparative analysis between standard numerical discretization (finite-difference) and PINN learning for solving Burgers and Eikonal equations, showing they converge to similar dynamical knowledge. 3. Identifies that PINNs explore a vast space of random matrices, unlike the structured matrices of finite-difference methods, leading to more parameters, higher training costs, and reduced explainability for 1D problems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the connection between neural chains (transformers without self-attention) and discrete dynamical systems. It compares solving PDEs like Burgers and Eikonal equations using standard finite-difference methods versus Physics-Informed Neural Networks (PINNs), finding both methods yield similar solutions but PINNs use many more random, less interpretable parameters. The authors conclude that for these 1D problems, PINNs offer no efficiency advantage over traditional methods, though their potential for high-dimensional problems remains open.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Neural Chains and Discrete Dynamical Systems] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[分析神经链与离散动力系统的类比/Analyze analogy between neural chains and discrete dynamical systems]
        B --> B2[比较PINN与传统数值方法/Compare PINNs vs. traditional numerical methods]
        C --> C1[将数值离散化表述为神经链/Cast numerical discretization as neural chains]
        C --> C2[使用PINN求解方程/Use PINNs to solve equations]
        C --> C3[比较矩阵结构与参数空间/Compare matrix structure and parameter space]
        D --> D1[两种方法获得相同动力学知识/Both methods acquire same dynamical knowledge]
        D --> D2[PINN使用更多随机参数/PINNs use more random parameters]
        D --> D3[1D问题中PINN无效率优势/No PINN efficiency advantage for 1D problems]
    ```

- **[arXiv260105] Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation**
  - **tags:** [mlsys], [agent system], [agentic AI, distributed agents, human-AI co-creation, progressive ideation, meta-cognitive workflow]
  - **authors:** Sankar B, Srinidhi Ranjini Girish, Aadya Bharti, Dibakar Sen
  - **institution:** Indian Institute of Science (IISc)
  - **link:** https://arxiv.org/pdf/2601.00475
  - **contributions:** 1. Proposes MIDAS, a novel framework that replaces single-AI systems with a distributed team of specialized AI agents for ideation. 2. Emulates a human meta-cognitive workflow to progressively refine ideas and assess them for both global and local novelty. 3. Establishes a new paradigm for human-AI co-creation, elevating the human from a passive filter to an active collaborative partner.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dee9a17774f8f69cd3f3a19a0507461cd7a05a21b3f1dec4db23b9a0a1c9bfa_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of AI systems generating semantically clustered ideas that hinder novel ideation in engineering design. It proposes the MIDAS framework, which uses a distributed team of specialized AI agents to progressively refine and assess ideas for novelty. This approach enables true human-AI co-creation, making the human designer an active partner rather than a passive filter.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Progressive Ideation using an Agentic AI Framework<br>渐进式构思的智能体AI框架] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI生成想法语义聚类<br>AI-generated ideas are semantically clustered]
        B --> B2[新手设计师构思困难<br>Ideation is challenging for novice designers]
        C --> C1[提出MIDAS框架<br>Propose MIDAS framework]
        C1 --> C2[分布式专业AI智能体团队<br>Distributed team of specialized AI agents]
        C2 --> C3[模拟元认知工作流<br>Emulate meta-cognitive workflow]
        D --> D1[渐进式提炼与评估想法<br>Progressively refines and assesses ideas]
        D --> D2[实现人-AI协同创造<br>Enables true human-AI co-creation]
    ```

- **[arXiv260105] MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability**
  - **tags:** [mlsys], [agent system], [multi-agent systems, evaluation suite, observability, execution traces, reliability]
  - **authors:** Tie Ma, Yixi Chen, Vaastav Anand, Alessandro Cornacchia, Amândio R. Faustino, Guanheng Liu, Shan Zhang, Hongbin Luo, Suhaib A. Fahmy, Zafar A. Qazi, Marco Canini
  - **institution:** KAUST, Beihang University, MPI-SWS, LUMS
  - **link:** https://arxiv.org/pdf/2601.00481
  - **contributions:** 1. Proposes MAESTRO, a unified evaluation suite for standardizing the configuration, execution, and observability of LLM-based Multi-Agent Systems (MAS). 2. Provides a repository of examples and adapters to integrate diverse MAS frameworks and exports framework-agnostic execution traces with system-level signals (e.g., latency, cost). 3. Through controlled experiments with 12 MAS, demonstrates that MAS architecture is the dominant factor affecting resource profiles, reproducibility, and trade-offs, often outweighing changes in backend models or tools.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8621fc3bb6613fb26f1974a1a10ce1d6783df8fc57e3246b8e7ef34f2a5f4391_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MAESTRO, a benchmark suite designed to systematically evaluate the testing, reliability, and observability of LLM-based multi-agent systems. It standardizes execution and provides detailed traces, enabling controlled comparisons. The study finds that MAS architecture is the primary driver of performance variance and system behavior, more so than model or tool changes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MAESTRO: Multi-Agent Evaluation Suite] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[LLM-based MAS are stochastic and hard to debug]
        Problem --> P2[Lack of standardized system-level benchmarks]
        Method[主要方法/Method] --> M1[Unified interface for MAS configuration]
        Method --> M2[Repository & adapters for integration]
        Method --> M3[Exports framework-agnostic traces & signals]
        Results[关键结果/Results] --> R1[MAS executions are structurally stable but temporally variable]
        Results --> R2[MAS architecture is the dominant driver of performance]
    ```

- **[arXiv260105] Multi-Agent Coordinated Rename Refactoring**
  - **tags:** [se], [refactoring], [multi-agent system, coordinated renaming, scope inference, refactoring automation, IDE integration]
  - **authors:** Abhiram Bellur, Mohammed Raihan Ullah, Fraol Batole, Mohit Kansara, Masaharu Morimoto, Kai Ishikawa, Haifeng Chen, Yaroslav Zharov, Timofey Bryksin, Tien N. Nguyen, Hridesh Rajan, Danny Dig
  - **institution:** University of Colorado, Tulane University, University of Texas at Dallas, NEC Corporation, NEC Laboratories America, JetBrains Research
  - **link:** https://arxiv.org/pdf/2601.00482
  - **contributions:** 1. Designed and implemented the first multi-agent framework for automating coordinated rename refactoring. 2. Introduced a novel approach where an initial developer refactoring is used as a clue to infer a Declared Scope, which guides subsequent automated refactorings. 3. Demonstrated significant performance improvements (2.3x-3.1x F1-score) over state-of-the-art methods through rigorous evaluation on established and new benchmarks, including successful integration into active open-source projects.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e6c46d36197d90e221024d6225c9bcd5170e720a7f8f29bef40ad31d151bbda_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the tedious and error-prone task of coordinated renaming in software development by proposing a novel multi-agent framework. The framework uses a developer's initial rename as a clue to infer a scope, which then guides specialized agents to safely identify and execute related refactorings using IDE APIs. The evaluation shows the system, CoRenameAgent, significantly outperforms existing methods in accuracy and demonstrates practical utility by having its automatically generated changes accepted into real projects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Agent Coordinated Rename Refactoring] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动重命名易错且繁琐/Manual renaming is tedious & error-prone]
        B --> B2[现有方法假阳性高或不完整/Existing methods have high false positives or are incomplete]
        C --> C1[范围推断代理/Scope Inference Agent]
        C --> C2[计划执行代理/Planned Execution Agent]
        C --> C3[复制代理/Replication Agent]
        D --> D1[F1分数显著提升/Significant F1-score improvement]
        D --> D2[实际项目验证/Practical validation in open-source projects]
    ```

- **[arXiv260105] MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation**
  - **tags:** [cv], [physics-based simulation], [motion distillation, differentiable simulation, multimodal large language model, material parameter estimation, video diffusion models]
  - **authors:** Miaowei Wang, Jakub Zadrożny, Oisin Mac Aodha, Amir Vaxman
  - **institution:** The University of Edinburgh
  - **link:** https://arxiv.org/pdf/2601.00504
  - **code:** https://wangmiaowei.github.io/MotionPhysics.github.io/
  - **contributions:** 1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[物理参数调优耗时且需专业知识/Physical parameter tuning is time-consuming and requires expertise]
        C --> C1[使用多模态大语言模型估计参数/Use multimodal LLM to estimate parameters]
        C --> C2[提出可学习运动蒸馏损失/Propose learnable motion distillation loss]
        C2 --> C2a[从视频扩散模型提取运动先验/Extract motion priors from video diffusion models]
        D --> D1[在30+场景中评估/Evaluated across 30+ scenarios]
        D --> D2[超越现有方法/Surpasses state-of-the-art]
        D --> D3[自动确定合理参数/Automatically determines plausible parameters]
    ```

- **[arXiv260105] The Illusion of Insight in Reasoning Models**
  - **tags:** [ai], [reasoning models], [reasoning shifts, self-correction, model uncertainty, intrinsic vs extrinsic, chain-of-thought]
  - **authors:** Liv G. d'Aliberti, Manoel Horta Ribeiro
  - **institution:** Princeton University
  - **link:** https://arxiv.org/pdf/2601.00514
  - **contributions:** 1. Conducted a large-scale empirical study analyzing over 1 million reasoning traces across multiple models, domains, and training stages to investigate the nature and impact of mid-reasoning "Aha!" moments. 2. Found that such intrinsic reasoning shifts are rare, do not increase with training, and seldom improve accuracy, challenging the perception that they represent genuine model insight or self-correction. 3. Demonstrated that while intrinsic shifts are not beneficial, artificially triggering extrinsic shifts under conditions of high model uncertainty (high entropy) can reliably improve accuracy, showing these shifts are symptoms of unstable inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c00fcaa573a1b7a6558433ae1348cc7cefec26d36175047b45df1cd217f2516_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether reasoning models experience genuine "Aha!" moments of intrinsic self-correction during inference. Through a large-scale analysis of reasoning traces across multiple models and training checkpoints, the authors find that such mid-reasoning shifts are rare and ineffective, but that artificially triggering shifts when the model is uncertain can improve accuracy. The main conclusion is that these shifts are symptoms of unstable inference behavior, not a mechanism for intrinsic self-improvement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Illusion of Insight in Reasoning Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[推理模型是否有真正的“顿悟”时刻？/Do reasoning models have genuine "Aha!" moments?]
        C --> C1[大规模分析推理轨迹与训练检查点/Large-scale analysis of reasoning traces & training checkpoints]
        D --> D1[内在转变罕见且无效/Intrinsic shifts are rare and ineffective]
        D --> D2[外在触发在高熵下可提升准确率/Extrinsic triggering under high entropy improves accuracy]
        D --> D3[转变是不稳定推理的症状/Shifts are symptoms of unstable inference]
    ```

- **[arXiv260105] Probability-Aware Parking Selection**
  - **tags:** [other], [urban computing, intelligent transportation systems], [dynamic programming, stochastic modeling, parking availability, travel time estimation, navigation]
  - **authors:** Cameron Hickert, Sirui Li, Zhengbing He, Cathy Wu
  - **institution:** Massachusetts Institute of Technology (MIT)
  - **link:** https://arxiv.org/pdf/2601.00521
  - **contributions:** 1. Introduces the probability-aware parking selection problem, shifting navigation from destination to optimal parking location. 2. Proposes an adaptable dynamic programming framework for decision-making under probabilistic parking availability. 3. Provides analytical and empirical error assessments for using stochastic observations to estimate parking availability, showing viability with real-world data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e593d7f649bbe1a9ad2d3e102fdbd08c74926a786541d28174a517648ac1dcd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underestimation of total travel time in navigation systems by introducing a probability-aware parking selection problem. It proposes a dynamic programming framework to direct drivers to the best parking location based on probabilistic availability, and demonstrates through simulations with Seattle data that this approach can yield significant time savings compared to probability-unaware baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Probability-Aware Parking Selection] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[导航系统低估总行程时间/Navigation systems underestimate total travel time]
        B --> B2[忽略停车搜索时间/Ignore parking search time]
        C --> C1[概率感知停车选择问题/Probability-aware parking selection problem]
        C --> C2[动态规划决策框架/Dynamic programming decision framework]
        C --> C3[基于概率可用性决策/Decision based on probabilistic availability]
        D --> D1[实验验证: 西雅图数据/Experiments: Seattle data]
        D --> D2[MAE从7%降至2%/MAE decreased from 7% to below 2%]
        D --> D3[节省时间高达66%/Time savings up to 66%]
    ```

- **[arXiv260105] Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI**
  - **tags:** [mlsys], [agent system], [Siamese Recurrent Autoencoder, hybrid loss, real-time anomaly detection]
  - **authors:** Laksh Advani
  - **institution:** Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)
  - **link:** https://arxiv.org/pdf/2601.00516
  - **contributions:** 1. Demonstrated the ineffectiveness of standard anomaly detection methods for agent trajectory validation, establishing the need for specialized models. 2. Proposed a novel, sequence-aware Siamese Recurrent Autoencoder with a hybrid loss function for real-time trajectory anomaly detection. 3. Demonstrated that the approach is over 17x faster than LLM Judge baselines, making it suitable for real-time deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of detecting anomalous action plans in autonomous LLM agents, where existing methods fail to capture sequential structure and context. It proposes Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss combining contrastive learning and reconstruction for unified anomaly detection. The method achieves high F1-scores (0.88-0.94) and significantly faster inference (32 ms) than LLM-based baselines, enabling real-time safety verification.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Trajectory Guard] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[现有方法不适用/Existing methods ill-suited]
        Problem --> P2[需要序列感知/Need for sequence-awareness]
        Method --> M1[孪生循环自编码器/Siamese Recurrent Autoencoder]
        Method --> M2[混合损失函数/Hybrid Loss Function]
        M2 --> M2a[对比学习/Contrastive Learning]
        M2 --> M2b[重建/Reconstruction]
        Results --> R1[高F1分数/High F1-scores (0.88-0.94)]
        Results --> R2[低延迟/32 ms latency]
        Results --> R3[实时部署/Real-time deployment]
    ```

- **[arXiv260105] Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [LSTM compression, model efficiency, retail forecasting, edge computing, hidden units]
  - **authors:** Ravi Teja Pagidoju
  - **institution:** Campbellsville University
  - **link:** https://arxiv.org/pdf/2601.00525
  - **code:** https://github.com/RaviTeja444/sales-forecast-LSTM
  - **contributions:** 1. Systematic evaluation of LSTM network sizes from 16 to 128 hidden units on real retail data. 2. Discovery that moderate compression (to 64 units) actually improves forecast accuracy. 3. Practical guidelines for model selection based on the accuracy-efficiency trade-off.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp
  - **Simple LLM Summary:** This paper studies LSTM model compression for resource-constrained retail sales forecasting by reducing the number of hidden units. The method involves systematically pruning the LSTM from 128 to 16 hidden units. The main conclusion is that reducing the model to 64 units not only makes it 73% smaller but also improves accuracy by 47%, showing larger models are not always better.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Optimizing LSTM for Resource-Constrained Retail Forecasting] --> B(核心问题/Problem: LSTM模型计算需求大，中小型零售商难以部署/LSTM models are computationally expensive for mid-to-small retailers)
        A --> C(主要方法/Method: 通过逐步减少隐藏单元进行模型压缩/Model compression by reducing hidden units from 128 to 16)
        A --> D(关键结果/Results: 64单元模型更小(76KB vs 280KB)且更准确(MAPE 12.4% vs 23.6%)/64-unit model is smaller and more accurate)
    ```

- **[arXiv260105] ECR: Manifold-Guided Semantic Cues for Compact Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [embedding consistency regulation, manifold structure, semantic anchors, compact language models, on-device AI]
  - **authors:** Chung-Wei Victor Yuan
  - **institution:** YVIC Research Lab
  - **link:** https://arxiv.org/pdf/2601.00543
  - **contributions:** 1. Proposes Embedding Consistency Regulation (ECR), a new framework that uses semantic anchors derived from teacher embeddings to preserve the underlying manifold structure in compact models. 2. Demonstrates that ECR stabilizes training and preserves semantic structure across tasks and languages without relying on matching logits or internal features, and adds minimal inference overhead. 3. Shows ECR is compatible with but independent of distillation, enabling better task alignment and deployment under strict efficiency or privacy constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3184228efac0008fa39e8578d4daa8e597c9d20bf57f0662c9080c6c11607590_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of semantic drift and loss of manifold structure in compact language models. It proposes the Embedding Consistency Regulation (ECR) framework, which uses offline-computed semantic anchors to guide the compact model's geometry. Experiments show ECR produces more compact, task-aligned representations, making low-capacity models more stable and easier to deploy.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[ECR: Manifold-Guided Semantic Cues for Compact Language Models] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[嵌入空间结构崩塌/Embedding Space Collapse]
        Problem --> P2[语义漂移/Semantic Drift]
        Method --> M1[提取语义锚点/Derive Semantic Anchors]
        Method --> M2[保持几何一致性/Maintain Geometric Consistency]
        Results --> R1[稳定训练/Stabilized Training]
        Results --> R2[保留语义结构/Preserved Semantic Structure]
        Results --> R3[紧凑任务对齐空间/Compact Task-Aligned Space]
    ```

- **[arXiv260105] CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge**
  - **tags:** [mlsys], [federated learning], [gradient compression, orthogonal superposition, low-rank optimization, O-RAN, communication efficiency]
  - **authors:** Zhiheng Guo, Zhaoyang Liu, Zihan Cen, Chenyuan Feng, Xinghua Sun, Xiang Chen, Tony Q. S. Quek, Xijun Wang
  - **institution:** Sun Yat-sen University, University of Exeter, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2601.00549
  - **contributions:** 1. Proposes a unified framework (CoCo-Fed) that jointly addresses local memory constraints and global communication bottlenecks in federated learning at the wireless edge. 2. Introduces a local double-dimension gradient down-projection technique to enable optimizer operation on low-rank structures, reducing memory footprint without adding inference overhead. 3. Designs a global transmission protocol using orthogonal subspace superposition to consolidate layer-wise updates into a single matrix per node, drastically cutting backhaul traffic.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec79bdf4a8b9f5a548f3d50e3ca33a9f5a73510543acb755bae04b50c1ce378_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes CoCo-Fed, a federated learning framework that compresses gradients locally and combines updates globally to reduce memory and communication overhead in O-RAN edge networks. It achieves this through low-rank gradient projection and orthogonal superposition for transmission. Experiments on angle-of-arrival estimation show it outperforms baselines in efficiency while maintaining convergence under non-IID data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoCo-Fed: 无线边缘内存与通信高效的联邦学习统一框架<br>CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge]
        A --> B[核心问题/Problem: 大规模神经网络在O-RAN中部署面临内存墙和回程链路带宽瓶颈<br>Core Problem: Memory wall and backhaul bandwidth bottleneck for large-scale NN deployment in O-RAN]
        A --> C[主要方法/Method: 本地梯度双维度下投影与基于正交子空间叠加的全局传输协议<br>Core Method: Local double-dimension gradient down-projection & global orthogonal subspace superposition transmission protocol]
        A --> D[关键结果/Results: 在非独立同分布数据下显著提升内存和通信效率，保持鲁棒收敛<br>Key Results: Significantly improves memory/communication efficiency and maintains robust convergence under non-IID data]
    ```

- **[arXiv260105] A Comprehensive Dataset for Human vs. AI Generated Image Detection**
  - **tags:** [cv], [image forensics], [AI-Generated Images, Detection Techniques, Synthetic Media, Generative AI, Multimodal AI]
  - **authors:** Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das
  - **institution:** Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San José State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI
  - **link:** https://arxiv.org/pdf/2601.00553
  - **contributions:** 1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Comprehensive Dataset for Human vs. AI Generated Image Detection"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["AI生成图像难以区分/AI-Generated Images Hard to Distinguish"]
        Problem --> P2["误导性内容传播/Spread of Misleading Content"]
        Method --> M1["构建MS COCOAI数据集/Build MS COCOAI Dataset"]
        Method --> M2["使用五种生成器/Use Five Generators"]
        Results --> R1["提供96000个数据点/Provide 96k Datapoints"]
        Results --> R2["定义两项检测任务/Define Two Detection Tasks"]
    ```

- **[arXiv260105] Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?**
  - **tags:** [sec], [IoT Security], [Interaction Threats, Static Analysis, Large Language Models, Trigger-Action-Condition Rules, Symbolic Reasoning]
  - **authors:** Jason Quantrill, Noura Khajehnouri, Zihan Guo, Manar H. Alalfi
  - **institution:** Toronto Metropolitan University
  - **link:** https://arxiv.org/pdf/2601.00559
  - **code:** https://github.com/JasonQuantrill/llm-v-static-results
  - **contributions:** 1. Conducted the first comprehensive evaluation of LLMs for detecting multi-category interaction threats in IoT TAC rules. 2. Introduced a structurally challenging Mutation dataset to test model robustness under rule transformations. 3. Demonstrated that symbolic reasoning baselines outperform LLMs in structural reasoning tasks, highlighting the need for hybrid architectures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88e5908c842c3a16fa536e15c334b07eda4e7076715625cdb48ee93058227761_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the ability of Large Language Models (LLMs) to detect security threats in IoT automation rules, comparing them against traditional static analysis. The results show that while LLMs have good semantic understanding, they struggle with structural reasoning and are outperformed by symbolic methods, indicating they are not yet reliable for this safety-critical task alone.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[IoT规则交互威胁/IoT Rule Interaction Threats]
        C --> C1[评估LLMs与静态分析/Evaluating LLMs vs. Static Analysis]
        C --> C2[使用原始与变异数据集/Using Original & Mutation Datasets]
        D --> D1[LLMs语义理解好但结构推理差/LLMs Good at Semantics, Poor at Structural Reasoning]
        D --> D2[符号方法稳定可靠/Symbolic Method Stable & Reliable]
    ```

- **[arXiv260105] Improving Scientific Document Retrieval with Academic Concept Index**
  - **tags:** [nlp], [information retrieval], [academic concept index, concept coverage-based generation (CCQGen), concept-focused auxiliary contexts (CCExpand)]
  - **authors:** Jeyun Lee, Junhyoung Lee, Wonbin Kweon, Bowen Jin, Yu Zhang, Susik Yoon, Dongha Lee, Hwanjo Yu, Jiawei Han, Seongku Kang
  - **institution:** Korea University, University of Illinois Urbana-Champaign, Texas A&M University, Yonsei University, Pohang University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00567
  - **contributions:** 1. Introduces an academic concept index that extracts and organizes key concepts from scientific papers using an academic taxonomy. 2. Proposes CCQGen, a concept coverage-based query generation method that adaptively conditions LLMs on uncovered concepts to produce complementary queries with broader coverage. 3. Develops CCExpand, a context augmentation technique that leverages document snippets as concise responses to concept-aware queries for improved relevance matching.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfdf054b52cb87f6cd4e63982623a18245a08006e97e04ad3614fc5ac270f85a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting general-domain retrievers to scientific domains by introducing an academic concept index. The proposed method improves synthetic query generation (CCQGen) and context augmentation (CCExpand) using this structured index, leading to higher-quality queries and better retrieval performance. Experiments demonstrate improved conceptual alignment and retrieval effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improving Scientific Document Retrieval with Academic Concept Index] --> B[核心问题/Problem: Adapting general-domain retrievers to scientific domains is challenging due to vocabulary mismatch and lack of domain-specific annotations.]
        A --> C[主要方法/Method: Introduces academic concept index to enhance synthetic query generation (CCQGen) and context augmentation (CCExpand).]
        A --> D[关键结果/Results: Leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.]
    ```

- **[arXiv260105] Learning to be Reproducible: Custom Loss Design for Robust Neural Networks**
  - **tags:** [ai], [training stability & reproducibility], [custom loss function, training robustness, reproducibility, stochastic factors, weight initialization]
  - **authors:** Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen
  - **institution:** Friedrich Schiller University Jena, University of Technology Chemnitz, Norwegian University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00578
  - **contributions:** 1. Identifies and empirically analyzes the critical gap in ensuring consistent performance across training runs due to stochastic factors like weight initialization and data shuffling. 2. Proposes a novel Custom Loss Function (CLF) designed to explicitly balance predictive accuracy with training stability, reducing sensitivity to these stochastic factors. 3. Demonstrates through extensive experiments on diverse architectures and tasks (image classification, time series forecasting) that CLF significantly improves training robustness without sacrificing predictive performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of inconsistent model performance across training runs due to stochastic factors. It proposes a Custom Loss Function (CLF) to explicitly balance accuracy and stability, which is shown to improve training robustness without harming predictive performance in experiments on image classification and time series forecasting.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Learning to be Reproducible: Custom Loss Design for Robust Neural Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("模型性能不一致/Inconsistent Model Performance")
        Problem --> P2("对随机因素敏感/Sensitive to Stochastic Factors")
        Method --> M1("提出自定义损失函数/Propose Custom Loss Function (CLF)")
        Results --> R1("提高训练鲁棒性/Improves Training Robustness")
        Results --> R2("保持预测性能/Maintains Predictive Performance")
    ```

- **[arXiv260105] Priority-Aware Multi-Robot Coverage Path Planning**
  - **tags:** [ai], [multi-robot path planning], [coverage path planning, priority-weighted latency, lexicographic optimization, spanning-tree, Steiner-tree]
  - **authors:** Kanghoon Lee, Hyeonjun Kim, Jiachen Li, Jinkyoo Park
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), Korea Military Academy (KMA), University of California, Riverside (UCR)
  - **link:** https://arxiv.org/pdf/2601.00580
  - **contributions:** 1. Formally defines the Priority-Aware Multi-Robot Coverage Path Planning (PA-MCPP) problem, introducing priority weights and a lexicographic objective to minimize priority-weighted latency and makespan. 2. Proposes a scalable two-phase framework combining greedy zone assignment with local search and Steiner-tree-guided residual coverage. 3. Demonstrates through experiments that the method significantly reduces priority-weighted latency compared to baselines while maintaining competitive makespan and scales well with the number of robots.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4091ae11f186a26844a6a1c27c13cf633fa92da4e6636d53275a7d839f775d9f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of standard multi-robot coverage path planning, which treats all areas equally, by introducing a priority-aware version (PA-MCPP) where certain zones have higher urgency. The authors propose a two-phase method that first assigns and covers priority zones efficiently and then handles the remaining area. Experiments show their approach successfully reduces coverage delay for high-priority zones without significantly compromising the overall completion time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Priority-Aware Multi-Robot Coverage Path Planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[标准MCPP忽视区域优先级/Standard MCPP ignores zone priority]
        C --> C1[两阶段框架: 贪心分配与斯坦纳树引导覆盖/Two-phase framework: greedy assignment & Steiner-tree-guided coverage]
        D --> D1[显著降低优先级加权延迟/Significantly reduces priority-weighted latency]
        D --> D2[保持有竞争力的完工时间/Maintains competitive makespan]
    ```

- **[arXiv260105] HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts**
  - **tags:** [mlsys], [federated learning], [Mixture-of-Experts, federated fine-tuning, resource-aware, expert selection, sparsity-aware aggregation]
  - **authors:** Zihan Fang, Zheng Lin, Senkang Hu, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang
  - **institution:** City University of Hong Kong, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2601.00583
  - **contributions:** 1. Introduces a method to identify expert importance based on contributions to fine-tuning performance, enabling informed expert selection. 2. Proposes an adaptive expert subset selection mechanism from an information bottleneck perspective to align with heterogeneous client computing budgets. 3. Designs a sparsity-aware model aggregation strategy that weights updates from actively fine-tuned experts and gating parameters to mitigate destructive interference during global aggregation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes HFedMoE, a heterogeneous federated learning framework for fine-tuning large language models using Mixture-of-Experts. It addresses challenges in expert selection, resource heterogeneity, and aggregation interference by customizing expert subsets per client and using importance-weighted aggregation. Experiments show HFedMoE outperforms state-of-the-art methods in accuracy and convergence speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM联邦微调中MoE面临的挑战/Challenges in MoE for FL fine-tuning]
        B1 --> B2[专家选择困难/Difficulty in expert selection]
        B1 --> B3[客户端资源异构性/Client resource heterogeneity]
        B1 --> B4[聚合干扰/Aggregation interference]
        C --> C1[定制化专家子集/Customized expert subset per client]
        C1 --> C2[基于重要性的专家选择/Importance-based expert selection]
        C1 --> C3[信息瓶颈视角的适配/Adaptation via information bottleneck]
        C --> C4[稀疏感知的模型聚合/Sparsity-aware model aggregation]
        D --> D1[更高的训练精度/Higher training accuracy]
        D --> D2[更快的收敛速度/Faster convergence speed]
    ```

- **[arXiv260105] Noise-Robust Tiny Object Localization with Flows**
  - **tags:** [cv], [object detection], [Tiny Object Detection, Noise Robustness, Normalizing Flows, Uncertainty-Guided Optimization, Flow-Based Error Modeling]
  - **authors:** Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao
  - **institution:** Beihang University, Communication University of China, National University of Singapore
  - **link:** https://arxiv.org/pdf/2601.00617
  - **contributions:** 1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Noise-Robust Tiny Object Localization with Flows] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[微小物体检测对标注噪声敏感/Tiny object detection is sensitive to annotation noise]
        C --> C1[基于归一化流的误差建模/Flow-based error modeling]
        C --> C2[不确定性引导的优化/Uncertainty-guided optimization]
        D --> D1[在多个数据集上验证有效/Validated on multiple datasets]
        D --> D2[提升DINO基线1.2% AP/Boosts DINO baseline by 1.2% AP]
    ```

- **[arXiv260105] Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization**
  - **tags:** [ai], [submodular optimization], [weakly DR-submodular, continuous-greedy, Frank-Wolfe, approximation algorithm, down-closed convex body]
  - **authors:** Hareshkumar Jadav, Ranveer Singh, Vaneet Aggarwal
  - **institution:** IIT Indore, Purdue University
  - **link:** https://arxiv.org/pdf/2601.00611
  - **contributions:** 1. A novel approximation algorithm for maximizing non-monotone γ-weakly DR-submodular functions over down-closed convex bodies. 2. A smooth approximation guarantee that recovers the 0.401 factor for DR-submodular (γ=1) and degrades gracefully for γ&lt;1, improving upon prior bounds. 3. A hybrid algorithmic framework combining Frank-Wolfe-guided continuous-greedy with a γ-aware double-greedy step to handle non-monotonicity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of maximizing non-monotone, nonnegative γ-weakly DR-submodular functions over down-closed convex bodies. The authors propose a new algorithm that integrates a Frank-Wolfe-guided continuous-greedy approach with a γ-aware double-greedy step. This method achieves state-of-the-art approximation guarantees that depend smoothly on the parameter γ, improving upon previous results for this class of functions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization<br>非单调γ-弱DR-子模最大化的更强近似保证] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Maximize non-monotone γ-weakly DR-submodular function<br>最大化非单调γ-弱DR-子模函数]
        B --> B2[Over down-closed convex body<br>在下封闭凸体上]
        C --> C1[Frank-Wolfe-guided continuous-greedy<br>Frank-Wolfe引导的连续贪心]
        C --> C2[γ-aware double-greedy step<br>γ感知的双贪心步骤]
        D --> D1[Smooth approximation guarantee based on γ<br>基于γ的平滑近似保证]
        D --> D2[Recovers 0.401 for γ=1 (DR-submodular)<br>γ=1时恢复0.401因子]
        D --> D3[Improves prior bounds for γ<1<br>改进了γ<1时的现有界限]
    ```

- **[arXiv260105] DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations**
  - **tags:** [ai], [multimodal learning], [Direct Preference Optimization, hallucination mitigation, difficulty-aware learning, multimodal large language models, preference data imbalance]
  - **authors:** Longtian Qiu, Shan Ning, Chuyu Zhang, Jiaxuan Sun, Xuming He
  - **institution:** ShanghaiTech University, Lingang Laboratory, Shanghai Engineering Research Center of Intelligent Vision and Imaging
  - **link:** https://arxiv.org/pdf/2601.00623
  - **code:** https://artanic30.github.io/project_pages/DA-DPO
  - **contributions:** 1. Identifies and analyzes the problem of difficulty imbalance in multimodal DPO training, where models overfit to easy preference pairs. 2. Proposes a novel Difficulty-Aware DPO (DA-DPO) framework with a training-free difficulty estimation module using pre-trained VLMs and a distribution-aware voting strategy. 3. Introduces a difficulty-aware training mechanism that reweights preference pairs to prioritize harder examples, improving hallucination suppression and generalization without extra data or fine-tuning stages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66cedf9b64154fde20f2a21a0c0dbc301932351057de033cf8e0323686aec2bc_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of Multimodal Large Language Models (MLLMs) overfitting to easy samples during Direct Preference Optimization (DPO), which limits hallucination reduction. It proposes DA-DPO, a cost-efficient framework that estimates sample difficulty without training and reweights the loss to focus on harder examples. Experiments show DA-DPO improves robustness to hallucinations and generalization across benchmarks while remaining computationally efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DA-DPO: Cost-efficient Difficulty-aware Preference Optimization] --> B
        A --> C
        A --> D
        B[核心问题/Problem: MLLM幻觉 & DPO过拟合/Hallucinations & DPO Overfitting]
        C[主要方法/Method: 难度感知DPO/Difficulty-Aware DPO]
        D[关键结果/Results: 更强鲁棒性 & 更好泛化性/Stronger Robustness & Better Generalization]
        C --> C1[难度估计/Difficulty Estimation]
        C --> C2[难度感知训练/Difficulty-Aware Training]
        C1 --> C1a[预训练VLM/Pre-trained VLMs]
        C1 --> C1b[分布感知投票/Distribution-Aware Voting]
        C2 --> C2a[重加权偏好对/Reweight Preference Pairs]
    ```

- **[arXiv260105] Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability**
  - **tags:** [ai], [interpretable machine learning], [Bi-objective Optimization, Temporal Integrated Gradients, Optimal Path Oracle, Directed Acyclic Graph, Structured Regularization]
  - **authors:** Kasra Fouladi, Hamta Rahmani
  - **institution:** Not explicitly stated; inferred from email domains as independent researchers.
  - **link:** https://arxiv.org/pdf/2601.00655
  - **contributions:** 1. Proposes the IGBO framework that trains interpretable models by formalizing the task as a bi-objective optimization problem, jointly optimizing for accuracy and adherence to domain knowledge constraints. 2. Introduces an Optimal Path Oracle to generate data-manifold-aware integration paths, addressing the Out-of-Distribution problem in Temporal Integrated Gradients computation. 3. Provides theoretical analysis proving convergence properties and robustness to mini-batch noise, and demonstrates empirical effectiveness on time-series data with minimal accuracy loss.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains models to be both accurate and interpretable by jointly optimizing a task loss and an interpretability loss derived from domain knowledge encoded as a DAG. It addresses a key challenge in gradient-based attribution (the OOD problem) by learning an Optimal Path Oracle. Empirical results show IGBO effectively enforces interpretability constraints with minimal impact on accuracy, outperforming standard regularization methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretability-Guided Bi-objective Optimization] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[黑盒模型缺乏可解释性/Black-box models lack interpretability]
        B --> B2[事后解释方法无法保证约束/Post-hoc methods don't guarantee constraints]
        B --> B3[梯度归因存在OOD问题/Gradient attribution has OOD problem]
        C --> C1[双目标优化框架/Bi-objective Optimization Framework]
        C --> C2[使用DAG编码领域知识/Encode knowledge via DAG]
        C --> C3[最优路径预言机/Optimal Path Oracle]
        D --> D1[理论收敛性证明/Theoretical convergence proof]
        D --> D2[实证效果优于基线/Empirically outperforms baselines]
        D --> D3[最小精度损失/Minimal accuracy loss]
    ```

- **[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation**
  - **tags:** [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]
  - **authors:** Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang
  - **institution:** KAIST, NTU Singapore, DeepAuto.ai
  - **link:** https://arxiv.org/pdf/2601.00664
  - **code:** https://taekyungki.github.io/AvatarForcing
  - **contributions:** 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[缺乏真正互动/Lacks truly interactive communication]
        Problem --> P2[单向反应缺乏情感/One-way responses lack emotional engagement]
        Method[主要方法/Method] --> M1[扩散驱动框架/Diffusion forcing framework]
        Method --> M2[无标签直接偏好优化/Label-free direct preference optimization]
        Results[关键结果/Results] --> R1[低延迟实时交互/Low-latency real-time interaction (~500ms)]
        Results --> R2[6.8倍加速/6.8x speedup]
        Results --> R3[80%用户偏好/Over 80% user preference]
    ```

- **[arXiv260105] Fast-weight Product Key Memory**
  - **tags:** [nlp], [long-context language modeling], [product key memory, fast weights, episodic memory, gradient descent, long-context]
  - **authors:** Tianyu Zhao, Llion Jones
  - **institution:** Sakana AI
  - **link:** https://arxiv.org/pdf/2601.00671
  - **contributions:** 1. Proposes Fast-weight Product Key Memory (FwPKM), a novel architecture that transforms static Product Key Memory into a dynamic, fast-weight episodic memory., 2. Introduces a mechanism for dynamic parameter updates at both training and inference time via local chunk-level gradient descent, enabling rapid memorization and retrieval., 3. Demonstrates that FwPKM effectively complements semantic memory, significantly reduces perplexity on long-context data, and shows strong generalization to contexts much longer than those seen during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc5e7437ccdc396cc0c89f8bda772596a4ad71d82cd906f8eae81c22b107608_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the trade-off between storage capacity and computational efficiency in sequence modeling layers. It proposes Fast-weight Product Key Memory (FwPKM), a dynamic architecture that updates parameters via local gradient descent to act as an episodic memory. Experiments show FwPKM reduces perplexity on long-context datasets and generalizes well to sequences much longer than those in training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fast-weight Product Key Memory] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[存储容量与计算效率的权衡/Trade-off: Storage Capacity vs. Computational Efficiency]
        C --> C1[动态快速权重产品键记忆/Dynamic Fast-weight Product Key Memory (FwPKM)]
        C --> C2[本地块级梯度下降更新/Local Chunk-level Gradient Descent Updates]
        D --> D1[长上下文困惑度降低/Long-context Perplexity Reduction]
        D --> D2[从4K到128K的泛化/Generalization from 4K to 128K Tokens]
    ```

- **[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]
  - **authors:** Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang
  - **institution:** HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00677
  - **contributions:** 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[IRPO: Scaling the Bradley-Terry Model via RL] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Pairwise GRMs create O(n²) bottleneck in RL/成对GRM在RL中造成O(n²)瓶颈]
        C --> C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: 将Bradley-Terry融入GRPO实现逐点评分]
        D --> D1[SOTA among pointwise GRMs/在逐点GRM中达到SOTA]
        D --> D2[Outperforms pairwise GRMs in post-training/在训练后评估中优于成对GRM]
    ```

- **[arXiv260105] QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantization, spike-driven language models (SLMs), memory footprint, tiered search, embedded systems]
  - **authors:** Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique
  - **institution:** New York University (NYU) Abu Dhabi
  - **link:** https://arxiv.org/pdf/2601.00679
  - **contributions:** 1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: SLMs内存占用大，难以部署在资源受限的嵌入式设备/SLMs have large memory footprints, challenging for resource-constrained embedded deployment]
        C[主要方法/Method: 自动化分层量化策略，结合网络层次、层敏感性和多目标权衡函数/Automated tiered quantization strategy using network hierarchy, layer sensitivity, and multi-objective trade-off]
        D[关键结果/Results: 内存占用减少高达86.5%，功耗降低高达20%，性能接近原始模型/Memory footprint reduced by up to 86.5%, power by up to 20%, performance close to original model]
    ```

- **[arXiv260105] A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference**
  - **tags:** [mlsys], [multi-modal training], [Large Language Models, Pedestrian Crossing Behavior, Vision-Augmented Reasoning, Domain Knowledge Adaptation, Low-Rank Adaptation (LoRA)]
  - **authors:** Qingwen Pu, Kun Xie, Hong Yang, Guocong Zhai
  - **institution:** Old Dominion University, Southwest Jiaotong University
  - **link:** https://arxiv.org/pdf/2601.00694
  - **contributions:** 1. Introduces PedX-LLM, a vision-and-knowledge enhanced LLM framework for generalizable pedestrian crossing behavior inference, shifting from site-specific pattern recognition to semantic reasoning. 2. Proposes integrating LLaVA-extracted visual features with textual data and domain knowledge to fine-tune a LLaMA-2-7B model via LoRA. 3. Demonstrates strong cross-site generalizability, where the model significantly outperforms baseline methods in zero-shot and few-shot settings on unseen environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e948e9bb60b0f41e32522739a2caeb9dea0028b52570cf642f19e2d94faefef7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes PedX-LLM, a framework that enhances a large language model (LLaMA-2-7B) with visual features and domain knowledge to infer pedestrian crossing decisions. It achieves higher accuracy than traditional methods and demonstrates strong generalization to unseen sites, showing that vision-and-knowledge-enhanced reasoning overcomes the limitations of purely data-driven approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference<br>用于可泛化行人过街行为推理的视觉与知识增强大语言模型] --> B(Problem: 现有方法泛化性差<br>Problem: Poor Generalizability of Existing Methods)
        A --> C(Method: 提出PedX-LLM，集成视觉特征与领域知识<br>Method: Propose PedX-LLM, Integrating Visual Features and Domain Knowledge)
        A --> D(Results: 准确率提升，展现出强大的跨场景泛化能力<br>Results: Improved Accuracy and Strong Cross-Site Generalizability)
    ```

- **[arXiv260105] Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model**
  - **tags:** [ai], [domain adaptation], [data shift detection, performance degradation monitoring, vision-language model, confidence-based indicator, digital pathology]
  - **authors:** Hao Guan, Li Zhou
  - **institution:** Brigham and Women's Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2601.00716
  - **contributions:** 1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[VLM性能在数据偏移后下降/VLM performance degrades after data shift]
        C --> C1[开发输入数据偏移检测工具/DomainSAT toolbox for input shift]
        C --> C2[提出基于置信度的输出监测指标/Confidence-based output indicator]
        D --> D1[输入偏移检测有效但不总对应性能下降/Input shift detection effective but not always correlates with degradation]
        D --> D2[置信度指标与性能下降密切相关/Confidence indicator closely related to degradation]
        D --> D3[结合两者实现更可靠的监测/Combining both enables more reliable monitoring]
    ```

- **[arXiv260105] An Agentic Framework for Neuro-Symbolic Programming**
  - **tags:** [mlsys], [agent system], [neuro-symbolic programming, agentic workflow, declarative programming, DomiKnowS, human-in-the-loop]
  - **authors:** Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi
  - **institution:** Michigan State University
  - **link:** https://arxiv.org/pdf/2601.00743
  - **contributions:** 1. Introduces AgenticDomiKnowS (ADS), a framework that translates natural language task descriptions into complete neuro-symbolic programs for the DomiKnowS library. 2. Employs an agentic workflow that breaks program generation into stages, generating, executing, and refining each component independently for higher accuracy. 3. Supports optional human-in-the-loop intervention, allowing both experienced and novice users to rapidly build programs, reducing development time from hours to minutes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8ba19ac112f8b6ab345bafea964dbc29f08a66659e47452cb34cd423893719c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of authoring neuro-symbolic programs, which is time-consuming and requires expertise in specific library syntax. It proposes AgenticDomiKnowS (ADS), an agentic framework that generates complete programs from free-form descriptions by creating and testing components in stages, with optional human refinement. This approach enables both experts and non-users to construct programs much faster, reducing development time significantly.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Agentic Framework for Neuro-Symbolic Programming] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Integrating symbolic constraints is time-consuming and requires library expertise]
        C[主要方法/Method: Agentic workflow translates free-form descriptions into code with human-in-the-loop refinement]
        D[关键结果/Results: Reduces development time from hours to 10-15 minutes for users and non-users]
    ```

- **[arXiv260105] Exploring the Performance of Large Language Models on Subjective Span Identification Tasks**
  - **tags:** [nlp], [span identification], [large language models, in-context learning, chain of thought, aspect-based sentiment analysis, subjective spans]
  - **authors:** Alphaeus Dmonte, Roland Oruche, Tharindu Ranasinghe, Marcos Zampieri, Prasad Calyam
  - **institution:** George Mason University, University of Missouri, Lancaster University
  - **link:** https://arxiv.org/pdf/2601.00736
  - **contributions:** 1. Evaluates LLMs on subjective span identification tasks (sentiment analysis, offensive language identification, claim verification), an underexplored area compared to explicit tasks like NER. 2. Explores multiple LLM strategies including instruction tuning, in-context learning, and chain of thought for span identification. 3. Provides empirical results indicating that underlying textual relationships aid LLMs in identifying precise text spans.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33271e2c603477b20beb01a7eafd35b6ae5eb8b9dfab2b53139ccf619c75074_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the performance of Large Language Models on subjective text span identification tasks, such as sentiment analysis and offensive language detection, using strategies like in-context learning and chain of thought. The study finds that LLMs benefit from underlying relationships within the text to identify accurate spans, addressing a gap in current research focused on explicit span tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploring LLMs on Subjective Span Identification<br/>探索大型语言模型在主观跨度识别任务中的表现] --> B(核心问题/Problem: Subjective span identification with LLMs is underexplored compared to explicit tasks like NER.<br/>与NER等显式任务相比，LLM在主观跨度识别方面的研究不足。)
        A --> C(主要方法/Method: Evaluate LLMs using instruction tuning, in-context learning, and chain of thought on three tasks.<br/>使用指令调优、上下文学习和思维链在三个任务上评估LLMs。)
        A --> D(关键结果/Results: Underlying text relationships aid LLMs in identifying precise spans.<br/>文本中的潜在关系有助于LLMs识别精确的跨度。)
    ```

- **[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty**
  - **tags:** [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]
  - **authors:** Uğurcan Özalp
  - **institution:** Turkish Aerospace
  - **link:** https://arxiv.org/pdf/2601.00737
  - **contributions:** 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Critic网络系统性高估价值/Critic Networks Systematically Overestimate Value Estimates]
        C --> C1[使用单分布评论家建模时序偶然不确定性/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]
        C --> C2[在TD更新中应用基于不确定性的悲观偏差/Apply Uncertainty-Based Pessimistic Bias in TD Updates]
        C --> C3[对评论家和行动者网络使用Dropout正则化/Use Dropout Regularization on Critic and Actor Networks]
        D --> D1[缓解高估偏差/Mitigates Overestimation Bias]
        D --> D2[在随机环境中产生风险规避行为/Leads to Risk-Averse Behavior in Stochastic Environments]
        D --> D3[提高计算效率和训练稳定性/Improves Computational Efficiency and Training Stability]
    ```

- **[arXiv260105] LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization**
  - **tags:** [mlsys], [agent system], [LLM agents, combinatorial optimization, portfolio optimization, heuristic algorithms, mixed-integer quadratic programming]
  - **authors:** Simon Paquette-Greenbaum, Jiangbo Yu
  - **institution:** McGill University
  - **link:** https://arxiv.org/pdf/2601.00770
  - **contributions:** 1. Implements a novel LLM agentic framework for the Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem. 2. Explores and evaluates several concrete agentic architectures for automating complex portfolio optimization workflows. 3. Demonstrates that the framework matches state-of-the-art algorithms in benchmarks, alleviating development effort while maintaining acceptable performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/675ab12face3ffa5a24cbba64bf47553eac6b08cadcc7ab86dc8312ae824f240_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using LLM agents to automate the complex workflows and heuristic algorithm development for Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO), a challenging combinatorial problem. The implemented agentic framework matches state-of-the-art algorithm performance on benchmarks, significantly reducing manual effort in the optimization process.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CCPO: 混合整数二次规划问题/MIQP Problem]
        B --> B2[启发式算法开发繁琐/Heuristic Algorithm Development is Laborious]
        C --> C1[LLM智能体框架/LLM Agentic Framework]
        C --> C2[探索多种架构/Explore Multiple Architectures]
        D --> D1[匹配最先进算法/Matches SOTA Algorithms]
        D --> D2[减轻工作流程负担/Alleviates Complex Workflows]
    ```

- **[arXiv260105] Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning**
  - **tags:** [nlp], [reasoning verification], [spectral graph analysis, attention patterns, Fiedler value, high-frequency energy ratio, sliding window attention]
  - **authors:** Valentin Noël
  - **institution:** Devoteam
  - **link:** https://arxiv.org/pdf/2601.00791
  - **contributions:** 1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model's attention architecture.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning") --> Problem("核心问题/Problem: Detecting valid mathematical reasoning in LLMs")
        Root --> Method("主要方法/Method: Spectral analysis of attention patterns as dynamic graphs")
        Root --> Results("关键结果/Results: High classification accuracy, detects logical coherence, architectural dependency identified")
    ```

- **[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing**
  - **tags:** [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]
  - **authors:** Sunny Gupta, Amit Sethi
  - **institution:** Indian Institute of Technology Bombay
  - **link:** https://arxiv.org/pdf/2601.00785
  - **code:** github.com/sunnyinAI/FedHypeVAE
  - **contributions:** 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FedHypeVAE] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[非IID数据与隐私挑战/non-IID & Privacy]
        C --> C1[超网络生成条件VAE/Hypernetwork-Generated Conditional VAE]
        C --> C2[差分隐私训练/Differentially Private Training]
        C --> C3[MMD对齐与正则化/MMD Alignment & Regularization]
        D --> D1[个性化与隐私统一/Unified Personalization & Privacy]
        D --> D2[可控多域合成/Controllable Multi-Domain Synthesis]
    ```

- **[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes**
  - **tags:** [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]
  - **authors:** Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani
  - **institution:** Ben Gurion University of the Negev, Tel Aviv University
  - **link:** https://arxiv.org/pdf/2601.00012
  - **code:** https://github.com/Shaharak88/neural-brain-fields
  - **contributions:** 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>EEG data challenges: low SNR, variability, limited datasets"] --> P1["具体挑战/Challenges<br>Varying length, low SNR, participant differences"]
        Method["主要方法/Method<br>NeRF-inspired neural network for EEG"] --> M1["核心类比/Core Analogy<br>Viewpoints (NeRF) ↔ Electrodes (EEG)"]
        Method --> M2["技术实现/Technique<br>Train on single sample to get fixed weight vector"]
        Results["关键结果/Results<br>Enables continuous visualization & reconstruction"] --> R1["功能一/Capability 1<br>Render signal at unseen times/positions"]
        Results --> R2["功能二/Capability 2<br>Simulate non-existent electrodes"]
        Results --> R3["实证结果/Empirical Result<br>Improves standard EEG network performance"]
    ```

- **[arXiv260105] Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI**
  - **tags:** [ai], [health informatics], [deep learning, Holter ECG, explainable AI, time series analysis, risk prediction]
  - **authors:** Eran Zvuloni, Ronit Almog, Michael Glikson, Shany Brimer Biton, Ilan Green, Izhar Laufer, Offer Amir, Joachim A. Behar
  - **institution:** Technion - Israel Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00014
  - **contributions:** 1. Developed DeepHHF, a deep learning model that uses full 24-hour single-lead ECG recordings for heart failure risk prediction, outperforming models using short segments and clinical scores. 2. Created and utilized the large-scale Technion-Leumit Holter ECG (TLHE) dataset, comprising 69,663 recordings from 47,729 patients collected over 20 years. 3. Provided explainability analysis showing the model focuses on arrhythmias and heart abnormalities, with key attention patterns during daytime hours (8 AM to 3 PM), linking model decisions to clinically relevant features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes DeepHHF, a deep learning model that analyzes 24-hour single-lead ECG data to predict the 5-year risk of heart failure. The model achieved an AUC of 0.80, outperforming baseline methods, and identified high-risk individuals with a two-fold increased chance of hospitalization or death. The study demonstrates the feasibility of using long-term, continuous ECG data and explainable AI for non-invasive and accessible heart failure risk prediction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[预测心力衰竭风险/Predict Heart Failure Risk]
        B --> B2[使用长时程ECG数据/Using Long-term ECG Data]
        C --> C1[深度学习模型DeepHHF/Deep Learning Model DeepHHF]
        C --> C2[分析24小时单导联ECG/Analyze 24-hour Single-lead ECG]
        C --> C3[可解释性分析/Explainability Analysis]
        D --> D1[AUC达到0.80/AUC of 0.80]
        D --> D2[识别高风险个体/Identify High-risk Individuals]
        D --> D3[关注心律失常与日间模式/Focus on Arrhythmias & Daytime Patterns]
    ```

- **[arXiv260105] MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection**
  - **tags:** [ai], [computational biology / bioinformatics], [DNA methylation, transformer, explainable AI (XAI), cross-tissue analysis, Alzheimer's disease]
  - **authors:** Gang Qu, Guanghao Li, Zhongming Zhao
  - **institution:** The University of Texas Health Science Center at Houston
  - **link:** https://arxiv.org/pdf/2601.00143
  - **contributions:** 1. Proposed MethConvTransformer, a novel deep learning framework integrating convolutional and self-attention layers to capture local and long-range dependencies in DNA methylation data for Alzheimer's disease detection. 2. Introduced a method to incorporate subject-level covariates and tissue embeddings to disentangle shared and tissue-specific epigenetic effects, enabling robust cross-tissue biomarker discovery. 3. Demonstrated the model's superior performance and generalizability across multiple datasets and provided multi-resolution interpretability linking methylation patterns to known AD biological pathways.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd7853219944463e4a18ac7001e74a3c4474f9652d2f82ee38649d92981bed5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting Alzheimer's disease using DNA methylation data, which varies across tissues. The authors propose MethConvTransformer, a deep learning model that combines convolutional and transformer layers to analyze methylation patterns from brain and peripheral tissues. The model outperforms baselines, provides interpretable biomarkers, and identifies disease-relevant biological pathways.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection] --> B(核心问题/Problem: DNA甲基化特征因组织和研究而异，限制了AD生物标志物的可重复性/DNA methylation signatures vary across tissues and studies, limiting reproducible AD biomarkers)
        A --> C(主要方法/Method: 提出MethConvTransformer，结合卷积、自注意力和组织嵌入/Propose MethConvTransformer, coupling CNN, self-attention, and tissue embeddings)
        A --> D(关键结果/Results: 模型性能优于基线，提供可解释的生物标志物，揭示AD相关通路/Model outperforms baselines, provides interpretable biomarkers, reveals AD-associated pathways)
    ```

- **[arXiv260105] Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow**
  - **tags:** [mlsys], [compiler & ir], [electronic-photonic design automation, cross-layer co-design, inverse photonic design, AI-accelerated Maxwell solvers, photonic AI system]
  - **authors:** Hongjian Zhou, Ziang Yin, Jiaqi Gu
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2601.00130
  - **contributions:** 1. Proposed a cross-layer co-design framework for scalable photonic edge AI and Transformer inference architectures. 2. Introduced SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. 3. Developed AI-enabled photonic design automation techniques, including physical AI-based Maxwell solvers and a fabrication-aware inverse design framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76312a14ab1d9b01be6967c891d86f1c36fb6eebdf382d27578721d3ff3e1c24_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of designing electronic-photonic AI systems by proposing an open-source, AI-infused cross-layer co-design and automation framework. The method includes architecture designs for photonic AI, a modeling tool called SimPhony, and AI-powered design automation tools. The work aims to democratize and accelerate the development of next-generation photonic AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Democratizing Electronic-Photonic AI Systems<br>电子-光子AI系统民主化] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Challenging EPIC AI System Design<br>EPIC AI系统设计挑战]
        B --> B2[Lack of Mature EPDA Toolchain<br>缺乏成熟的EPDA工具链]
        C --> C1[Cross-Layer Co-Design Framework<br>跨层协同设计框架]
        C --> C2[SimPhony Modeling Tool<br>SimPhony建模工具]
        C --> C3[AI-Enabled EPDA Stack<br>AI赋能的EPDA堆栈]
        D --> D1[Democratizes Development<br>民主化开发]
        D --> D2[Enables Scalable EPDA<br>实现可扩展EPDA]
    ```

- **[arXiv260105] Neural Minimum Weight Perfect Matching for Quantum Error Codes**
  - **tags:** [mlsys], [others], [Quantum Error Correction, Minimum Weight Perfect Matching, Graph Neural Networks, Transformer, Hybrid Decoder]
  - **authors:** Yotam Peled, David Zenati, Eliya Nachmani
  - **institution:** Ben-Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2601.00242
  - **contributions:** 1. Proposed a hybrid decoder (NMWPM) that integrates GNNs and Transformers to predict dynamic edge weights for the MWPM algorithm. 2. Formulated a novel proxy loss function to enable end-to-end training through the non-differentiable MWPM algorithm. 3. Demonstrated a significant reduction in Logical Error Rate (LER) compared to standard baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a neural-enhanced decoder for quantum error correction called Neural Minimum Weight Perfect Matching (NMWPM). It uses a hybrid architecture of Graph Neural Networks and Transformers to predict dynamic edge weights for the classical MWPM decoder, trained with a novel proxy loss. The method significantly reduces the Logical Error Rate, showing the advantage of combining neural networks with classical algorithms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Neural Minimum Weight Perfect Matching for Quantum Error Codes"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Quantum error correction requires effective decoding."]
        Method["主要方法/Method<br>Hybrid GNN+Transformer predicts weights for MWPM."]
        Results["关键结果/Results<br>Significant reduction in Logical Error Rate."]
    ```

- **[arXiv260105] Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration**
  - **tags:** [mlsys], [others], [photonic AI, electronic-photonic design automation (EPDA), system-algorithm co-exploration, cross-layer toolchain, physical design automation]
  - **authors:** Ziang Yin, Hongjian Zhou, Nicholas Gangi, Meng Zhang, Jeff Zhang, Zhaoran Rena Huang, Jiaqi Gu
  - **institution:** Arizona State University, Rensselaer Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2601.00129
  - **contributions:** 1. Identified three essential considerations for scaling practical photonic AI systems: dynamic tensor operation support, systematic management of overheads, and robustness under hardware non-idealities. 2. Built a cross-layer toolchain (SimPhony, ADEPT, ADEPT-Z, Apollo, LiDAR) for quantitative, physically-grounded co-design from system exploration to physical layout. 3. Established a co-design loop that bridges architectural intent and deployable photonic hardware by translating physical costs into system-level metrics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd0ce87d1ffb64d07fd82b197f024f167052c6fb76c061ecf0885e18f056796_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scaling photonic AI systems by identifying key design considerations and developing a cross-layer toolchain for system-algorithm co-exploration. The proposed method uses tools like SimPhony and Apollo to model physical costs and automate design, enabling quantitative trade-off analysis under real implementation constraints. The main conclusion is that this approach creates a physically-grounded co-design loop essential for realizing large-scale, deployable photonic AI hardware.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Large-Scale Photonics-Empowered AI Systems<br/>大规模光子赋能AI系统] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Scaling AI constrained by data movement & efficiency<br/>AI扩展受限于数据移动与能效] --> B1[挑战1: 动态张量操作支持<br/>Dynamic tensor operation support]
        B --> B2[挑战2: 开销系统管理<br/>Systematic overhead management]
        B --> B3[挑战3: 硬件非理想性鲁棒性<br/>Robustness under hardware non-idealities]
        C[主要方法/Method<br/>Cross-layer toolchain for co-design<br/>跨层工具链协同设计] --> C1[SimPhony: 实现感知建模<br/>Implementation-aware modeling]
        C --> C2[ADEPT/ADEPT-Z: 电路与拓扑探索<br/>Circuit & topology exploration]
        C --> C3[Apollo/LiDAR: 物理设计自动化<br/>Physical design automation]
        D[关键结果/Results<br/>Quantitative & physically-grounded co-design loop<br/>定量且物理基础的协同设计循环] --> D1[连接系统目标与可行硬件<br/>Connects system objectives to feasible hardware]
        D --> D2[产生可制造布局<br/>Produces manufacturable layouts]
    ```

- **[arXiv260105] Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics**
  - **tags:** [ai], [biometrics], [ECG biometrics, phase-aware representation, hierarchical fusion, multi-prototype enrollment, graph neural networks]
  - **authors:** Jintao Huang, Lu Leng, Yi Zhang, Ziyuan Yang
  - **institution:** Nanchang Hangkong University, Sichuan University
  - **link:** https://arxiv.org/pdf/2601.00170
  - **contributions:** 1. Proposed a Hierarchical Phase-Aware Fusion (HPAF) framework with a three-stage design (Intra-Phase Representation, Phase-Grouped Hierarchical Fusion, Global Representation Fusion) to explicitly model phase-specific characteristics within the cardiac cycle and avoid cross-feature entanglement. 2. Introduced a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy to construct a multi-prototype gallery template set, mitigating the impact of heartbeat-specific noise and variability. 3. Demonstrated state-of-the-art performance on three public datasets under both closed and open-set settings, validating the effectiveness of the physiologically grounded approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2278c4a79bd346845b69356eeced50030b7ff8a854aa0dc0165503f22a96a9e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that existing ECG biometric methods treat heartbeats as homogeneous, overlooking phase-specific characteristics. It proposes a Hierarchical Phase-Aware Fusion (HPAF) framework to independently model and hierarchically fuse features from different cardiac phases, along with a multi-prototype enrollment strategy. The method achieves state-of-the-art results on public datasets, showing the benefit of a physiologically grounded, phase-aware approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Hear the Heartbeat in Phases<br/>Physiologically Grounded Phase-Aware ECG Biometrics] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法将心跳视为同质信号<br/>Existing methods treat heartbeats as homogeneous]
        B --> B2[忽略心动周期内的相位特异性<br/>Overlook phase-specific characteristics]
        C --> C1[分层相位感知融合框架 HPAF<br/>Hierarchical Phase-Aware Fusion (HPAF) framework]
        C1 --> C1_1[阶段1: 相位内表示 IPR<br/>Stage 1: Intra-Phase Representation (IPR)]
        C1 --> C1_2[阶段2: 相位分组融合 PGHF<br/>Stage 2: Phase-Grouped Hierarchical Fusion (PGHF)]
        C1 --> C1_3[阶段3: 全局表示融合 GRF<br/>Stage 3: Global Representation Fusion (GRF)]
        C --> C2[心跳感知多原型注册 HAM<br/>Heartbeat-Aware Multi-prototype (HAM) enrollment]
        D --> D1[在三个公开数据集上SOTA<br/>State-of-the-art on three public datasets]
        D --> D2[闭集和开集设置<br/>Closed and open-set settings]
    ```

- **[arXiv260105] Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics**
  - **tags:** [other], [computational biology], [single-cell genomics, data integration, normalization, dimensionality reduction, benchmarking]
  - **authors:** Ali Anaissi, Seid Miad Zandavi, Weidong Huang, Junaid Akram, Basem Suleiman, Ali Braytee, Jie Hua
  - **institution:** University of Technology Sydney, University of Sydney, Broad Institute, University of New South Wales, Shaoyang University
  - **link:** https://arxiv.org/pdf/2601.00277
  - **contributions:** 1. Conducted a systematic evaluation of preprocessing and integration methods for single-cell multimodal data, which had been lacking. 2. Benchmarked a comprehensive pipeline involving combinations of seven normalization, five integration, and four dimensionality reduction methods across six diverse datasets. 3. Provided empirical findings on the performance and time-efficiency of leading methods, identifying Seurat and Harmony as top performers and highlighting the compatibility of UMAP.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7cf9641df8a7176dd7483adbf7e7c16827ddac25d332fe518d19b7c39109f9_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically benchmarks combinations of preprocessing and integration methods for single-cell multimodal genomic data. The study evaluates various normalization, integration, and dimensionality reduction techniques across diverse datasets. The results show that Seurat and Harmony excel at integration, with Harmony being more time-efficient, and UMAP is the most compatible dimensionality reduction method.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[整合单细胞多模态数据/Integrating single-cell multimodal data]
        B --> B2[缺乏预处理策略的系统评估/Lack of systematic evaluation with preprocessing]
        C --> C1[评估多种方法组合/Evaluating combinations of methods]
        C --> C2[使用六个数据集和三个指标/Using six datasets and three metrics]
        D --> D1[Seurat与Harmony表现优异/Seurat and Harmony excel]
        D --> D2[Harmony时间效率更高/Harmony is more time-efficient]
        D --> D3[UMAP兼容性最好/UMAP is most compatible]
    ```

- **[arXiv260105] Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks**
  - **tags:** [ai], [reinforcement learning], [multi-functional RIS, NOMA, energy efficiency, hybrid deep reinforcement learning, parametrized sharing]
  - **authors:** Chi-Te Kuo, Li-Hsiang Shen, Jyun-Jhe Huang
  - **institution:** National Central University
  - **link:** https://arxiv.org/pdf/2601.00538
  - **contributions:** 1. Formulates an energy efficiency maximization problem for a multi-MF-RIS-aided NOMA downlink network, jointly optimizing power, beamforming, RIS configurations, and RIS positions. 2. Proposes a Parametrized Sharing scheme for Multi-Agent Hybrid Deep Reinforcement Learning (PMHRL) that combines multi-agent PPO for continuous variables and DQN for discrete variables. 3. Demonstrates through simulations that the proposed PMHRL and multi-MF-RIS architecture achieve superior energy efficiency compared to various benchmarks and alternative system scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of maximizing energy efficiency in downlink NOMA networks assisted by multiple multi-functional RISs. The authors propose a novel parametrized sharing scheme for a multi-agent hybrid deep reinforcement learning algorithm (PMHRL) to jointly optimize power, beamforming, and RIS parameters. Simulation results show that the proposed method achieves the highest energy efficiency compared to other benchmarks and system configurations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("最大化多MF-RIS辅助NOMA网络的能效/Maximize EE of multi-MF-RIS-aided NOMA network")
        Method --> M1("参数共享的多智能体混合DRL (PMHRL)/Parametrized Sharing Multi-Agent Hybrid DRL (PMHRL)")
        M1 --> M1_1("PPO处理连续变量/PPO for continuous variables")
        M1 --> M1_2("DQN处理离散变量/DQN for discrete variables")
        Results --> R1("PMHRL能效最高/PMHRL achieves highest EE")
        Results --> R2("优于无参数共享、纯PPO/DQN基准/Superior to benchmarks without sharing, pure PPO/DQN")
        Results --> R3("多MF-RIS NOMA优于传统RIS等场景/Multi-MF-RIS NOMA outperforms traditional RIS, etc.")
    ```
