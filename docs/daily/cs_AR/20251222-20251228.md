---
slug: /daily/csar/20251222-20251228
---
# 20251222-20251228 (cs.AR)

## 2025-12-22

- **[arXiv251222] LLM-based Behaviour Driven Development for Hardware Design**
  - **tags:** [mlsys], [others], [Behavior Driven Development (BDD), Large Language Models (LLMs), hardware design, test and verification, natural language processing, Electronic Design Automation (EDA)]
  - **authors:** Rolf Drechsler, Qian Liu
  - **institution:** University of Bremen, DFKI
  - **link:** https://arxiv.org/pdf/2512.17814
  - **Simple LLM Summary:** This paper investigates the use of Large Language Models (LLMs) to automate the generation of behavioral scenarios from textual specifications for Behavior Driven Development (BDD) in hardware design. The core method involves applying LLM-based techniques to interpret specifications and produce high-level behavioral descriptions. The main conclusion is that LLMs offer a promising opportunity to support and automate BDD workflows in hardware design, addressing the manual effort and complexity of current verification practices.

## 2025-12-23

- **[arXiv251223] Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs**
  - **tags:** TBD
  - **authors:** Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18134
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp
  - **Simple LLM Summary:** Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs

- **[arXiv251223] Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference**
  - **tags:** TBD
  - **authors:** Rui Xie, Yunhua Fang, Asad Ul Haq, Linsen Ma, Sanchari Sen, Swagath Venkataramani, Liu Liu, Tong Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18152
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9099b3e60f5155323e33ca2663b734d9691284bb3f33ff23ded103fa2caa02e8_w640_q70.webp
  - **Simple LLM Summary:** Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference

- **[arXiv251223] PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference**
  - **tags:** TBD
  - **authors:** Nuntipat Narkthong, Xiaolin Xu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18132
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5249dae70a6ea951da7229a4606840eb6080d2d0b091828439fd7dc722b3fe4e_w640_q70.webp
  - **Simple LLM Summary:** PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference

- **[arXiv251223] PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM**
  - **tags:** TBD
  - **authors:** Tsung-Han Lu, Zheyu Li, Minxuan Zhou, Tajana Rosing
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18158
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c0e2dda9beb3cd09d48e66d5bd1e2e0d25dcf0ff68f729ae40eea17980532c9_w640_q70.webp
  - **Simple LLM Summary:** PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM

- **[arXiv251223] BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism**
  - **tags:** TBD
  - **authors:** Suhas Vittal, Moinuddin Qureshi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18300
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d48040e52a6d8197d1665d09704221d54a7d6dab03a13f7863e398d9cb759275_w640_q70.webp
  - **Simple LLM Summary:** BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism

- **[arXiv251223] Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration**
  - **tags:** TBD
  - **authors:** Wonseok Choi, Hyunah Yu, Jongmin Kim, Hyesung Ji, Jaiyoung Park, Jung Ho Ahn
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18345
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e4c078425f57860f1303fe0449ae500adae2d4acf07294f16f8ed22a154bad_w640_q70.webp
  - **Simple LLM Summary:** Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration

- **[arXiv251223] Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework**
  - **tags:** TBD
  - **authors:** Akul Malhotra, Sumeet Kumar Gupta
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18459
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97e8d699ae3dc2331ca317ecfd53fe4db9ae0d861417dcb1143bdade9d3ecfa0_w640_q70.webp
  - **Simple LLM Summary:** Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework

- **[arXiv251223] DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge**
  - **tags:** TBD
  - **authors:** Yifan Zhao, Xinglong Yu, Yi Sun, Honglin Kuang, Jun Han
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18589
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1feca70dc6f2a05e35a48f6366ccdcc1accb232952639d7388e6316ca2ff652_w640_q70.webp
  - **Simple LLM Summary:** DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge

- **[arXiv251223] Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA**
  - **tags:** TBD
  - **authors:** Emir Devlet Ertörer, Cem Ünsalan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19304
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f352565f0530b4ee02ed15da3cf6f6c724c3dee282c3ce8dcec4f2bb7a8bed00_w640_q70.webp
  - **Simple LLM Summary:** Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA

- **[arXiv251223] Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory**
  - **tags:** TBD
  - **authors:** Guan-Cheng Chen, Chieh-Lin Tsai, Pei-Hsuan Tsai, Yuan-Hao Chang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19445
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/731a9d0dfb14219a43a9ad49271f63aef43ed484776bc0682810b6c431e5f06e_w640_q70.webp
  - **Simple LLM Summary:** Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory

## 2025-12-24

- **[arXiv251224] 3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras**
  - **tags:** TBD
  - **authors:** Hongyang Shang, Shuai Dong, Ye Ke, Arindam Basu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20073
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32bc75b2d6c95f1b2dc70a4aff51224520c7f222f61a22edbcd56ede5fa299a5_w640_q70.webp
  - **Simple LLM Summary:** 3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras

- **[arXiv251224] Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling**
  - **tags:** TBD
  - **authors:** Huizheng Wang, Taiquan Wei, Hongbin Wang, Zichuan Wang, Xinru Tang, Zhiheng Yue, Shaojun Wei, Yang Hu, Shouyi Yin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20198
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e2bc2f765accf0fbb24e05e708bd3a2b62c961a86def9137c9976aa8b753f85_w640_q70.webp
  - **Simple LLM Summary:** Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling

- **[arXiv251224] Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization**
  - **tags:** TBD
  - **authors:** He Zhu, Zheng Liu, Xingyang Li, Anbang Wu, Jieru Zhao, Fangxin Liu, Yiming Gan, Jingwen Leng, Yu Feng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20495
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1ba8a7f58cc0b9807b31889823cb7497448ae885dece28021d7b9eda1828c58_w640_q70.webp
  - **Simple LLM Summary:** Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization

- **[arXiv251224] Composing Mini Oscilloscope on Embedded Systems**
  - **tags:** TBD
  - **authors:** Brennan Romero, D.G. Perera
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20571
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c64819e1e69202dbb50470d8750cc2e3fb023b25140bd80e5d0c7dddb4b62612_w640_q70.webp
  - **Simple LLM Summary:** Composing Mini Oscilloscope on Embedded Systems

## 2025-12-25

- **[arXiv251225] NotSoTiny: A Large, Living Benchmark for RTL Code Generation**
  - **tags:** [mlsys], [llm training], [RTL code generation, benchmark, hardware design, data contamination, verification]
  - **authors:** Razine Moundir Ghorab, Emanuele Parisi, Cristian Gutierrez, Miquel Alberti-Binimelis, Miquel Moreto, Dario Garcia-Gasulla, Gokcen Kestor
  - **institution:** Barcelona Supercomputing Center, Universitat Politecnica de Catalunya
  - **link:** https://arxiv.org/pdf/2512.20823
  - **contributions:** 1. Introduces NotSoTiny, a large-scale, living benchmark for evaluating LLMs on RTL code generation, built from real hardware designs. 2. Proposes an automated pipeline to ensure benchmark quality by removing duplicates, verifying correctness, and periodically updating to mitigate data contamination. 3. Demonstrates that NotSoTiny presents more challenging tasks than prior benchmarks, effectively highlighting current LLM limitations in hardware design.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ee59b5723cdae955454bd94bdc8872b40c0eaccf59a4e54b86951d040529325_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces NotSoTiny, a benchmark for evaluating LLMs on generating Register-Transfer Level (RTL) code, addressing limitations of prior benchmarks by using real, complex hardware designs and a pipeline to ensure correctness and reduce data contamination. The results show that NotSoTiny tasks are more challenging, effectively guiding the improvement of LLMs for hardware design.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NotSoTiny: A Large, Living Benchmark for RTL Code Generation] --> B(核心问题/Problem: LLM RTL代码生成评估挑战 / LLM RTL Code Generation Evaluation Challenge)
    A --> C(主要方法/Method: 基于真实硬件设计的自动化基准测试 / Automated Benchmark from Real Hardware Designs)
    A --> D(关键结果/Results: 任务更具挑战性，有效指导改进 / Tasks More Challenging, Effectively Guides Improvement)
    ```

- **[arXiv251225] ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update**
  - **tags:** [mlsys], [on-device ai], [spiking neural network, self-supervised learning, structured sparsity, activity-dependent update, event-driven processing]
  - **authors:** Zhe Su, Giacomo Indiveri
  - **institution:** Institute of Neuroinformatics, University of Zurich and ETH Zurich
  - **link:** https://arxiv.org/pdf/2512.21153
  - **contributions:** 1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[ElfCore] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]
    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]
    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]
    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]
    D --> D1[功耗降低16倍/16x Lower Power]
    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]
    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]
    ```
