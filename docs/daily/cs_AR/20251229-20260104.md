---
slug: /daily/csar/20251229-20260104
---
# 20251229-20260104 (cs.AR)

## 2025-12-29

- **[arXiv251229] Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide**
  - **tags:** [sec], [side-channel analysis], [RISC-V, CVA6, Correlation Power Analysis (CPA), RTL simulation, power side-channel]
  - **authors:** Behnam Farnaghinejad, Antonio Porsia, Annachiara Ruospo, Alessandro Savino, Stefano Di Carlo, Ernesto Sanchez
  - **institution:** Politecnico di Torino
  - **link:** https://arxiv.org/pdf/2512.21362
  - **contributions:** 1. Presents the first side-channel vulnerability evaluation of the CVA6 RISC-V processor core. 2. Demonstrates the application of the VeriSide RTL-level power profiling framework for efficient power trace extraction without waveform files. 3. Shows that Correlation Power Analysis (CPA) on the CVA6 during software-based AES encryption enables key recovery, highlighting the need for early-stage RTL security assessments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f1045d9314e37006547d2c94a7c4490ff95449687fd13d7c467003b4c095bac_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the power side-channel vulnerability of the CVA6 RISC-V core using the VeriSide RTL simulation framework. By applying Correlation Power Analysis (CPA) to power traces during software AES execution, the authors successfully recover the secret key. The findings demonstrate significant leakage in the CVA6 design, emphasizing the importance of pre-silicon RTL-level security evaluation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现代RISC-V处理器需要抗侧信道攻击能力 / Modern RISC-V processors require resilience to side-channel attacks]
        C --> C1[使用VeriSide框架在RTL级进行功耗分析 / Use VeriSide framework for RTL-level power analysis]
        C --> C2[对软件AES执行进行相关性功耗分析(CPA) / Perform Correlation Power Analysis (CPA) on software AES execution]
        D --> D1[CVA6设计存在显著泄漏 / CVA6 design exhibits significant leakage]
        D --> D2[成功恢复AES密钥 / Successful AES key recovery]
    ```

- **[arXiv251229] Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation**
  - **tags:** [mlsys], [on-device ai], [predictive plasticity rule, extreme learning machine, FPGA implementation, online learning, low-complexity training]
  - **authors:** Zhenya Zang, Xingda Li, David Day Uei Li
  - **institution:** University of Strathclyde
  - **link:** https://arxiv.org/pdf/2512.21777
  - **contributions:** 1. Proposed a simplified, biologically inspired predictive local learning rule that eliminates global backpropagation and membrane integration, using sparse, binary-driven vector additions triggered only by prediction errors. 2. Integrated this rule into an Extreme Learning Machine (ELM), replacing the conventional matrix inversion, thereby reducing training complexity from O(M^3) to O(M) for M hidden nodes. 3. Demonstrated an FPGA implementation showing significant reductions in computational and memory requirements, highlighting its potential for energy-efficient online learning on low-cost edge devices.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fd9efea1b6faa314e48c1a0c90ad6c7bf79112ed59bfb1db6f14146d142848_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a low-complexity online learning method by integrating a simplified predictive plasticity rule into an Extreme Learning Machine (ELM), which reduces training complexity from cubic to linear order. The approach is implemented on FPGA, showing reduced computational and memory needs while maintaining comparable accuracy. The work demonstrates strong potential for enabling efficient online learning on resource-constrained edge devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Online Learning ELM with Low-Complexity Predictive Plasticity Rule and FPGA Implementation<br/>在线学习ELM与低复杂度预测可塑性规则及FPGA实现"]
        Root --> Problem["Problem: High complexity of online learning for edge devices<br/>问题：面向边缘设备的在线学习复杂度高"]
        Root --> Method["Method: Simplified predictive plasticity rule + ELM<br/>方法：简化的预测可塑性规则 + ELM"]
        Root --> Results["Results: O(M) training, FPGA implementation, low resource use<br/>结果：O(M)训练，FPGA实现，低资源消耗"]
    ```

- **[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling**
  - **tags:** [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]
  - **authors:** Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras
  - **institution:** Uppsala University
  - **link:** https://arxiv.org/pdf/2512.22066
  - **contributions:** 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同"] --> Problem_Sub1["SRAM大小与频率如何影响能效？"]
        Problem --> Problem_Sub2["内存带宽如何限制性能？"]
        Method["主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法"] --> Method_Sub1["能耗建模/Energy Modeling"]
        Method --> Method_Sub2["延迟模拟/Latency Simulation"]
        Method --> Method_Sub3["操作强度分析/Operational Intensity"]
        Results["关键结果/Results"] --> Results_Sub1["总能耗主要由SRAM大小决定<br>大缓存增加静态能耗"]
        Results --> Results_Sub2["高频可降低总能耗<br>（减少静态能耗）"]
        Results --> Results_Sub3["最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)"]
    ```

## 2025-12-30

- **[arXiv251230] An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator**
  - **tags:** [mlsys], [on-device ai], [RFET, stochastic computing, SCNN, stochastic number generator, accelerator]
  - **authors:** Sheng Lu, Qianhou Qu, Sungyong Jung, Qilian Liang, Chenyun Pan
  - **institution:** University of Texas at Arlington (inferred from IEEE affiliation and author "Qilian Liang, Fellow, IEEE" known association)
  - **link:** https://arxiv.org/pdf/2512.22131
  - **contributions:** 1. Proposes a novel SCNN architecture leveraging Reconfigurable Field-Effect Transistors (RFETs) for device-level reconfigurability. 2. Designs highly efficient and compact core modules (e.g., SNGs, APCs) enabled by RFET technology. 3. Develops and evaluates a dedicated RFET-based SCNN accelerator, showing significant improvements in area, latency, and energy over a FinFET baseline.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d718d7962f59eda2ed3430de0579c28b976cb6dd1e7da5e6fb355787c2b15ee_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high resource consumption of Stochastic Computing Neural Networks (SCNNs) by proposing a novel accelerator architecture based on Reconfigurable Field-Effect Transistors (RFETs). The inherent reconfigurability of RFETs enables the design of compact and efficient core components like stochastic number generators. Experimental results demonstrate that the proposed RFET-based accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator] --> B
        A --> C
        A --> D
        B[核心问题/Problem: SCNN资源消耗高/High SCNN Resource Usage]
        C[主要方法/Method: 基于RFET的架构/RFET-Based Architecture]
        D[关键结果/Results: 面积、延迟、能耗降低/Reduced Area, Latency, Energy]
    ```

- **[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA**
  - **tags:** [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]
  - **authors:** Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn
  - **institution:** National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)
  - **link:** https://arxiv.org/pdf/2512.22139
  - **code:** https://github.com/dll-ncai/HLS4PC
  - **contributions:** 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]
        P1 --> P2[High memory/computation demand hinders real-time performance]
        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]
        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]
        M2 --> M3[Creates PointMLP-Lite model]
        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]
        R1 --> R2[3.56x higher throughput vs. prior work]
        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]
    ```

- **[arXiv251230] AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience**
  - **tags:** [mlsys], [agent system], [analog circuit design, multi-agent framework, stratified memory, simulation-grounded feedback, self-evolving]
  - **authors:** Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang
  - **institution:** Northeastern University, Kansas State University
  - **link:** https://arxiv.org/pdf/2512.22435
  - **contributions:** 1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Analog circuit design is knowledge-intensive and relies on human intuition]
        B --> B2[Existing LLM-based methods lack feedback and generalization]
        C --> C1[Self-evolving multi-agent framework]
        C --> C2[Three-stage agent explorations with stratified memory]
        C --> C3[Iterative refinement via simulation-grounded feedback]
        D --> D1[10x overall pass rate improvement]
        D --> D2[48x Pass@1 improvement]
        D --> D3[4x reduction in parameter search space]
    ```

- **[arXiv251230] TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators**
  - **tags:** [mlsys], [on-device ai], [activation function, hardware accelerator, taylor series, energy efficiency, edge inference]
  - **authors:** Soham Pramanik, Vimal William, Arnab Raha, Debayan Das, Amitava Mukherjee, Janet L. Paluh
  - **institution:** Jadavpur University, SandLogic Technologies, Intel Corporation, Indian Institute of Science, Amrita University, SUNY Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2512.23062
  - **contributions:** 1. Proposes TYTAN, a Taylor-series based Generalized Non-linear Approximation Engine (G-NAE) for accelerating non-linear activation functions in deep learning. 2. Integrates a re-configurable hardware design with a specialized algorithm to dynamically estimate approximations, aiming for minimal accuracy deviation. 3. Demonstrates significant performance gains and efficiency improvements, including ~2x performance, ~56% power reduction, and ~35x lower area compared to a baseline NVDLA implementation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/233e926fc1401824c658e53f6ee69cc2fa91152f36cad6ff674b919cf9a3aa0e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TYTAN, a hardware-software co-designed engine that uses Taylor series approximations to accelerate non-linear activation functions for energy-efficient AI inference at the edge. The system dynamically configures the approximation to maintain accuracy. Evaluations show it achieves high frequency operation (&gt;950 MHz) with substantial improvements in performance, power, and area compared to a standard accelerator.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TYTAN: Taylor-series based Non-Linear Activation Engine] --> B[核心问题/Problem: Edge AI inference requires acceleration and energy efficiency, limited by high-power operations like activation functions.]
        A --> C[主要方法/Method: Proposes a Generalized Non-linear Approximation Engine (G-NAE) using Taylor series and re-configurable hardware with a dynamic approximation algorithm.]
        A --> D[关键结果/Results: >950 MHz operation, ~2x performance, ~56% power reduction, ~35x lower area vs. NVDLA baseline.]
    ```

- **[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta**
  - **tags:** [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]
  - **authors:** Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu
  - **institution:** Meta Platforms
  - **link:** https://arxiv.org/pdf/2512.23236
  - **contributions:** 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp
  - **Simple LLM Summary:** This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]
        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]
        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]
        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]
        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]
        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]
        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]
        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]
    ```

- **[arXiv251230] Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space**
  - **tags:** [other], [digital signal processing, computer arithmetic, high-performance computing], [energy-efficient computing, integer-friendly approximation, conflict-free memory access, fast Fourier transform, fast Schur algorithm]
  - **authors:** Sergey Salishev
  - **institution:** Saint Petersburg State University
  - **link:** https://arxiv.org/pdf/2512.22676
  - **contributions:** 1. A power/energy consumption model for clocked CMOS logic to select optimal parallelism. 2. Integer-friendly approximation methods for elementary functions using constrained piecewise-polynomials to reduce lookup-table size. 3. Provably conflict-free data placement and execution order schemes for mixed-radix streaming FFT on multi-bank/single-port memories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fe50589b6f9e67a8e1acb929b6cfc7dcaecddcc5c1837d218c89e297ca79994_w640_q70.webp
  - **Simple LLM Summary:** This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space to improve energy efficiency. It proposes a power model, approximation methods, and conflict-free memory access schemes for FFT and fast Schur algorithms. The results provide constructive theorems and design trade-offs for building efficient specialized accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space<br>信号处理算法在最小并行度和内存空间约束下的综合"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Improving energy efficiency of low-power computing hardware<br>提高低功耗计算硬件的能效"]
        Method["主要方法/Method<br>1. Power/energy model for CMOS logic<br>CMOS逻辑功耗/能耗模型<br>2. Integer-friendly function approximation<br>整数友好函数近似<br>3. Conflict-free FFT schedules<br>无冲突FFT调度<br>4. Parallelism/memory analysis for fast Schur algorithm<br>快速Schur算法的并行度/内存分析"]
        Results["关键结果/Results<br>Constructive theorems, schedules, and design trade-offs for efficient specialized accelerators<br>为高效专用加速器提供构造性定理、调度方案和设计权衡"]
    ```

## 2026-01-01

- **[arXiv260101] Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics**
  - **tags:** [mlsys], [on-device ai], [FPGA acceleration, model recovery, hardware-software co-design, GRU, Neural ODE]
  - **authors:** Bin Xu, Ayan Banerjee, Sandeep Gupta
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.23767
  - **contributions:** 1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enabling Physical AI at the Edge<br>在边缘实现物理人工智能] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Model recovery methods (Neural ODEs) are inefficient for edge hardware<br>模型恢复方法在边缘硬件上效率低下]
        C[主要方法/Method<br>MERINDA: FPGA-accelerated, hardware-friendly framework<br>MERINDA: FPGA加速的硬件友好框架]
        D[关键结果/Results<br>114x lower energy, 28x smaller memory, 1.68x faster training<br>能耗降低114倍, 内存占用减少28倍, 训练速度提升1.68倍]
    ```

- **[arXiv260101] HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation**
  - **tags:** [hpc], [gpu kernels], [SPHINCS+, GPU Optimization, Tree Fusion, Adaptive Compilation, Kernel Overlapping]
  - **authors:** Yaoyun Zhou, Qian Wang
  - **institution:** University of California, Merced
  - **link:** https://arxiv.org/pdf/2512.23969
  - **contributions:** 1. Introduces a Tree Fusion strategy for the FORS component, guided by an automated Tree Tuning search algorithm to adapt to different GPU architectures. 2. Employs an adaptive compilation strategy that automatically selects between PTX and native code paths for different SPHINCS+ kernels to maximize efficiency. 3. Optimizes batched signature generation using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29ca58021f9583efba2be9c3f10082df113be98d3f93c0f5ef9cc70c6fa8e481_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes HERO-Sign, a GPU-accelerated implementation for the post-quantum signature scheme SPHINCS+. It uses hierarchical tuning, including a Tree Fusion strategy and adaptive compilation, to better exploit GPU parallelism and reduce overhead. The method achieves significant throughput improvements and latency reduction compared to state-of-the-art GPU implementations across multiple architectures.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HERO-Sign: SPHINCS+签名生成优化 / HERO-Sign: SPHINCS+ Signature Generation Optimization"]
        Root --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["SPHINCS+签名生成慢 / Slow SPHINCS+ Signature Generation"]
        Problem --> P2["现有GPU优化未充分利用并行性 / Existing GPU Optimizations Underutilize Parallelism"]
        Method --> M1["树融合策略 / Tree Fusion Strategy"]
        Method --> M2["自适应编译 / Adaptive Compilation"]
        Method --> M3["任务图构建 / Task Graph Construction"]
        Results --> R1["吞吐量提升1.24-3.13倍 / Throughput Improvement 1.24-3.13x"]
        Results --> R2["内核启动延迟降低两个数量级 / Kernel Launch Latency Reduced by Two Orders of Magnitude"]
    ```

- **[arXiv260101] FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [N:M structured pruning, 4-bit quantization, systolic array, FPGA accelerator, hardware-software co-design]
  - **authors:** Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu
  - **institution:** Institute of Information Science, Academia Sinica
  - **link:** https://arxiv.org/pdf/2512.24713
  - **contributions:** 1. Proposes an automation framework and unified pipeline for applying N:M structured pruning and 4-bit integer quantization to compress LLMs. 2. Presents a hardware-software co-design method that generates a custom systolic-array-based FPGA accelerator for efficient inference. 3. Demonstrates the synergy of fine-grained sparsity and quantization, achieving significant reductions in storage and latency while offering flexibility beyond fixed hardware sparsity patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a594f5eb4f19e15f5f182ee786cc270613c6a3d07553a78731a54b9a3ae90ea_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational and memory demands of LLMs by proposing a hardware-software co-design framework. The method combines N:M structured pruning and 4-bit quantization to compress models, and implements a custom FPGA accelerator for efficient inference. The results show significant reductions in storage and latency, demonstrating the effectiveness of the approach for deployable LLM inference.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LLM部署困难/LLM Deployment Challenge")
        P1 --> P2("高计算与内存需求/High Computation & Memory Requirements")
        Method --> M1("模型压缩/Model Compression")
        M1 --> M2("N:M结构化剪枝与4-bit量化/N:M Structured Pruning & 4-bit Quantization")
        Method --> M3("软硬件协同设计/Hardware-Software Co-Design")
        M3 --> M4("生成基于脉动阵列的FPGA加速器/Generating Systolic-Array-based FPGA Accelerator")
        Results --> R1("存储减少4倍/4x Weight Storage Reduction")
        Results --> R2("矩阵乘法加速1.71倍/1.71x Matrix Multiplication Speedup")
        Results --> R3("端到端延迟降低1.29倍/1.29x End-to-End Latency Reduction")
    ```

- **[arXiv260101] Advances in Agentic AI: Back to the Future**
  - **tags:** [mlsys], [agent system], [Agentic AI, Algorithmization, M1 (first Machine in Machine Learning), M2 (second Machine in Machine Learning), Strategies-based Agentic AI]
  - **authors:** Sergio Alvarez-Telena, Marta Diez-Fernandez
  - **institution:** University College London, SciTheWorld
  - **link:** https://arxiv.org/pdf/2512.24856
  - **contributions:** 1. Proposes precise definitions and a structured analytical framework for Agentic AI and its convergence with Algorithmization. 2. Introduces and distinguishes the concepts of the first Machine in Machine Learning (M1) as the platform for LLM-based Agentic AI and the second Machine (M2) as the architectural prerequisite for production-grade B2B transformation. 3. Provides conceptual and technical insight into what is claimed to be the first fully realized implementation of an M2 system and outlines a forward-looking research agenda.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/840ae0fbf1973bad5d93eb708489774eab0a0df7bdc273f111d764666bd471bc_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the fragmented discourse around Agentic AI by proposing a clarifying framework and distinguishing between two key architectural concepts: M1 for current LLM-based systems and M2 for future, robust B2B transformation. It positions M2, or Strategies-based Agentic AI, as the necessary evolution to overcome operational barriers. The conclusion outlines a research agenda for the next two decades based on the authors' prior work in Algorithmization.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Advances in Agentic AI: Back to the Future<br/>Agentic AI进展：回到未来"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br/>Fragmented discourse on Agentic AI<br/>Agentic AI领域论述碎片化"] --> P1["缺乏清晰定义/Lack of clear definitions"]
        Problem --> P2["需要B2B转型框架/Need for B2B transformation framework"]
    
        Method["主要方法/Method<br/>Propose analytical framework<br/>提出分析框架"] --> M1["区分M1与M2/Distinguish M1 & M2"]
        M1 --> M1a["M1: LLM-based Agentic AI platform<br/>M1: 基于LLM的Agentic AI平台"]
        M1 --> M1b["M2: Strategies-based Agentic AI<br/>M2: 基于策略的Agentic AI"]
        Method --> M2["回顾算法化工作/Review Algorithmization work"]
    
        Results["关键结果/Results<br/>Framework & Future Agenda<br/>框架与未来议程"] --> R1["提供清晰定义与框架/Provide clear definitions & framework"]
        Results --> R2["介绍首个M2实现/Introduce first M2 implementation"]
        Results --> R3["提出未来20年议程/Propose 20-year future agenda"]
    ```
