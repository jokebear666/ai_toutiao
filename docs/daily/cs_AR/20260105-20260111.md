---
slug: /daily/csar/20260105-20260111
---
# 20260105-20260111 (cs.AR)

## 2026-01-05

- **[arXiv260105] Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation**
  - **tags:** [sys], [memory & caching], [STT-MRAM, read disturbance, cache reliability, ECC, REAP-cache]
  - **authors:** Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi
  - **institution:** Sharif University of Technology, Amirkabir University of Technology
  - **link:** https://arxiv.org/pdf/2601.00450
  - **contributions:** 1. Introduces and formulates the phenomenon of read disturbance accumulation in STT-MRAM caches caused by speculative parallel reads during tag comparison. 2. Proposes the REAP-cache (Read Error Accumulation Preventer) scheme to eliminate this accumulation without compromising cache performance. 3. Demonstrates that REAP-cache significantly improves reliability (extending MTTF by 171x) with minimal overhead (less than 1% area and 2.7% energy increase).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7f78ea49ca100ac89e593165b5879dec1063a4cf12be706bf9dd1db3c8f455_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that the conventional parallel read of all blocks in a cache set for tag comparison leads to the accumulation of uncorrected read disturbance errors in STT-MRAM caches, degrading reliability. To solve this, the authors propose the REAP-cache scheme, which prevents this error accumulation. Their evaluation shows REAP-cache dramatically improves cache reliability (171x longer MTTF) with very low area and energy overheads.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enhancing Reliability of STT-MRAM Caches<br>提升STT-MRAM缓存可靠性] --> B
        A --> C
        A --> D
        B[问题: 读取干扰累积降低可靠性<br>Problem: Read Disturbance Accumulation Degrades Reliability]
        C[方法: 提出REAP-cache方案<br>Method: Propose REAP-cache Scheme]
        D[结果: MTTF提升171倍，开销极小<br>Results: 171x MTTF Improvement, Minimal Overhead]
    ```

- **[arXiv260105] ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches**
  - **tags:** [sys], [memory & caching], [STT-MRAM, Error-Correcting Codes (ECC), cache reliability, write failure, interleaving]
  - **authors:** Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi
  - **institution:** Sharif University of Technology, Amirkabir University of Technology
  - **link:** https://arxiv.org/pdf/2601.00456
  - **contributions:** 1. Conducted a comprehensive analysis revealing the inefficiency of conventional ECC configurations (per-word and interleaved) in STT-MRAM caches due to non-uniform distribution of bit transitions across codewords. 2. Proposed a novel ECC configuration called ROBIN (Incremental Oblique Interleaved ECC) designed to uniformly distribute bit transitions among codewords to maximize error correction capability. 3. Demonstrated significant reliability improvement, showing that ROBIN reduces the increased cache error rate caused by conventional ECC inefficiency by more than 28.6 times.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/220a026abe75f2e65296ee9d7d5c4aebb0b4aac8cebaceddd46efd3084ecae49_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high error rate problem in STT-MRAM caches by identifying that conventional Error-Correcting Codes (ECCs) are inefficient due to data-dependent error patterns. The authors propose a new ECC configuration called ROBIN, which uses an incremental oblique interleaving technique to uniformly distribute bit transitions and improve correction capability. Evaluations show ROBIN reduces the cache error rate increase by over 28.6x compared to conventional ECCs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[STT-MRAM缓存错误率高 / High error rate in STT-MRAM caches]
        B --> B2[传统ECC因数据依赖错误模式效率低 / Conventional ECC inefficient due to data-dependent error patterns]
        C --> C1[提出ROBIN ECC配置 / Propose ROBIN ECC configuration]
        C --> C2[增量斜交错分布比特翻转 / Incremental oblique interleaving to distribute bit transitions uniformly]
        D --> D1[降低缓存错误率提升超过28.6倍 / Reduces cache error rate increase by >28.6x]
    ```

- **[arXiv260105] Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow**
  - **tags:** [mlsys], [compiler & ir], [electronic-photonic design automation, cross-layer co-design, inverse photonic design, AI-accelerated Maxwell solvers, photonic AI system]
  - **authors:** Hongjian Zhou, Ziang Yin, Jiaqi Gu
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2601.00130
  - **contributions:** 1. Proposed a cross-layer co-design framework for scalable photonic edge AI and Transformer inference architectures. 2. Introduced SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. 3. Developed AI-enabled photonic design automation techniques, including physical AI-based Maxwell solvers and a fabrication-aware inverse design framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76312a14ab1d9b01be6967c891d86f1c36fb6eebdf382d27578721d3ff3e1c24_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of designing electronic-photonic AI systems by proposing an open-source, AI-infused cross-layer co-design and automation framework. The method includes architecture designs for photonic AI, a modeling tool called SimPhony, and AI-powered design automation tools. The work aims to democratize and accelerate the development of next-generation photonic AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Democratizing Electronic-Photonic AI Systems<br>电子-光子AI系统民主化] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Challenging EPIC AI System Design<br>EPIC AI系统设计挑战]
        B --> B2[Lack of Mature EPDA Toolchain<br>缺乏成熟的EPDA工具链]
        C --> C1[Cross-Layer Co-Design Framework<br>跨层协同设计框架]
        C --> C2[SimPhony Modeling Tool<br>SimPhony建模工具]
        C --> C3[AI-Enabled EPDA Stack<br>AI赋能的EPDA堆栈]
        D --> D1[Democratizes Development<br>民主化开发]
        D --> D2[Enables Scalable EPDA<br>实现可扩展EPDA]
    ```

- **[arXiv260105] Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration**
  - **tags:** [mlsys], [others], [photonic AI, electronic-photonic design automation (EPDA), system-algorithm co-exploration, cross-layer toolchain, physical design automation]
  - **authors:** Ziang Yin, Hongjian Zhou, Nicholas Gangi, Meng Zhang, Jeff Zhang, Zhaoran Rena Huang, Jiaqi Gu
  - **institution:** Arizona State University, Rensselaer Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2601.00129
  - **contributions:** 1. Identified three essential considerations for scaling practical photonic AI systems: dynamic tensor operation support, systematic management of overheads, and robustness under hardware non-idealities. 2. Built a cross-layer toolchain (SimPhony, ADEPT, ADEPT-Z, Apollo, LiDAR) for quantitative, physically-grounded co-design from system exploration to physical layout. 3. Established a co-design loop that bridges architectural intent and deployable photonic hardware by translating physical costs into system-level metrics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd0ce87d1ffb64d07fd82b197f024f167052c6fb76c061ecf0885e18f056796_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of scaling photonic AI systems by identifying key design considerations and developing a cross-layer toolchain for system-algorithm co-exploration. The proposed method uses tools like SimPhony and Apollo to model physical costs and automate design, enabling quantitative trade-off analysis under real implementation constraints. The main conclusion is that this approach creates a physically-grounded co-design loop essential for realizing large-scale, deployable photonic AI hardware.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Large-Scale Photonics-Empowered AI Systems<br/>大规模光子赋能AI系统] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Scaling AI constrained by data movement & efficiency<br/>AI扩展受限于数据移动与能效] --> B1[挑战1: 动态张量操作支持<br/>Dynamic tensor operation support]
        B --> B2[挑战2: 开销系统管理<br/>Systematic overhead management]
        B --> B3[挑战3: 硬件非理想性鲁棒性<br/>Robustness under hardware non-idealities]
        C[主要方法/Method<br/>Cross-layer toolchain for co-design<br/>跨层工具链协同设计] --> C1[SimPhony: 实现感知建模<br/>Implementation-aware modeling]
        C --> C2[ADEPT/ADEPT-Z: 电路与拓扑探索<br/>Circuit & topology exploration]
        C --> C3[Apollo/LiDAR: 物理设计自动化<br/>Physical design automation]
        D[关键结果/Results<br/>Quantitative & physically-grounded co-design loop<br/>定量且物理基础的协同设计循环] --> D1[连接系统目标与可行硬件<br/>Connects system objectives to feasible hardware]
        D --> D2[产生可制造布局<br/>Produces manufacturable layouts]
    ```

- **[arXiv260105] Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO**
  - **tags:** [sys], [wireless communication systems], [splitting precoding, subspace selection, quantized refinement, massive MIMO, fronthaul capacity]
  - **authors:** Yasaman Khorsandmanesh, Emil Bjornson, Joakim Jalden
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00616
  - **contributions:** 1. Proposes a novel splitting precoding architecture that separates precoding computation between the Advanced Antenna System (AAS) and Baseband Unit (BBU) to address fronthaul bottlenecks. 2. Introduces a local subspace selection method at the AAS to reduce channel dimensionality before fronthaul transmission. 3. Develops a quantization-aware refinement precoding algorithm at the BBU that operates on the reduced effective channel to optimize performance under limited fronthaul capacity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f66e91bfdb08909724d80ba8bae4b67df9824da0d5fae8eb15678f2e7897b484_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited fronthaul capacity problem in massive MIMO 5G systems by proposing a splitting precoding architecture. The method separates processing between the antenna system (which performs subspace selection) and the baseband unit (which computes quantized refinement precoding), achieving higher spectral efficiency than conventional one-stage precoding approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root["Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO"] --> Problem["核心问题/Problem: Limited fronthaul capacity in massive MIMO 5G architectures"]
    Root --> Method["主要方法/Method: Splitting architecture with AAS subspace selection and BBU quantized refinement precoding"]
    Root --> Results["关键结果/Results: Achieves higher sum spectral efficiency than conventional one-stage precoding"]
    ```
