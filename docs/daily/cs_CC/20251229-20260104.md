---
slug: /daily/cscc/20251229-20260104
---
# 20251229-20260104 (cs.CC)

## 2025-12-29

- **[arXiv251229] A Note on the NP-Hardness of PARTITION Via First-Order Projections**
  - **tags:** [other], [computational complexity theory], [NP-hardness, first-order reductions, AC0 reductions, PARTITION problem, descriptive complexity]
  - **authors:** Paúl Risco Iturralde
  - **institution:** Independent researcher
  - **link:** https://arxiv.org/pdf/2512.21448
  - **contributions:** 1. Demonstrates NP-hardness of the PARTITION problem via first-order projections, 2. Overcomes the obstacle of requiring large sums in the standard reduction by using descriptive complexity techniques, 3. Fills a gap in the literature regarding the hardness of PARTITION under restricted reductions like AC0.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b5d9f8d4b42755b482904c0cf03df329316dc9a10936a82fe27b6ce034a1e56_w640_q70.webp
  - **Simple LLM Summary:** This note addresses the open question of whether the PARTITION problem is NP-hard under restricted reductions like AC0. It modifies classic reductions from 3SAT to SUBSET-SUM to PARTITION, defining them using first-order logical formulas (first-order projections). The main conclusion is that PARTITION is indeed NP-hard via first-order projections, which implies hardness under polynomial-size AC0 reductions, thereby resolving the gap mentioned in prior work.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: A Note on the NP-Hardness of PARTITION Via First-Order Projections] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[PARTITION的NP-hardness在受限归约下是否成立?/Is PARTITION NP-hard under restricted reductions?]
        C --> C1[使用一阶逻辑公式定义归约/Define reductions using first-order logic formulas]
        C --> C2[修改经典归约(3SAT到SUBSET-SUM到PARTITION)/Modify classic reductions (3SAT to SUBSET-SUM to PARTITION)]
        D --> D1[PARTITION对一阶投影是NP-hard的/PARTITION is NP-hard via first-order projections]
        D --> D2[暗示对多项式大小AC0归约也是NP-hard的/Implies NP-hard under polynomial-size AC0 reductions]
        D --> D3[填补了文献中的空白/Fills a gap in the literature]
    ```

- **[arXiv251229] A Note on Avoid vs MCSP**
  - **tags:** [other], [computational complexity theory], [Range Avoidance Problem, Minimal Circuit Size Problem, AM ∩ coAM, Turing reductions]
  - **authors:** Edward A. Hirsch, Ilya Volkovich
  - **institution:** Ariel University, Boston College
  - **link:** https://arxiv.org/pdf/2512.21764
  - **contributions:** 1. Presents an alternative approach to a known result linking languages reducible to the Range Avoidance Problem (Avoid) to the complexity class AM ∩ coAM. 2. Proposes using the Minimal Circuit Size Problem (MCSP) as a potential avenue to derive this containment result. 3. Highlights the connection between two central problems in complexity theory (Avoid and MCSP) for understanding the power of reductions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/918e4b50a4df8fcc5a358172ac9f315b25fd82440914a13e94eb45224863b78a_w640_q70.webp
  - **Simple LLM Summary:** This note explores the complexity of the Range Avoidance Problem (Avoid). It proposes a new potential method, using the Minimal Circuit Size Problem (MCSP), to show that any language reducible to Avoid via deterministic or randomized Turing reductions is contained in the complexity class AM ∩ coAM, offering an alternative to a recent proof.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Note on Avoid vs MCSP"] --> Problem["核心问题/Problem<br>Complexity of Range Avoidance (Avoid)"]
        Root --> Method["主要方法/Method<br>Using Minimal Circuit Size Problem (MCSP)"]
        Root --> Results["关键结果/Results<br>Languages reducible to Avoid are in AM ∩ coAM"]
    ```

- **[arXiv251229] Conserved active information**
  - **tags:** [ai], [information theory], [conserved active information, No-Free-Lunch, KL divergence, search space, information conservation]
  - **authors:** Yanchen Chen, Daniel Andrés Díaz-Pachón
  - **institution:** University of Miami
  - **link:** https://arxiv.org/pdf/2512.21834
  - **contributions:** 1. Introduces conserved active information (I⊕), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I⊕ can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new information-theoretic measure called conserved active information (I⊕) to quantify net information change in search problems while respecting conservation laws. It shows that I⊕ uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Conserved active information] --> Problem[核心问题/Problem: Limitations of average-focused information measures like KL divergence]
        Root --> Method[主要方法/Method: Introduce conserved active information I⊕, a symmetric extension respecting No-Free-Lunch]
        Root --> Results[关键结果/Results: I⊕ reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]
    ```

- **[arXiv251229] Poincaré Duality and Multiplicative Structures on Quantum Codes**
  - **tags:** [other], [quantum error correction], [sheaf codes, Poincaré duality, quantum LDPC codes, transversal gates, cup/cap product]
  - **authors:** Yiming Li, Zimu Li, Zi-Wen Liu, Quynh T. Nguyen
  - **institution:** Tsinghua University, Harvard University
  - **link:** https://arxiv.org/pdf/2512.21922
  - **contributions:** 1. Generalizing Poincaré duality from manifolds to sheaf-based classical and quantum codes, establishing a rigorous duality relationship between chain and cochain complexes. 2. Constructing multiplicative structures (cup and cap products) on sheaved chain complexes, leading to an explicit isomorphism between (co)homology groups. 3. Applying the framework to obtain transversal logical gates (CZ, CCZ, higher-order controlled-Z) on families of good qLDPC and quantum locally testable codes, pointing towards fault-tolerant non-Clifford gates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fcb4213084226768ada35e1a65f3fff10489254aaaafade43391ca7096cc47_w640_q70.webp
  - **Simple LLM Summary:** This paper generalizes Poincaré duality and multiplicative structures from topology to sheaf-based quantum codes. The authors rigorously prove duality relationships and construct cup/cap products, leading to an isomorphism between homology groups. As an application, they demonstrate how to construct transversal logical non-Clifford gates on good quantum LDPC codes, advancing fault-tolerant quantum computing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Poincaré Duality and Multiplicative Structures on Quantum Codes<br>量子代码的庞加莱对偶与乘法结构")
        Root --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Generalize Poincaré duality to codes<br>将对偶性推广至编码")
        Method --> M1("Sheaf theory on cell complexes<br>胞腔复形上的层理论")
        Method --> M2("Build cup/cap products<br>构建杯积/卡积")
        Results --> R1("Duality & isomorphism proven<br>证明对偶与同构")
        Results --> R2("Transversal logical gates<br>横截逻辑门")
    ```

## 2025-12-30

- **[arXiv251230] A Study of NP-Completeness and Undecidable Word Problems in Semigroups**
  - **tags:** [other], [computational complexity theory], [NP-completeness, undecidable word problem, associative calculus, polynomial reducibility, Turing machines]
  - **authors:** Duaa Abdullah, Jasem Hamoud
  - **institution:** Moscow Institute of Physics and Technology
  - **link:** https://arxiv.org/pdf/2512.22123
  - **contributions:** 1. Explores the relationship between complexity classes P and NP and the concept of polynomial reducibility. 2. Demonstrates the construction of an associative calculus (semigroup) with an algorithmically undecidable word problem. 3. Establishes a direct connection between a Turing machine computing a non-recursive function and the equivalence condition in the constructed calculus, linking computational complexity and algebraic undecidability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9ade75d90324f1eb4f6ee92ec609932e78897f10c8103547d100966beb841a9_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates fundamental limits in computation by studying NP-completeness and undecidable problems. It constructs an associative calculus whose word problem is undecidable, linking it to a Turing machine that computes a non-recursive function. The work highlights the intrinsic boundaries of algorithmic solutions by connecting computational complexity theory with algebraic undecidability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Study of NP-Completeness and Undecidable Word Problems in Semigroups"] --> Problem["核心问题/Problem<br>Computational complexity & decidability limits"]
        Root --> Method["主要方法/Method<br>Polynomial reducibility & Associative calculus construction"]
        Root --> Results["关键结果/Results<br>Undecidable word problem linked to non-recursive Turing machine"]
    ```

- **[arXiv251230] Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width**
  - **tags:** [other], [parameterized complexity], [tropical circuits, pathwidth, communication complexity, Traveling Salesperson Problem, dynamic programming]
  - **authors:** Kacper Kluk, Jesper Nederlof
  - **institution:** University of Warsaw, Utrecht University
  - **link:** https://arxiv.org/pdf/2512.23121
  - **contributions:** 1. Proves unconditional lower bounds on the size of tropical circuits (modeling pure dynamic programming) for solving connectivity problems like TSP on graphs of bounded pathwidth. 2. Establishes a connection between tropical circuit complexity and the nondeterministic communication complexity of compatibility matrices. 3. Shows that any tropical circuit for TSP on a certain graph of pathwidth k requires at least 2^Ω(k log log k) gates, which is higher than known algebraic algorithms, suggesting algebra is necessary for competitive worst-case times.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86590df9819e19ad7303e5df124f5b2189c5e6ab6d542206119582e1e6c83135_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the limitations of pure dynamic programming, modeled by tropical circuits, for solving connectivity problems like the Traveling Salesperson Problem on graphs with small pathwidth. It proves an unconditional lower bound of 2^Ω(k log log k) gates for any tropical circuit solving TSP on a specific graph of pathwidth k. This result, established via a link to communication complexity, suggests that algebraic techniques are unavoidable for achieving the fastest known worst-case running times for these problems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[评估纯动态规划对连通性问题的能力/Assess capability of pure DP for connectivity problems]
        C --> C1[将热带电路复杂度与通信复杂性关联/Link tropical circuit complexity to communication complexity]
        D --> D1[证明下界高于已知代数算法/Prove lower bound higher than known algebraic algorithms]
    ```

- **[arXiv251230] Pseudodeterministic Algorithms for Minimum Cut Problems**
  - **tags:** [other], [graph algorithms], [pseudodeterministic algorithms, global minimum cut, minimum s-t cut, streaming algorithms, cut-query models]
  - **authors:** Aryan Agarwala, Nithin Varma
  - **institution:** Max-Planck-Institut für Informatik, Saarland Informatics Campus; University of Cologne
  - **link:** https://arxiv.org/pdf/2512.23468
  - **contributions:** 1. Presents efficient pseudodeterministic algorithms for the global minimum cut and minimum s-t cut problems. 2. Achieves an asymptotic running time for global minimum cut that is better than the fastest known sequential deterministic algorithm. 3. Implements the algorithm in multiple computational models (sequential, streaming, PRAM, cut-query) where efficient deterministic algorithms were previously unknown.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c06a5f7252dc9f1337880cbdb49ac90f3b180fa63fe8a5b0eb2c22a487cedbbd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces pseudodeterministic algorithms for finding global minimum cuts and minimum s-t cuts in graphs. The proposed method offers replicability by consistently outputting the same answer with high probability, while being faster than the best known deterministic algorithm for global minimum cut. The algorithms are also successfully adapted to work in sequential, streaming, PRAM, and cut-query models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pseudodeterministic Algorithms for Minimum Cut Problems] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(确定性算法效率低 / Deterministic algorithms are inefficient)
        B --> B2(随机算法输出不一致 / Randomized algorithms lack replicability)
        C --> C1(伪确定性算法 / Pseudodeterministic Algorithms)
        C --> C2(高概率输出相同解 / Outputs same solution with high probability)
        D --> D1(渐近更快 / Asymptotically faster)
        D --> D2(多模型实现 / Implemented in multiple models)
    ```

- **[arXiv251230] Coloring Hardness on Low Twin-Width Graphs**
  - **tags:** [other], [graph theory, computational complexity], [twin-width, graph coloring, NP-hardness, computational complexity, graph classes]
  - **authors:** Édouard Bonnet
  - **institution:** Univ Lyon, CNRS, ENS de Lyon, Université Claude Bernard Lyon 1, LIP UMR5668
  - **link:** https://arxiv.org/pdf/2512.23680
  - **contributions:** 1. Proves that the Min Coloring problem is NP-hard on the class of graphs with twin-width at most 3 (T3). 2. Proves that for every k &gt;= 3, the k-Coloring problem is NP-hard on the class of graphs with twin-width at most 4 (T4). 3. Provides structural observations about the T3 and T4 classes, highlighting their distinct properties and raising open questions about complexity transitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77a33f460e61bf2610b2bc5a51fb237df8e78440e7279dd902d4d1e4a4dbc60_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the computational complexity of graph coloring problems on graphs with low twin-width. It proves that Min Coloring is NP-hard on graphs of twin-width at most 3, and that k-Coloring is NP-hard on graphs of twin-width at most 4 for all k&gt;=3. These results establish the first hardness for a problem on T3 that is easy on simpler graph classes like trees and cographs.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Coloring Hardness on Low Twin-Width Graphs] --> B[核心问题/Problem: Coloring complexity on bounded twin-width graphs]
    A --> C[主要方法/Method: NP-hardness proofs via reductions]
    A --> D[关键结果/Results: Min Coloring hard on T3, k-Coloring hard on T4]
    ```

## 2026-01-01

- **[arXiv260101] Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds**
  - **tags:** [other], [parameterized complexity], [kidney exchange, FPT algorithm, W[1]-hardness, pathwidth, treewidth]
  - **authors:** Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu
  - **institution:** National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur
  - **link:** https://arxiv.org/pdf/2512.24037
  - **contributions:** 1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) ≈ O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>肾脏交换：更快的参数化算法与更紧的下界"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Kidney exchange is NP-complete<br>肾脏交换问题是NP完全问题"] --> P1["限制/Constraint<br>Exchange via small cycles & paths<br>通过小环和路径交换"]
        Method["主要方法/Method<br>Parameterized Complexity<br>参数化复杂度"] --> M1["参数/Parameter<br>Number of patients (t)<br>患者数量(t)"]
        Method --> M2["参数/Parameter<br>Graph pathwidth<br>图路径宽度"]
        Results["关键结果/Results"] --> R1["算法改进/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT算法: O*((4e)^t)"]
        Results --> R2["下界/Lower Bound<br>W[1]-hard for pathwidth<br>对路径宽度是W[1]-难的"]
    ```

- **[arXiv260101] From FPT Decision to FPT Enumeration**
  - **tags:** [other], [parameterized complexity], [fixed-parameter tractable (FPT), enumeration, DelayP, kernelization, branching]
  - **authors:** Nadia Creignou, Timo Camillo Merkl, Reinhard Pichler, Daniel Unterberger
  - **institution:** Aix Marseille Université, TU Wien
  - **link:** https://arxiv.org/pdf/2512.24137
  - **contributions:** 1. Proposes a framework for studying how to extend FPT decision algorithms to FPT enumeration algorithms. 2. Inspects fundamental FPT design approaches (e.g., kernelization, branching) for their potential to yield enumeration algorithms. 3. Presents ideas and methodologies for transforming decision/optimization FPT techniques into ones suitable for enumerating all solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ea5a12796e6b1045900cdb4da84a92c69ff6ce730801a1cf14697d7fae1b84_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in applying fixed-parameter tractable (FPT) algorithms to enumeration problems. It investigates how fundamental techniques for designing FPT decision algorithms, such as kernelization and branching, can be adapted to create FPT algorithms for enumerating all solutions. The main conclusion is that a systematic methodology can be developed to bridge FPT decision and FPT enumeration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From FPT Decision to FPT Enumeration] --> B[核心问题/Problem: Intractable enumeration problems lack FPT algorithmic focus.]
        A --> C[主要方法/Method: Inspect and extend fundamental FPT design approaches for enumeration.]
        A --> D[关键结果/Results: Presents ideas and a framework for turning FPT decision into FPT enumeration algorithms.]
    ```

- **[arXiv260101] Diffusion Language Models are Provably Optimal Parallel Samplers**
  - **tags:** [mlsys], [diffusion models], [diffusion language models, parallel sampling, chain-of-thought, remasking, revision]
  - **authors:** Haozhe Jiang, Nika Haghtalab, Lijie Chen
  - **institution:** University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.25014
  - **contributions:** 1. Formalized a model of parallel sampling and proved that DLMs with CoT can simulate any parallel sampling algorithm with an optimal number of sequential steps. 2. Showed that enabling remasking or revision with CoT allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. 3. Established a strict expressivity gap, proving DLMs with revision or remasking are strictly more expressive than those without.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp
  - **Simple LLM Summary:** This paper provides a theoretical foundation for the efficiency of Diffusion Language Models (DLMs) as parallel samplers. It proves that DLMs augmented with chain-of-thought reasoning can simulate any parallel sampling algorithm with optimal sequential steps and, when further equipped with token remasking or revision, also achieve optimal space complexity. The results theoretically justify DLMs as highly efficient parallel samplers and advocate for enabling revision capabilities in such models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Diffusion Language Models are Provably Optimal Parallel Samplers<br>扩散语言模型是可证明最优的并行采样器"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>DLMs的理论优势与效率极限未明<br>Theoretical advantages and efficiency limits of DLMs are unclear"] --> P1["并行采样效率/Parallel Sampling Efficiency"]
        Problem --> P2["空间复杂度/Space Complexity"]
        Method["主要方法/Method<br>形式化并行采样模型与电路复杂度<br>Formalize parallel sampling model & circuit complexity"] --> M1["增强CoT/Augment with CoT"]
        Method --> M2["引入重掩码或修订/Introduce Remasking or Revision"]
        Results["关键结果/Results"] --> R1["最优顺序步骤/Optimal Sequential Steps"]
        Results --> R2["最优空间复杂度/Optimal Space Complexity"]
        Results --> R3["严格表达能力差距/Strict Expressivity Gap"]
    ```

- **[arXiv260101] Thin Tree Verification is coNP-Complete**
  - **tags:** [other], [computational complexity], [thin tree, coNP-complete, spanning tree, graph cuts, edge-connectivity]
  - **authors:** Alice Moayyedi
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.25043
  - **contributions:** 1. Proves that the problem of verifying if a given spanning tree is α-thin is coNP-hard, resolving an open question about its computational complexity. 2. Provides a formal proof for a problem previously speculated to be NP-hard or lacking a polynomially-checkable certificate. 3. Establishes a negative result (coNP-hardness) that impacts the algorithmic prospects related to the Thin Tree Conjecture and its applications, such as approximating the Asymmetric Travelling Salesman Problem (ATSP).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57fee7ec29f3a9f8d8859080b76ea70b44eeb72ddf00dece29f07228a749a454_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the computational complexity of verifying whether a given spanning tree in a graph is α-thin, meaning it contains at most an α proportion of edges in any graph cut. The authors prove that this verification problem is coNP-hard. This result shows that efficiently checking a candidate solution is computationally difficult, which has implications for the Thin Tree Conjecture and related approximation algorithms for problems like ATSP.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Thin Tree Verification is coNP-Complete] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[验证给定生成树是否为α-thin树/Verify if a given spanning tree is α-thin]
        C --> C1[证明复杂度下界/Prove complexity lower bound]
        D --> D1[验证问题是coNP-hard的/Verification problem is coNP-hard]
    ```

- **[arXiv260101] Syndrome aware mitigation of logical errors**
  - **tags:** [other], [quantum error correction and mitigation], [logical error mitigation, syndrome-aware, fault-tolerance, runtime overhead, quantum error correction]
  - **authors:** Dorit Aharonov, Yosi Atia, Eyal Bairey, Zvika Brakerski, Itsik Cohen, Omri Golan, Ilya Gurwich, Netanel H. Lindner, Maor Shutman
  - **institution:** Qedma Quantum Computing, Hebrew University, Weizmann Institute of Science, Technion
  - **link:** https://arxiv.org/pdf/2512.23810
  - **contributions:** 1. Introduces Syndrome-Aware Logical Error Mitigation (SALEM), a novel method that leverages syndrome data from error correction to mitigate logical errors. 2. Demonstrates that SALEM achieves an exponentially lower runtime overhead compared to prior logical error mitigation schemes, enabling the execution of significantly larger logical circuits. 3. Shows that SALEM can outperform physical error mitigation even above the standard fault-tolerance threshold, making error correction useful in physical error rate regimes where it was previously considered ineffective.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28c546f1791a48c23de235687f3df199f96775597dcac09ee65b2ab0f41d74e5_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Syndrome-Aware Logical Error Mitigation (SALEM), a method that uses syndrome data from quantum error correction to more efficiently mitigate residual logical errors. SALEM drastically reduces the runtime overhead compared to previous approaches, allowing for the accurate execution of much larger quantum circuits. The work demonstrates that this tight integration of error correction and mitigation can be beneficial even when physical error rates are above the conventional fault-tolerance threshold.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Syndrome aware mitigation of logical errors] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[有限物理比特导致残留逻辑错误/Limited physical qubits cause residual logical errors]
        C --> C1[利用纠错伴随的综合征数据进行缓解/Use syndrome data from error correction for mitigation]
        D --> D1[运行时开销指数级降低/Runtime overhead reduced exponentially]
        D --> D2[可执行电路规模数量级提升/Executable circuit volume increased by orders of magnitude]
        D --> D3[可在容错阈值以上超越物理错误缓解/Can outperform physical error mitigation above fault-tolerance threshold]
    ```

- **[arXiv260101] Proper colorings of a graph in linear time using a number of colors linear in the maximum degree of the graph**
  - **tags:** [other], [graph algorithms], [proper coloring, graph sampling, linear-time algorithm, maximum degree, Markov chain Monte Carlo]
  - **authors:** Kritika Bhandari, Mark Huber
  - **institution:** None (Inferred from arXiv submission; no explicit affiliation provided on first page)
  - **link:** https://arxiv.org/pdf/2512.24522
  - **contributions:** 1. A new algorithm for exact sampling from the set of proper colorings of a graph., 2. The algorithm achieves an expected running time linear in the graph size for graphs with maximum degree Δ., 3. It is the first algorithm with this guarantee when the number of colors exceeds 3.637Δ + 1.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2df3f6d8f9edf2c385953da77f4effcefb8b3b532fac0ef601829b8505fab4b3_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a new algorithm for exactly sampling proper colorings of a graph. It is the first algorithm whose expected running time is guaranteed to be linear in the graph size when the number of colors is greater than 3.637 times the graph's maximum degree plus one.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Proper colorings of a graph in linear time<br/>图的线性时间正常着色] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Exact sampling from proper colorings<br/>从正常着色中精确采样]
        C --> C1[New sampling algorithm<br/>新的采样算法]
        D --> D1[Linear expected runtime<br/>线性期望运行时间]
        D --> D2[Colors > 3.637Δ + 1<br/>颜色数 > 3.637Δ + 1]
    ```

- **[arXiv260101] Approximate Computation via Le Cam Simulability**
  - **tags:** [other], [computational complexity theory], [Le Cam deficiency, computational deficiency, LeCam-P, approximate reduction, statistical experiments]
  - **authors:** Deniz Akdemir
  - **institution:** Not explicitly stated in the provided content. The author is Deniz Akdemir, but no affiliation or email domain is given.
  - **link:** https://arxiv.org/pdf/2512.24860
  - **contributions:** 1. Proposes a decision-theoretic framework for computational complexity based on Le Cam simulability and statistical experiments, shifting focus from syntactic exactness to semantic approximation. 2. Defines computational deficiency (δ_poly) and uses it to construct the complexity class LeCam-P (Decision-Robust Polynomial Time) for problems that are semantically easy to approximate. 3. Establishes the No-Free-Transfer Inequality, showing that strictly invariant representations inevitably destroy decision-relevant information.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eb21a7edcb8e5cb8e1788fcdd488db8897d0be53c937e14e03661081a0b7652_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new framework for computational complexity that views computation as the efficient simulation of a target statistical experiment with bounded risk distortion (Le Cam deficiency). It introduces the concept of computational deficiency and the class LeCam-P to characterize problems that are hard to solve exactly but easy to approximate for decision-making. The main conclusion is that this framework bridges algorithmic complexity and decision theory, showing that classical exact reductions are a special case of zero-deficiency simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Approximate Computation via Le Cam Simulability] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[经典精确计算理论对现代近似需求限制/Classical exactness paradigm is restrictive for modern approximate needs]
        C --> C1[基于Le Cam缺陷的统计实验模拟框架/Framework based on Le Cam deficiency for simulating statistical experiments]
        C --> C2[定义计算缺陷与LeCam-P类/Define computational deficiency and LeCam-P class]
        D --> D1[Karp归约是零缺陷模拟的特例/Karp reductions are special cases of zero-deficiency simulations]
        D --> D2[提出无免费转移不等式/Propose No-Free-Transfer Inequality]
        D --> D3[连接算法复杂度与决策理论/Bridge algorithmic complexity and decision theory]
    ```
