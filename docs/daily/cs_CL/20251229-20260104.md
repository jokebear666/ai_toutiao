---
slug: /daily/cscl/20251229-20260104
---
# 20251229-20260104 (cs.CL)

## 2025-12-29

- **[arXiv251229] Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks**
  - **tags:** [db], [text-to-SQL], [unanswerable question detection, few-shot prompting, biomedical databases]
  - **authors:** Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger
  - **institution:** Zurich University of Applied Sciences, University of Zurich
  - **link:** https://arxiv.org/pdf/2512.21345
  - **contributions:** 1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem") --> P1("Text-to-SQL对不可回答查询生成可执行SQL/Text-to-SQL generates executable SQL for unanswerable queries")
        P1 --> P2("生物医学领域风险高/High risk in biomedical contexts")
        Method("主要方法/Method") --> M1("Query Carefully 管道/Query Carefully pipeline")
        M1 --> M2("LLM (llama3.3:70b) + 模式感知提示 + 少样本/LLM (llama3.3:70b) + schema-aware prompts + few-shot")
        M2 --> M3("包含可回答与不可回答示例/Includes answerable and unanswerable examples")
        Results("关键结果/Results") --> R1("构建OncoMX-NAQ基准/Built OncoMX-NAQ benchmark")
        R1 --> R2("不可回答检测准确率0.8/Unanswerable-detection accuracy 0.8")
        R2 --> R3("结构性问题检测好，语义模糊挑战大/Good for structural, challenging for semantic ambiguity")
    ```

- **[arXiv251229] Teaching People LLM's Errors and Getting it Right**
  - **tags:** [nlp], [human-ai interaction], [overreliance, failure patterns, mental models, user study, meta-labels]
  - **authors:** Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasović
  - **institution:** University of Utah
  - **link:** https://arxiv.org/pdf/2512.21422
  - **contributions:** 1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness—assessing a user's ability to anticipate LLM errors using taught patterns—which showed a positive effect, unlike traditional human-AI team accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Teaching People LLM’s Errors and Getting it Right] --> B[核心问题/Problem: Users overrely on LLMs due to inaccurate mental models]
        A --> C[主要方法/Method: Analyze failure pattern existence, test discovery methods, propose new evaluation metric]
        A --> D[关键结果/Results: Patterns exist, discovery methods are mixed, new metric shows teaching is effective]
    ```

- **[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models**
  - **tags:** [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]
  - **authors:** Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi
  - **institution:** Institute of Intelligent Systems and Robotics, Sorbonne University
  - **link:** https://arxiv.org/pdf/2512.21439
  - **contributions:** 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]
        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]
        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]
    ```

- **[arXiv251229] Oogiri-Master: Benchmarking Humor Understanding via Oogiri**
  - **tags:** [nlp], [humor understanding], [Oogiri, benchmark, linguistic analysis, incongruity resolution, insight-augmented prompting]
  - **authors:** Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura
  - **institution:** CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo
  - **link:** https://arxiv.org/pdf/2512.21494
  - **contributions:** 1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Oogiri-Master: Benchmarking Humor Understanding via Oogiri] --> B[核心问题/Problem: What makes Oogiri responses funny to humans?]
        A --> C[主要方法/Method: Introduce Oogiri-Master benchmark and Oogiri-Corpus dataset with diverse responses and independent ratings]
        A --> D[关键结果/Results: LLMs approach human performance; insight-augmented prompting improves results]
    ```

- **[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding**
  - **tags:** [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]
  - **authors:** Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson
  - **institution:** Dartmouth College
  - **link:** https://arxiv.org/pdf/2512.21506
  - **contributions:** 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]
        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]
        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]
    ```

- **[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training**
  - **tags:** [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]
  - **authors:** Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren
  - **institution:** Ant Group, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.21515
  - **contributions:** 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of "perplexity landscapes" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Perplexity-Aware Data Scaling Law<br>困惑度感知数据缩放定律] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CPT中单纯增加数据收益递减<br>Diminishing returns from scaling data in CPT]
        C --> C1[提出基于困惑度景观的缩放定律<br>Propose perplexity-landscape-based scaling law]
        C1 --> C2[利用困惑度量化知识差距<br>Use perplexity to quantify knowledge gap]
        C2 --> C3[自适应选择高价值数据子集<br>Adaptively select high-utility data subsets]
        D --> D1[识别接近最优的训练子集<br>Identifies near-optimal training subsets]
        D1 --> D2[在领域基准上取得优越性能<br>Achieves superior performance on domain benchmarks]
    ```

- **[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures**
  - **tags:** [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]
  - **authors:** Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li
  - **institution:** NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.21551
  - **contributions:** 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp
  - **Simple LLM Summary:** This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-AI Interaction Alignment] --> B[核心问题/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]
        A --> C[主要方法/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]
        A --> D[关键结果/Results: Establishes agenda for reciprocal, responsible human-AI futures]
    ```

- **[arXiv251229] Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management**
  - **tags:** [mlsys], [agent system], [external memory, sequential decision-making, value functions, uncertainty estimators, hierarchical storage]
  - **authors:** Changzhi Sun, Xiangyu Chen, Jixiang Luo, Dell Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.21567
  - **code:** https://github.com/TeleAI-UAGI/telemem
  - **contributions:** 1. Proposes a decision-theoretic reframing of agent memory management as a sequential decision-making problem under uncertainty, 2. Introduces the DAM framework that decomposes memory operations into immediate access and hierarchical maintenance, 3. Provides a foundation for future research by evaluating operations via value functions and uncertainty estimators for long-term utility and risk
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d30748df565a671b29899dffbfb153dca58fed0398e13cc6871dfe6f450f11a1_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current heuristic-based memory management for LLM agents is inadequate due to delayed and uncertain utility. It proposes DAM, a decision-theoretic framework that uses value functions and uncertainty estimators to make memory decisions based on long-term consequences. The main contribution is a principled reframing of the problem to guide future research on uncertainty-aware memory systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[启发式内存管理缺乏对长期和不确定后果的洞察/Heuristic memory management lacks insight into long-term & uncertain consequences]
        C --> C1[提出DAM框架，将内存管理视为序列决策问题/Propose DAM framework, viewing memory as a sequential decision problem]
        C --> C2[使用价值函数和不确定性估计器评估操作/Evaluate operations via value functions & uncertainty estimators]
        D --> D1[原则性重构，为不确定性感知内存系统奠定基础/Principled reframing, provides foundation for uncertainty-aware memory systems]
    ```

- **[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid**
  - **tags:** [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]
  - **authors:** Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng
  - **institution:** Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers
  - **link:** https://arxiv.org/pdf/2512.21577
  - **contributions:** 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Problem["核心问题/Problem"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Method["主要方法/Method"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Results["关键结果/Results"]
        Problem --> P1["Hallucination persists in LLMs / 幻觉在LLM中持续存在"]
        Method --> M1["Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模"]
        Method --> M2["Framework: reference world & conflict policy / 框架：参考世界与冲突策略"]
        Results --> R1["Clarifies evaluation & terminology / 澄清评估与术语"]
        Results --> R2["Proposes new benchmark plans / 提出新基准计划"]
    ```

- **[arXiv251229] Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM**
  - **tags:** [nlp], [multilingual language modeling], [two-stage pre-training, cross-lingual alignment, English enrichment, cost-efficient training, Russian LLM]
  - **authors:** Alexander Podolskiy, Semen Molokov, Timofey Gerasin, Maksim Titov, Alexey Rukhovich, Artem Khrapov, Kirill Morozov, Evgeny Tetin, Constantine Korikov, Pavel Efimov, Polina Lazukova, Yuliya Skripkar, Nikita Okhotnikov, Irina Piontkovskaya, Meng Xiaojun, Zou Xueyi, Zhang Zhenhe
  - **institution:** Gamayun Team
  - **link:** https://arxiv.org/pdf/2512.21580
  - **contributions:** 1. Introduces a novel two-stage pre-training strategy (balanced multilingual training followed by high-quality English enrichment) for efficient cross-lingual knowledge transfer. 2. Presents Gamayun, a 1.5B-parameter multilingual LLM trained from scratch on 2.5T tokens, designed for resource-constrained environments. 3. Demonstrates state-of-the-art performance for its size (1-2B parameters) on Russian benchmarks and competitive results on English and multilingual tasks, despite a significantly smaller training budget than comparable models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b7ebd17e8a0b7536938c0d12aa8812a6376542abde4f40239a4622472c17ea0_w640_q70.webp
  - **Simple LLM Summary:** This paper presents Gamayun, a cost-efficient 1.5B-parameter multilingual language model. It addresses the lack of small non-English-centric LLMs through a novel two-stage pre-training strategy for cross-lingual alignment. The model achieves state-of-the-art results in Russian and outperforms larger models on many tasks, despite being trained on far fewer tokens.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Gamayun's Path to Multilingual Mastery<br/>Gamayun的多语言精通之路] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Lack of small, efficient, non-English-centric LLMs<br/>缺乏小型、高效、非英语中心的LLM]
        C[主要方法/Method<br/>Two-stage pre-training<br/>两阶段预训练<br/>1. Balanced multilingual training<br/>平衡多语言训练<br/>2. High-quality English enrichment<br/>高质量英语增强]
        D[关键结果/Results<br/>Outperforms LLaMA3.2-1B & Qwen2.5-1.5B<br/>超越LLaMA3.2-1B和Qwen2.5-1.5B<br/>SOTA in Russian (MERA)<br/>俄语任务达到SOTA]
    ```

- **[arXiv251229] Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards**
  - **tags:** [ai], [reinforcement learning], [RLVR, sample polarity, advantage shaping, policy optimization, reasoning models]
  - **authors:** Xinyu Tang, Yuliang Zhan, Zhixun Li, Wayne Xin Zhao, Zhenduo Zhang, Zujie Wen, Zhiqiang Zhang, Jun Zhou
  - **institution:** Renmin University of China, The Chinese University of Hong Kong, Ant Group
  - **link:** https://arxiv.org/pdf/2512.21625
  - **contributions:** 1. A systematic investigation into the distinct roles of positive and negative samples (sample polarity) in RLVR training dynamics, showing positive samples sharpen existing patterns while negative samples encourage exploration. 2. An exploration of how adjusting advantage values for different sample polarities at both the sample and token levels affects training. 3. The proposal of A3PO, an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, which precisely allocates advantage signals to key tokens.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66a5d8d013ee86ee35c80048dbd4d2b03bd023a52b180e92f678fb36aa1f6018_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the distinct roles of positive and negative samples in Reinforcement Learning with Verifiable Rewards (RLVR) for training large reasoning models. It finds positive samples refine correct patterns while negative samples promote exploration, and proposes a new method called A3PO for adaptive, asymmetric token-level advantage shaping. Experiments on five reasoning benchmarks demonstrate the effectiveness of the proposed approach.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Rethinking Sample Polarity in RLVR] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[RLVR中正负样本的角色?/Roles of +/- samples in RLVR?]
        Method[主要方法/Method] --> M1[分析样本极性/Analyze Sample Polarity]
        Method --> M2[提出A3PO方法/Propose A3PO Method]
        Results[关键结果/Results] --> R1[正样本锐化模式/Positive samples sharpen patterns]
        Results --> R2[负样本鼓励探索/Negative samples encourage exploration]
        Results --> R3[A3PO有效/A3PO is effective]
    ```

- **[arXiv251229] Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations**
  - **tags:** [nlp], [hallucination evaluation], [HIC-Bench, Intelligent Hallucinations, Defective Hallucinations, Torrance Tests of Creative Thinking, Dynamic Hallucination Prompt]
  - **authors:** Chengxu Yang, Jingling Yuan, Siqi Cai, Jiawei Jiang, Chuang Hu
  - **institution:** Wuhan University of Technology, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.21635
  - **code:** https://github.com/chujiguangniao/HIC-bench
  - **contributions:** 1. Proposes HIC-Bench, a novel evaluation framework that categorizes LLM hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH) for systematic study. 2. Introduces a structured multi-dimensional assessment matrix combining TTCT creativity metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation). 3. Features cross-domain applicability across ten scientific domains and a Dynamic Prompt Optimization technique (DHP) to guide model outputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb581f42c65746c28519ac82f0996a0d7ab8f6413a85636583ec1e0abecba3c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating LLM hallucinations beyond factual errors by proposing HIC-Bench, a framework that distinguishes between creative (Intelligent) and erroneous (Defective) hallucinations using a multi-metric assessment. It demonstrates that creativity and correctness can be jointly optimized, revealing a nonlinear relationship between the two types of hallucinations and positioning intelligent hallucinations as a catalyst for scientific innovation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法难以平衡LLM幻觉的创造性与准确性/Existing methods struggle to balance creativity and accuracy in LLM hallucinations]
        C --> C1[提出HIC-Bench评估框架/Propose HIC-Bench evaluation framework]
        C1 --> C2[分类智能与缺陷幻觉/Categorize IH and DH]
        C1 --> C3[多维度评估矩阵/Multi-dimensional metric matrix]
        C1 --> C4[动态提示优化/Dynamic Prompt Optimization]
        D --> D1[创造力与正确性可共同优化/Creativity and correctness can be jointly optimized]
        D --> D2[智能幻觉是创造力的催化剂/IH is a catalyst for creativity]
    ```

- **[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression**
  - **tags:** [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]
  - **authors:** Liuyang Bai, Weiyi Lu, Li Guo
  - **institution:** NYU Shanghai
  - **link:** https://arxiv.org/pdf/2512.21653
  - **contributions:** 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --> Problem["核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]
        Root --> Method["主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]
        Root --> Results["关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]
    ```

- **[arXiv251229] Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech**
  - **tags:** [nlp], [spoken dialogue systems], [Graph-of-Thoughts, full-duplex, speech acts, causal inference, multimodal transformer]
  - **authors:** Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian
  - **institution:** Zhejiang University, University of California, Berkeley, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.21706
  - **code:** https://got-duplex.github.io/
  - **contributions:** 1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Enabling Conversational Behavior Reasoning in Full-Duplex Speech<br/>实现全双工语音对话行为推理"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Current systems lack explicit reasoning for conversational behaviors."]
        Method["主要方法/Method<br/>Model reasoning as causal inference in a Graph-of-Thoughts (GoT)."]
        Results["关键结果/Results<br/>Robust behavior detection and interpretable reasoning chains."]
    ```

- **[arXiv251229] Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers**
  - **tags:** [nlp], [ai-generated text detection], [transformer, fine-tuning, zero-shot, Bengali, paraphrase detection]
  - **authors:** Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan
  - **institution:** Not specified in provided content.
  - **link:** https://arxiv.org/pdf/2512.21709
  - **contributions:** 1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Detecting AI-Generated Paraphrases in Bengali] --> B[核心问题/Problem: LLM misuse & lack of Bengali detection research]
        A --> C[主要方法/Method: Compare 5 transformers (Zero-Shot vs. Fine-Tuned)]
        A --> D[关键结果/Results: Fine-tuning needed; XLM-R, mDeBERTa, mBERT achieve ~91% accuracy]
    ```

- **[arXiv251229] MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles**
  - **tags:** [mlsys], [agent system], [Parameter-Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), Mixture-of-Roles (MoR), Agent Tuning, Data Generation Pipeline]
  - **authors:** Jing Han, Binwei Yan, Tianyu Guo, Zheyuan Bai, Mengyu Zheng, Hanting Chen, Ying Nie
  - **institution:** Beijing University of Posts and Telecommunications, Huawei Noah's Ark Lab
  - **link:** https://arxiv.org/pdf/2512.21708
  - **code:** https://mor-agent.github.io/
  - **contributions:** 1. Decomposes agent capabilities into three distinct roles (reasoner, executor, summarizer) based on the Reason+Action paradigm. 2. Proposes the Mixture-of-Roles (MoR) framework, which uses three specialized LoRA groups, each dedicated to a specific role, to collaboratively accomplish agent tasks. 3. Develops a multi-role data generation pipeline for effective fine-tuning, incorporating role-specific content completion and reliability verification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c303f17ce31b4315bdd80c394b9ba486dc14690a542d268175927115df139559_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underexplored area of parameter-efficient fine-tuning (PEFT) for AI agents. It proposes MoRAgent, a framework that decomposes agent tasks into three roles (reasoner, executor, summarizer) and assigns a specialized LoRA module to each, enabling efficient and collaborative task completion. Extensive experiments demonstrate the method's effectiveness in tuning LLMs for agent tasks while maintaining parameter efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: PEFT for agent tasks is largely unexplored] --> P1[挑战/Challenges: Full fine-tuning is resource-heavy and harms general capabilities]
        Method[主要方法/Method: Mixture-of-Roles (MoR) Framework] --> M1[策略1/Strategy 1: Decompose agent into three roles]
        M1 --> M1_1[角色/Roles: Reasoner, Executor, Summarizer]
        Method --> M2[策略2/Strategy 2: Three specialized LoRA groups for the three roles]
        Method --> M3[策略3/Strategy 3: Multi-role data generation pipeline]
        Results[关键结果/Results: Effectiveness demonstrated] --> R1[实验/Experiments: Extensive tests on various LLMs & benchmarks]
    ```

- **[arXiv251229] Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought**
  - **tags:** [nlp], [interpretability & analysis], [latent tokens, chain-of-thought, model reliability, causal analysis, shortcut learning]
  - **authors:** Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21711
  - **contributions:** 1. Introduces "Steering Experiments" to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts "Shortcut Experiments" to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a "pseudo-reasoning" mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Latent token mechanisms unclear, reliability concerns] --> B1[潜在令牌机制不明确/Unclear latent token mechanisms]
        B --> B2[可靠性问题/Reliability concerns]
        C[主要方法/Method: Causal & adversarial analysis] --> C1[引导实验/Steering experiments]
        C --> C2[捷径实验/Shortcut experiments]
        D[关键结果/Results: COCONUT is pseudo-reasoning] --> D1[令牌对扰动不敏感/Tokens insensitive to perturbation]
        D --> D2[利用数据集捷径/Exploits dataset shortcuts]
        D --> D3[性能提升不基于真实推理/Performance gains not from true reasoning]
    ```

- **[arXiv251229] CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation**
  - **tags:** [nlp], [dialogue systems], [theme detection, topic clustering, hierarchical generation]
  - **authors:** Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li
  - **institution:** The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.21715
  - **contributions:** 1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CATCH: 可控主题检测框架 / Controllable Theme Detection Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题 / Problem] --> P1[短话语稀疏语义 / Sparse, short utterances]
        Problem --> P2[跨对话主题对齐 / Cross-dialogue theme alignment]
        Problem --> P3[用户偏好整合 / Personalized user preferences]
        Method[主要方法 / Method] --> M1[上下文感知主题表示 / Context-aware topic representation]
        Method --> M2[偏好引导主题聚类 / Preference-guided topic clustering]
        Method --> M3[分层主题生成 / Hierarchical theme generation]
        Results[关键结果 / Results] --> R1[在DSTC-12基准测试有效 / Effective on DSTC-12 benchmark]
        Results --> R2[提升聚类与生成质量 / Improved clustering & generation quality with 8B LLM]
    ```

- **[arXiv251229] An Information Theoretic Perspective on Agentic System Design**
  - **tags:** [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]
  - **authors:** Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.21720
  - **contributions:** 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance")
        C --> C1("提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator")
        D --> D1("更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate")
        D --> D2("扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors")
    ```

- **[arXiv251229] Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation**
  - **tags:** [nlp], [machine translation], [dialectal arabic, modern standard arabic, post-editing evaluation, error taxonomy, human evaluation]
  - **authors:** Abdullah Alabdullah, Lifeng Han, Chenghua Lin
  - **institution:** University of Edinburgh, University of Manchester, Leiden University Medical Center (LUMC) / Leiden University
  - **link:** https://arxiv.org/pdf/2512.21787
  - **contributions:** 1. Introduces Ara-HOPE, a human-centric post-editing evaluation framework specifically designed for Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation. 2. Proposes a five-category error taxonomy and a decision-tree annotation protocol to systematically identify dialect-specific translation errors. 3. Provides a comparative evaluation of three MT systems (Jais, GPT-3.5, NLLB-200), highlighting persistent challenges like dialect-specific terminology and semantic preservation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81517a8ae79cb17e9997772074cacae30aa2d04d3b344341b5fa0c68a1bc551b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of evaluating machine translation from Dialectal Arabic (DA) to Modern Standard Arabic (MSA), where existing metrics fail to capture dialect-specific errors. It proposes Ara-HOPE, a human-centric post-editing evaluation framework with a specialized error taxonomy and annotation protocol. The framework's application reveals that dialect-specific terminology and semantic preservation are the most persistent challenges for current MT systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: DA-MSA翻译评估困难 / DA-MSA Translation Evaluation is Difficult]
        Method[主要方法/Method: 提出Ara-HOPE框架 / Proposes Ara-HOPE Framework]
        Results[关键结果/Results: 方言术语和语义保留是主要挑战 / Dialect Terminology & Semantic Preservation are Key Challenges]
    ```

- **[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning**
  - **tags:** [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]
  - **authors:** Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles
  - **institution:** The Pennsylvania State University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21789
  - **contributions:** 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]
        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]
        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]
        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]
        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]
        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]
        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]
    ```

- **[arXiv251229] On The Conceptualization and Societal Impact of Cross-Cultural Bias**
  - **tags:** [nlp], [bias and fairness], [cultural bias, literature survey, societal impact, harm evaluation, bias mitigation]
  - **authors:** Vitthal Bhandari
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21809
  - **contributions:**  1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("On The Conceptualization and Societal Impact of Cross-Cultural Bias") --> Problem("核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.")
    Root --> Method("主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.")
    Root --> Results("关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.")
    ```

- **[arXiv251229] Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments**
  - **tags:** [mlsys], [agent system], [method decoration, large language models, adaptive method generation, IoT intelligence, on-device reasoning]
  - **authors:** Hong Su
  - **institution:** Chengdu University of Information Technology
  - **link:** https://arxiv.org/pdf/2512.21817
  - **contributions:** 1. Proposes the Method Decoration (DeMe) framework, a novel approach that modifies an LLM's method-generation path using explicit, non-hardcoded decorations derived from hidden goals, learned methods, and environmental feedback. 2. Formalizes two major categories of decorations (whole-process and step-level) and mechanisms (pre-decoration, post-decoration, etc.) to enable context-aware and adaptive method reshaping. 3. Demonstrates experimentally that the framework allows IoT devices to generate more appropriate methods in unknown or faulty operating conditions without modifying the underlying LLM's internal weights.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c4e12db053492542eb54d5fe131cf8b2ac404f1af94254b9d2abeed75d055a7_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that LLM-driven IoT devices struggle to adapt to novel situations due to fixed, pre-trained models. It proposes the Method Decoration (DeMe) framework, which augments an LLM's reasoning path with contextual decorations from experience and environment to generate adaptive methods. Experimental results show DeMe enables devices to derive more appropriate methods for unseen or faulty conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments"] --> B["核心问题/Problem: LLMs in IoT lack adaptability to unseen situations and rely on fixed logic."]
        A --> C["主要方法/Method: DeMe framework modifies LLM method-generation using decorations from goals, experience, and feedback."]
        A --> D["关键结果/Results: Enables derivation of more appropriate methods for unknown/faulty conditions."]
    ```

- **[arXiv251229] Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco**
  - **tags:** [nlp], [knowledge-augmented reasoning], [GraphRAG, Knowledge Graph, Graph Neural Network, LoRA, ChatGLM]
  - **authors:** Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu
  - **institution:** Chongqing Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.21837
  - **contributions:** 1. Proposes an LLM-based approach integrating a domain-specific knowledge graph for reasoning in tobacco pest and disease control, built upon the GraphRAG framework. 2. Employs a GNN to learn expressive node representations that capture relational information within the knowledge graph, enhancing the model's reasoning capability. 3. Demonstrates effective parameter-efficient fine-tuning of a ChatGLM backbone using LoRA, achieving superior performance in complex reasoning scenarios like multi-hop and comparative reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85aaa4f0b61b3033af1966c2105126130730ca9d346945b5a0ca02e3f706eb1a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that enhances large language models for agricultural knowledge reasoning by integrating graph-structured information. It constructs a tobacco pest and disease knowledge graph, uses a GNN to learn node representations, and fine-tunes a ChatGLM model with LoRA. The approach outperforms baselines, significantly improving reasoning accuracy and depth, especially in complex scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Knowledge Reasoning of LLMs Integrating Graph Information for Tobacco Pest Control<br>LLM集成图信息的烟草病虫害知识推理] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[传统方法依赖专家经验，效率低、错误率高<br>Traditional methods rely on expert experience, low efficiency & high error]
    C --> C1[基于GraphRAG框架，构建烟草病虫害知识图谱<br>Built on GraphRAG, construct tobacco pest/disease KG]
    C --> C2[使用GNN学习图谱节点表示，ChatGLM+LoRA微调<br>Use GNN for node representations, fine-tune ChatGLM with LoRA]
    D --> D1[在多指标上超越基线方法<br>Outperforms baselines across multiple metrics]
    D --> D2[显著提升复杂推理（多跳、比较）的准确性和深度<br>Significantly improves accuracy & depth in complex reasoning]
    ```

- **[arXiv251229] AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts**
  - **tags:** [nlp], [machine translation], [sentence alignment, parallel corpora, Arabic-English, legal texts, large language models (LLMs)]
  - **authors:** Baorong Huang, Ali Asiri
  - **institution:** Huaihua University, Umm al-Qura University
  - **link:** https://arxiv.org/pdf/2512.21842
  - **code:** https://github.com/XXX
  - **contributions:** 1. Proposed AlignAR, a generative sentence alignment method for Arabic-English parallel corpora. 2. Introduced a new dataset of complex legal and literary texts, featuring a "Hard" subset with reduced one-to-one mappings to better evaluate alignment methods. 3. Developed a hybrid LLM-plus-human-validation workflow and a bilingual annotation tool for creating gold-standard alignments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f00c37d022ef71644588277bad96472223331ff4180059a6a3d353133f3a205_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the scarcity of high-quality Arabic-English parallel corpora by proposing AlignAR, a generative sentence alignment method. The method, along with a new dataset of complex legal and literary texts, demonstrates that LLM-based approaches are more robust than traditional methods, achieving an 85.5% F1-score and a 9% improvement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts] --> B[核心问题/Problem: Arabic-English parallel corpora are scarce and lack complex mappings]
        A --> C[主要方法/Method: Proposes AlignAR, a generative sentence alignment method using LLMs and a new dataset]
        A --> D[关键结果/Results: LLM-based methods show superior robustness, achieving 85.5% F1-score, a 9% improvement]
    ```

- **[arXiv251229] HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs**
  - **tags:** [nlp], [evaluation], [anthropomorphic intelligence, benchmark, psychological counseling, rubric-based evaluation, reasoning-before-scoring]
  - **authors:** Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang
  - **institution:** Ant Group, Xiamen University, Beijing Normal University, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.21849
  - **code:** https://github.com/inclusionAI/HeartBench
  - **contributions:** 1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based "reasoning-before-scoring" evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLMs缺乏拟人化智能 / LLMs lack anthropomorphic intelligence]
        B --> B2[中文语境缺乏评估框架 / Lack of evaluation frameworks in Chinese context]
        C --> C1[基于心理咨询场景的基准 / Benchmark based on psychological counseling scenarios]
        C --> C2[理论驱动的分类法 / Theory-driven taxonomy]
        C --> C3[基于量规的推理评分法 / Rubric-based reasoning-before-scoring]
        D --> D1[模型性能存在上限 / Performance ceiling in models]
        D --> D2[复杂场景表现显著下降 / Significant decay in complex scenarios]
    ```

- **[arXiv251229] TimeBill: Time-Budgeted Inference for Large Language Models**
  - **tags:** [mlsys], [llm inference], [time-budgeted inference, KV cache eviction, response length prediction, execution time estimation]
  - **authors:** Qi Fan, An Zou, Yehan Ma
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21859
  - **contributions:** 1. Proposed a fine-grained response length predictor (RLP) and an execution time estimator (ETE) for accurate end-to-end LLM inference time modeling. 2. Developed a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on predicted execution time and a given time budget. 3. Demonstrated through experiments that TimeBill improves task completion rate and maintains response performance under various time constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a21a4204d1665c895b35788196ab3a0e5b32216d06abc37bfaaa9aefac4cb2f5_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TimeBill, a framework for performing LLM inference within a strict time budget. It uses predictors to estimate response length and execution time, then dynamically adjusts the KV cache eviction ratio to meet deadlines while preserving output quality. Experiments show it improves task completion rates and maintains performance compared to fixed strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TimeBill: Time-Budgeted Inference for Large Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM推理时间不确定/Uncertain LLM Inference Time]
        B --> B2[固定KV缓存策略不灵活/Fixed KV Cache Strategy Inflexible]
        C --> C1[响应长度预测器 (RLP)/Response Length Predictor (RLP)]
        C --> C2[执行时间估计器 (ETE)/Execution Time Estimator (ETE)]
        C --> C3[自适应KV缓存驱逐/Adaptive KV Cache Eviction]
        D --> D1[提高任务完成率/Improves Task Completion Rate]
        D --> D2[保持响应性能/Maintains Response Performance]
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics**
  - **tags:** [nlp], [text-to-sql], [benchmark, multilingual, domain-specific, large language models, sports analytics]
  - **authors:** Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa
  - **institution:** Birla Institute of Technology and Science (BITS), Pilani
  - **link:** https://arxiv.org/pdf/2512.21877
  - **contributions:** 1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a "Gold Standard" dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs在专业领域Text-to-SQL能力未充分探索/LLMs' Text-to-SQL capability in specialized domains is under-explored]
        B --> B2[现有基准缺乏多语言和体育分析特性/Existing benchmarks lack multilingual and sports analytics features]
        C --> C1[构建板球领域专业多语言基准/Build a specialized multilingual benchmark for cricket]
        C --> C2[与专家合作创建"黄金标准"查询/Collaborate with experts to create "Gold Standard" queries]
        C --> C3[评估六个最先进的LLMs/Evaluate six state-of-the-art LLMs]
        D --> D1[专业领域性能显著下降/Significant performance drop in specialized domain]
        D --> D2[DeepSeek R1表现最佳/DeepSeek R1 achieves SOTA]
        D --> D3[印地语查询准确率可比或更高/Hindi queries yield parity or higher accuracy]
    ```

- **[arXiv251229] Explainable Statute Prediction via Attention-based Model and LLM Prompting**
  - **tags:** [nlp], [legal text processing], [statute prediction, explainable AI, attention mechanism, large language models, chain-of-thought prompting]
  - **authors:** Sachin Pawar, Girish Keshav Palshikar, Anindita Sinha Banerjee, Nitin Ramrakhiyani, Basit Ali
  - **institution:** TCS Research, Tata Consultancy Services Limited
  - **link:** https://arxiv.org/pdf/2512.21902
  - **contributions:** 1. Proposes AoS, an attention-based supervised model using sentence transformers for explainable statute prediction. 2. Proposes LLMPrompt, a zero-shot method using large language models with standard and Chain-of-Thought prompting for prediction and explanation. 3. Evaluates both prediction performance and explanation quality across two datasets using automated and human evaluation methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0a88ac95c85beb5da693179a57fced56221862f49b2b3f82a7923e814deb844_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of automatically predicting relevant legal statutes from case descriptions and providing human-understandable explanations. It proposes two methods: a supervised attention-based model (AoS) and a zero-shot LLM prompting approach (LLMPrompt). The study compares their prediction performance against baselines and evaluates the quality of the generated explanations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Explainable Statute Prediction via Attention-based Model and LLM Prompting<br>基于注意力模型和LLM提示的可解释法规预测"] --> Problem["核心问题/Problem<br>Automatic prediction of relevant statutes from case descriptions with explanations<br>从案例描述中自动预测相关法规并提供解释"]
        Root --> Method["主要方法/Method<br>Two proposed techniques: AoS (supervised attention) and LLMPrompt (zero-shot LLM prompting)<br>两种方法: AoS(监督注意力)和LLMPrompt(零样本LLM提示)"]
        Root --> Results["关键结果/Results<br>Comparison of prediction performance and evaluation of explanation quality<br>比较预测性能并评估解释质量"]
    ```

- **[arXiv251229] Accelerate Speculative Decoding with Sparse Computation in Verification**
  - **tags:** [mlsys], [llm inference], [speculative decoding, sparse computation, verification stage, mixture-of-experts (MoE), efficiency-accuracy trade-off]
  - **authors:** Jikai Wang, Jianchao Tan, Yuxuan Hu, Jiayu Qin, Yerui Sun, Yuchen Xie, Xunliang Cai, Juntao Li, Min Zhang
  - **institution:** Soochow University, Meituan
  - **link:** https://arxiv.org/pdf/2512.21911
  - **contributions:** 1. Systematically analyzes and identifies structured computational redundancy across attention, FFN, and MoE components during the verification stage of speculative decoding. 2. Proposes a sparse verification framework that jointly sparsifies these components to reduce the dominant computation cost. 3. Introduces an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without additional training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0540488d64a1ff85171c147d2e74adf0477a0d2787eadf86a76357c955ab86be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational bottleneck in the verification stage of speculative decoding for LLMs, especially for long-context and MoE models. It proposes a framework that applies sparse computation techniques to the verification stage and employs a retrieval reuse strategy to reduce redundant calculations. Experiments show the method achieves a favorable efficiency-accuracy trade-off while maintaining stable acceptance length.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Accelerate Speculative Decoding with Sparse Computation in Verification] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[验证阶段成为瓶颈/Verification stage is bottleneck]
        B1 --> B2[长上下文与MoE模型/Long-context & MoE models]
        C --> C1[稀疏验证框架/Sparse Verification Framework]
        C1 --> C2[联合稀疏化注意力、FFN、MoE/Jointly sparsifies Attention, FFN, MoE]
        C1 --> C3[检索重用策略/Retrieval Reuse Strategy]
        D --> D1[有利的效率-精度权衡/Favorable efficiency-accuracy trade-off]
        D --> D2[稳定的接受长度/Stable acceptance length]
    ```

- **[arXiv251229] SWE-RM: Execution-free Feedback For Software Engineering Agents**
  - **tags:** [se], [software engineering agents], [reward model, test-time scaling, reinforcement learning, mixture-of-experts, SWE-Bench]
  - **authors:** KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He
  - **institution:** The Hong Kong University of Science and Technology, Alibaba Group (Qwen Team)
  - **link:** https://arxiv.org/pdf/2512.21919
  - **contributions:** 1. Identified that high TTS performance does not guarantee effective RL training, and introduced classification accuracy and calibration as crucial metrics for robust reward models. 2. Conducted comprehensive experiments to analyze factors (data scale, policy mixtures, data source) impacting reward model training for SWE agents. 3. Proposed SWE-RM, a large-scale mixture-of-experts reward model that significantly improves agent performance on both TTS and RL, achieving new SOTA on SWE-Bench Verified.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcb1e3e885f771ebf5ee27ef70da96ceb4c030de77fdee247afd5d854761a72f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of execution-based feedback for software engineering agents by proposing an execution-free reward model. It introduces SWE-RM, a robust reward model trained with insights from controlled experiments, which substantially improves agent performance on both test-time scaling and reinforcement learning, setting a new state-of-the-art on the SWE-Bench benchmark.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SWE-RM: Execution-free Feedback For Software Engineering Agents"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["执行反馈的局限性/Limitations of Execution-based Feedback"]
        Problem --> P2["无执行反馈未被充分探索/Execution-free Feedback Underexplored"]
        Method --> M1["识别RL关键指标/Identify Key RL Metrics (Accuracy, Calibration)"]
        Method --> M2["可控实验分析/Controlled Experiments on Training Factors"]
        Method --> M3["提出SWE-RM模型/Propose SWE-RM (MoE Reward Model)"]
        Results --> R1["提升TTS性能/Improves TTS Performance (e.g., Qwen3-Coder-Max to 74.6%)"]
        Results --> R2["提升RL性能/Improves RL Performance (+3 points)"]
        Results --> R3["开源模型SOTA/New SOTA Among Open-Source Models"]
    ```

- **[arXiv251229] Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs**
  - **tags:** [nlp], [tokenization], [tokenization penalty, large language models, byte-pair encoding, vocabulary size, natural word splitting]
  - **authors:** Sachin Pawar, Manoj Apte, Kshitij Jadhav, Girish Keshav Palshikar, Nitin Ramrakhiyani
  - **institution:** TCS Research, Tata Consultancy Services Limited
  - **link:** https://arxiv.org/pdf/2512.21933
  - **contributions:** 1. Proposes the hypothesis that breaking natural words into multiple tokens negatively impacts LLM performance on NLP tasks. 2. Introduces a set of penalty functions to quantify the "badness" of tokenization for a given text and LLM. 3. Establishes the statistical significance of the hypothesis across multiple NLP tasks and different LLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/015399929e56260633eb709a56e41948acd324ce4065b0fcb286daa6d5ea6e33_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how tokenization, specifically the splitting of natural words into multiple sub-tokens due to limited vocabulary, affects the performance of Large Language Models (LLMs). The authors propose penalty functions to measure this tokenization effect and demonstrate its statistically significant negative impact on various NLP tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM分词将自然词拆分为多个子词/LLM tokenization splits natural words into multiple sub-tokens]
        B --> B2[假设这会损害模型性能/Hypothesized to hurt model performance]
        C --> C1[提出量化分词影响的惩罚函数/Propose penalty functions to quantify tokenization effect]
        D --> D1[在多任务和多模型上验证假设的显著性/Validate hypothesis significance on multiple tasks & models]
    ```

- **[arXiv251229] Self-attention vector output similarities reveal how machines pay attention**
  - **tags:** [nlp], [attention mechanisms], [self-attention, BERT, attention heads, vector similarity, token representation]
  - **authors:** Tal Halevi, Yarden Tzach, Ronit D. Gross, Shalom Rosner, Ido Kanter
  - **institution:** Bar-Ilan University
  - **link:** https://arxiv.org/pdf/2512.21956
  - **contributions:** 1. Introduced a new method for quantifying information processing within the self-attention mechanism using a context similarity matrix derived from token vectors. 2. Revealed that different attention heads specialize in distinct linguistic features, such as identifying token repetitions or common contextual tokens. 3. Demonstrated a progression from long-range to short-range token similarities across layers, culminating in a focus on intra-sentence relationships and unique token-centric similarity patterns in final layers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ef3aef5be4139745b138137e89a3f21de053bd91a9fedbe345ad6f90900a98b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel approach to analyze the self-attention mechanism in transformer models by examining vector output similarities. The analysis on BERT-12 shows that attention heads specialize in different linguistic features and that similarity patterns evolve from long-range to short-range, focusing on sentence-level structures in deeper layers.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Self-attention vector output similarities reveal how machines pay attention<br/>自注意力向量输出相似性揭示机器如何关注"] --> Problem["核心问题/Problem: Quantitative characterization of self-attention learning process<br/>自注意力学习过程的定量表征"]
        Root --> Method["主要方法/Method: Context similarity matrix from self-attention head vectors<br/>基于自注意力头向量的上下文相似性矩阵"]
        Root --> Results["关键结果/Results: Heads specialize linguistically; similarity shifts from long to short range<br/>头部语言专业化;相似性从长程转向短程"]
    ```

- **[arXiv251229] Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management**
  - **tags:** [nlp], [AI Governance & Compliance], [lifecycle management, bias detection, differential privacy, federated learning, terminology drift]
  - **authors:** Sunil Arora, John Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.22060
  - **contributions:** 1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks."]
        Method["主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy)."]
        Results["关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study."]
    ```

- **[arXiv251229] Context as a Tool: Context Management for Long-Horizon SWE-Agents**
  - **tags:** [mlsys], [agent system], [context management, long-horizon reasoning, SWE-agents, trajectory compression, structured workspace]
  - **authors:** Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, Bryan Dai
  - **institution:** Beihang University, University of Manchester, Ubiquant
  - **link:** https://arxiv.org/pdf/2512.22087
  - **contributions:** 1. Proposes CAT, a new paradigm that treats context management as an integrated, callable tool for agents, enabling proactive control. 2. Introduces a structured context workspace with stable semantics, condensed long-term memory, and high-fidelity short-term interactions. 3. Presents CAT-GENERATOR, a trajectory-level supervision framework for training the SWE-Compressor model, which achieves state-of-the-art performance on SWE-Bench-Verified.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554c080cfd26463c6d73be16144f677075ea893401e3c8ae26ee7321c48b2be8_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of context explosion and semantic drift in long-horizon software engineering agents by proposing CAT, a paradigm that integrates proactive context management as a tool. It introduces a structured workspace and a training framework to produce the SWE-Compressor model. Experiments show this model significantly outperforms existing baselines on a software engineering benchmark while maintaining stable reasoning under a bounded context budget.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Context as a Tool: Context Management for Long-Horizon SWE-Agents] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有代理上下文爆炸、语义漂移/Existing agents suffer context explosion & semantic drift]
        C --> C1[CAT: 将上下文管理作为可调用工具/CAT: Context management as a callable tool]
        C --> C2[结构化上下文工作区/Structured context workspace]
        C --> C3[CAT-GENERATOR 训练框架/CAT-GENERATOR training framework]
        D --> D1[SWE-Compressor 达到 57.6% 解决率/SWE-Compressor achieves 57.6% solved rate]
        D --> D2[显著优于基准/Significantly outperforms baselines]
    ```

- **[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law**
  - **tags:** [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]
  - **authors:** Chiwun Yang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22088
  - **contributions:** 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\{-1/6\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]
        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]
        C --> C2[近似为核行为 / Approximate to kernel behaviors]
        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]
        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]
        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]
        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]
        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]
    ```

- **[arXiv251229] Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis**
  - **tags:** [nlp], [benchmark construction], [Turkish NLU benchmark, semi-automated annotation, sentiment analysis dataset]
  - **authors:** Duygu Altinok
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22100
  - **contributions:** 1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Introducing TrGLUE and SentiTurca] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏土耳其语综合基准/Lack of Turkish NLU Benchmark]
        C --> C1[半自动标注流程/Semi-automated Pipeline]
        C1 --> C2[LLM标注 + 交叉验证 + 人工校验/LLM Annotation + Cross-check + Human Validation]
        D --> D1[发布TrGLUE & SentiTurca/Release TrGLUE & SentiTurca]
        D --> D2[提供代码与资源/Provide Code & Resources]
    ```

- **[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting**
  - **tags:** [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]
  - **authors:** Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22101
  - **contributions:** 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A2P-Vis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]
        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]
        B1 --> B3[组装成专业报告/Assembling coherent professional report]
        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]
        C1 --> C2[数据分析器/Data Analyzer]
        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]
        C1 --> C4[报告呈现器/Presenter]
        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]
        D --> D1[端到端协同分析/End-to-end co-analysis]
        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]
    ```
