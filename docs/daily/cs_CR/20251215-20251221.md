# 20251215-20251221 (cs.CR)

## 2025-12-18

- **[arXiv251218] Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol**
  - **tags:** [mlsys], [others], [zero-knowledge proof, Model Context Protocol, privacy-preserving audit, Circom, mutual auditing]
  - **authors:** Guanlin Jing, Huayi Qi
  - **institution:** Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.14737
  - **Simple LLM Summary:** This paper introduces a framework that integrates zero-knowledge proofs with the Model Context Protocol (MCP) to enable verifiable, privacy-preserving audits of agent communications without revealing message content. It achieves mutual auditing for compliance and billing while maintaining data authenticity and privacy with negligible latency overhead. The authors implement the system, claiming it is the first to offer such verifiable mutual auditing for agent communications.

- **[arXiv251218] Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)**
  - **tags:** [mlsys], [others], [quantum machine learning, hybrid quantum computing, amplitude encoding, entanglement encoding, hierarchical threat detection, anomaly detection, intrusion confirmation, multiattack classification, deep neural networks, ensemble classifiers]
  - **authors:** Tan Le, Van Le, Sachin Shetty
  - **institution:** Hampton University, Virginia Polytechnic Institute and State University, Old Dominion University
  - **link:** https://arxiv.org/pdf/2512.14742
  - **Simple LLM Summary:** This paper proposes a hierarchical cybersecurity framework for Open Radio Access Networks (O-RAN) that integrates hybrid quantum computing and machine learning. The method uses quantum-inspired feature encodings (amplitude- and entanglement-based) with deep and ensemble classifiers for anomaly detection, intrusion confirmation, and multiattack classification. The framework demonstrates near-perfect accuracy, high recall, and strong interpretability, indicating readiness for scalable deployment in O-RAN environments.

- **[arXiv251218] Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs**
  - **tags:** [mlsys], [post-training], [backdoor attack, continual fine-tuning, gradient alignment, P-Trojan, persistence]
  - **authors:** Jing Cui, Yufei Han, Jianbin Jiao, Junge Zhang
  - **institution:** University of Chinese Academy of Sciences, INRIA, Institute of Automation, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.14741
  - **Simple LLM Summary:** The paper proposes P-Trojan, a backdoor attack method that optimizes for persistence by aligning poisoned gradients with clean task gradients on token embeddings during model poisoning. Experiments show it maintains over 99% backdoor persistence across continual fine-tuning updates on models like Qwen2.5 and LLaMA3 without harming clean-task accuracy. This highlights the need for stronger defenses and persistence-aware evaluation in real-world LLM adaptation pipelines.

- **[arXiv251218] Factor(U,T): Controlling Untrusted AI by Monitoring their Plans**
  - **tags:** [ai], [ai safety], [factored cognition, control protocols, task decomposition, untrusted decomposer, monitoring, BigCodeBench, AUROC]
  - **authors:** Edward Lue Chee Lip, Anthony Channg, Diana Kim, Aaron Sandoval, Kevin Zhu
  - **institution:** Algoverse AI Research, Colorado State University, Orange Coast College
  - **link:** https://arxiv.org/pdf/2512.14745
  - **Simple LLM Summary:** This paper introduces Factor(U,T), a protocol where an untrusted, potentially malicious AI model decomposes a complex task into subtasks, which are then implemented by trusted models. It finds that monitoring only the natural language decomposition plans is ineffective for detecting attacks, whereas monitoring the concrete code solutions of the subtasks provides strong safety and discrimination. The main conclusion is that implementation-context monitoring is crucial for safety when using untrusted models for task decomposition.

- **[arXiv251218] CODE ACROSTIC: Robust Watermarking for Code Generation**
  - **tags:** [mlsys], [post-training], [watermarking, cue list, low-entropy, high-entropy, comment removal attack]
  - **authors:** Li Lin, Siyuan Xin, Yang Cao, Xiaochun Cao
  - **institution:** Institute of Science Tokyo, Shanghai University, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.14753
  - **Simple LLM Summary:** This paper proposes a robust watermarking method for LLM-generated code that uses a Cue List to distinguish between low and high-entropy parts of the code for targeted watermark injection. It addresses the vulnerability of existing methods to comment removal attacks. The evaluation shows the method achieves higher detectability and usability compared to state-of-the-art techniques.

- **[arXiv251218] One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs**
  - **tags:** [mlsys], [post-training], [jailbreak attack, adversarial prompt, representation-level probing, linear separability, Probe-Guided Projection (PGP), transferability, finetuning, pretrained model]
  - **authors:** Yixin Tan, Zhe Yu, Jun Sakuma
  - **institution:** Institute of Science Tokyo, Riken AIP
  - **link:** https://arxiv.org/pdf/2512.14751
  - **Simple LLM Summary:** This paper investigates how jailbreak vulnerabilities transfer from pretrained to finetuned LLMs. It introduces the Probe-Guided Projection (PGP) attack, which uses representation-level probing to guide adversarial prompt optimization for better transferability. The main conclusion is that the pretrain-finetune paradigm inherently amplifies security risks, as vulnerabilities encoded in the pretrained model's representations are inherited by its finetuned variants.

- **[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation**
  - **tags:** [mlsys], [others], [Shapley-CMI, Private Set Intersection, Conditional Mutual Information, Vertical Federated Learning, data valuation]
  - **authors:** Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza
  - **institution:** Mondragon Unibertsitatea
  - **link:** https://arxiv.org/pdf/2512.14767
  - **Simple LLM Summary:** This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.

- **[arXiv251218] MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber**
  - **tags:** [mlsys], [llm inference], [multi-agent system, secure communication layer, ontology-aligned messaging, MITRE ATT&CK, Lightweight Random Forest]
  - **authors:** Arth Bhardwaj, Sia Godika, Yuvam Loonker
  - **institution:** Saint Francis High School, Massachusetts Institute of Technology, JBCN International School
  - **link:** https://arxiv.org/pdf/2512.14846
  - **Simple LLM Summary:** The paper proposes MALCDF, a real-time cyber defense framework where four specialized LLM agents (Detection, Intelligence, Response, Analysis) collaborate via a secure communication layer. It demonstrates that this multi-agent approach with encrypted, ontology-aligned messaging outperforms a lightweight ML baseline and a single-LLM setup in detection accuracy and F1-score on a test stream. The conclusion is that coordinating simple LLM agents improves practical, real-time cyber defense.

- **[arXiv251218] Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks**
  - **tags:** [mlsys], [llm inference], [penetration testing, prompt injection, agentic ai, autogen, crewai, ssrf, sql injection]
  - **authors:** Viet K. Nguyen, Mohammad I. Husain
  - **institution:** Cal Poly Pomona
  - **link:** https://arxiv.org/pdf/2512.14860
  - **Simple LLM Summary:** This paper conducts systematic penetration testing on agentic AI systems, comparing five LLM models across two frameworks using 13 attack scenarios. It finds significant security vulnerabilities, with over half of malicious prompts succeeding, and identifies novel defensive patterns like "hallucinated compliance".

- **[arXiv251218] Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection**
  - **tags:** [mlsys], [others], [AI-Augmented Security Operations Center (AISOC), malware detection, log-anomaly detection, score fusion, Elasticsearch, Kibana, Metasploit, EC2]
  - **authors:** Nnamdi Philip Okonkwo, Lubna Luxmi Dhirani
  - **institution:** University of Limerick
  - **link:** https://arxiv.org/pdf/2512.14935
  - **Simple LLM Summary:** The paper implements an AI-Augmented Security Operations Center (AISOC) on AWS that fuses outputs from a malware detector and a log-anomaly detector to triage threats. The method uses calibrated score fusion to classify activity as NORMAL, SUSPICIOUS, or HIGH_CONFIDENCE_ATTACK. It concludes that this simple, fused approach can effectively enhance cloud SOC capabilities in cost-sensitive environments.

- **[arXiv251218] Intrusion Detection in Internet of Vehicles Using Machine Learning**
  - **tags:** [mlsys], [others], [intrusion detection system, machine learning, multi-class classification, CAN bus, CICIoV2024 dataset]
  - **authors:** Hop Le, Izzat Alsmadi
  - **institution:** Texas A&M University-San Antonio
  - **link:** https://arxiv.org/pdf/2512.14958
  - **Simple LLM Summary:** This paper develops a machine learning-based intrusion detection system to classify malicious CAN bus traffic in the Internet of Vehicles. It uses the CICIoV2024 benchmark dataset to analyze attack patterns like DoS and spoofing. The initial findings confirm a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.

- **[arXiv251218] SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports**
  - **tags:** [mlsys], [others], [masked language model, fine-tuning, semantic surrogates, deep neural network, BERT]
  - **authors:** Sogol Masoumzadeh, Yufei Li, Shane McIntosh, Dániel Varró, Lili Wei
  - **institution:** McGill University, University of Waterloo, Linköping University
  - **link:** https://arxiv.org/pdf/2512.15003
  - **Simple LLM Summary:** The paper proposes SEBERTIS, a framework that fine-tunes bidirectional transformer models (like BERT) as Masked Language Models using semantically equivalent vocabulary (Semantic Surrogates) to create classifiers for security-related issue reports. This method reduces reliance on lexical shortcuts, enabling better detection of complex issues. The resulting classifier significantly outperforms existing ML and LLM baselines in precision, recall, and F1-score, demonstrating high effectiveness for real-time issue triage.

- **[arXiv251218] Quantifying Return on Security Controls in LLM Systems**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation (RAG), Monte Carlo simulation, loss exceedance curves, Laplace's Rule of Succession, adversarial probing, Garak, attribute-based access control (ABAC), named entity recognition (NER) redaction, NeMo Guardrails]
  - **authors:** Richard Helder Moulton, Austin O'Brien, John D. Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.15081
  - **Simple LLM Summary:** This paper introduces a framework to quantify the financial return on security controls for LLM systems by simulating attacks on a RAG service, estimating attack success probabilities, and modeling potential losses with Monte Carlo methods. The main conclusion is that controls like ABAC and NER redaction significantly reduce expected financial losses and offer high return-on-control, whereas NeMo Guardrails provides minimal benefit in the tested scenarios.

- **[arXiv251218] An Efficient Gradient-Based Inference Attack for Federated Learning**
  - **tags:** [mlsys], [others], [membership inference attack, federated learning, gradient-based methods, shadow training, attribute inference]
  - **authors:** Pablo Montaña-Fernández, Ines Ortega-Fernandez
  - **institution:** Gradiant
  - **link:** https://arxiv.org/pdf/2512.15143
  - **Simple LLM Summary:** This paper proposes a new gradient-based inference attack for federated learning that analyzes the temporal evolution of last-layer gradients across multiple training rounds. The attack uses a shadow model technique to learn gradient patterns and can be extended to attribute inference. The findings show that multi-round federated learning increases vulnerability to such attacks, with aggregators posing a greater threat than data owners.

- **[arXiv251218] Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions**
  - **tags:** [mlsys], [others], [Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), Quantum Generative Adversarial Networks (QGANs)]
  - **authors:** Siva Sai, Ishika Goyal, Shubham Sharma, Sri Harshita Manuri, Vinay Chamola, Rajkumar Buyya
  - **institution:** The University of Melbourne, Birla Institute of Technology and Science, Pilani, APPCAIR
  - **link:** https://arxiv.org/pdf/2512.15286
  - **Simple LLM Summary:** This survey paper explores Quantum Machine Learning (QML) techniques, including QNNs, QSVMs, VQCs, and QGANs, for cybersecurity applications like intrusion detection and malware classification. It concludes that QML offers potential advantages for processing high-dimensional data and enhancing security in areas like cloud computing, but also discusses current limitations and future research directions needed to address them.

- **[arXiv251218] Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference**
  - **tags:** [mlsys], [post-training], [post-training quantization, membership inference, AdaRound, BRECQ, OBC, quantization, privacy-utility trade-off]
  - **authors:** Chenxiang Zhang, Tongxi Qu, Zhong Li, Tian Zhang, Jun Pang, Sjouke Mauw
  - **institution:** University of Luxembourg, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.15335
  - **Simple LLM Summary:** This paper systematically evaluates how post-training quantization (PTQ) affects privacy by using membership inference attacks on models quantized with algorithms like AdaRound, BRECQ, and OBC. The study finds that lower-precision PTQ (e.g., 4-bit, 2-bit, 1.58-bit) significantly reduces privacy leakage, offering up to an order of magnitude less vulnerability compared to full-precision models, though at the cost of decreased utility. The results provide practical insights for balancing efficiency, accuracy, and privacy in model deployment.

- **[arXiv251218] Remotely Detectable Robot Policy Watermarking**
  - **tags:** [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]
  - **authors:** Michael Amir, Manon Flageat, Amanda Prorok
  - **institution:** University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.15379
  - **Simple LLM Summary:** This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot's motions using the policy's inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies.

- **[arXiv251218] How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?**
  - **tags:** [mlsys], [llm training], [membership inference, semantically equivalent code transformation, variable renaming, causal analysis, code obfuscation]
  - **authors:** Hua Yang, Alejandro Velasco, Thanh Le-Cong, Md Nazmul Haque, Bowen Xu, Denys Poshyvanyk
  - **institution:** North Carolina State University, William & Mary, The University of Melbourne
  - **link:** https://arxiv.org/pdf/2512.15468
  - **Simple LLM Summary:** This paper investigates how semantically equivalent code transformations, such as variable renaming, can be used to evade membership inference detection in large language models for code. It finds that these transformations, especially RenameVariable, can significantly reduce the success of membership inference attacks without substantially harming model performance. The results reveal a critical vulnerability in license compliance enforcement for code LLMs, showing that transformation-based obfuscation can weaken detection of unauthorized code usage.

- **[arXiv251218] Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection**
  - **tags:** [mlsys], [others], [transformer, multi-head self-attention, positional encoding, precision-focused loss, edge deployment, TensorFlow Lite, ONNX, TensorRT]
  - **authors:** Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos
  - **institution:** KTH Royal Institute of Technology, Lenovo
  - **link:** https://arxiv.org/pdf/2512.15503
  - **Simple LLM Summary:** This paper proposes AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons. It uses multi-head self-attention to capture spatiotemporal dependencies and a precision-focused loss to minimize false positives. The method demonstrates high performance and achieves sub-millisecond inference latency, making it suitable for deployment on resource-constrained edge platforms.

- **[arXiv251218] BashArena: A Control Setting for Highly Privileged AI Agents**
  - **tags:** [mlsys], [cluster infrastructure], [AI control, red teaming, blue teaming, adversarial evaluation, system administration tasks, sabotage detection, Linux environment, control protocols]
  - **authors:** Adam Kaufman, James Lucassen, Tyler Tracy, Cody Rushing, Aryan Bhatt
  - **institution:** Redwood Research
  - **link:** https://arxiv.org/pdf/2512.15688
  - **Simple LLM Summary:** The paper introduces BashArena, a control setting with 637 Linux system administration tasks and sabotage objectives to study AI control techniques for highly privileged agents. It evaluates frontier LLMs in an adversarial game between red teams (performing sabotage) and blue teams (detecting it), finding that Claude Sonnet 4.5 can evade detection by GPT-4.1 mini 26% of the time, establishing a baseline for more effective control protocols.

## 2025-12-19

- **[arXiv251219] PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling**
  - **tags:** [mlsys], [others], [adversarial variational framework, progressive training, dual-path VAE-GAN, domain-specific feature matching, synthetic data generation]
  - **authors:** Jamal Al-Karaki, Muhammad Al-Zafar Khan, Rand Derar Mohammad Al Athamneh
  - **institution:** Zayed University, The Hashemite University
  - **link:** https://arxiv.org/pdf/2512.15768
  - **Simple LLM Summary:** This paper introduces PHANTOM, a progressive adversarial variational framework that uses a dual-path VAE-GAN architecture with domain-specific feature matching to generate high-fidelity synthetic cyberattack data. The method achieves 98% weighted accuracy in intrusion detection when models are trained on its synthetic data, demonstrating its effectiveness for augmenting training datasets, though it faces challenges with generating rare attack types.

- **[arXiv251219] Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?**
  - **tags:** [mlsys], [diffusion inference], [Data-Chain Backdoor (DCB), clean-label attack, Early-Stage Trigger Manifestation (ESTM), backdoor triggers, synthetic data generation]
  - **authors:** Junchi Lu, Xinke Li, Yuheng Liu, Qi Alfred Chen
  - **institution:** University of California, Irvine, City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.15769
  - **Simple LLM Summary:** This paper investigates the Data-Chain Backdoor (DCB) threat, where backdoor triggers are injected into and propagated through open-source diffusion models used for synthetic data generation. The method reveals that these triggers are memorized and reproduced in the generated data, subsequently poisoning downstream models, even in clean-label attack scenarios. The main conclusion is that this poses a severe, previously underexplored security risk in generative data pipelines, highlighting the need for mitigation strategies.

- **[arXiv251219] Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence**
  - **tags:** [ai], [adversarial robustness], [adversarial training, FGSM, PGD, SHAP, Expected Calibration Error (ECE), Value-at-Risk (VaR), Expected Shortfall (ES), bootstrap inference]
  - **authors:** Samruddhi Baviskar
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.15780
  - **Simple LLM Summary:** The paper introduces a dataset-agnostic pipeline to evaluate adversarial robustness in tabular financial ML models using gradient-based attacks like FGSM and PGD. It finds that small perturbations significantly degrade model performance and increase financial risk, but adversarial training can partially recover utility. The study also suggests SHAP stability as an early-warning indicator for adversarial influence.

- **[arXiv251219] Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection**
  - **tags:** [mlsys], [others], [hyperparameter tuning, grid search, random search, recursive feature elimination, support vector machine, KDD CUP 1999 dataset, tenfold cross-validation]
  - **authors:** Sudhanshu Sekhar Tripathy, Bichitrananda Behera
  - **institution:** C.V. Raman Global University
  - **link:** https://arxiv.org/pdf/2512.15779
  - **Simple LLM Summary:** This paper evaluates multiple machine learning algorithms for network intrusion detection, applying hyperparameter tuning via grid and random search along with Recursive Feature Elimination. The optimized Support Vector Machine classifier achieved the highest accuracy of 99.12% on the KDD CUP 1999 dataset, demonstrating that systematic tuning significantly enhances detection performance.

- **[arXiv251219] Auto-Tuning Safety Guardrails for Black-Box Large Language Models**
  - **tags:** [mlsys], [llm inference], [hyperparameter optimization, system prompts, content filters, jailbreak detection, malware generation, Optuna, ModernBERT, grid search]
  - **authors:** Perry Abdulkadir
  - **institution:** University of St. Thomas
  - **link:** https://arxiv.org/pdf/2512.15782
  - **Simple LLM Summary:** This paper proposes treating the design of safety guardrails (like system prompts and content filters) for a frozen black-box LLM as a hyperparameter optimization problem. Using a proof-of-concept with Mistral-7B-Instruct and ModernBERT, it shows that a black-box optimizer (Optuna) can efficiently find safe configurations, matching the best grid search results with far fewer evaluations and less time. The conclusion is that this auto-tuning approach is a feasible method to harden LLM deployments under practical constraints.

- **[arXiv251219] RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization**
  - **tags:** [mlsys], [fault-tolerance], [bit-flip attack, state-space models, Mamba, hardware faults, adversarial robustness]
  - **authors:** Sanjay Das, Swastik Bhattacharya, Shamik Kundu, Arnab Raha, Souvik Kundu, Kanad Basu
  - **institution:** University of Texas at Dallas, Intel Corporation, Rensselaer Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2512.15778
  - **Simple LLM Summary:** This paper introduces RAMBO, a framework for conducting bit-flip attacks on Mamba-based state-space models to analyze their reliability. It demonstrates that flipping a single critical bit can catastrophically degrade model performance, reducing accuracy to 0% and drastically increasing perplexity. The results highlight the pronounced vulnerability of these efficient sequence models to hardware-level adversarial perturbations.

- **[arXiv251219] An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative**
  - **tags:** [mlsys], [others], [CVSS, vulnerability classification, machine learning, deep learning, predictive modeling]
  - **authors:** Apurva Shet, Izzat Alsmadi
  - **institution:** Texas A&M University - San Antonio
  - **link:** https://arxiv.org/pdf/2512.15803
  - **Simple LLM Summary:** This paper analyzes zero-day vulnerabilities disclosed by the Zero Day Initiative, employing machine learning and deep learning models on structured metadata and textual descriptions to classify severity. It aims to identify trends and characteristics indicative of high-severity vulnerabilities. The findings are intended to improve patch prioritization and vulnerability management strategies.

- **[arXiv251219] Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India**
  - **tags:** [ai], [cybersecurity], [computer forensics, explainable AI (XAI), data minimization, algorithmic bias, deepfakes, adversarial AI, Digital Personal Data Protection Act]
  - **authors:** Sahibpreet Singh, Shikha Dhiman
  - **institution:** Guru Nanak Dev University, Amritsar
  - **link:** https://arxiv.org/pdf/2512.15799
  - **Simple LLM Summary:** This paper employs a doctrinal legal methodology to analyze the integration of AI into cybercrime and forensics in India, focusing on the Digital Personal Data Protection Act, 2023. It concludes that there is a critical tension between privacy principles and forensic needs, and proposes a human-centric forensic model using explainable AI (XAI) to ensure evidence admissibility while advocating for legislative synchronization with international standards.

- **[arXiv251219] Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications**
  - **tags:** [mlsys], [multi-modal inference], [point cloud super-resolution, attribute-based encryption, downsampling, upscaling]
  - **authors:** Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi
  - **institution:** University of Massachusetts Amherst, Tennessee Technological University
  - **link:** https://arxiv.org/pdf/2512.15823
  - **Simple LLM Summary:** This paper proposes a system to reduce latency in AR/VR streaming by downsampling and partially encrypting point cloud content at the server, then using a machine learning-based super-resolution model to reconstruct it at the client. The evaluation shows this approach effectively reduces bandwidth and encryption overhead while accurately reconstructing the original point clouds with minimal error.

- **[arXiv251219] VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces**
  - **tags:** [mlsys], [others], [verifiable execution traces, agent identity document, web proofs, tee proxy, host-independent authentication]
  - **authors:** Artem Grigor, Christian Schroeder de Witt, Simon Birnbach, Ivan Martinovic
  - **institution:** University of Oxford
  - **link:** https://arxiv.org/pdf/2512.15892
  - **Simple LLM Summary:** The paper introduces VET, a framework for achieving host-independent authentication of autonomous agent outputs using verifiable execution traces and an Agent Identity Document. It demonstrates that practical authentication is possible by composing proof mechanisms like Web Proofs and TEE Proxies, with a case study showing overhead typically under 3x for API calls.

- **[arXiv251219] Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection**
  - **tags:** [mlsys], [others], [autoencoder, denoising, adversarial attack, Perlin noise, YOLOv5, object detection]
  - **authors:** Min Geun Song, Gang Min Kim, Woonmin Kim, Yongsik Kim, Jeonghyun Sim, Sangbeom Park, Huy Kang Kim
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.16123
  - **Simple LLM Summary:** This paper proposes an autoencoder-based denoising defense to mitigate adversarial attacks on object detection models. The method uses a single-layer convolutional autoencoder to remove Perlin noise perturbations from images before feeding them to a YOLOv5 detector. The results show that this approach provides a partial recovery of detection performance without requiring model retraining.

- **[arXiv251219] In-Context Probing for Membership Inference in Fine-Tuned Language Models**
  - **tags:** [mlsys], [llm inference], [membership inference attacks, in-context probing, optimization gap, black-box attacks, fine-tuning, PEFT]
  - **authors:** Zhexi Lu, Hongliang Chi, Nathalie Baracaldo, Swanand Ravindra Kadhe, Yuseok Jeon, Lei Yu
  - **institution:** Rensselaer Polytechnic Institute, IBM Research, Korea University
  - **link:** https://arxiv.org/pdf/2512.16292
  - **Simple LLM Summary:** The paper proposes ICP-MIA, a membership inference attack framework that estimates the optimization gap via in-context probing to simulate fine-tuning behavior without retraining. It shows that this method outperforms prior black-box attacks, especially at low false positive rates, providing a practical tool for auditing privacy risks in deployed LLMs.

- **[arXiv251219] Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams**
  - **tags:** [mlsys], [llm inference], [large language models, safety filters, conversation study, social engineering, automation]
  - **authors:** Gilad Gressel, Rahul Pankajakshan, Shir Rozenfeld, Ling Li, Ivan Franceschini, Krishnahsree Achuthan, Yisroel Mirsky
  - **institution:** Amrita Vishwa Vidyapeetham, Ca’ Foscari University of Venice, University of Melbourne, Ben Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2512.16280
  - **Simple LLM Summary:** The paper investigates the role of LLMs in romance-baiting scams through interviews with insiders and victims, a blinded long-term conversation study comparing LLM agents to human operators, and an evaluation of commercial safety filters. It finds that LLMs are already widely used in scams, can elicit greater trust and compliance than humans, and that current safety filters are ineffective at detecting such dialogues, suggesting scams are ripe for full LLM automation.

- **[arXiv251219] Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks**
  - **tags:** [mlsys], [llm inference], [prompt injection defense, goal-hijacking, Chain Of Thoughts, automatic defense generation, LLaMA]
  - **authors:** Safwan Shaheer, G.M. Refatul Islam, Mohammad Rafid Hamid, Tahsin Zaman Jilan
  - **institution:** BRAC University
  - **link:** https://arxiv.org/pdf/2512.16307
  - **Simple LLM Summary:** This paper introduces a novel defense framework against prompt injection attacks for small open-source LLMs like LLaMA, using an iterative refinement process based on a Chain Of Thoughts seed to generate automatic defenses. The method significantly improves the detection of goal-hijacking attacks, reducing both attack success rates and false detection rates. The work demonstrates that this approach enables more secure and efficient deployment of LLMs in resource-constrained environments.

- **[arXiv251219] Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation**
  - **tags:** [mlsys], [llm inference], [Tools Orchestration Privacy Risk (TOP-R), TOP-Bench, Privacy Enhancement Principle (PEP), H-Score, Risk Leakage Rate (RLR)]
  - **authors:** Yuxuan Qiao, Dongqin Liu, Hongchang Yang, Wei Zhou, Songlin Hu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.16310
  - **Simple LLM Summary:** This paper identifies and studies a new privacy risk in single-agent, multi-tool LLM architectures, termed Tools Orchestration Privacy Risk (TOP-R), where agents can synthesize sensitive information from aggregated tool outputs. It introduces a benchmark (TOP-Bench) and a mitigation method called the Privacy Enhancement Principle (PEP). The main conclusion is that TOP-R is a severe and prevalent risk in current models, but the proposed PEP method can effectively reduce leakage and improve the safety-robustness trade-off.

- **[arXiv251219] A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection**
  - **tags:** [mlsys], [llm inference], [code obfuscation, layout obfuscation, data flow obfuscation, control flow obfuscation, LLM-driven implementation, vulnerability detection, adversarial robustness]
  - **authors:** Xiao Li, Yue Li, Hao Wu, Yue Zhang, Yechao Zhang, Fengyuan Xu, Sheng Zhong
  - **institution:** Nanjing University, Shandong University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.16538
  - **Simple LLM Summary:** This paper systematically studies the impact of code obfuscation on LLM-based vulnerability detection by categorizing obfuscation techniques into layout, data flow, and control flow classes and implementing them across multiple programming languages. It evaluates these techniques against 15 LLMs and two coding agents, finding that obfuscation can both improve and degrade detection performance depending on the vulnerability type, code properties, and model attributes. The study highlights the need to enhance LLM robustness for reliable real-world security auditing.

- **[arXiv251219] Prefix Probing: Lightweight Harmful Content Detection for Large Language Models**
  - **tags:** [mlsys], [llm inference], [prefix probing, black-box detection, log-probability comparison, prefix caching, prefix construction algorithm, harmful content detection]
  - **authors:** Jirui Yang, Hengqi Guo, Zhihui Lu, Yi Zhao, Yuansen Zhang, Shijing Hu, Qiang Duan, Yinggui Wang, Tao Wei
  - **institution:** Fudan University, Ant Group, Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.16650
  - **Simple LLM Summary:** This paper introduces Prefix Probing, a lightweight method for detecting harmful content in LLMs by comparing the model's log-probabilities for "agreement" versus "refusal" response prefixes. It uses prefix caching to reduce latency and an algorithm to automatically construct effective probe prefixes. The method achieves detection accuracy comparable to external safety models with minimal computational overhead, requiring no extra model deployment.

- **[arXiv251219] Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking**
  - **tags:** [mlsys], [others], [white-box watermarking, chaotic sequences, logistic map, genetic algorithm, weight injection]
  - **authors:** Sangeeth B, Serena Nicolazzo, Deepa K., Vinod P
  - **institution:** Cochin University of Science and Technology, University of Eastern Piedmont
  - **link:** https://arxiv.org/pdf/2512.16658
  - **Simple LLM Summary:** The paper proposes a white-box watermarking method for deep neural networks that embeds ownership information into model weights using chaotic sequences generated by a logistic map. Ownership verification is performed via a genetic algorithm to recover the original chaotic parameters. The approach maintains model accuracy and remains detectable after fine-tuning, offering a scalable solution for intellectual property protection.

- **[arXiv251219] Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering**
  - **tags:** [mlsys], [others], [ensemble learning, character-level CNN, LightGBM, feature engineering, URL analysis, FastAPI]
  - **authors:** Rudra Dubey, Arpit Mani Tripathi, Archit Srivastava, Sarvpal Singh
  - **institution:** Madan Mohan Malaviya University of Technology
  - **link:** https://arxiv.org/pdf/2512.16717
  - **Simple LLM Summary:** This paper proposes a phishing detection system using an ensemble model that combines a character-level Convolutional Neural Network (CNN) with a LightGBM classifier based on engineered URL features. The system achieves high performance metrics, such as 99.819% accuracy, and is deployed as a real-time service via FastAPI. The results show the ensemble approach outperforms individual models, effectively identifying modern phishing techniques with low false positive rates.

- **[arXiv251219] PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy**
  - **tags:** [mlsys], [others], [explainable AI, differential privacy, membership inference attack, re-identification attack, post-hoc explanations, feature selection, transformer models]
  - **authors:** Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.16851
  - **Simple LLM Summary:** The paper proposes a framework that uses explainable AI (XAI) to identify the most influential features in AI-XR models and selectively applies differential privacy (DP) to those features during inference to defend against privacy attacks. This XAI-guided DP approach reduces the success rates of membership inference and re-identification attacks while preserving model accuracy and improving inference time compared to traditional DP. The method is deployed as a system called PrivateXR on an HTC VIVE Pro headset, allowing users to adjust privacy levels in real-time during XR gameplay.

- **[arXiv251219] Pixel Seal: Adversarial-only training for invisible image and video watermarking**
  - **tags:** [mlsys], [multi-modal training], [adversarial-only training, three-stage training schedule, JND-based attenuation, temporal watermark pooling]
  - **authors:** Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko
  - **institution:** Meta FAIR
  - **link:** https://arxiv.org/pdf/2512.16874
  - **Simple LLM Summary:** Pixel Seal introduces an adversarial-only training paradigm for invisible watermarking, eliminating unreliable perceptual losses and using a three-stage schedule to decouple robustness and imperceptibility. It addresses high-resolution scaling with JND-based attenuation and adapts to video via temporal pooling. The method achieves state-of-the-art robustness and imperceptibility for image and video watermarking.

- **[arXiv251219] Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences**
  - **tags:** [ai], [quantum information theory], [strong data-processing inequalities, hockey-stick divergence, quantum channels, contraction coefficients, Fγ curves, reverse-Pinsker inequalities]
  - **authors:** Theshani Nuradha, Ian George, Christoph Hirche
  - **institution:** University of Illinois Urbana-Champaign, National University of Singapore, Leibniz Universität Hannover
  - **link:** https://arxiv.org/pdf/2512.16778
  - **Simple LLM Summary:** This paper establishes non-linear strong data-processing inequalities (SDPI) for quantum hockey-stick divergences, improving upon existing linear bounds. The method involves defining Fγ curves to characterize SDPI for sequential compositions of noisy quantum channels. The results enable tighter finite mixing time bounds and stronger privacy guarantees for sequential private quantum channels.
