# 20251215-20251221 (cs.CR)

## 2025-12-18

- **[arXiv251218] Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol**
  - **tags:** [mlsys], [others], [zero-knowledge proof, Model Context Protocol, privacy-preserving audit, Circom, mutual auditing]
  - **authors:** Guanlin Jing, Huayi Qi
  - **institution:** Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.14737
  - **Simple LLM Summary:** This paper introduces a framework that integrates zero-knowledge proofs with the Model Context Protocol (MCP) to enable verifiable, privacy-preserving audits of agent communications without revealing message content. It achieves mutual auditing for compliance and billing while maintaining data authenticity and privacy with negligible latency overhead. The authors implement the system, claiming it is the first to offer such verifiable mutual auditing for agent communications.

- **[arXiv251218] Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)**
  - **tags:** [mlsys], [others], [quantum machine learning, hybrid quantum computing, amplitude encoding, entanglement encoding, hierarchical threat detection, anomaly detection, intrusion confirmation, multiattack classification, deep neural networks, ensemble classifiers]
  - **authors:** Tan Le, Van Le, Sachin Shetty
  - **institution:** Hampton University, Virginia Polytechnic Institute and State University, Old Dominion University
  - **link:** https://arxiv.org/pdf/2512.14742
  - **Simple LLM Summary:** This paper proposes a hierarchical cybersecurity framework for Open Radio Access Networks (O-RAN) that integrates hybrid quantum computing and machine learning. The method uses quantum-inspired feature encodings (amplitude- and entanglement-based) with deep and ensemble classifiers for anomaly detection, intrusion confirmation, and multiattack classification. The framework demonstrates near-perfect accuracy, high recall, and strong interpretability, indicating readiness for scalable deployment in O-RAN environments.

- **[arXiv251218] Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs**
  - **tags:** [mlsys], [post-training], [backdoor attack, continual fine-tuning, gradient alignment, P-Trojan, persistence]
  - **authors:** Jing Cui, Yufei Han, Jianbin Jiao, Junge Zhang
  - **institution:** University of Chinese Academy of Sciences, INRIA, Institute of Automation, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.14741
  - **Simple LLM Summary:** The paper proposes P-Trojan, a backdoor attack method that optimizes for persistence by aligning poisoned gradients with clean task gradients on token embeddings during model poisoning. Experiments show it maintains over 99% backdoor persistence across continual fine-tuning updates on models like Qwen2.5 and LLaMA3 without harming clean-task accuracy. This highlights the need for stronger defenses and persistence-aware evaluation in real-world LLM adaptation pipelines.

- **[arXiv251218] Factor(U,T): Controlling Untrusted AI by Monitoring their Plans**
  - **tags:** [ai], [ai safety], [factored cognition, control protocols, task decomposition, untrusted decomposer, monitoring, BigCodeBench, AUROC]
  - **authors:** Edward Lue Chee Lip, Anthony Channg, Diana Kim, Aaron Sandoval, Kevin Zhu
  - **institution:** Algoverse AI Research, Colorado State University, Orange Coast College
  - **link:** https://arxiv.org/pdf/2512.14745
  - **Simple LLM Summary:** This paper introduces Factor(U,T), a protocol where an untrusted, potentially malicious AI model decomposes a complex task into subtasks, which are then implemented by trusted models. It finds that monitoring only the natural language decomposition plans is ineffective for detecting attacks, whereas monitoring the concrete code solutions of the subtasks provides strong safety and discrimination. The main conclusion is that implementation-context monitoring is crucial for safety when using untrusted models for task decomposition.

- **[arXiv251218] CODE ACROSTIC: Robust Watermarking for Code Generation**
  - **tags:** [mlsys], [post-training], [watermarking, cue list, low-entropy, high-entropy, comment removal attack]
  - **authors:** Li Lin, Siyuan Xin, Yang Cao, Xiaochun Cao
  - **institution:** Institute of Science Tokyo, Shanghai University, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.14753
  - **Simple LLM Summary:** This paper proposes a robust watermarking method for LLM-generated code that uses a Cue List to distinguish between low and high-entropy parts of the code for targeted watermark injection. It addresses the vulnerability of existing methods to comment removal attacks. The evaluation shows the method achieves higher detectability and usability compared to state-of-the-art techniques.

- **[arXiv251218] One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs**
  - **tags:** [mlsys], [post-training], [jailbreak attack, adversarial prompt, representation-level probing, linear separability, Probe-Guided Projection (PGP), transferability, finetuning, pretrained model]
  - **authors:** Yixin Tan, Zhe Yu, Jun Sakuma
  - **institution:** Institute of Science Tokyo, Riken AIP
  - **link:** https://arxiv.org/pdf/2512.14751
  - **Simple LLM Summary:** This paper investigates how jailbreak vulnerabilities transfer from pretrained to finetuned LLMs. It introduces the Probe-Guided Projection (PGP) attack, which uses representation-level probing to guide adversarial prompt optimization for better transferability. The main conclusion is that the pretrain-finetune paradigm inherently amplifies security risks, as vulnerabilities encoded in the pretrained model's representations are inherited by its finetuned variants.

- **[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation**
  - **tags:** [mlsys], [others], [Shapley-CMI, Private Set Intersection, Conditional Mutual Information, Vertical Federated Learning, data valuation]
  - **authors:** Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza
  - **institution:** Mondragon Unibertsitatea
  - **link:** https://arxiv.org/pdf/2512.14767
  - **Simple LLM Summary:** This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.

- **[arXiv251218] MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber**
  - **tags:** [mlsys], [llm inference], [multi-agent system, secure communication layer, ontology-aligned messaging, MITRE ATT&CK, Lightweight Random Forest]
  - **authors:** Arth Bhardwaj, Sia Godika, Yuvam Loonker
  - **institution:** Saint Francis High School, Massachusetts Institute of Technology, JBCN International School
  - **link:** https://arxiv.org/pdf/2512.14846
  - **Simple LLM Summary:** The paper proposes MALCDF, a real-time cyber defense framework where four specialized LLM agents (Detection, Intelligence, Response, Analysis) collaborate via a secure communication layer. It demonstrates that this multi-agent approach with encrypted, ontology-aligned messaging outperforms a lightweight ML baseline and a single-LLM setup in detection accuracy and F1-score on a test stream. The conclusion is that coordinating simple LLM agents improves practical, real-time cyber defense.

- **[arXiv251218] Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks**
  - **tags:** [mlsys], [llm inference], [penetration testing, prompt injection, agentic ai, autogen, crewai, ssrf, sql injection]
  - **authors:** Viet K. Nguyen, Mohammad I. Husain
  - **institution:** Cal Poly Pomona
  - **link:** https://arxiv.org/pdf/2512.14860
  - **Simple LLM Summary:** This paper conducts systematic penetration testing on agentic AI systems, comparing five LLM models across two frameworks using 13 attack scenarios. It finds significant security vulnerabilities, with over half of malicious prompts succeeding, and identifies novel defensive patterns like "hallucinated compliance".

- **[arXiv251218] Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection**
  - **tags:** [mlsys], [others], [AI-Augmented Security Operations Center (AISOC), malware detection, log-anomaly detection, score fusion, Elasticsearch, Kibana, Metasploit, EC2]
  - **authors:** Nnamdi Philip Okonkwo, Lubna Luxmi Dhirani
  - **institution:** University of Limerick
  - **link:** https://arxiv.org/pdf/2512.14935
  - **Simple LLM Summary:** The paper implements an AI-Augmented Security Operations Center (AISOC) on AWS that fuses outputs from a malware detector and a log-anomaly detector to triage threats. The method uses calibrated score fusion to classify activity as NORMAL, SUSPICIOUS, or HIGH_CONFIDENCE_ATTACK. It concludes that this simple, fused approach can effectively enhance cloud SOC capabilities in cost-sensitive environments.

- **[arXiv251218] Intrusion Detection in Internet of Vehicles Using Machine Learning**
  - **tags:** [mlsys], [others], [intrusion detection system, machine learning, multi-class classification, CAN bus, CICIoV2024 dataset]
  - **authors:** Hop Le, Izzat Alsmadi
  - **institution:** Texas A&M University-San Antonio
  - **link:** https://arxiv.org/pdf/2512.14958
  - **Simple LLM Summary:** This paper develops a machine learning-based intrusion detection system to classify malicious CAN bus traffic in the Internet of Vehicles. It uses the CICIoV2024 benchmark dataset to analyze attack patterns like DoS and spoofing. The initial findings confirm a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.

- **[arXiv251218] SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports**
  - **tags:** [mlsys], [others], [masked language model, fine-tuning, semantic surrogates, deep neural network, BERT]
  - **authors:** Sogol Masoumzadeh, Yufei Li, Shane McIntosh, Dániel Varró, Lili Wei
  - **institution:** McGill University, University of Waterloo, Linköping University
  - **link:** https://arxiv.org/pdf/2512.15003
  - **Simple LLM Summary:** The paper proposes SEBERTIS, a framework that fine-tunes bidirectional transformer models (like BERT) as Masked Language Models using semantically equivalent vocabulary (Semantic Surrogates) to create classifiers for security-related issue reports. This method reduces reliance on lexical shortcuts, enabling better detection of complex issues. The resulting classifier significantly outperforms existing ML and LLM baselines in precision, recall, and F1-score, demonstrating high effectiveness for real-time issue triage.

- **[arXiv251218] Quantifying Return on Security Controls in LLM Systems**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation (RAG), Monte Carlo simulation, loss exceedance curves, Laplace's Rule of Succession, adversarial probing, Garak, attribute-based access control (ABAC), named entity recognition (NER) redaction, NeMo Guardrails]
  - **authors:** Richard Helder Moulton, Austin O'Brien, John D. Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.15081
  - **Simple LLM Summary:** This paper introduces a framework to quantify the financial return on security controls for LLM systems by simulating attacks on a RAG service, estimating attack success probabilities, and modeling potential losses with Monte Carlo methods. The main conclusion is that controls like ABAC and NER redaction significantly reduce expected financial losses and offer high return-on-control, whereas NeMo Guardrails provides minimal benefit in the tested scenarios.

- **[arXiv251218] An Efficient Gradient-Based Inference Attack for Federated Learning**
  - **tags:** [mlsys], [others], [membership inference attack, federated learning, gradient-based methods, shadow training, attribute inference]
  - **authors:** Pablo Montaña-Fernández, Ines Ortega-Fernandez
  - **institution:** Gradiant
  - **link:** https://arxiv.org/pdf/2512.15143
  - **Simple LLM Summary:** This paper proposes a new gradient-based inference attack for federated learning that analyzes the temporal evolution of last-layer gradients across multiple training rounds. The attack uses a shadow model technique to learn gradient patterns and can be extended to attribute inference. The findings show that multi-round federated learning increases vulnerability to such attacks, with aggregators posing a greater threat than data owners.

- **[arXiv251218] Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions**
  - **tags:** [mlsys], [others], [Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), Quantum Generative Adversarial Networks (QGANs)]
  - **authors:** Siva Sai, Ishika Goyal, Shubham Sharma, Sri Harshita Manuri, Vinay Chamola, Rajkumar Buyya
  - **institution:** The University of Melbourne, Birla Institute of Technology and Science, Pilani, APPCAIR
  - **link:** https://arxiv.org/pdf/2512.15286
  - **Simple LLM Summary:** This survey paper explores Quantum Machine Learning (QML) techniques, including QNNs, QSVMs, VQCs, and QGANs, for cybersecurity applications like intrusion detection and malware classification. It concludes that QML offers potential advantages for processing high-dimensional data and enhancing security in areas like cloud computing, but also discusses current limitations and future research directions needed to address them.

- **[arXiv251218] Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference**
  - **tags:** [mlsys], [post-training], [post-training quantization, membership inference, AdaRound, BRECQ, OBC, quantization, privacy-utility trade-off]
  - **authors:** Chenxiang Zhang, Tongxi Qu, Zhong Li, Tian Zhang, Jun Pang, Sjouke Mauw
  - **institution:** University of Luxembourg, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.15335
  - **Simple LLM Summary:** This paper systematically evaluates how post-training quantization (PTQ) affects privacy by using membership inference attacks on models quantized with algorithms like AdaRound, BRECQ, and OBC. The study finds that lower-precision PTQ (e.g., 4-bit, 2-bit, 1.58-bit) significantly reduces privacy leakage, offering up to an order of magnitude less vulnerability compared to full-precision models, though at the cost of decreased utility. The results provide practical insights for balancing efficiency, accuracy, and privacy in model deployment.

- **[arXiv251218] Remotely Detectable Robot Policy Watermarking**
  - **tags:** [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]
  - **authors:** Michael Amir, Manon Flageat, Amanda Prorok
  - **institution:** University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.15379
  - **Simple LLM Summary:** This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot's motions using the policy's inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies.

- **[arXiv251218] How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?**
  - **tags:** [mlsys], [llm training], [membership inference, semantically equivalent code transformation, variable renaming, causal analysis, code obfuscation]
  - **authors:** Hua Yang, Alejandro Velasco, Thanh Le-Cong, Md Nazmul Haque, Bowen Xu, Denys Poshyvanyk
  - **institution:** North Carolina State University, William & Mary, The University of Melbourne
  - **link:** https://arxiv.org/pdf/2512.15468
  - **Simple LLM Summary:** This paper investigates how semantically equivalent code transformations, such as variable renaming, can be used to evade membership inference detection in large language models for code. It finds that these transformations, especially RenameVariable, can significantly reduce the success of membership inference attacks without substantially harming model performance. The results reveal a critical vulnerability in license compliance enforcement for code LLMs, showing that transformation-based obfuscation can weaken detection of unauthorized code usage.

- **[arXiv251218] Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection**
  - **tags:** [mlsys], [others], [transformer, multi-head self-attention, positional encoding, precision-focused loss, edge deployment, TensorFlow Lite, ONNX, TensorRT]
  - **authors:** Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos
  - **institution:** KTH Royal Institute of Technology, Lenovo
  - **link:** https://arxiv.org/pdf/2512.15503
  - **Simple LLM Summary:** This paper proposes AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons. It uses multi-head self-attention to capture spatiotemporal dependencies and a precision-focused loss to minimize false positives. The method demonstrates high performance and achieves sub-millisecond inference latency, making it suitable for deployment on resource-constrained edge platforms.

- **[arXiv251218] BashArena: A Control Setting for Highly Privileged AI Agents**
  - **tags:** [mlsys], [cluster infrastructure], [AI control, red teaming, blue teaming, adversarial evaluation, system administration tasks, sabotage detection, Linux environment, control protocols]
  - **authors:** Adam Kaufman, James Lucassen, Tyler Tracy, Cody Rushing, Aryan Bhatt
  - **institution:** Redwood Research
  - **link:** https://arxiv.org/pdf/2512.15688
  - **Simple LLM Summary:** The paper introduces BashArena, a control setting with 637 Linux system administration tasks and sabotage objectives to study AI control techniques for highly privileged agents. It evaluates frontier LLMs in an adversarial game between red teams (performing sabotage) and blue teams (detecting it), finding that Claude Sonnet 4.5 can evade detection by GPT-4.1 mini 26% of the time, establishing a baseline for more effective control protocols.
