---
slug: /daily/cscr/20251222-20251228
---
# 20251222-20251228 (cs.CR)

## 2025-12-22

- **[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]
  - **authors:** Saksham Sahai Srivastava, Haoyu He
  - **institution:** University of Georgia
  - **link:** https://arxiv.org/pdf/2512.16962
  - **Simple LLM Summary:** This paper introduces MemoryGraft, a novel attack that poisons an LLM agent's long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent's semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT's DataInterpreter.

- **[arXiv251222] Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation**
  - **tags:** [mlsys], [others], [adversarial attacks, deep learning, cybersickness detection, visual tunneling, MI-FGSM, PGD, C&W, DeepTCN, Transformer]
  - **authors:** Istiak Ahmed, Ripan Kumar Kundu, Khaza Anuarul Hoque
  - **institution:** University of Missouri-Columbia
  - **link:** https://arxiv.org/pdf/2512.17029
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Adversarial-VR, an open-source Unity testbed that integrates DeepTCN and Transformer models for real-time cybersickness detection and mitigation, and evaluates their robustness against adversarial attacks like MI-FGSM, PGD, and C&W. The results show these attacks can successfully fool the system, significantly degrading model accuracy and preventing correct mitigation.

- **[arXiv251222] Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors**
  - **tags:** [mlsys], [others], [soft prompt attacks, adversarial auditing, agentic framework, risk metrics, embedding-space robustness]
  - **authors:** Huixin Zhan
  - **institution:** New Mexico Institute of Mining and Technology
  - **link:** https://arxiv.org/pdf/2512.17146
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SAGE, an agentic framework that audits genomic foundation models by injecting soft prompt perturbations and evaluating performance degradation. It finds that models like ESM2 are vulnerable to such attacks, revealing hidden security risks in biomedical applications.

- **[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs**
  - **tags:** [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]
  - **authors:** Madhava Gaikwad
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.17251
  - **Simple LLM Summary:** The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.

- **[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning**
  - **tags:** [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]
  - **authors:** Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu
  - **institution:** Nankai University, University of Louisville, University of North Texas
  - **link:** https://arxiv.org/pdf/2512.17254
  - **Simple LLM Summary:** The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.

- **[arXiv251222] AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens**
  - **tags:** [mlsys], [post-training], [adversarial control tokens, beam-search exploration, last-layer logit gap, LoRA-based adversarial training, reward hacking]
  - **authors:** Tung-Ling Li, Yuhao Wu, Hongliang Liu
  - **institution:** Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.17375
  - **Simple LLM Summary:** The paper introduces AdvJudge-Zero, a method that uses beam-search on a model's next-token distribution to discover short, low-perplexity control token sequences that can flip the binary decisions of LLM-as-a-Judge systems from "No" to "Yes". It concludes that these tokens represent a realistic reward-hacking vulnerability in post-training pipelines, and shows that adversarial training can mitigate the issue while preserving evaluation quality.

- **[arXiv251222] DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference**
  - **tags:** [mlsys], [others], [Private Inference, ReLU sharing, DReLU, cryptographic protocols, activation sharing]
  - **authors:** Yonathan Bornfeld, Shai Avidan
  - **institution:** Tel Aviv University
  - **link:** https://arxiv.org/pdf/2512.17398
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DeepShare, a method for efficient Private Inference (PI) that reduces computational costs by sharing the DReLU (the non-linear step function of ReLU) across channels and layers within a neural network. It achieves state-of-the-art results by drastically decreasing the number of expensive DReLU operations while maintaining model performance on tasks like classification and segmentation.

- **[arXiv251222] Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques**
  - **tags:** [mlsys], [others], [FastText, NSFWJS, sentiment analysis, data restoration, classification]
  - **authors:** Xingyu Feng
  - **institution:** Hainan University
  - **link:** https://arxiv.org/pdf/2512.17411
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method for detecting sensitive and illegal content on the Ethereum blockchain using machine learning. It employs a data restoration algorithm and uses FastText for sentiment analysis and NSFWJS for image detection. The study concludes that harmful content, including personal data and explicit images, coexists with benign data on the blockchain, highlighting privacy and security concerns.

- **[arXiv251222] Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models**
  - **tags:** [mlsys], [llm inference], [LoRA, PEFT, orthonormal transform, hidden-state scrambling, access control, instruction tuning, 4-bit quantization]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17519
  - **Simple LLM Summary:** This paper proposes K-OTG, a method for secret-key access control in language models. It uses a training-time dual-path corpus and inference-time orthonormal transforms to scramble hidden states, making the model unusable without the correct key while preserving authorized performance. The method is compatible with LoRA and 4-bit quantization, showing effective locking with minimal utility loss for authorized users.

- **[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals**
  - **tags:** [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17527
  - **Simple LLM Summary:** This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at â‰¤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.

- **[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification**
  - **tags:** [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]
  - **authors:** Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli
  - **institution:** The University of Texas at El Paso, University of North Carolina
  - **link:** https://arxiv.org/pdf/2512.17594
  - **Simple LLM Summary:** This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.

- **[arXiv251222] STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting**
  - **tags:** [mlsys], [multi-modal inference], [zero-shot learning, cross-modal retrieval, dual-encoder architecture, contrastive learning, structure-aware augmentation, semantic-traffic alignment]
  - **authors:** Yifei Cheng, Yujia Zhu, Baiyang Li, Xinhao Deng, Yitong Cai, Yaochen Ren, Qingyun Liu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.17667
  - **Simple LLM Summary:** The paper introduces STAR, a method that reformulates website fingerprinting as a zero-shot cross-modal retrieval problem, using a dual-encoder architecture to learn a joint embedding space for encrypted traffic traces and website logic profiles. It achieves high accuracy on unseen websites by aligning semantic and traffic features, demonstrating that semantic leakage is a major privacy risk in encrypted HTTPS traffic.

- **[arXiv251222] Digital and Web Forensics Model Cards, V1**
  - **tags:** [ai], [knowledge representation], [model cards, controlled vocabularies, web-based generator, digital forensics, web forensics]
  - **authors:** Paola Di Maio
  - **institution:** Ronin Institute, W3C AI Knowledge Representation Community Group
  - **link:** https://arxiv.org/pdf/2512.17722
  - **Simple LLM Summary:** This paper introduces a web-based framework for generating standardized model cards to represent knowledge in digital and web forensics, featuring controlled vocabularies for classification, reasoning, bias, and error. The main conclusion is the presentation of this beta framework and tool to establish an emerging standard, inviting community feedback for refinement.

- **[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models**
  - **tags:** [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]
  - **authors:** Jonathon Fox, William J Buchanan, Pavlos Papadopoulos
  - **institution:** Edinburgh Napier University
  - **link:** https://arxiv.org/pdf/2512.17902
  - **Simple LLM Summary:** This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.
