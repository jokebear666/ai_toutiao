---
slug: /daily/cscv/20251229-20260104
---
# 20251229-20260104 (cs.CV)

## 2025-12-29

- **[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation**
  - **tags:** [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]
  - **authors:** Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani; GenimeLabs
  - **link:** https://arxiv.org/pdf/2512.21402
  - **contributions:** 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传统指标无法评估真实观众参与度/Traditional metrics fail to assess real viewer engagement]
        Method[主要方法/Method] --> M1[使用VLM提取无监督视听特征/Use VLM to extract unsupervised audiovisual features]
        Method --> M2[聚类特征并训练回归评估器/Cluster features and train regression evaluator]
        Results[关键结果/Results] --> R1[预测与真实参与度强相关/Strong correlation between predicted and actual engagement]
        Results --> R2[提供可解释且可扩展的评估/Provides interpretable and scalable assessment]
    ```

- **[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding**
  - **tags:** [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]
  - **authors:** Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli
  - **institution:** California Institute of Technology, Stanford University
  - **link:** https://arxiv.org/pdf/2512.21414
  - **code:** https://github.com/christinaliu2020/tool-bottleneck-framework
  - **contributions:** 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]
        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]
        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]
    ```

- **[arXiv251229] Scalable Deep Subspace Clustering Network**
  - **tags:** [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]
  - **authors:** Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami
  - **institution:** University of Quebec at Montreal
  - **link:** https://arxiv.org/pdf/2512.21434
  - **contributions:** 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Deep Subspace Clustering Network"] --> Problem["核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity"]
        Root --> Method["主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization"]
        Root --> Results["关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance"]
    ```

- **[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset**
  - **tags:** [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]
  - **authors:** Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh
  - **institution:** Simon Fraser University, AIP Labs
  - **link:** https://arxiv.org/pdf/2512.21472
  - **code:** /githubsfu-mial/IMAplusplus
  - **contributions:** 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"] --> Problem["核心问题/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets"]
        Root --> Method["主要方法/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata"]
        Root --> Results["关键结果/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research"]
    ```

- **[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism**
  - **tags:** [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]
  - **authors:** Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21452
  - **contributions:** 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
    
        Problem --> P1("Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效")
        Problem --> P2("Data scarcity <br/> 数据稀缺")
    
        Method --> M1("DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强")
        Method --> M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络")
        Method --> M3("MS COCO Transfer Learning <br/> MS COCO迁移学习")
    
        Results --> R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能")
        Results --> R2("Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒")
        Results --> R3("New paradigm for automated detection <br/> 自动化检测新范式")
    ```

- **[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection**
  - **tags:** [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]
  - **authors:** Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu
  - **institution:** Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC
  - **link:** https://arxiv.org/pdf/2512.21459
  - **code:** https://github.com/chloeqxq/CCAD
  - **contributions:** 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]
        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]
        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]
    ```

- **[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification**
  - **tags:** [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]
  - **authors:** Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian
  - **institution:** Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital
  - **link:** https://arxiv.org/pdf/2512.21476
  - **code:** https://github.com/JeremyXSC/GPF-Net
  - **contributions:** 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]
        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]
        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]
    ```

- **[arXiv251229] Generative Multi-Focus Image Fusion**
  - **tags:** [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]
  - **authors:** Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li
  - **institution:** Zhejiang University, Donghai Laboratory
  - **link:** https://arxiv.org/pdf/2512.21495
  - **code:** https://github.com/Xinzhe99/StackMFF-Series
  - **contributions:** 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Multi-Focus Image Fusion] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法假设总有对焦图像/Existing methods assume always an in-focus image]
        B --> B2[边缘伪影/Edge artifacts]
        C --> C1[阶段一: 确定性融合/Stage 1: Deterministic Fusion (StackMFF V4)]
        C --> C2[阶段二: 生成式恢复/Stage 2: Generative Restoration (IFControlNet)]
        D --> D1[SOTA性能/State-of-the-art performance]
        D --> D2[处理复杂多焦内容/Handles complex multi-focal content]
    ```

- **[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering**
  - **tags:** [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]
  - **authors:** Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu
  - **institution:** University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University
  - **link:** https://arxiv.org/pdf/2512.21510
  - **contributions:** 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --> Problem["核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]
        Root --> Method["主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]
        Root --> Results["关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness."]
    ```

- **[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art**
  - **tags:** [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]
  - **authors:** Md Ashik Khan, Arafat Alam Jion
  - **institution:** Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology
  - **link:** https://arxiv.org/pdf/2512.21512
  - **contributions:** 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection."]
        Method["主要方法/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion."]
        Results["关键结果/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance."]
    ```

- **[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning**
  - **tags:** [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]
  - **authors:** Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang
  - **institution:** Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21507
  - **code:** https://github.com/Gloria2tt/SVBench-Evaluation
  - **contributions:** 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SVBench: 视频生成模型的社会推理评估<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有视频生成模型缺乏社会推理能力<br>Current models lack social reasoning]
        C --> C1[构建基于社会心理学范式的基准<br>Build benchmark based on social psychology paradigms]
        C --> C2[使用基于智能体的训练免费流程进行评估<br>Use training-free agent-based pipeline for evaluation]
        D --> D1[模型在表面合理性上表现良好<br>Models perform well on surface-level plausibility]
        D --> D2[模型在社会推理维度上系统性失败<br>Models fail systematically on social reasoning dimensions]
    ```

- **[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification**
  - **tags:** [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]
  - **authors:** Md Ashik Khan, Md Nahid Siddique
  - **institution:** Indian Institute of Technology Kharagpur, Florida International University
  - **link:** https://arxiv.org/pdf/2512.21508
  - **contributions:** 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 多模态胸部X光分析的计算成本高，泛化性差 / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]
        C[主要方法/Method: 使用冻结编码器和参数高效训练策略 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]
        D[关键结果/Results: PET方法性能优于全微调，但校准性差 / PET methods outperform full fine-tuning but have degraded calibration]
    ```

- **[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO**
  - **tags:** [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]
  - **authors:** Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji
  - **institution:** Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21514
  - **contributions:** 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]
        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]
        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]
        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]
        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]
        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]
        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]
    ```

- **[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions**
  - **tags:** [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]
  - **authors:** Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu
  - **institution:** Fuzhou University, Research Institute of Highway, Ministry of Transport
  - **link:** https://arxiv.org/pdf/2512.21513
  - **code:** https://github.com/WangPuyun/MuS-Polar3D
  - **contributions:** 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(现有数据集缺乏多样性的散射和观测条件/Existing datasets lack diversity in scattering and observation conditions)
        C --> C1(构建包含42个物体、7种散射条件、5个视角的偏振图像基准数据集/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)
        C --> C2(提出解耦的两阶段成像流程：去散射后3D重建/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)
        D --> D1(实现最佳平均角度误差15.49度/Achieve best mean angular error of 15.49 degrees)
        D --> D2(首个公开的定量浑浊水下偏振3D成像基准数据集/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)
    ```

- **[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data**
  - **tags:** [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]
  - **authors:** Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu
  - **institution:** Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University
  - **link:** https://arxiv.org/pdf/2512.21516
  - **contributions:** 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]
        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]
        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]
        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]
        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]
        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]
        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]
    ```

- **[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models**
  - **tags:** [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]
  - **authors:** Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu
  - **institution:** University of Washington, Intel
  - **link:** https://arxiv.org/pdf/2512.21529
  - **contributions:** 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification")
        Problem --> P2("标准方法预测不一致/Standard methods produce inconsistent predictions")
        Method --> M1("提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework")
        Method --> M2("结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses")
        Method --> M3("集成轻量级LoRA适配/Integrate lightweight LoRA adaptation")
        Results --> R1("提升全路径精度/Improves Full-Path Accuracy")
        Results --> R2("降低不一致性错误/Reduces Tree-based Inconsistency Error")
        Results --> R3("参数开销最小/Minimal parameter overhead")
    ```

- **[arXiv251229] Vision Transformers are Circulant Attention Learners**
  - **tags:** [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]
  - **authors:** Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21542
  - **code:** https://github.com/LeapLabTHU/Circulant-Attention
  - **contributions:** 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Vision Transformers are Circulant Attention Learners] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Self-attention quadratic complexity O(N²) is computationally heavy for high-resolution vision tasks.]
        C[主要方法/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]
        D[关键结果/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]
    ```

- **[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration**
  - **tags:** [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]
  - **authors:** Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani
  - **link:** https://arxiv.org/pdf/2512.21560
  - **contributions:** 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Intelligent Scene Augmentation<br>智能场景增强] --> B(Problem: Existing image editing lacks contextual appropriateness<br>核心问题: 现有图像编辑缺乏上下文合理性)
        A --> C(Method: Introduce two new tasks & build datasets<br>主要方法: 提出两个新任务并构建数据集)
        A --> D(Results: Identifies research gap for joint reasoning & generation<br>关键结果: 指出了联合推理与生成的研究空白)
        B --> E(Task 1: Context-aware object insertion<br>任务1: 上下文感知物体插入)
        B --> F(Task 2: Sponsor-product logo augmentation<br>任务2: 赞助商产品商标增强)
        C --> G(Utilize VLMs and diffusion models<br>利用视觉语言模型和扩散模型)
        C --> H(Build annotated datasets<br>构建带标注的数据集)
    ```

- **[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal**
  - **tags:** [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]
  - **authors:** Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim
  - **institution:** Seoul National University, OGQ
  - **link:** https://arxiv.org/pdf/2512.21545
  - **contributions:** 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]
        C[主要方法/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]
        D[关键结果/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]
    ```

- **[arXiv251229] Exploration of Reproducible Generated Image Detection**
  - **tags:** [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]
  - **authors:** Yihang Duan
  - **institution:** Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)
  - **link:** https://arxiv.org/pdf/2512.21562
  - **contributions:** 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploration of Reproducible Generated Image Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]
        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]
        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]
    ```

- **[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation**
  - **tags:** [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]
  - **authors:** Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li
  - **institution:** Beihang University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2512.21576
  - **contributions:** 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]
        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]
        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]
    ```

- **[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings**
  - **tags:** [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]
  - **authors:** Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura
  - **institution:** Keio University
  - **link:** https://arxiv.org/pdf/2512.21582
  - **contributions:** 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("LLM-Free Image Captioning Evaluation in Reference-Flexible Settings") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LLM-based metrics lack neutrality/LLM指标缺乏中立性")
        Problem --> P2("LLM-free metrics lack performance/无LLM指标性能不足")
        Method --> M1("Propose Pearl metric/提出Pearl指标")
        Method --> M2("Learn image-caption & caption-caption similarity/学习图像-描述与描述-描述相似性")
        Method --> M3("Construct large human-annotated dataset/构建大规模人工标注数据集")
        Results --> R1("Outperforms LLM-free metrics on benchmarks/在基准测试中超越其他无LLM指标")
        Results --> R2("Works in reference-based & reference-free settings/适用于有参考和无参考设置")
        Results --> R3("Fast & aligned with human judgment/快速且与人类判断一致")
    ```

- **[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation**
  - **tags:** [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]
  - **authors:** Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu
  - **institution:** Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.
  - **link:** https://arxiv.org/pdf/2512.21584
  - **code:** this https URL
  - **contributions:** 1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]
        C[主要方法/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]
        D[关键结果/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]
    ```

- **[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement**
  - **tags:** [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]
  - **authors:** Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou
  - **institution:** University of Electronic Science and Technology of China, Southwestern University of Finance and Economics
  - **link:** https://arxiv.org/pdf/2512.21598
  - **contributions:** 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]
        C[主要方法/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]
        D[关键结果/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]
    ```

- **[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians**
  - **tags:** [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]
  - **authors:** Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han
  - **institution:** Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)
  - **link:** https://arxiv.org/pdf/2512.21599
  - **contributions:** 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --> B(核心问题/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)
        A --> C(主要方法/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)
        A --> D(关键结果/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)
    ```

- **[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care**
  - **tags:** [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]
  - **authors:** Yusuf Brima, Marcellin Atemkeng
  - **institution:** Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)
  - **link:** https://arxiv.org/pdf/2512.21602
  - **contributions:** 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]
        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]
        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]
    ```

- **[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]
  - **authors:** Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou
  - **institution:** University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21616
  - **contributions:** 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TAMEing Long Contexts in Personalization<br>论文标题] --> B[Problem: Existing MLLM personalization lacks long-context support<br>核心问题: 现有MLLM个性化方法缺乏长上下文支持]
        A --> C[Method: Proposes TAME framework with double memory & RA2G<br>主要方法: 提出TAME框架, 包含双重记忆和RA2G范式]
        A --> D[Results: TAME achieves best performance on LCMP benchmark<br>关键结果: TAME在LCMP基准测试中取得最佳性能]
    ```

- **[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration**
  - **tags:** [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]
  - **authors:** Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang
  - **institution:** Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University
  - **link:** https://arxiv.org/pdf/2512.21618
  - **contributions:** 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp
  - **Simple LLM Summary:** SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --> B[核心问题/Problem: 现有方法难以同时实现高保真渲染和交互式交通编辑 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]
        A --> C[主要方法/Method: 提出对称自回归在线修复范式与免训练协调机制 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]
        A --> D[关键结果/Results: 在新视角增强和3D车辆插入中实现最先进性能 / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]
    ```

- **[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective**
  - **tags:** [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]
  - **authors:** Zhiwen Yang, Jinglin Xu, Yuxin Pen
  - **institution:** Peking University, University of Science and Technology Beijing
  - **link:** https://arxiv.org/pdf/2512.21617
  - **code:** https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM
  - **contributions:** 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]
        Root --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[Support set as confounder/支持集作为混淆变量]
        Problem --> P2[Biased data distribution/有偏数据分布]
        Problem --> P3[Spurious correlations/虚假关联]
        Method --> M1[Causal Intervention/因果干预]
        Method --> M2[IMSE: Sample-level/IMSE: 样本层面]
        Method --> M3[IMFR: Feature-level/IMFR: 特征层面]
        Results --> R1[SOTA Performance/最优性能]
        Results --> R2[Datasets: CUB, Dogs, Cars/数据集: CUB, Dogs, Cars]
    ```

- **[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints**
  - **tags:** [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]
  - **authors:** Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira
  - **institution:** Universitas Brawijaya
  - **link:** https://arxiv.org/pdf/2512.21637
  - **contributions:** 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --> B(核心问题/Problem: Attribute Entanglement in Text-Driven Image Editing)
        A --> C(主要方法/Method: Sparse Latent Constraints via L1 Regularization)
        A --> D(关键结果/Results: More Focused Edits, Reduced Unintended Changes)
    ```

- **[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding**
  - **tags:** [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]
  - **authors:** Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai
  - **institution:** Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA
  - **link:** https://arxiv.org/pdf/2512.21643
  - **code:** https://github.com/Zhouzone/OmniWeather
  - **contributions:** 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Omni-Weather: 统一多模态天气基础模型 / Unified Multimodal Foundation Model for Weather] --> B
        A --> C
        A --> D
        B[核心问题 / Problem: 天气建模中生成与理解任务分离 / Separation of generation and understanding in weather modeling]
        C[主要方法 / Method: 统一架构，共享自注意力，思维链数据集 / Unified architecture, shared self-attention, Chain-of-Thought dataset]
        D[关键结果 / Results: SOTA性能，任务互增强 / State-of-the-art performance, mutual enhancement of tasks]
    ```

- **[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References**
  - **tags:** [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]
  - **authors:** Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
  - **institution:** Zhejiang University, Fudan University, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.21641
  - **contributions:** 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]
        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]
        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]
        C1 --> C3[构建UniScene表示/Build UniScene Representation]
        C1 --> C4[利用运动历史推理/Reason with Motion History]
        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]
        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]
        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]
    ```

- **[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds**
  - **tags:** [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]
  - **authors:** Subramanyam Sahoo, Jared Junkin
  - **institution:** University of California, Berkeley, Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.21670
  - **code:** https://github.com/SubramanyamSahoo/The-Deepfake-Detective
  - **contributions:** 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the "black box" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]
        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]
        C --> C2[法证流形分析/Forensic Manifold Analysis]
        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]
        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]
    ```

- **[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles**
  - **tags:** [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]
  - **authors:** Jalal Khan
  - **institution:** United Arab Emirates University
  - **link:** https://arxiv.org/pdf/2512.21673
  - **contributions:** 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp
  - **Simple LLM Summary:** This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]
    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]
    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]
    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]
    ```

- **[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture**
  - **tags:** [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]
  - **authors:** Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu
  - **institution:** Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI
  - **link:** https://arxiv.org/pdf/2512.21675
  - **code:** https://github.com/thunderbolt215/UniPercept
  - **contributions:** 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["UniPercept: 统一感知级图像理解 / Unified Perceptual-Level Image Understanding"] --> Problem["MLLMs感知能力有限 / MLLMs' Perceptual Ability is Limited"]
        Root --> Method["提出统一基准与模型 / Proposes Unified Benchmark & Model"]
        Root --> Results["性能超越现有模型 / Outperforms Existing Models"]
        Problem --> P1["感知级特征理解不足 / Limited Perceptual-Level Feature Understanding"]
        Method --> M1["UniPercept-Bench基准 / UniPercept-Bench Benchmark"]
        Method --> M2["DAPT与Task-Aligned RL训练 / DAPT & Task-Aligned RL Training"]
        Results --> R1["在VR与VQA任务上泛化 / Generalizes on VR & VQA Tasks"]
        Results --> R2["可作为图像生成奖励模型 / Serves as Reward Model for Generation"]
    ```

- **[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration**
  - **tags:** [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]
  - **authors:** Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang
  - **institution:** Rensselaer Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2512.21684
  - **contributions:** 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --> B[核心问题/Problem: VLM语义输出难以验证、复现和审计，存在不一致性 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]
        A --> C[主要方法/Method: 基于区块链的溯源框架，从幻灯片中提取概念和关系三元组，哈希上链 / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]
        A --> D[关键结果/Results: 揭示模型间显著差异，实现完美篡改检测和确定性复现，提供可扩展的完整性 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]
    ```

- **[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]
  - **authors:** Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao
  - **institution:** Nanjing University of Science and Technology, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21683
  - **code:** https://github.com/primebo1/C-Graph
  - **contributions:** 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法过滤域特定信息，损害性能/Existing methods filter domain-specific info, harming performance]
        C --> C1[利用结构一致性作为先验/Use structural consistency as prior]
        C --> C2[图建模: SPG层, SMD解码, CNC损失/Graph Modeling: SPG layer, SMD decoding, CNC loss]
        D --> D1[跨域性能SOTA/Cross-domain SOTA performance]
        D --> D2[保持源域精度/Preserves source-domain accuracy]
    ```

- **[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective**
  - **tags:** [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]
  - **authors:** Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu
  - **institution:** Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21691
  - **contributions:** 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[VGGT中的注意力崩溃现象/Attention Collapse in VGGT]
        C --> C1[将注意力迭代建模为退化扩散过程/Model Attention Iteration as Degenerate Diffusion]
        D --> D1[推导出预测崩溃的均值场PDE/Derive Mean-Field PDE Predicting Collapse]
        D --> D2[理论解释令牌合并的缓解作用/Theory Explains Token-Merging Remedy]
    ```

- **[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields**
  - **tags:** [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]
  - **authors:** Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch
  - **institution:** Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC
  - **link:** https://arxiv.org/pdf/2512.21692
  - **contributions:** 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法难以建模各向异性高光/Existing methods struggle with anisotropic specular reflections]
        C --> C1[提出ShinyNeRF框架/Propose ShinyNeRF framework]
        C1 --> C2[使用各向同性vMF混合近似出射辐射度/Use isotropic vMF mixture to approximate outgoing radiance]
        C2 --> C3[联合估计法线、切线、高光参数/Jointly estimate normals, tangents, specular parameters]
        D --> D1[实现SOTA性能/Achieves SOTA performance]
        D --> D2[提供物理解释和材质编辑/Provides physical interpretation and material editing]
    ```

- **[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating**
  - **tags:** [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]
  - **authors:** Li Yang, Yuting Liu
  - **institution:** Wannan Medical College
  - **link:** https://arxiv.org/pdf/2512.21693
  - **contributions:** 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Prior-AttUNet: 视网膜OCT液体分割 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[模糊边界与设备差异 / Ambiguous Boundaries & Device Heterogeneity]
        C --> C1[双路径架构 / Dual-Path Architecture]
        C --> C2[变分自编码器先验 / VAE Priors]
        C --> C3[三重注意力机制 / Triple-Attention Mechanism]
        D --> D1[高Dice分数 / High Dice Scores]
        D --> D2[低计算成本 / Low Computational Cost]
    ```

- **[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks**
  - **tags:** [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]
  - **authors:** Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu
  - **institution:** Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.
  - **link:** https://arxiv.org/pdf/2512.21694
  - **contributions:** 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]
        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]
        C --> C2[Use self-collected dataset<br/>使用自收集数据集]
        C --> C3[Pre-process images<br/>预处理图像]
        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]
        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]
    ```

- **[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection**
  - **tags:** [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]
  - **authors:** Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam
  - **institution:** Not explicitly stated in provided content. Affiliation/email domain not present.
  - **link:** https://arxiv.org/pdf/2512.21695
  - **contributions:** 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"] --> Problem["核心问题/Problem: Reliable detection of AI-generated images"]
        Root --> Method["主要方法/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training"]
        Root --> Results["关键结果/Results: SOTA on Chameleon, strong generalization across datasets"]
    ```

- **[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction**
  - **tags:** [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]
  - **authors:** Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang
  - **institution:** Nanjing University of Science and Technology, Nanjing Forestry University
  - **link:** https://arxiv.org/pdf/2512.21707
  - **code:** https://github.com/alanyz106/ST-MoE
  - **contributions:** 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("不灵活的时空表示/Inflexible spatiotemporal representation")
        Problem --> P2("高计算成本/High computational cost")
        Method --> M1("提出ST-MoE模型/Propose ST-MoE model")
        M1 --> M2("四种时空专家/Four spatiotemporal experts")
        M2 --> M3("双向时空Mamba/Bidirectional spatiotemporal Mamba")
        Results --> R1("精度超越SOTA/Outperforms SOTA in accuracy")
        Results --> R2("参数减少41.38%/Reduces parameters by 41.38%")
        Results --> R3("训练加速3.6倍/3.6x training speedup")
    ```

- **[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention**
  - **tags:** [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]
  - **authors:** Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang
  - **institution:** Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21710
  - **code:** https://github.com/Thelegendzz/RAPTOR
  - **contributions:** 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RAPTOR: 实时高分辨率无人机视频预测<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>视频预测的"三难困境": 速度、分辨率、质量<br>Video Prediction Trilemma: Speed, Resolution, Quality]
        C[主要方法/Method<br>高效视频注意力 (EVA) 模块<br>Efficient Video Attention (EVA) Module]
        D[关键结果/Results<br>首个在Jetson上512^2视频>30 FPS<br>First >30 FPS for 512^2 video on Jetson]
    ```

- **[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency**
  - **tags:** [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]
  - **authors:** Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang
  - **institution:** Amap Alibaba, Peking University (PKU), Tsinghua University (THU)
  - **link:** https://arxiv.org/pdf/2512.21714
  - **code:** https://astra-amap.github.io/AstraNav-World.github.io/
  - **contributions:** 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AstraNav-World] --> B[核心问题/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]
        A --> C[主要方法/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]
        A --> D[关键结果/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]
    ```

- **[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation**
  - **tags:** [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]
  - **authors:** Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21734
  - **code:** https://humanaigc.github.io/knot_forcing_demo_page/
  - **contributions:** 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate to maintain long-term coherence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: 驯服自回归视频扩散模型"] --> B["核心问题/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>实时肖像动画需要低延迟和一致性，但自回归模型存在误差累积和运动不连续问题。"]
        A --> C["主要方法/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and 'Running Ahead' mechanism.<br>分块生成与KV缓存，用于平滑过渡的时序结模块，以及'超前运行'机制。"]
        A --> D["关键结果/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>在消费级GPU上实现高保真、无限、交互式的实时肖像动画。"]
    ```

- **[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild**
  - **tags:** [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]
  - **authors:** Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21736
  - **code:** https://humanaigc.github.io/sync_anyone_demo_page/
  - **contributions:** 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SyncAnyone] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["现有方法破坏时空上下文 / Existing methods disrupt spatiotemporal context"]
        C --> C1["阶段1: 基于掩码的扩散变换器训练 / Stage 1: Mask-based DiT training"]
        C --> C2["阶段2: 无掩码调优与自校正 / Stage 2: Mask-free tuning & self-correction"]
        D --> D1["SOTA视觉质量与时间一致性 / SOTA visual quality & temporal coherence"]
        D --> D2["更好的身份与背景保持 / Better identity & background preservation"]
    ```

- **[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning**
  - **tags:** [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]
  - **authors:** Hengyi Wu, Zhenyi Wang, Heng Huang
  - **institution:** University of Maryland, College Park, University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.21743
  - **contributions:** 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]
        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]
        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]
        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]
        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]
        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]
    ```

- **[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG**
  - **tags:** [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]
  - **authors:** Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy
  - **institution:** Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad
  - **link:** https://arxiv.org/pdf/2512.21747
  - **contributions:** 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --> B(核心问题/Problem: Driver drowsiness detection for road safety)
        A --> C(主要方法/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)
        A --> D(关键结果/Results: Improved stability on SEED-VIG, SOTA on STEW)
    ```

- **[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets**
  - **tags:** [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]
  - **authors:** Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli
  - **link:** https://arxiv.org/pdf/2512.21760
  - **contributions:** 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]
        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]
        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]
    ```

- **[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization**
  - **tags:** [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]
  - **authors:** Evgeny Alves Limarenko, Anastasiia Studenikina
  - **institution:** Moscow Institute of Physics and Technology
  - **link:** https://arxiv.org/pdf/2512.21769
  - **contributions:** 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("BertsWin: 3D MAE优化") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE")
        Problem --> P2("破坏空间关系/Destroys Spatial Context")
        Method --> M1("BertsWin混合架构/BertsWin Hybrid Architecture")
        Method --> M2("完整3D令牌网格/Full 3D Token Grid")
        Method --> M3("Swin窗口 & 结构损失/Swin Windows & Structural Loss")
        Results --> R1("5.8x语义收敛加速/5.8x Faster Convergence")
        Results --> R2("15倍训练轮次减少/15x Fewer Epochs")
        Results --> R3("FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction")
    ```

- **[arXiv251229] Inference-based GAN Video Generation**
  - **tags:** [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]
  - **authors:** Jingbo Yang, Adrian G. Bors
  - **institution:** University of York
  - **link:** https://arxiv.org/pdf/2512.21776
  - **contributions:** 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Inference-based GAN Video Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]
        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]
        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]
        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]
        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]
        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]
        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]
    ```

- **[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models**
  - **tags:** [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]
  - **authors:** Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky
  - **institution:** Ben-Gurion University, Amazon Prime Video, Tel-Aviv University
  - **link:** https://arxiv.org/pdf/2512.21778
  - **contributions:** 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Scene-VLM: Multimodal Video Scene Segmentation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有方法限制/Limitations of existing methods")
        P1 --> P1_1("视觉中心偏见/Visual-centric bias")
        P1 --> P1_2("孤立分类/Isolated shot classification")
        P1 --> P1_3("缺乏可解释性/Lack explainability")
        Method --> M1("微调VLM框架/Fine-tuned VLM framework")
        M1 --> M1_1("多模态推理/Multimodal reasoning")
        M1 --> M1_2("序列预测/Sequential prediction")
        M1 --> M1_3("上下文聚焦窗口/Context-focus window")
        Method --> M2("置信度提取/Confidence score extraction")
        Method --> M3("生成解释/Generate rationales")
        Results --> R1("SOTA性能/State-of-the-art performance")
        Results --> R2("显著提升/Significant improvement on MovieNet")
    ```

- **[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning**
  - **tags:** [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]
  - **authors:** Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles
  - **institution:** The Pennsylvania State University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21789
  - **contributions:** 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]
        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]
        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]
        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]
        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]
        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]
        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]
    ```

- **[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation**
  - **tags:** [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]
  - **authors:** Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan
  - **institution:** ByteDance Inc., Rutgers University
  - **link:** https://arxiv.org/pdf/2512.21788
  - **code:** https://github.com/yanq095/InstructMoLE
  - **contributions:** 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]
        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]
        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]
    ```

- **[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge**
  - **tags:** [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]
  - **authors:** Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane
  - **institution:** University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds
  - **link:** https://arxiv.org/pdf/2512.21792
  - **contributions:** 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI用于组织病理学图像中的足菌肿诊断：MICCAI 2024挑战赛") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("足菌肿诊断困难，尤其在资源匮乏地区<br>Mycetoma diagnosis is challenging, especially in low-resource settings")
        Method --> M1("组织mAIcetoma挑战赛，开发AI模型<br>Organized mAIcetoma challenge to develop AI models")
        Method --> M2("提供MyData数据集，用于分割和分类<br>Provided MyData dataset for segmentation and classification")
        Results --> R1("模型实现高精度分割<br>Models achieved high segmentation accuracy")
        Results --> R2("顶级模型分类性能显著<br>Top models showed significant classification performance")
    ```

- **[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise**
  - **tags:** [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]
  - **authors:** Abu Hanif Muhammad Syarubany
  - **institution:** Korea Advanced Institute of Science & Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2512.21797
  - **contributions:** 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Diffusion Posterior Sampling for Super-Resolution<br>扩散后验采样用于超分辨率"] --> Problem["核心问题/Problem<br>Single-image super-resolution under Gaussian noise<br>高斯噪声下的单图像超分辨率"]
        Root --> Method["主要方法/Method<br>Diffusion Posterior Sampling (DPS)<br>扩散后验采样<br>Unconditional prior + likelihood-guided conditioning<br>无条件先验 + 似然引导的条件化"]
        Root --> Results["关键结果/Results<br>Optimal PS scale 0.95, noise σ=0.01<br>最佳PS尺度0.95, 噪声σ=0.01<br>Balancing prior and data yields sharp details<br>平衡先验与数据得到清晰细节"]
    ```

- **[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection**
  - **tags:** [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]
  - **authors:** Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu
  - **institution:** University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21803
  - **contributions:** 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]
        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]
        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]
    ```

- **[arXiv251229] S&P 500 Stock's Movement Prediction using CNN**
  - **tags:** [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]
  - **authors:** Rahul Gupta
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.21804
  - **contributions:** 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势"] --> P1["传统方法依赖特征工程<br>Traditional methods rely on engineered features"]
        Problem --> P2["现有研究多使用单维数据<br>Existing research often uses single-dimension data"]
        Method["主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN"] --> M1["将历史数据矩阵视为图像<br>Treat historical data matrices as images"]
        Method --> M2["包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)"]
        Results["关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果"] --> R1["支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction"]
    ```

- **[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models**
  - **tags:** [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]
  - **authors:** Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang
  - **institution:** Australian National University, The University of Queensland, GE Research
  - **link:** https://arxiv.org/pdf/2512.21815
  - **contributions:** 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp
  - **Simple LLM Summary:** This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]
        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]
        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]
        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]
        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]
        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]
        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]
    ```

- **[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration**
  - **tags:** [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]
  - **authors:** Zhenwei Yang, Yibo Ai, Weidong Zhang
  - **institution:** University of Science and Technology Beijing, National Center for Materials Service Safety
  - **link:** https://arxiv.org/pdf/2512.21831
  - **contributions:** 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[感知挑战:遮挡,视角限制,通信延迟/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]
    C --> C1[提出XET-V2X框架/Proposes XET-V2X Framework]
    C1 --> C2[双层级空间交叉注意力模块/Dual-layer Spatial Cross-Attention Module]
    C2 --> C3[多视图图像特征聚合/Multi-view Image Feature Aggregation]
    C2 --> C4[点云融合/Point Cloud Fusion]
    D --> D1[在V2X-Seq-SPD等数据集上性能提升/Performance Improvements on V2X-Seq-SPD, etc.]
    D1 --> D2[检测与跟踪性能增强/Enhanced Detection & Tracking Performance]
    D2 --> D3[鲁棒且时序稳定的感知/Robust & Temporally Stable Perception]
    ```

- **[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse**
  - **tags:** [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]
  - **authors:** Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen
  - **institution:** Not explicitly stated in the provided content. Affiliation information is not included.
  - **link:** https://arxiv.org/pdf/2512.21845
  - **code:** this https URLETF2
  - **contributions:** 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题 / Problem"] --> P1["过拟合新数据 / Overfitting to new data"]
        Problem --> P2["灾难性遗忘旧数据 / Catastrophic forgetting of old data"]
        Problem --> P3["特征差异与类别错位 / Feature difference & Class misalignment"]
    
        Method["主要方法 / Method"] --> M1["SCL-PNC方法 / SCL-PNC Method"]
        M1 --> M1_1["自适应层扩展主干 / Adapt-layer for backbone expansion"]
        M1 --> M1_2["动态参数化ETF分类器 / Dynamic Parametric ETF Classifier"]
        M1 --> M1_3["并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation"]
    
        Results["关键结果 / Results"] --> R1["高效处理类别增长 / Efficiently handles increasing categories"]
        Results --> R2["解决类别错位 / Addresses class misalignment"]
        Results --> R3["确保特征一致性 / Ensures feature consistency"]
    ```

- **[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection**
  - **tags:** [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]
  - **authors:** Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li
  - **institution:** Dalian Minzu University, Shandong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21856
  - **code:** https://github.com/HTUTU2/TPS-SCL
  - **contributions:** 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Breaking Alignment Barriers: TPS-SCL<br>突破对齐壁垒: TPS-SCL] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖对齐数据集<br>Existing methods rely on aligned datasets]
        B --> B2[真实场景图像未对齐<br>Real-world images are unaligned]
        C --> C1[双流MobileViT编码器<br>Dual-stream MobileViT encoder]
        C --> C2[TPS对齐模块<br>TPS Alignment Module]
        C --> C3[语义关联约束模块<br>Semantic Correlation Constraint Module]
        C --> C4[跨模态关联模块<br>Cross-Modal Correlation Module]
        D --> D1[轻量级SOTA性能<br>Lightweight SOTA performance]
        D --> D2[超越主流RGB-T方法<br>Outperforms mainstream RGB-T methods]
    ```

- **[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models**
  - **tags:** [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]
  - **authors:** Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue
  - **institution:** CyberAgent
  - **link:** https://arxiv.org/pdf/2512.21860
  - **code:** https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings
  - **contributions:** 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 全局图像嵌入无法聚焦于特定文本条件]
        C[主要方法/Method: DIOR - 利用LVLM生成单字描述并提取最后token的隐藏状态]
        D[关键结果/Results: 在条件图像相似性任务上超越现有方法]
    ```

- **[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees**
  - **tags:** [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]
  - **authors:** Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou
  - **institution:** Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.21857
  - **code:** https://github.com/Haodong-Lei-Ray/ADT-Tree
  - **contributions:** 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fast Inference of Visual AR Model with ADT-Tree<br>视觉自回归模型快速推理与ADT-Tree"]
        Root --> Problem["核心问题/Problem<br>Visual AR models have slow sequential inference.<br>视觉AR模型推理慢"]
        Root --> Method["主要方法/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>提出邻接自适应动态草稿树"]
        Root --> Results["关键结果/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>在基准测试上实现3.13x/3.05x加速"]
        Problem --> P1["Spatially varying token prediction difficulty.<br>空间变化的token预测难度"]
        Method --> M1["Dynamically adjusts tree depth & width.<br>动态调整树深度与宽度"]
        Method --> M2["Leverages adjacency & prior acceptance rates.<br>利用邻接关系和先验接受率"]
        Results --> R1["Integrates with relaxed sampling.<br>可与松弛采样方法结合"]
    ```

- **[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening**
  - **tags:** [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]
  - **authors:** Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan
  - **institution:** North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh
  - **link:** https://arxiv.org/pdf/2512.21861
  - **contributions:** 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]
        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]
        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]
    ```

- **[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition**
  - **tags:** [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]
  - **authors:** Yihan Hu, Xuelin Chen, Xiaodong Cun
  - **institution:** Great Bay University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21865
  - **code:** https://github.com/GVCLab/EasyOmnimatte
  - **contributions:** 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EasyOmnimatte] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法慢且次优/Existing methods are slow and suboptimal]
        C --> C1[双专家微调/Dual-Expert Finetuning]
        C1 --> C2[效果专家/Effect Expert]
        C1 --> C3[质量专家/Quality Expert]
        D --> D1[高质量分解/High-quality decomposition]
        D --> D2[高效快速/Efficient and fast]
    ```

- **[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation**
  - **tags:** [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]
  - **authors:** Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball
  - **institution:** University of California, San Diego, Dolby Laboratories
  - **link:** https://arxiv.org/pdf/2512.21867
  - **contributions:** 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]
        Method[主要方法/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]
        Results[关键结果/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]
    ```

- **[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer**
  - **tags:** [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]
  - **authors:** Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang
  - **institution:** Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University
  - **link:** https://arxiv.org/pdf/2512.21883
  - **code:** https://github.com/dtc111111/Reloc-VGGT
  - **contributions:** 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --> B[核心问题/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]
        A --> C[主要方法/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]
        A --> D[关键结果/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]
    ```

- **[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis**
  - **tags:** [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]
  - **authors:** Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen
  - **institution:** Southern University of Science and Technology, University of Warwick, Fudan University
  - **link:** https://arxiv.org/pdf/2512.21881
  - **contributions:** 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法瓶颈/Bottlenecks of Existing Methods]
        B1 --> B1_1[图谱方法: 丢失细节, 需大数据/Atlas-based: lose details, need big data]
        B1 --> B1_2[无图谱方法: 内存计算成本高/Atlas-free: high memory & compute cost]
        C --> C1[两阶段自适应设计/Two-stage Adaptive Design]
        C1 --> C1_1[轻量时序提取器: 全局上下文与显著性排序/Lightweight Temporal Extractor: global context & saliency ranking]
        C1 --> C1_2[4D分层编码器: 从Top-k窗口学习/4D Hierarchical Encoder: learn from top-k windows]
        D --> D1[性能/Performance]
        D --> D2[效率/Efficiency]
        D1 --> D1_1[在七个基准上达到SOTA/Achieves SOTA on seven benchmarks]
        D2 --> D2_1[仅需4千次预训练会话/Only 4k pre-training sessions]
        D2 --> D2_2[GPU内存降至30%/GPU memory reduced to ~30%]
    ```

- **[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model**
  - **tags:** [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]
  - **authors:** Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen
  - **institution:** Hong Kong University of Science and Technology, University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.21890
  - **contributions:** 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp
  - **Simple LLM Summary:** This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CrownGen: Patient-customized Crown Generation<br>基于点扩散模型的个性化牙冠生成] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Digital crown design is labor-intensive<br>数字化牙冠设计费时费力]
        C --> C1[Uses a point diffusion model<br>使用点扩散模型]
        C --> C2[Has a boundary prediction module<br>包含边界预测模块]
        D --> D1[Surpasses SOTA in geometric fidelity<br>几何保真度超越现有方法]
        D --> D2[Reduces active design time<br>减少主动设计时间]
        D --> D3[Crowns are clinically non-inferior<br>临床质量不劣于人工设计]
    ```

- **[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer**
  - **tags:** [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]
  - **authors:** Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang
  - **institution:** Guangzhou Quwan Network Technology, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21905
  - **contributions:** 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>基于扩散Transformer的高保真长时人体图像动画"] --> B["核心问题/Problem"]
        A --> C["主要方法/Method"]
        A --> D["关键结果/Results"]
        B --> B1["长视频生成挑战<br>Long-duration Video Generation"]
        B --> B2["面部与手部细节合成不足<br>Lack of Fine-grained Facial/Hand Details"]
        C --> C1["混合隐式引导信号<br>Hybrid Implicit Guidance"]
        C --> C2["位置偏移自适应模块<br>Position Shift Adaptive Module"]
        C --> C3["数据增强与骨架对齐<br>Data Augmentation & Skeleton Alignment"]
        D --> D1["超越现有SOTA方法<br>Outperforms SOTA"]
        D --> D2["实现超1分钟动画<br>Exceeds 1-minute Animation"]
    ```

- **[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition**
  - **tags:** [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]
  - **authors:** Zeyu Liang, Hailun Xia, Naichuan Zheng
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.21916
  - **contributions:** 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[RGB与骨架模态异构融合困难/RGB-Skeleton Heterogeneous Fusion Difficulty]
        C --> C1[以人为中心的图表示学习/Human-Centric Graph Representation Learning]
        C1 --> C2[基于注意力的后校准/Attention-Based Post Calibration]
        C1 --> C3[双变体: PAN-Ensemble与PAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]
        D --> D1[三个数据集上SOTA性能/SOTA Performance on Three Datasets]
    ```

- **[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning**
  - **tags:** [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]
  - **authors:** Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21924
  - **contributions:** 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[泛化性差与异常残留/Generalizability & Residuals]
        C --> C1[解耦表示模块/Disentangled Representation Module]
        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]
        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]
    ```

- **[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization**
  - **tags:** [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]
  - **authors:** Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law
  - **institution:** JD.COM
  - **link:** https://arxiv.org/pdf/2512.21921
  - **code:** https://github.com/JD-GenX/AutoPP
  - **contributions:** 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]
        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]
        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]
        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]
        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]
        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]
        D --> D2[代码与数据集公开/Code & dataset released]
    ```

- **[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement**
  - **tags:** [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]
  - **authors:** Yiquan Gao, John See
  - **institution:** Heriot-Watt University
  - **link:** https://arxiv.org/pdf/2512.21944
  - **contributions:** 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")
        Method("主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")
        Results("关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")
    ```

- **[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials**
  - **tags:** [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]
  - **authors:** Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe
  - **institution:** University of Saskatchewan, New Mexico State University
  - **link:** https://arxiv.org/pdf/2512.21948
  - **contributions:** 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[自动化发现用于植被分类的紧凑光谱指数/Automated discovery of compact spectral indices for vegetation classification]
        C --> C1[生成归一化差异多项式候选特征/Generate candidate features via normalized difference polynomials]
        C --> C2[使用特征选择方法挑选指数/Use feature selection methods to pick indices]
        D --> D1[单个二阶指数达到96.26%准确率/Single degree-2 index achieves 96.26% accuracy]
        D --> D2[指数简单，可直接部署于GEE/Indices are simple and deployable on GEE]
    ```

- **[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models**
  - **tags:** [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]
  - **authors:** Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng
  - **institution:** The Chinese University of Hong Kong (CUHK)
  - **link:** https://arxiv.org/pdf/2512.21964
  - **contributions:** 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --> B(核心问题/Problem: MLLMs are sensitive to input noise, undermining clinical use)
        A --> C(主要方法/Method: Training-free IMC framework with PDC for vision and SMS for text)
        A --> D(关键结果/Results: SOTA performance on new multi-modal noise benchmark)
    ```

- **[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models**
  - **tags:** [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]
  - **authors:** Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner
  - **institution:** Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)
  - **link:** https://arxiv.org/pdf/2512.21985
  - **contributions:** 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]
        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]
        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]
        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]
        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]
        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]
    ```

- **[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation**
  - **tags:** [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]
  - **authors:** Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang
  - **institution:** The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.
  - **link:** https://arxiv.org/pdf/2512.21984
  - **code:** https://github.com/hhwmortal/PELD-Instance-segmentation
  - **contributions:** 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[狭窄视野, 伪影, 硬件限制/Narrow FOV, Artifacts, Hardware Constraints]
        C --> C1[轻量级多尺度注意力框架/Lightweight Multi-scale Attention Framework]
        C1 --> C2[主干: C2f-Pro (重参数化卷积+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]
        C1 --> C3[颈部: SSFF与TFE/Neck: SSFF and TFE]
        C1 --> C4[头部: 轻量共享头 (LMSH)/Head: Lightweight Shared Head (LMSH)]
        D --> D1[性能优越, 参数量少 (1.8M)/High Performance, Few Params (1.8M)]
        D --> D2[发布PELD数据集/Release PELD Dataset]
        D --> D3[良好泛化性/Good Generalization]
    ```

- **[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs**
  - **tags:** [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]
  - **authors:** Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He
  - **institution:** Chongqing University, Xinjiang University
  - **link:** https://arxiv.org/pdf/2512.21999
  - **code:** https://github.com/hujiayu1223/ALEAHallu
  - **contributions:** 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]
        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]
        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]
        C --> C1[激活数据集/Activation Dataset]
        C --> C2[定位关键参数/Locate Critical Parameters]
        C --> C3[对抗性编辑/Adversarial Editing]
    ```

- **[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception**
  - **tags:** [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]
  - **authors:** Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian
  - **institution:** Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad
  - **link:** https://arxiv.org/pdf/2512.22009
  - **contributions:** 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision "slow mode" and an efficient "fast mode" based on task demands. 3. Employs special perception tokens to dynamically guide the model's visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --> B[核心问题/Problem: Building efficient and precise GUI agents is challenging]
        A --> C[主要方法/Method: Slow-fast hybrid inference with adaptive perception tokens]
        A --> D[关键结果/Results: Matches SOTA performance with compact 2.5B size]
    ```

- **[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration**
  - **tags:** [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]
  - **authors:** Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji
  - **institution:** Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University
  - **link:** https://arxiv.org/pdf/2512.22010
  - **contributions:** 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]
        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]
        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]
    ```

- **[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching**
  - **tags:** [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]
  - **authors:** Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu
  - **institution:** Beijing Technology and Business University, The University of Sydney
  - **link:** https://arxiv.org/pdf/2512.22016
  - **contributions:** 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --> B[核心问题/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]
        A --> C[主要方法/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]
        A --> D[关键结果/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]
    ```

- **[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection**
  - **tags:** [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]
  - **authors:** Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma
  - **institution:** Shanghai Jiao Tong University, Chinese Academy of Military Science
  - **link:** https://arxiv.org/pdf/2512.22027
  - **code:** https://gendf.github.io/
  - **contributions:** 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Patch-Discontinuity Mining for Generalized Deepfake Detection"] --> B["核心问题/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns."]
        A --> C["主要方法/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model."]
        A --> D["关键结果/Results: Achieves SOTA generalization with only 0.28M parameters."]
    ```

- **[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models**
  - **tags:** [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]
  - **authors:** Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang
  - **institution:** Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University
  - **link:** https://arxiv.org/pdf/2512.22046
  - **contributions:** 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]
        C[主要方法/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]
        D[关键结果/Results: High ASR, preserves clean performance, defenses ineffective]
    ```

- **[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars**
  - **tags:** [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]
  - **authors:** Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu
  - **institution:** Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22065
  - **code:** https://streamavatar.github.io
  - **contributions:** 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]
        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]
        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]
    ```

- **[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents**
  - **tags:** [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]
  - **authors:** Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22047
  - **code:** https://github.com/Tongyi-MAI/MAI-UI
  - **contributions:** 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp
  - **Simple LLM Summary:** The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1(缺乏原生人机交互/Lack of Native Agent-User Interaction)
        B1 --> C2(仅UI操作的限制/Limits of UI-Only Operation)
        B1 --> C3(缺乏实用部署架构/Absence of Practical Deployment Architecture)
        B1 --> C4(动态环境脆弱性/Brittleness in Dynamic Environments)
        B2 --> D1(自演进数据管道/Self-Evolving Data Pipeline)
        B2 --> D2(原生设备-云协作系统/Native Device-Cloud Collaboration System)
        B2 --> D3(在线RL框架/Online RL Framework)
        D1 --> E1(包含用户交互/Includes User Interaction)
        D1 --> E2(包含MCP工具调用/Includes MCP Tool Calls)
        D3 --> E3(扩展并行环境/Scales Parallel Environments)
        D3 --> E4(扩展上下文长度/Scales Context Length)
        B3 --> F1(GUI Grounding SOTA/GUI Grounding SOTA)
        B3 --> F2(移动导航SOTA/Mobile Navigation SOTA)
        B3 --> F3(系统性能提升/System Performance Gains)
        F1 --> G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)
        F1 --> G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)
        F2 --> G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)
        F2 --> G4(MobileWorld: 41.7%/MobileWorld: 41.7%)
        F3 --> G5(设备性能提升33%/On-Device Perf. +33%)
        F3 --> G6(云调用减少40%/Cloud Calls -40%)
    ```

- **[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model**
  - **tags:** [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]
  - **authors:** Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang
  - **institution:** Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22096
  - **code:** https://github.com/stdstu12/YUME
  - **contributions:** 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1[大模型参数与慢推理/Large Model & Slow Inference]
        Problem --> P2[缺乏文本控制/Lack of Text Control]
        Method --> M1[长视频生成框架/Long-Video Gen Framework]
        Method --> M2[实时流加速策略/Real-time Streaming]
        Method --> M3[文本控制事件生成/Text-Controlled Events]
        M1 --> M1_Sub[统一上下文压缩与线性注意力/Unified Context Compression & Linear Attention]
        M2 --> M2_Sub[双向注意力蒸馏与文本嵌入/Bidirectional Attention Distillation & Text Embedding]
        Results --> R1[生成交互式世界/Generates Interactive Worlds]
        Results --> R2[支持键盘探索/Supports Keyboard Exploration]
        Results --> R3[公开代码库/Public Codebase]
    ```

- **[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking**
  - **tags:** [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]
  - **authors:** Momir Adžemović
  - **institution:** University of Belgrade
  - **link:** https://arxiv.org/pdf/2512.22105
  - **code:** https://github.com/Robotmurlock/TDLP
  - **contributions:** 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]
        C[主要方法/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]
        D[关键结果/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]
    ```

- **[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning**
  - **tags:** [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]
  - **authors:** Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang
  - **institution:** Microsoft Research, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.22120
  - **code:** https://github.com/zss02/BiPS
  - **contributions:** 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional "where-to-look" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]
        C[主要方法/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]
        D[关键结果/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]
    ```

- **[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right**
  - **tags:** [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]
  - **authors:** Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng
  - **institution:** Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.22118
  - **code:** https://isee-laboratory.github.io/ProEdit/
  - **contributions:** 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ProEdit: Inversion-based Editing From Prompts Done Right] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法过度注入源图像信息/Existing methods overly inject source info]
        B1 --> B2[阻碍属性编辑如姿态、数量、颜色/Hinders editing attributes like pose, number, color]
        C --> C1[注意力层面: KV-mix/Attention Aspect: KV-mix]
        C1 --> C11[混合源与目标KV特征/Mix source & target KV features]
        C --> C2[潜在层面: Latents-Shift/Latent Aspect: Latents-Shift]
        C2 --> C21[扰动编辑区域的源潜在表示/Perturb source latent in edited region]
        D --> D1[SOTA性能/SOTA performance]
        D --> D2[即插即用设计/Plug-and-play design]
    ```

- **[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI**
  - **tags:** [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]
  - **authors:** Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud
  - **institution:** Daffodil International University
  - **link:** https://arxiv.org/pdf/2512.21372
  - **contributions:** 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --> B(核心问题 / Problem: 胃肠道疾病图像分类挑战 / GI Disease Image Classification Challenge)
        A --> C(主要方法 / Method: 基于知识蒸馏的双流Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)
        A --> D(关键结果 / Results: 高准确率与可解释性 / High Accuracy & Explainability)
        B --> B1[数据量大, 类间差异小 / Large Data Volume, Subtle Inter-class Variation]
        C --> C1[教师模型: Swin + ViT / Teacher: Swin + ViT]
        C --> C2[学生模型: Tiny-ViT / Student: Tiny-ViT]
        C --> C3[可解释性分析: Grad-CAM等 / XAI: Grad-CAM etc.]
        D --> D1[准确率 > 0.99 / Accuracy > 0.99]
        D --> D2[AUC = 1.0000]
        D --> D3[适用于临床环境 / Suitable for Clinical Settings]
    ```

- **[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models**
  - **tags:** [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]
  - **authors:** Takuro Kutsuna
  - **institution:** Toyota Central R&D Labs., Inc.
  - **link:** https://arxiv.org/pdf/2512.21593
  - **contributions:** 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Residual Prior Diffusion (RPD) / 残差先验扩散模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details"]
        Method --> M1["两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model"]
        Method --> M2["概率模型与可处理ELBO / Probabilistic model with tractable ELBO"]
        Results --> R1["在合成数据上准确捕捉细节 / Accurately captures details on synthetic data"]
        Results --> R2["自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines"]
        Results --> R3["少步推理保持性能 / Maintains performance with few inference steps"]
    ```

- **[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring**
  - **tags:** [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]
  - **authors:** Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li
  - **institution:** Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21975
  - **code:** https://github.com/ReaganWu/RT-Focuser
  - **contributions:** 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[运动模糊降低图像质量 / Motion blur degrades image quality]
        B --> B2[实时应用挑战 / Challenges for real-time applications]
        C --> C1[轻量级U型网络 / Lightweight U-shaped network]
        C --> C2[三个关键组件 / Three key components: LD, MLIA, X-Fuse]
        D --> D1[30.67 dB PSNR / 30.67 dB PSNR]
        D --> D2[5.85M参数, 15.76 GMACs / 5.85M params, 15.76 GMACs]
        D --> D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]
    ```

- **[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology**
  - **tags:** [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]
  - **authors:** Sungwoo Kang
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.21988
  - **contributions:** 1. Identifies and defines the "color-clinical decoupling" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker's disproportionate sensitivity to noise in the b* color channel.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed "color-clinical decoupling," primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Color-Clinical Decoupling<br>颜色-临床解耦] --> B[核心问题/Problem<br>Does color calibration ensure clinical reliability?<br>色彩校准能否确保临床可靠性？]
        A --> C[主要方法/Method<br>Analyze 43,425 images across devices with CCM<br>使用CCM分析43,425张跨设备图像]
        A --> D[关键结果/Results<br>Color-clinical decoupling: ∆E↓ but ICC(ITA) poor<br>颜色-临床解耦：∆E下降但ITA的ICC差]
        B --> D
        C --> D
    ```

## 2025-12-30

- **[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [Structured Pruning, Multi-Objective Optimization, Edge Inference, MVCNN, View-Adaptive Compression]
  - **authors:** Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee
  - **institution:** Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science
  - **link:** https://arxiv.org/pdf/2512.22136
  - **contributions:** 1. Proposes a framework for lightweight DNN deployment that integrates structured pruning with multi-objective optimization to meet heterogeneous hardware constraints. 2. Demonstrates the framework on MVCNN by quantifying the contribution of individual views to accuracy for view-adaptive pruning budget allocation. 3. Shows experimentally that the compressed models meet user-specified accuracy and memory bounds while achieving 1.2x to 5.0x inference speedup across diverse hardware.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of deploying large DNNs on resource-constrained edge devices. It proposes SlimEdge, a method that combines structured pruning and multi-objective optimization to compress models like MVCNN while preserving task performance. The results show that this approach successfully meets specified accuracy and memory constraints while significantly reducing inference latency on various edge hardware platforms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SlimEdge: Lightweight Distributed DNN Deployment] --> B(核心问题/Problem: DNN部署在资源受限的边缘设备上/DNN deployment on resource-constrained edge devices)
        A --> C(主要方法/Method: 结构化剪枝与多目标优化/Structured Pruning & Multi-Objective Optimization)
        A --> D(关键结果/Results: 满足精度与内存约束，推理延迟降低1.2x-5.0x/Meets accuracy & memory bounds, 1.2x-5.0x latency reduction)
    ```

- **[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]
  - **authors:** Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan
  - **institution:** Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22170
  - **contributions:** 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据标注噪声/Annotation Noise]
        B --> B2[奖励黑客攻击/Reward Hacking]
        B --> B3[模型架构设计不足/Under-explored RM Architecture]
        C --> C1[单项目二元标注与跨提示配对/Single-item Binary Annotations & Cross-prompt Pairing]
        C --> C2[分层渐进查询注意力/Hierarchical Progressive Query Attention]
        C --> C3[改进的BT损失函数/Modified BT Loss for Win-Tie]
        D --> D1[奖励模型评估指标提升/Improved Direct RM Evaluation Metrics]
        D --> D2[视频生成后训练效果增强/Enhanced Efficacy of Post-training on Video Generation Models]
    ```

- **[arXiv251230] Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment**
  - **tags:** [cv], [sign language recognition], [3D CNN, LSTM, real-time processing, spatial-temporal features, edge deployment]
  - **authors:** Dawnena Key
  - **institution:** University of Denver
  - **link:** https://arxiv.org/pdf/2512.22177
  - **contributions:** 1. A hybrid 3D CNN-LSTM architecture for capturing spatial and temporal features in ASL signs. 2. A training methodology using multiple complementary datasets (WLASL, ASL-LEX, expert-annotated signs). 3. A deployment architecture supporting both cloud (AWS) and edge (OAK-D camera) inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a real-time American Sign Language recognition system that uses a hybrid 3D CNN and LSTM architecture to process video streams. The system, trained on multiple datasets, achieves high F1-scores and is deployed for practical use on both cloud and edge devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Real-Time ASL Recognition Using 3D CNN and LSTM] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[沟通障碍/Communication Barrier]
        B --> B2[实时翻译挑战/Real-time Translation Challenge]
        C --> C1[混合架构/Hybrid Architecture]
        C1 --> C1_1[3D CNN: 时空特征/Spatial-temporal Features]
        C1 --> C1_2[LSTM: 序列依赖/Sequential Dependencies]
        C --> C2[多数据集训练/Multi-dataset Training]
        C --> C3[云边部署/Cloud-Edge Deployment]
        D --> D1[高F1分数/High F1-scores (0.71-0.99)]
        D --> D2[实时推理/Real-time Inference]
    ```

- **[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps**
  - **tags:** [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]
  - **authors:** Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava
  - **institution:** University of Maryland
  - **link:** https://arxiv.org/pdf/2512.22175
  - **contributions:** 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Characterizing Motion Encoding in Video Diffusion Timesteps] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[视频扩散模型中运动编码机制不明确 / Motion encoding in video diffusion is poorly understood]
        C --> C1[通过条件注入量化运动-外观权衡 / Quantify motion-appearance trade-off via conditional injection]
        C --> C2[大规模定量研究 / Large-scale quantitative study]
        D --> D1[识别早期运动主导与后期外观主导阶段 / Identify early motion-dominant and late appearance-dominant regimes]
        D --> D2[简化单样本运动定制范式 / Simplify one-shot motion customization paradigm]
    ```

- **[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery**
  - **tags:** [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]
  - **authors:** Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir
  - **institution:** National University of Science and Technology (NUST)
  - **link:** https://arxiv.org/pdf/2512.22182
  - **contributions:** 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)
    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))
    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)
    ```

- **[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs**
  - **tags:** [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]
  - **authors:** Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou
  - **institution:** Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis
  - **link:** https://arxiv.org/pdf/2512.22183
  - **contributions:** 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unbiased Visual Reasoning with Controlled Visual Inputs] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[VLMs exploit spurious correlations/VLMs利用虚假关联]
        C --> C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: 解耦感知与推理的模块化框架]
        C1 --> C2[Frozen VLM sensor + LLM reasoner/冻结VLM感知器 + LLM推理器]
        C2 --> C3[Train with RL (GRPO)/使用强化学习(GRPO)训练]
        D --> D1[Improved robustness on SpuriVerse/在SpuriVerse上鲁棒性提升]
        D --> D2[Competitive on MMVP & SeedBench/在MMVP & SeedBench上保持竞争力]
        D --> D3[Transferable & interpretable/可迁移且可解释]
    ```

- **[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology**
  - **tags:** [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]
  - **authors:** Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He
  - **institution:** Tsinghua University, Xiamen University, Westlake University, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.22188
  - **code:** https://github.com/lingxitong/HookMIL
  - **contributions:** 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MIL loses context; Transformers are inefficient] --> P1[传统MIL丢失上下文/Traditional MIL loses context]
        Problem --> P2[基于Transformer的MIL计算复杂/Transformer-based MIL has quadratic complexity]
        Method[主要方法/Method: HookMIL Framework] --> M1[使用可学习的Hook Tokens/Use learnable Hook Tokens]
        Method --> M2[多模态初始化/Multimodal Initialization]
        Method --> M3[Hook多样性损失与通信机制/Hook Diversity Loss & Communication]
        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art Performance]
        Results --> R2[计算高效/Computationally Efficient]
        Results --> R3[可解释性/Interpretability]
    ```

- **[arXiv251230] SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening**
  - **tags:** [cv], [medical image analysis], [dual-encoder, data augmentation, transfer learning, intracranial aneurysm detection, Grad-CAM]
  - **authors:** Antara Titikhsha, Divyanshu Tak
  - **institution:** Carnegie Mellon University, Harvard Medical School, Mass General Brigham, Dana-Farber Cancer Institute, Brigham and Women’s Hospital
  - **link:** https://arxiv.org/pdf/2512.22185
  - **code:** https://github.com/antitikhsha/SAMM2D
  - **contributions:** 1. Introduced SAMM2D, a scale-aware multi-modal 2D dual-encoder framework for high-sensitivity intracranial aneurysm screening. 2. Demonstrated through ablation that data augmentation degrades performance when using a strong pretrained backbone, challenging a common assumption in low-data medical imaging. 3. Showed the model achieves 95% sensitivity (surpassing average radiologist performance) and provides interpretable visualizations (Grad-CAM) with clinically relevant focus.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SAMM2D, a dual-encoder framework for detecting intracranial aneurysms in medical images. The key finding is that, contrary to common practice, data augmentation harms performance when using a strong ImageNet-pretrained backbone, suggesting robust pretraining is more beneficial than complex augmentation in low-data medical settings. The model achieves high sensitivity and demonstrates cost-saving potential in clinical screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[颅内动脉瘤检测挑战/Intracranial Aneurysm Detection Challenges]
        B1 --> B2[形态细微/Subtle Morphology]
        B1 --> B3[类别不平衡/Class Imbalance]
        B1 --> B4[标注数据稀缺/Scarce Annotated Data]
        C --> C1[双编码器框架/Dual-Encoder Framework]
        C --> C2[使用强预训练骨干/Strong Pretrained Backbone]
        C --> C3[无数据增强/No Data Augmentation]
        D --> D1[AUC 0.686 (提升32%)/AUC 0.686 (32% Improvement)]
        D --> D2[95% 灵敏度 (超越放射科医生)/95% Sensitivity (Surpasses Radiologists)]
        D --> D3[数据增强降低性能/Augmentation Degrades Performance]
        D --> D4[可解释性可视化/Interpretable Visualizations (Grad-CAM)]
    ```

- **[arXiv251230] Tiny-YOLOSAM: Fast Hybrid Image Segmentation**
  - **tags:** [cv], [image segmentation], [Segment Anything Model (SAM), YOLO, hybrid prompting, inference acceleration, object detection]
  - **authors:** Kenneth Xu, Songhan Wu
  - **institution:** University of Michigan
  - **link:** https://arxiv.org/pdf/2512.22193
  - **contributions:** 1. Proposes Tiny-YOLOSAM, a fast hybrid segmentation pipeline that combines YOLO object detection with TinySAM for efficient prompting. 2. Introduces a targeted sparse prompting strategy that samples points only in regions not covered by detector-guided masks to improve coverage. 3. Demonstrates significant improvements in both segmentation coverage (AR, mIoU) and inference speed (4.7x faster) on COCO val2017 compared to the baseline TinySAM "segment-everything" mode.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of the Segment Anything Model (SAM) for full-scene segmentation. It proposes Tiny-YOLOSAM, a hybrid method that uses a YOLO detector to generate box prompts for salient objects and supplements uncovered areas with sparse point prompts. The approach achieves a 4.7x speedup and significantly better coverage compared to the baseline, offering a practical alternative to dense "segment-everything" prompting.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Tiny-YOLOSAM: Fast Hybrid Image Segmentation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: SAM/TinySAM "segment-everything" mode is computationally expensive and slow for full-scene segmentation.]
        C[主要方法/Method: Hybrid pipeline using YOLO for box prompts on foreground objects and sparse point prompts for uncovered regions.]
        D[关键结果/Results: 4.7x speedup (10.39s vs 49.20s/image) and improved coverage (mIoU: 67.8% vs 19.2%) on COCO val2017.]
    ```

- **[arXiv251230] Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy**
  - **tags:** [cv], [medical image analysis], [Vision-Language Model (VLM), Few-Shot Learning, Grad-CAM, Multimodal Explainability, Diabetic Retinopathy]
  - **authors:** Shivum Telang
  - **institution:** University of Pittsburgh Rangos Research Center, North Allegheny Senior High School
  - **link:** https://arxiv.org/pdf/2512.22197
  - **contributions:** 1. Proposes a novel multimodal explainability model that combines fundus and OCT images for Diabetic Retinopathy severity classification, mimicking an ophthalmologist's reasoning. 2. Introduces a Vision-Language Model (VLM) with few-shot learning adaptation to analyze lesion distributions within retinal quadrants and generate natural language explanations. 3. Develops a method to generate paired Grad-CAM heatmaps across both imaging modalities to visually highlight regions contributing to the classification decision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of explainability in AI models for Diabetic Retinopathy (DR) diagnosis by proposing a multimodal Vision-Language Model (VLM). The model uses few-shot learning to analyze lesion distributions in retinal quadrants from both fundus and OCT images and generates paired Grad-CAM heatmaps and natural language explanations for its severity classifications. This approach aims to provide a more interpretable and clinically practical tool for DR diagnostics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Quadrant Segmentation VLM for Diabetic Retinopathy] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI模型缺乏可解释性 / AI Models Lack Explainability]
        B --> B2[依赖单一成像模态 / Reliance on Single Imaging Modality]
        C --> C1[多模态VLM与少样本学习 / Multimodal VLM with Few-Shot Learning]
        C --> C2[象限分析与Grad-CAM热图 / Quadrant Analysis & Grad-CAM Heatmaps]
        D --> D1[提供自然语言解释 / Provides Natural Language Explanations]
        D --> D2[提升临床实用性 / Improves Clinical Practicality]
    ```

- **[arXiv251230] A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability**
  - **tags:** [cv], [medical image classification], [Convolutional Neural Network, SHAP, LIME, Saliency Maps, Malaria Diagnosis]
  - **authors:** Md. Ismiel Hossen Abir, Awolad Hossain
  - **institution:** Department of Computer Science & Engineering, International Standard University, Dhaka, Bangladesh
  - **link:** https://arxiv.org/pdf/2512.22205
  - **contributions:** 1. Proposes a custom CNN model for automated malaria diagnosis from blood cell images, achieving high accuracy (96%). 2. Compares the performance of the custom CNN with established deep learning architectures like ResNet50 and VGG16. 3. Enhances model interpretability for clinical trust by applying Explainable AI techniques, including SHAP, LIME, and Saliency Maps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a deep learning-based system using a custom Convolutional Neural Network (CNN) to automatically diagnose malaria from blood cell images, achieving high accuracy. It compares this model against several established architectures and applies Explainable AI (XAI) techniques like SHAP and LIME to make the model's decisions interpretable. The study concludes that this approach can provide a quick, accurate, and understandable diagnostic tool, particularly valuable in resource-limited settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability"] --> Problem["核心问题/Problem: Traditional microscopic diagnosis is slow, subjective, and resource-intensive."]
        Root --> Method["主要方法/Method: A custom CNN model for classification, compared with other architectures, enhanced with SHAP, LIME, and Saliency Maps for explainability."]
        Root --> Results["关键结果/Results: Achieves 96% accuracy, high precision/recall, providing a fast and interpretable diagnostic tool."]
    ```

- **[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting**
  - **tags:** [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]
  - **authors:** Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu
  - **institution:** Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.22203
  - **contributions:** 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[TCFormer: 5M参数Transformer用于弱监督人群计数] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[标注成本高/High Annotation Cost]
        Problem --> P2[计算复杂度高/High Computational Complexity]
        Method[主要方法/Method] --> M1[高效视觉Transformer特征提取器/Efficient ViT Feature Extractor]
        Method --> M2[可学习密度加权平均模块/Learnable Density-Weighted Averaging]
        Method --> M3[密度等级分类损失/Density-Level Classification Loss]
        Results[关键结果/Results] --> R1[仅5M参数/Only 5M Parameters]
        Results --> R2[弱监督下竞争性性能/Competitive Performance under Weak Supervision]
        Results --> R3[适用于边缘设备/Suitable for Edge Devices]
    ```

- **[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA**
  - **tags:** [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]
  - **authors:** Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang
  - **institution:** Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC
  - **link:** https://arxiv.org/pdf/2512.22208
  - **code:** https://github.com/moxin-org/Moxin-LLM
  - **contributions:** 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Open-Source Multimodal Moxin Models<br/>开源多模态Moxin模型] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Proprietary vs. Open-Source LLMs<br/>闭源与开源大语言模型]
        B --> B2[Need for transparent, capable open models<br/>需要透明、强大的开源模型]
        C --> C1[Develop Moxin 7B under Model Openness Framework<br/>基于模型开放框架开发Moxin 7B]
        C --> C2[Create variants: VLM, VLA, Chinese<br/>创建变体: VLM, VLA, 中文模型]
        D --> D1[Superior performance in evaluations<br/>在评估中表现优异]
        D --> D2[Full release of models, code, data<br/>完整发布模型、代码、数据]
    ```

- **[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition**
  - **tags:** [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]
  - **authors:** Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid
  - **institution:** University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi
  - **link:** https://arxiv.org/pdf/2512.22217
  - **contributions:** 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>类别不平衡, 属性依赖, 域偏移]
        C[主要方法/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>视觉语言框架与跨注意力融合]
        D[关键结果/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K上SOTA, PETA & Market-1501上提升]
    ```

- **[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition**
  - **tags:** [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]
  - **authors:** Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.22214
  - **contributions:** 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Signal-SGN++<br/>论文标题/Paper Title] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GCNs能耗高<br/>GCNs High Energy Cost]
        B --> B2[SNNs难以捕捉时空-频率与拓扑依赖<br/>SNNs Limited in Capturing Time-Freq & Topology]
        C --> C1[主干网络: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]
        C --> C2[拓扑转移自注意力 TSSA<br/>Topology-Shift Self-Attention TSSA]
        C --> C3[多尺度小波变换融合 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]
        D --> D1[优于现有SNN方法<br/>Outperforms Existing SNN Methods]
        D --> D2[与先进GCNs结果相当<br/>Competitive with SOTA GCNs]
        D --> D3[能耗显著降低<br/>Substantially Reduced Energy Consumption]
    ```

- **[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects**
  - **tags:** [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]
  - **authors:** Tasha Pais, Nikhilesh Belulkar
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2512.22220
  - **contributions:** 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("On Extending Semantic Abstraction for Efficient Search of Hidden Objects") --> Problem("核心问题/Problem: Localizing hidden/occluded objects")
        Root --> Method("主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search")
        Root --> Results("关键结果/Results: Faster and accurate 3D localization vs. random search")
    ```

- **[arXiv251230] Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark**
  - **tags:** [cv], [visual question answering], [signboard VQA, OCR-integrated VQA, multimodal dataset, Vietnamese, multi-agent framework]
  - **authors:** Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen
  - **institution:** University of Information Technology, Vietnam National University, Ho Chi Minh City
  - **link:** https://arxiv.org/pdf/2512.22218
  - **contributions:** 1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark"] --> B["核心问题/Problem: 理解自然场景中的招牌文本对于VQA的现实应用至关重要，但在低资源语言中仍未充分探索。"]
        A --> C["主要方法/Method: 1. 引入首个大规模越南语招牌VQA数据集ViSignVQA。 2. 通过集成越南语OCR和语言模型来适配SOTA VQA模型。 3. 提出结合感知与推理智能体及GPT-4的多智能体VQA框架。"]
        A --> D["关键结果/Results: 1. 添加OCR文本使F1分数提升高达209%。 2. 多智能体框架通过多数投票达到75.98%准确率。 3. 创建了首个捕获真实世界场景文本特征的大规模越南语多模态基准。"]
    ```

- **[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs**
  - **tags:** [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]
  - **authors:** Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao
  - **institution:** University of Science and Technology of China, Ant Group
  - **link:** https://arxiv.org/pdf/2512.22226
  - **code:** https://github.com/zheng980629/VideoScaffold
  - **contributions:** 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]
        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]
        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]
    ```

- **[arXiv251230] KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation**
  - **tags:** [cv], [pose estimation], [KAN, Feature Pyramid Network, Vision Transformer, multi-scale fusion, convolutional layer]
  - **authors:** HaoNan Tang
  - **institution:** WuHan University of Technology
  - **link:** https://arxiv.org/pdf/2512.22228
  - **contributions:** 1. Identified that the performance bottleneck in ViT front-ends for pose estimation lies in the post-fusion smoothing step, not in feature refinement via attention modules. 2. Proposed a novel KAN-enhanced FPN-Stem architecture that replaces the standard linear 3x3 convolution in the FPN with a KAN-based convolutional layer for superior non-linear smoothing. 3. Demonstrated a significant performance improvement of up to +2.0 AP on COCO dataset over the ViTPose-S baseline, providing a plug-and-play module and a new direction for improving feature fusion quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the performance bottleneck in Vision Transformer (ViT) based pose estimation models, which stems from simplistic front-end designs that cause information loss and poor multi-scale handling. The authors propose KAN-FPN-Stem, an architecture that enhances the Feature Pyramid Network by replacing its final linear smoothing convolution with a KAN-based layer to better rectify fusion artifacts. Experiments show a significant performance boost, revealing that improving feature fusion, not just refinement, is key to advancing ViT-based dense prediction tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["KAN-FPN-Stem: A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation"] --> Problem["核心问题/Problem: ViT front-end (e.g., ViTPose) has simplistic design causing multi-scale handling issues & information loss"]
        Root --> Method["主要方法/Method: Replace FPN's terminal linear 3x3 conv with a KAN-based convolutional layer for adaptive non-linear smoothing"]
        Root --> Results["关键结果/Results: Achieves +2.0 AP boost on COCO over ViTPose-S; reveals fusion quality is the key bottleneck"]
    ```

- **[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction**
  - **tags:** [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]
  - **authors:** Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu
  - **institution:** Nanchang University, Hefei Comprehensive National Science Center
  - **link:** https://arxiv.org/pdf/2512.22237
  - **contributions:** 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --> B(核心问题/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)
        A --> C(主要方法/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)
        A --> D(关键结果/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)
    ```

- **[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening**
  - **tags:** [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]
  - **authors:** Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf
  - **institution:** Radboud University Medical Center, Radboud University
  - **link:** https://arxiv.org/pdf/2512.22242
  - **contributions:** 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]
        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]
        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]
    ```

- **[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]
  - **authors:** Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.22238
  - **contributions:** 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]
        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]
        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]
        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]
        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]
        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]
        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]
        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]
    ```

- **[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture**
  - **tags:** [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]
  - **authors:** Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan
  - **institution:** FPT University
  - **link:** https://arxiv.org/pdf/2512.22239
  - **contributions:** 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>面向智慧农业的高效深度学习的多目标混合知识蒸馏"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Deploying deep models on edge devices in smart agriculture<br>在智慧农业中于边缘设备部署深度模型"] --> P1["挑战/Challenge<br>Trade-off between efficiency and accuracy<br>效率与准确性的权衡"]
        Method["主要方法/Method<br>Hybrid knowledge distillation framework<br>混合知识蒸馏框架"] --> M1["学生模型/Student Model<br>Customized CNN with inverted residual & dense blocks<br>定制化CNN，含倒残差与密集连接块"]
        Method --> M2["教师模型/Teacher Model<br>ResNet18 guidance<br>ResNet18教师网络指导"]
        Method --> M3["多目标策略/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>整合硬标签、特征级、响应级与自蒸馏"]
        Results["关键结果/Results<br>Experiments on agricultural datasets<br>在农业数据集上的实验"] --> R1["性能/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>水稻种子分类准确率98.56%（教师模型98.65%）"]
        Results --> R2["效率/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs，约107万参数（比教师模型小10倍）"]
        Results --> R3["泛化/Generalization<br>Consistent gains on plant leaf disease datasets<br>在植物叶片病害数据集上一致性能提升"]
    ```

- **[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models**
  - **tags:** [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]
  - **authors:** Zheng Xing, Weibing Zhao
  - **institution:** Shenzhen University, Shenzhen MSU-BIT University
  - **link:** https://arxiv.org/pdf/2512.22249
  - **contributions:** 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题 / Paper Title: Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models] --> B(核心问题 / Problem: 传统无监督人体运动分割方法忽略了时序语义的作用 / Traditional unsupervised HMS overlooks temporal semantics)
        A --> C(主要方法 / Method: 利用LLM提取时序视觉语义，并融入子空间聚类框架 / Use LLM to extract TVS and integrate it into subspace clustering)
        A --> D(关键结果 / Results: 在四个基准数据集上性能超越现有方法 / Outperforms SOTA on four benchmark datasets)
        C --> C1(LLM查询 / LLM Query: 判断相邻帧是否描述相同运动 / Determine if consecutive frames depict the same motion)
        C --> C2(时序正则化 / Temporal Regularizer: 诱导相邻帧共享相似子空间嵌入 / Induces similar subspace embeddings for temporal neighbors)
        C --> C3(反馈优化 / Feedback Optimization: 基于分割结果迭代优化嵌入 / Iteratively optimizes embedding based on segmentation output)
    ```

- **[arXiv251230] Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models**
  - **tags:** [cv], [generative models], [text-to-image diffusion, geometric control, human perception embedding, latent guidance, teacher model]
  - **authors:** Antara Titikhsha, Om Kulkarni, Dharun Muthaiah
  - **institution:** Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.22272
  - **code:** https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception
  - **contributions:** 1. Proposing a Human Perception Embedding (HPE) teacher trained on human similarity data to capture geometry-sensitive features. 2. Introducing a model-agnostic latent guidance framework for steering various generative architectures. 3. Demonstrating improved geometric control and semantic alignment in text-to-image synthesis without specialized training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem where text-to-image models prioritize style over geometric constraints. The proposed method uses a lightweight "teacher" model, trained on human perceptual data, to guide the diffusion process towards desired shapes. The results show that this approach significantly improves geometric control across different model architectures without requiring retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-Aligned Generative Perception<br>人类对齐的生成式感知] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Generative models ignore geometry for style<br>生成模型为风格忽略几何]
        C --> C1[Use HPE teacher for guidance<br>使用HPE教师模型进行引导]
        D --> D1[~80% better alignment<br>对齐度提升约80%]
        D --> D2[Zero-shot shape transfer<br>零样本形状迁移]
    ```

- **[arXiv251230] GeCo: A Differentiable Geometric Consistency Metric for Video Generation**
  - **tags:** [cv], [video generation evaluation], [geometric consistency, video generation, differentiable metric, depth prior, motion prior]
  - **authors:** Leslie Gu, Junhwa Hur, Charles Herrmann, Fangneng Zhan, Todd Zickler, Deqing Sun, Hanspeter Pfister
  - **institution:** Harvard University, Google DeepMind, Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22274
  - **contributions:** 1. Proposes GeCo, a novel differentiable metric that fuses motion and depth priors to detect geometric deformation and occlusion inconsistency in generated videos. 2. Introduces two synthetic datasets, WarpBench and OccluBench, to validate the metric's performance on isolated artifacts. 3. Demonstrates GeCo's dual utility for benchmarking state-of-the-art video generation models and as a training-free guidance loss to reduce geometric artifacts during generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces GeCo, a differentiable metric for evaluating geometric consistency in generated videos by combining motion and depth cues. It validates the metric on new synthetic datasets and uses it to benchmark models and improve video generation by reducing artifacts as a guidance loss.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GeCo: A Differentiable Geometric Consistency Metric for Video Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[生成视频违反3D几何/Generated videos violate 3D geometry]
        C --> C1[融合运动和深度先验/Fuse motion & depth priors]
        C --> C2[创建合成数据集/Create synthetic datasets]
        D --> D1[系统评估模型/Systematically benchmark models]
        D --> D2[用作无训练引导损失/Used as training-free guidance loss]
    ```

- **[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions**
  - **tags:** [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]
  - **authors:** Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton
  - **institution:** George Mason University
  - **link:** https://arxiv.org/pdf/2512.22263
  - **contributions:** 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Evaluating an Adaptive Multispectral Turret System] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[RGB在低光下表现差/RGB struggles in low-light]
        Problem --> P2[热成像缺乏颜色纹理/Thermal lacks color & texture]
        Method[主要方法/Method] --> M1[自适应RGB-LWIR融合框架/Adaptive RGB-LWIR fusion framework]
        Method --> M2[训练33个YOLO模型/Trained 33 YOLO models]
        Method --> M3[11种融合比例/11 fusion ratios]
        Results[关键结果/Results] --> R1[全光模型: 92.8%置信度/Full-light model: 92.8% confidence]
        Results --> R2[微光模型: 92.0%置信度/Dim-light model: 92.0% confidence]
        Results --> R3[无光模型: 71.0%置信度/No-light model: 71.0% confidence]
    ```

- **[arXiv251230] FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound**
  - **tags:** [cv], [medical imaging], [vision-language models, fetal ultrasound, visual question answering, benchmark, multimodal learning]
  - **authors:** Hussain Alasmawi, Numan Saeed, Mohammad Yaqub
  - **institution:** Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)
  - **link:** https://arxiv.org/pdf/2512.22278
  - **contributions:** 1. Introduces Fetal-Gauge, the first and largest visual question answering benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound, comprising over 42,000 images and 93,000 question-answer pairs. 2. Systematically evaluates state-of-the-art general-purpose and medical-specific VLMs, revealing a substantial performance gap where the best model achieves only 55% accuracy. 3. Identifies critical limitations of current VLMs in this domain and establishes a foundation for advancing domain-adapted architectures and specialized training for prenatal care.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of a standardized benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound by introducing Fetal-Gauge, a large-scale visual question answering dataset. The authors evaluate multiple VLMs and find their performance (max 55% accuracy) is far below clinical requirements, highlighting the urgent need for specialized models in this medical domain.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[全球超声医师短缺 / Global Sonographer Shortage]
        B --> B2[缺乏标准化评估基准 / Lack of Standardized Benchmark]
        C --> C1[创建胎儿超声VQA基准 / Create Fetal Ultrasound VQA Benchmark]
        C --> C2[系统评估VLMs / Systematically Evaluate VLMs]
        D --> D1[最佳模型准确率55% / Best Model Accuracy 55%]
        D --> D2[性能远低于临床要求 / Performance Far Below Clinical Requirements]
    ```

- **[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency**
  - **tags:** [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]
  - **authors:** Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang
  - **institution:** Peking University Third Hospital, Beihang University
  - **link:** https://arxiv.org/pdf/2512.22275
  - **contributions:** 1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Illusion of Clinical Reasoning<br>临床推理的假象] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>现有基准无法捕捉综合、多模态临床推理。]
        C --> C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>开发了包含1245个真实世界问题、涵盖7项任务的B&J基准。]
        D --> D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>巨大性能差距：选择题表现好，开放式多模态任务表现差。]
        D --> D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM在图像解释方面存在局限并出现幻觉。]
        D --> D3[Medically fine-tuned models show no consistent advantage.<br>医学微调模型未显示一致优势。]
    ```

- **[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model**
  - **tags:** [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]
  - **authors:** Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang
  - **institution:** Tsinghua University (Leap Lab), Anyverse Dynamics
  - **link:** https://arxiv.org/pdf/2512.22288
  - **code:** https://co-grpo.github.io
  - **contributions:** 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]
        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]
        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]
        C --> C1[统一MDP/Unified MDP]
        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]
        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]
        D --> D1[提升生成质量/Improved Generation Quality]
        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]
    ```

- **[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation**
  - **tags:** [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]
  - **authors:** Philip Xu, David Elizondo, Raouf Hamzaoui
  - **institution:** De Montfort University
  - **link:** https://arxiv.org/pdf/2512.22294
  - **contributions:** 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大规模3D检索与可控4D生成的挑战/Challenges in large-scale 3D retrieval and controlled 4D generation]
        C --> C1[三级对齐框架: 文本-3D-图像/Three-level alignment: text-3D-image]
        C --> C2[ATMS模型优化检索/ATMS model optimizes retrieval]
        D --> D1[高质量3D检索/High-quality 3D retrieval]
        D --> D2[可控4D生成/Controlled 4D generation]
    ```

- **[arXiv251230] Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors**
  - **tags:** [cv], [3D scene reconstruction], [SirenPose, sinusoidal representation networks, geometric priors, spatiotemporal consistency, dynamic 3D reconstruction]
  - **authors:** Tian Guo, Hui Yuan, Philip Xu, David Elizondo
  - **institution:** De Montfort University
  - **link:** https://arxiv.org/pdf/2512.22295
  - **contributions:** 1. Proposes a novel loss function "SirenPose" that combines periodic activation from SIREN networks with geometric priors from keypoint structures. 2. Introduces physics-inspired constraint mechanisms to enforce coherent keypoint predictions across spatial and temporal dimensions. 3. Expands the training dataset to 600,000 annotated instances to support robust learning and demonstrates significant improvements in spatiotemporal consistency metrics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of maintaining motion accuracy and spatiotemporal consistency in dynamic 3D scene reconstruction from monocular videos. It proposes a novel loss function called SirenPose, which integrates sinusoidal representation networks with geometric keypoint priors and physics-based constraints. Experiments show that models using SirenPose achieve superior performance in handling rapid motion and complex scene changes compared to prior methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法在快速运动多目标场景中难以保持运动建模精度和时空一致性/Existing methods struggle with motion accuracy and spatiotemporal consistency in fast-moving multi-target scenes]
        C --> C1[提出SirenPose损失函数，结合SIREN的周期激活特性和关键点几何先验/Propose SirenPose loss, combining SIREN's periodic activation and keypoint geometric priors]
        C --> C2[引入物理启发的约束机制，确保时空维度上关键点预测的一致性/Introduce physics-inspired constraints for coherent keypoint predictions across space and time]
        D --> D1[模型在时空一致性指标上相比现有方法有显著提升/Models show significant improvement in spatiotemporal consistency metrics vs. prior methods]
        D --> D2[在处理快速运动和复杂场景变化上表现出优越性能/Demonstrates superior performance in handling rapid motion and complex scene changes]
    ```

- **[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware**
  - **tags:** [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]
  - **authors:** Vesal Ahsani, Babak Hossein Khalaj
  - **institution:** Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.22298
  - **contributions:** 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[实时DMS需求 / Real-time DMS needs low latency, low cost, low power]
        C --> C1[紧凑单摄像头系统 / Compact single-camera pipeline]
        C1 --> C2[紧凑视觉模型 / Compact per-frame vision model]
        C1 --> C3[抗混淆标签设计 / Confounder-aware label design]
        C1 --> C4[时序决策头 / Temporal decision head]
        D --> D1[性能: 16 FPS (RPi5), 25 FPS (Edge TPU) / Performance: 16 FPS (RPi5), 25 FPS (Edge TPU)]
        D --> D2[验证: 真实车辆测试 / Validation: Real in-vehicle tests]
    ```

- **[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations**
  - **tags:** [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]
  - **authors:** Noor Fatima, Hasan Faraz Khan, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22303
  - **contributions:** 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"] --> B["核心问题/Problem: Robust detection under realistic counter-forensic attacks"]
        A --> C["主要方法/Method: Red-team training + Test-time defense in a two-stream architecture"]
        A --> D["关键结果/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps"]
    ```

- **[arXiv251230] PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation**
  - **tags:** [cv], [food volume and nutrition estimation], [knowledge distillation, cross-modal learning, point cloud, RGB-to-Geometry Adapter, dual-mode training]
  - **authors:** Darrin Bright, Rakshith Raj, Kanchan Keisham
  - **institution:** Vellore Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.22304
  - **contributions:** 1. Proposed PortionNet, a cross-modal knowledge distillation framework that enables RGB models to learn 3D geometric features from point clouds during training, eliminating the need for depth sensors at inference. 2. Introduced a dual-mode training strategy with a lightweight RGB-to-Geometry Adapter that learns to generate pseudo-3D features from standard RGB images. 3. Achieved state-of-the-art performance on MetaFood3D for volume and energy estimation and demonstrated strong generalization on SimpleFood45.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of estimating food nutrition from a single RGB image, which lacks 3D information. It proposes PortionNet, a framework that uses knowledge distillation to teach an RGB model geometric reasoning from point cloud data during training, requiring only RGB images at test time. The method achieves state-of-the-art accuracy on benchmark datasets, demonstrating effective 3D knowledge transfer without specialized hardware.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation] --> B(核心问题/Problem: 从单张RGB图像进行食物营养估计缺乏3D信息/Accurate food nutrition estimation from single RGB images lacks 3D information.)
        A --> C(主要方法/Method: 跨模态知识蒸馏，从点云学习几何特征/Cross-modal knowledge distillation learns geometric features from point clouds.)
        A --> D(关键结果/Results: 在MetaFood3D上取得SOTA，在SimpleFood45上展示强泛化性/Achieves SOTA on MetaFood3D, shows strong generalization on SimpleFood45.)
    ```

- **[arXiv251230] MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation**
  - **tags:** [cv], [video generation], [multi-subject video generation, scale-aware modulation, fourier fusion, permutation invariance]
  - **authors:** Run Ling, Ke Cao, Jian Lu, Ao Ma, Haowei Liu, Runze He, Changwei Wang, Rongtao Xu, Yihua Shao, Zhanjie Zhang, Peng Wu, Guibing Guo, Wei Feng, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Xingwei Wang
  - **institution:** JD.com, Inc., Northeastern University, University of Science and Technology of China, Chongqing University of Post and Telecommunications, University of Chinese Academy of Sciences, Northwestern Polytechnical University
  - **link:** https://arxiv.org/pdf/2512.22310
  - **contributions:** 1. Proposes Scale-Aware Modulation (SMO), an LLM-guided module to extract implicit scale cues from text prompts to address scale inconsistency. 2. Introduces a Fourier Fusion strategy using Fast Fourier Transform to process reference features for permutation-invariant generation. 3. Designs a dedicated benchmark and a Scale-Permutation Stability Loss to evaluate and jointly optimize for scale consistency and permutation invariance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes MoFu, a framework for multi-subject video generation that tackles scale inconsistency and permutation sensitivity. It introduces a Scale-Aware Modulation module and a Fourier Fusion strategy, validated by a new benchmark and a stability loss. Experiments show MoFu outperforms existing methods in preserving natural scale, subject fidelity, and visual quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MoFu: Multi-Subject Video Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[尺度不一致/Scale Inconsistency]
        B --> B2[排列敏感性/Permutation Sensitivity]
        C --> C1[尺度感知调制/Scale-Aware Modulation (SMO)]
        C --> C2[傅里叶融合/Fourier Fusion]
        C --> C3[稳定性损失/Scale-Permutation Stability Loss]
        D --> D1[超越现有方法/Outperforms Existing Methods]
        D --> D2[保持自然尺度与保真度/Preserves Natural Scale & Fidelity]
    ```

- **[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting**
  - **tags:** [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]
  - **authors:** Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan
  - **institution:** University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University
  - **link:** https://arxiv.org/pdf/2512.22317
  - **contributions:** 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]
        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]
        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]
    ```

- **[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning**
  - **tags:** [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]
  - **authors:** Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang
  - **institution:** Tsinghua University, The Chinese University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.22315
  - **code:** https://github.com/zsgvivo/VideoZoomer
  - **contributions:** 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[长视频理解受限/Limited Long Video Understanding]
        B1 --> B2[上下文窗口限制/Context Window Limitation]
        B1 --> B3[均匀采样忽略关键证据/Uniform Sampling Overlooks Evidence]
        C --> C1[代理框架/Agentic Framework]
        C1 --> C2[动态时间聚焦/Dynamic Temporal Focusing]
        C2 --> C3[从粗到细推理/Coarse-to-Fine Reasoning]
        C --> C4[两阶段训练/Two-Stage Training]
        C4 --> C5[监督微调/Supervised Fine-Tuning]
        C4 --> C6[强化学习/Reinforcement Learning]
        D --> D1[性能强劲/Strong Performance]
        D1 --> D2[超越开源模型/Surpasses Open-Source Models]
        D1 --> D3[媲美专有系统/Rivals Proprietary Systems]
        D --> D4[高效推理/Efficient Reasoning]
        D4 --> D5[低帧预算/Reduced Frame Budget]
    ```

- **[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers**
  - **tags:** [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]
  - **authors:** Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang
  - **institution:** National University of Singapore, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.22323
  - **code:** https://biangbiang0321.github.io/SpotEdit.github.io/
  - **contributions:** 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpotEdit: Selective Region Editing in Diffusion Transformers] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[全图去噪冗余/Full-image denoising is redundant for small edits]
        C --> C1[SpotSelector: 识别稳定区域/Identifies stable regions via perceptual similarity]
        C --> C2[SpotFusion: 动态特征融合/Dynamically fuses features for coherence]
        D --> D1[高效编辑/Efficient editing]
        D --> D2[保持未修改区域保真度/Preserves fidelity in unchanged areas]
    ```

- **[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents**
  - **tags:** [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]
  - **authors:** Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun
  - **institution:** Peking University, Tencent
  - **link:** https://arxiv.org/pdf/2512.22322
  - **code:** https://huggingface.co/collections/yolay/smartsnap
  - **contributions:** 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]
        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]
        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]
    ```

- **[arXiv251230] DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models**
  - **tags:** [cv], [human motion generation], [energy-based diffusion model, motion decomposition, compositional training]
  - **authors:** Jianrong Zhang, Hehe Fan, Yi Yang
  - **institution:** University of Technology Sydney, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.22324
  - **code:** https://jiro-zhang.github.io/DeMoGen/
  - **contributions:** 1. Proposes DeMoGen, a compositional training paradigm using an energy-based diffusion model to decompose holistic motions into semantic sub-components without needing ground-truth for individual concepts., 2. Introduces three training variants (DeMoGen-Exp, DeMoGen-OSS, DeMoGen-SC) to encourage decompositional understanding and disentangle reusable motion primitives., 3. Constructs a text-decomposed dataset to support compositional training and demonstrates that decomposed concepts can be recombined to generate novel motions beyond the training distribution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of decomposing holistic human motions into simpler, reusable primitives. It proposes DeMoGen, an energy-based diffusion model training paradigm with three variants to learn this decomposition without individual concept supervision. The method successfully disentangles motion concepts, which can then be flexibly recombined to generate diverse and novel motions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DeMoGen: Decompositional Human Motion Generation] --> B[核心问题/Problem: 如何将整体运动分解为语义子组件/How to decompose holistic motion into semantic sub-components?]
        A --> C[主要方法/Method: 基于能量的扩散模型与三种训练变体/Energy-based diffusion model with three training variants]
        A --> D[关键结果/Results: 解耦可重用运动基元并支持重组生成/Disentangle reusable motion primitives and support recombination generation]
    ```

- **[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma**
  - **tags:** [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]
  - **authors:** Mariya Miteva, Maria Nisheva-Pavlova
  - **institution:** Not explicitly stated in provided content.
  - **link:** https://arxiv.org/pdf/2512.22331
  - **contributions:** 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]
        C[主要方法/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]
        D[关键结果/Results: Latent embeddings used for MGMT promoter methylation classification]
    ```

- **[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement**
  - **tags:** [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]
  - **authors:** Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo
  - **institution:** Stanford University, Google, Google DeepMind, New York University
  - **link:** https://arxiv.org/pdf/2512.22351
  - **code:** vulcan-3d.github.io
  - **contributions:** 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>MLLMs在复杂3D场景操控中的应用未被充分探索<br>Application of MLLMs to complex 3D scene manipulation is underexplored]
        C[主要方法/Method<br>引入MCP API、视觉工具套件和多智能体协作框架<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]
        D[关键结果/Results<br>在25个复杂任务上显著超越基线<br>Significantly outperforms baselines on 25 complex tasks]
    ```

- **[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides**
  - **tags:** [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]
  - **authors:** Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye
  - **institution:** North Carolina A&T State University, University of Chichester, Yale University
  - **link:** https://arxiv.org/pdf/2512.22335
  - **contributions:** 1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[挑战: 联合分析H&E和IHC图像进行HER2评分/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]
    B --> B2[难点: 现有方法无法提供像素级HER2状态定位/Issue: Existing methods lack pixel-level HER2 status localization]
    C --> C1[方法: 端到端多阶段视觉Transformer管道/Method: End-to-end multi-stage Vision Transformer pipeline]
    C --> C2[创新: 新颖的映射函数关联H&E与IHC区域/Innovation: Novel mapping function to correlate H&E and IHC regions]
    C --> C3[机制: 临床启发的4级HER2评分机制/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]
    D --> D1[结果: 肿瘤定位分类准确率高/Result: Good classification accuracy for tumor localization]
    D --> D2[结果: HER2状态预测准确率0.94/Result: 0.94 accuracy for HER2 status prediction]
    D --> D3[结论: 端到端ViT模型可用于联合评估H&E和IHC图像/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]
    ```

- **[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data**
  - **tags:** [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]
  - **authors:** Alaa Alahmadi, Mohamed Hasan
  - **institution:** Newcastle University, University of Leeds
  - **link:** https://arxiv.org/pdf/2512.22349
  - **contributions:** 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1
        A --> B2
        A --> B3
        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]
        B1 --> C2[可解释性差/Limited explainability]
        B1 --> C3[临床可靠性受限/Constrained clinical reliability]
        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]
        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]
        D1 --> E2[结构化颜色表示/Structured colour representations]
        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]
        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]
        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]
        B3 --> F2[提升可解释性/Improve explainability (guide attention)]
        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]
    ```

- **[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation**
  - **tags:** [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]
  - **authors:** Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan
  - **institution:** The University of Hong Kong, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.22374
  - **contributions:** 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables "any-step" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]
        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]
        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]
    ```

- **[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition**
  - **tags:** [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]
  - **authors:** Elsen Ronando, Sozo Inoue
  - **institution:** Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya
  - **link:** https://arxiv.org/pdf/2512.22385
  - **contributions:** 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLM-Guided Exemplar Selection for Few-Shot HAR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[依赖大数据集与几何选择 / Reliance on large datasets & geometric selection]
        B --> B2[难以区分相似活动 / Hard to distinguish similar activities]
        C --> C1[LLM生成语义先验 / LLM-generated semantic priors]
        C --> C2[结合多线索优化 / Combine multiple cues for optimization]
        D --> D1[性能超越基线 / Outperforms baselines (88.78% F1)]
        D --> D2[语义先验有效 / Semantic priors are effective]
    ```

- **[arXiv251230] iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI**
  - **tags:** [cv], [semantic segmentation], [on-device AI, LiDAR depth estimation, fused GPS/IMU, sidewalk mapping, accessibility]
  - **authors:** Himanshu Naidu, Yuxiang Zhang, Sachin Mehta, Anat Caspi
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.22392
  - **contributions:** 1. Introduces iOSPointMapper, a mobile app for real-time, privacy-conscious sidewalk mapping using iPhones/iPads. 2. Leverages on-device semantic segmentation, LiDAR depth, and fused GPS/IMU to detect and localize sidewalk features like signs and poles. 3. Incorporates a user-guided annotation interface for validation and integrates collected data with the Transportation Data Exchange Initiative (TDEI).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the costly and fragmented collection of sidewalk data by proposing iOSPointMapper, a mobile application that uses on-device AI, LiDAR, and sensor fusion for real-time detection and mapping of pedestrian infrastructure. The system includes a user validation interface and integrates data into a broader transportation dataset. Evaluations show its potential for scalable and enhanced pedestrian mapping.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(人行道数据收集成本高、碎片化且难以扩展/Sidewalk data collection is costly, fragmented, and hard to scale)
        Method --> M1(移动应用使用设备端AI、LiDAR和传感器融合/Mobile app uses on-device AI, LiDAR, and sensor fusion)
        Method --> M2(用户引导的标注界面进行验证/User-guided annotation interface for validation)
        Results --> R1(评估显示人行道特征检测和空间映射的潜力/Evaluation shows potential for feature detection and spatial mapping)
        Results --> R2(为可扩展的、以用户为中心的数据收集提供方法/Offers a scalable, user-centered approach to data collection)
    ```

- **[arXiv251230] DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization**
  - **tags:** [cv], [object detection], [Conditional Flow Matching, Generative Object Detection, Rectified Flow, Ordinary Differential Equation, Magnetic Resonance Enterography]
  - **authors:** Hansang Lee, Chaelin Lee, Nieun Seo, Joon Seok Lim, Helen Hong
  - **institution:** Seoul Women's University, Severance Hospital, Yonsei University College of Medicine
  - **link:** https://arxiv.org/pdf/2512.22406
  - **contributions:** 1. Proposes DeFloMat, a novel generative object detection framework that replaces the slow multi-step stochastic denoising of diffusion models with a fast, deterministic flow field based on Conditional Flow Matching and Rectified Flow. 2. Achieves state-of-the-art accuracy with only 3 inference steps, significantly outperforming prior diffusion-based detectors in speed and performance on a clinical MRE dataset. 3. Demonstrates superior localization stability and recall in the few-step regime, effectively resolving the trade-off between generative accuracy and inference efficiency for time-sensitive applications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high latency of diffusion-based object detectors, which is impractical for clinical use. It proposes DeFloMat, a new framework that uses Conditional Flow Matching to create a deterministic flow for fast, stable object localization via an ODE solver. The method achieves superior accuracy with only 3 inference steps on a medical imaging dataset, setting a new standard for efficient generative detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DeFloMat: Detection with Flow Matching] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Diffusion-based detectors are too slow for clinical applications (e.g., Crohn's Disease detection in MRE)]
        C[主要方法/Method: Replaces stochastic diffusion with deterministic flow field (Conditional Flow Matching, Rectified Flow) solved via ODE]
        D[关键结果/Results: Achieves SOTA accuracy in only 3 steps, 1.4x performance gain, superior stability and recall]
    ```

- **[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy**
  - **tags:** [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]
  - **authors:** Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath
  - **institution:** UC Santa Barbara, Allen Institute for Cell Sciences
  - **link:** https://arxiv.org/pdf/2512.22423
  - **contributions:** 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)
        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)
        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)
    ```

- **[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning**
  - **tags:** [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]
  - **authors:** Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu
  - **institution:** Wayne State University, Henry Ford Health
  - **link:** https://arxiv.org/pdf/2512.22425
  - **contributions:** 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Ill-posed inverse problem: complex anatomy-beam relationship / 病态逆问题: 解剖结构与射束强度的复杂关系]
        B --> B2[CNN struggles with long-range dependencies / CNN难以捕捉长程依赖]
        C --> C1[Two-stage transformer framework / 两阶段Transformer框架]
        C1 --> C1_1[Stage 1: Global dose prior / 阶段1: 全局剂量先验]
        C1 --> C1_2[Stage 2: Geometry-conditioned fluence regression / 阶段2: 几何条件化的注量图回归]
        C --> C2[Fluence-Aware Regression (FAR) loss / 注量感知回归损失]
        D --> D1[Reduced Energy Error to 4.5% / 能量误差降低至4.5%]
        D --> D2[Improved structural fidelity (p<0.05) / 结构保真度显著提升]
        D --> D3[Outperformed benchmark CNN & single-stage methods / 超越基准CNN与单阶段方法]
    ```

- **[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems**
  - **tags:** [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]
  - **authors:** Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad
  - **institution:** SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego
  - **link:** https://arxiv.org/pdf/2512.22439
  - **contributions:** 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --> B[核心问题/Problem: LiDAR垂直分辨率固定与光束丢失导致点云稀疏]
        A --> C[主要方法/Method: 基于光束感知图与门控残差融合的图注意力网络]
        A --> D[关键结果/Results: 重建误差更低，几何一致性更好，结构完整性保持]
    ```

- **[arXiv251230] EmoCtrl: Controllable Emotional Image Content Generation**
  - **tags:** [cv], [image generation], [controllable generation, emotion-aware generation, diffusion models, affective computing, multi-modal learning]
  - **authors:** Jingyuan Yang, Weibin Luo, Hui Huang
  - **institution:** Shenzhen University
  - **link:** https://arxiv.org/pdf/2512.22437
  - **contributions:** 1. Introduces the novel task of Controllable Emotional Image Content Generation (C-EICG), which aims to generate images faithful to a content description while expressing a target emotion. 2. Proposes the EmoCtrl model, which incorporates textual and visual emotion enhancement modules to bridge abstract emotions to visual cues using learned emotion tokens. 3. Constructs a supporting dataset annotated with content, emotion, and affective prompts, and demonstrates through experiments and user studies that EmoCtrl outperforms existing methods in achieving both content faithfulness and expressive emotion control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap between content-faithful and emotion-expressive image generation by proposing EmoCtrl, a model for Controllable Emotional Image Content Generation. EmoCtrl uses textual and visual modules to learn emotion tokens that enrich affective expression while maintaining semantic content. Experiments and user studies show it outperforms existing methods in balancing content accuracy and emotional tone.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EmoCtrl: Controllable Emotional Image Content Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有模型缺乏情感控制或内容失真<br>Existing models lack emotion control or distort content]
        C --> C1[提出EmoCtrl模型与数据集<br>Propose EmoCtrl model and dataset]
        C1 --> C2[文本与视觉情感增强模块<br>Textual and visual emotion enhancement modules]
        C2 --> C3[学习情感令牌<br>Learn emotion tokens]
        D --> D1[定量定性实验表现优异<br>Quantitative and qualitative experiments show superiority]
        D --> D2[用户研究符合人类偏好<br>User studies align with human preference]
        D --> D3[泛化至创意应用<br>Generalizes to creative applications]
    ```

- **[arXiv251230] LECalib: Line-Based Event Camera Calibration**
  - **tags:** [cv], [camera calibration], [event camera, line detection, geometric calibration, non-linear optimization, stereo calibration]
  - **authors:** Zibin Liu, Banglei Guana, Yang Shanga, Zhenbao Yu, Yifei Bian, Qifeng Yu
  - **institution:** National University of Defense Technology, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.22441
  - **code:** https://github.com/Zibin6/line_based_event_camera_calib
  - **contributions:** 1. Proposes a line-based calibration framework for event cameras that uses geometric lines from common man-made objects, eliminating the need for dedicated flashing patterns or calibration boards. 2. Introduces a method to detect lines directly from raw event streams and leverages an event-line calibration model to generate an initial parameter guess suitable for both planar and non-planar lines. 3. Validates the method's feasibility and accuracy through both simulation and real-world experiments on monocular and stereo event camera setups.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of time-consuming and manually intensive calibration for event cameras. It proposes LECalib, a framework that calibrates event cameras by detecting geometric lines directly from event streams and using them in a linear initialization and non-linear refinement process. The method is validated as feasible and accurate for both monocular and stereo setups.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LECalib: Line-Based Event Camera Calibration] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法耗时且需人工标定物 / Existing methods are time-consuming and require manual calibration objects]
        C --> C1[从事件流直接检测线特征 / Detect lines directly from event streams]
        C --> C2[使用事件-线标定模型初始化 / Use event-line model for initial guess]
        C --> C3[非线性优化精修参数 / Refine parameters with non-linear optimization]
        D --> D1[仿真与真实实验验证 / Validated in simulation and real-world]
        D --> D2[支持单目与立体事件相机 / Works for monocular and stereo event cameras]
    ```

- **[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework**
  - **tags:** [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]
  - **authors:** Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang
  - **institution:** Anhui University, China Electronics Technology Group Corporation (38th Research Institute)
  - **link:** https://arxiv.org/pdf/2512.22447
  - **contributions:** 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework"] --> Problem["核心问题/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection."]
        Root --> Method["主要方法/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules."]
        Root --> Results["关键结果/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data."]
    ```

- **[arXiv251230] SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues**
  - **tags:** [cv], [object detection], [EfficientDet-D2, sound cues, on-device inference, assistive technology, Flutter]
  - **authors:** Md Abu Obaida Zishan, Annajiat Alim Rasel
  - **institution:** BRAC University
  - **link:** https://arxiv.org/pdf/2512.22449
  - **code:** https://github.com/MohammedZ666/SonoVision
  - **contributions:** 1. Developed SonoVision, a smartphone application that uses real-time object detection and spatialized sound cues to help visually impaired individuals locate objects independently. 2. Implemented the system using the Flutter framework and the EfficientDet-D2 model, enabling it to function completely offline for safety and user-friendliness. 3. Designed an intuitive auditory interface where object location (left, center, right) is indicated by playing sinusoidal sounds in the corresponding ear(s) of the user's headphones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp
  - **Simple LLM Summary:** The paper presents SonoVision, a smartphone app that helps visually impaired people locate objects by using the EfficientDet-D2 model for real-time detection and providing directional sound cues through headphones. The application is built with Flutter and works offline, aiming to increase user independence and safety. The authors conclude that this approach can significantly assist users in a user-friendly manner.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues] --> B(核心问题/Problem: Visually impaired individuals struggle to locate objects, hindering independence and safety.)
        A --> C(主要方法/Method: Smartphone app uses EfficientDet-D2 for object detection and provides directional sound cues (sinusoidal tones) via headphones.)
        A --> D(关键结果/Results: An offline, user-friendly application that can help visually impaired users locate objects more independently.)
    ```

- **[arXiv251230] SAM 3D for 3D Object Reconstruction from Remote Sensing Images**
  - **tags:** [cv], [3D reconstruction], [SAM 3D, monocular reconstruction, urban modeling, remote sensing, foundation model]
  - **authors:** Junsheng Yao, Lichao Mou, Qingyu Li
  - **institution:** The Chinese University of Hong Kong, Shenzhen; MedAI Technology
  - **link:** https://arxiv.org/pdf/2512.22452
  - **contributions:** 1. Presents the first systematic evaluation of the general-purpose foundation model SAM 3D for monocular remote sensing building reconstruction. 2. Benchmarks SAM 3D against TRELLIS on the NYC urban dataset using FID and CMMD metrics, showing superior roof geometry and boundary sharpness. 3. Extends SAM 3D to urban scene reconstruction via a novel segment-reconstruct-compose pipeline, demonstrating its potential for broader urban modeling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the SAM 3D foundation model for reconstructing 3D buildings from single remote sensing images. It benchmarks SAM 3D against TRELLIS, finding it produces more coherent geometry and sharper boundaries. The work also proposes a pipeline to extend the model for urban scene reconstruction, highlighting its potential and practical limitations for scalable urban modeling.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("SAM 3D for 3D Object Reconstruction from Remote Sensing Images<br>论文标题") --> Problem("Monocular 3D building reconstruction is essential but challenging<br>核心问题：单目3D建筑重建重要但困难")
        Root --> Method("Systematic evaluation & benchmark of SAM 3D; Segment-Reconstruct-Compose pipeline<br>主要方法：系统评估SAM 3D；分割-重建-组合流程")
        Root --> Results("SAM 3D outperforms TRELLIS; Potential for urban scene modeling demonstrated<br>关键结果：SAM 3D优于TRELLIS；展示了城市场景建模潜力")
    ```

- **[arXiv251230] Comparing Object Detection Models for Electrical Substation Component Mapping**
  - **tags:** [cv], [object detection], [YOLOv8, YOLOv11, RF-DETR, substation mapping, computer vision]
  - **authors:** Haley Mody, Namish Bansal, Dennies Kiprono Bor, Edward J. Oughton
  - **institution:** None (No affiliations or email domains provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.22454
  - **contributions:** 1. Training and comparative evaluation of three state-of-the-art object detection models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US electrical substation images. 2. Analysis of model performance based on detection accuracy, precision, and efficiency to identify strengths and limitations for the specific application. 3. Demonstration of a practical use case by utilizing the best-performing model(s) to effectively map substation components across the United States, showcasing autonomous infrastructure assessment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the labor-intensive problem of manually mapping electrical substation components by training and comparing three computer vision models (YOLOv8, YOLOv11, RF-DETR) on a labeled dataset of US substation images. The models are evaluated on accuracy, precision, and efficiency to determine the most reliable solution for large-scale, autonomous substation component mapping. The research concludes by identifying the best model and demonstrating its application for mapping substation infrastructure across the United States.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Comparing Object Detection Models for Electrical Substation Component Mapping] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Manual substation mapping is time-consuming and labor-intensive] --> B1[关键影响/Impact<br>Grid failure has economic & safety implications]
        C[主要方法/Method<br>Train & compare 3 CV models (YOLOv8, YOLOv11, RF-DETR)] --> C1[评估指标/Evaluation<br>Accuracy, Precision, Efficiency]
        D[关键结果/Results<br>Identify best model for reliable, large-scale mapping] --> D1[应用案例/Use Case<br>Map US substation components]
    ```

- **[arXiv251230] Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing**
  - **tags:** [cv], [motion generation and editing], [residual vector quantization (RVQ), pose code, transformer, text-to-motion, motion editing]
  - **authors:** Sukhyun Jeong, Yong-Hoon Choi
  - **institution:** Kwangwoon University
  - **link:** https://arxiv.org/pdf/2512.22464
  - **code:** https://github.com/jayze3736/PGR2M
  - **contributions:** 1. Proposes a hybrid motion representation (PGR²M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR²M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Pose-code frameworks struggle to capture subtle temporal dynamics and high-frequency details.]
        C[主要方法/Method: Hybrid representation (PGR²M) with pose codes and residual codes (RVQ), using a two-stage Transformer.]
        D[关键结果/Results: Improves FID and reconstruction metrics; enables intuitive, structure-preserving edits.]
    ```

- **[arXiv251230] Event-based high temporal resolution measurement of shock wave motion field**
  - **tags:** [cv], [event-based vision], [event cameras, shock wave measurement, motion field reconstruction, asymmetry estimation, polar coordinate encoding]
  - **authors:** Taihang Lei, Banglei Guan, Minzu Liang, Pengju Sun, Jing Tao, Yang Shang, Qifeng Yu
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.22474
  - **contributions:** 1. A novel framework using multiple event cameras to measure shock wave asymmetry with high spatiotemporal resolution. 2. A method involving polar coordinate event encoding and adaptive ROI extraction for revealing propagation patterns. 3. The derivation of a geometric model for event-based shock wave parameter estimation and 3D motion field reconstruction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method for high-resolution shock wave measurement using multiple event cameras. It establishes a polar coordinate system to encode events, extracts shock fronts via iterative slope analysis, and derives models for parameter estimation and 3D reconstruction. The method achieves high-precision measurements with errors as low as 0.06% compared to pressure sensors, demonstrating significant progress in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Event-based high temporal resolution measurement of shock wave motion field] --> B(核心问题/Problem: Fast, uneven shock wave propagation under unstable conditions)
        A --> C(主要方法/Method: Multi-event-camera framework with polar encoding, adaptive ROI, iterative slope analysis)
        A --> D(关键结果/Results: High-precision measurement, max error 5.20%, min error 0.06%)
    ```

- **[arXiv251230] Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection**
  - **tags:** [cv], [object detection], [Segment Anything Model, semi-supervised learning, knowledge distillation, Mixture of Experts, infrared small object detection]
  - **authors:** Zihan Liu, Xiangning Ren, Dezhang Kong, Yipeng Zhang, Meng Han
  - **institution:** Chengdu University, China University of Geosciences (Beijing), Zhejiang University, Zhejiang Lab
  - **link:** https://arxiv.org/pdf/2512.22483
  - **contributions:** 1. Proposed a Hierarchical Mixture of Experts (MoE) Adapter to adapt the Segment Anything Model (SAM) to the infrared domain and encode physical priors. 2. Introduced a novel two-stage semi-supervised paradigm (Scalpel-SAM) for knowledge distillation and transfer, requiring only 10% labeled data. 3. Demonstrated that the paradigm enables training lightweight downstream models with pseudo-labels, achieving performance comparable to or surpassing fully supervised models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data scarcity and domain gap challenges in Infrared Small Object Detection (IR-SOT) by proposing Scalpel-SAM, a semi-supervised paradigm. It adapts the Segment Anything Model (SAM) using a Hierarchical MoE Adapter and a two-stage knowledge distillation/transfer process, enabling efficient downstream models to be trained with minimal annotations. Experiments show the method achieves performance on par with or better than fully supervised counterparts.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[高标注成本与领域鸿沟/High annotation cost & domain gap]
        C --> C1[两阶段半监督范式/Two-stage semi-supervised paradigm]
        C1 --> C1_1[先验引导知识蒸馏/Prior-Guided Knowledge Distillation]
        C1 --> C1_2[部署导向知识迁移/Deployment-Oriented Knowledge Transfer]
        C1_1 --> C1_1a[使用MoE适配器蒸馏SAM/Use MoE Adapter to distill SAM]
        D --> D1[下游模型性能媲美全监督/Downstream models match or surpass fully supervised performance]
    ```

- **[arXiv251230] Tracking by Predicting 3-D Gaussians Over Time**
  - **tags:** [cv], [video representation learning], [self-supervised learning, Gaussian splatting, masked autoencoder, point tracking, video understanding]
  - **authors:** Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik
  - **institution:** University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.22489
  - **code:** https://github.com/tekotan/video-gmae
  - **contributions:** 1. Proposes Video-GMAE, a novel self-supervised method that learns video representations by encoding a sequence into a set of moving 3D Gaussians, enforcing temporal correspondence as an inductive bias. 2. Discovers that tracking emerges naturally from this pretraining, enabling zero-shot point tracking performance comparable to state-of-the-art methods. 3. Demonstrates superior performance after fine-tuning, achieving significant improvements on Kinetics and Kubric datasets over existing self-supervised video approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Video-GMAE, a self-supervised learning method that represents a video as a set of 3D Gaussian primitives moving over time. This representation enforces temporal consistency, allowing the model to learn strong correspondences and enabling zero-shot point tracking. The method outperforms existing self-supervised approaches on video understanding and tracking benchmarks after fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Tracking by Predicting 3-D Gaussians Over Time") --> Problem("核心问题/Problem: Self-supervised video representations lack strong temporal correspondence for tracking.")
        Root --> Method("主要方法/Method: Video-GMAE: Masked Autoencoder predicts moving 3D Gaussian splats from video frames.")
        Root --> Results("关键结果/Results: Emergent zero-shot tracking; SOTA performance after fine-tuning on Kinetics & Kubric.")
    ```

- **[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment**
  - **tags:** [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]
  - **authors:** Hassan Wasswa, Timothy Lynar
  - **institution:** University of New South Wales
  - **link:** https://arxiv.org/pdf/2512.22488
  - **contributions:** 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework's effectiveness in dynamic IoT environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection / 面向真实世界物联网安全：概念漂移鲁棒的物联网僵尸网络检测"]
        Root --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["AI模型依赖静态数据集 / AI models rely on stationary datasets"]
        Problem --> P2["真实流量存在概念漂移 / Real-world traffic suffers concept drift"]
        Problem --> P3["现有方案重训练开销大 / Existing solutions have high retraining cost"]
        Method --> M1["学习历史流量的潜在空间表示 / Learn latent space of historical traffic"]
        Method --> M2["对齐模型映射新流量 / Alignment model maps new traffic"]
        Method --> M3["图神经网络分类 / Graph Neural Network for classification"]
        Results --> R1["保持鲁棒检测性能 / Maintains robust detection performance"]
        Results --> R2["适用于动态大规模环境 / Suitable for dynamic, large-scale environments"]
    ```

- **[arXiv251230] SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration**
  - **tags:** [cv], [object detection], [multimodal fusion, BEV perception, coordinate attention, feature alignment, small object detection]
  - **authors:** Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.22503
  - **contributions:** 1. A Section-aware Coordinate Attention (SCA) module to enhance feature discrimination for small, irregular objects. 2. A parameter-efficient Cognitive Adapter for efficient camera backbone tuning. 3. A Contrastive Alignment Module (CAM) to enforce consistency between camera and LiDAR features.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SCAFusion, a multimodal 3D object detection framework built upon BEVFusion to address the challenge of detecting small objects like rocks in lunar exploration. It introduces several modules including a Section-aware Coordinate Attention mechanism to improve small object detection, achieving significant performance gains over the baseline on both standard and simulated lunar datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(现有方法在月球环境表现不佳/Existing methods underperform in lunar environments)
        Problem --> P2(特征未对齐，小物体检测弱/Poor feature alignment, weak small-object detection)
        Method --> M1(基于BEVFusion构建/Built upon BEVFusion)
        Method --> M2(集成认知适配器、对比对齐模块、相机辅助训练分支/Integrates Cognitive Adapter, Contrastive Alignment Module, Camera Auxiliary Training Branch)
        Method --> M3(引入分段坐标注意力机制/Introduces Section-aware Coordinate Attention mechanism)
        Results --> R1(在nuScenes上mAP 69.7%, NDS 72.1%/69.7% mAP, 72.1% NDS on nuScenes)
        Results --> R2(在模拟月球环境上mAP 90.93%/90.93% mAP on simulated lunar environment)
    ```

- **[arXiv251230] DreamOmni3: Scribble-based Editing and Generation**
  - **tags:** [cv], [image editing and generation], [scribble-based editing, unified model, joint input scheme, data synthesis, multimodal instruction]
  - **authors:** Bin Xia, Bohao Peng, Jiyang Liu, Sitong Wu, Jingyao Li, Junjia Huang, Xu Zhao, Yitong Wang, Ruihang Chu, Bei Yu, Jiaya Jia
  - **institution:** The Chinese University of Hong Kong (CUHK), ByteDance Inc, The Hong Kong University of Science and Technology (HKUST)
  - **link:** https://arxiv.org/pdf/2512.22525
  - **code:** https://github.com/dvlab-research/DreamOmni3
  - **contributions:** 1. Proposes two new tasks—scribble-based editing and generation—that combine text, images, and freehand sketches for more flexible and precise user control. 2. Introduces a comprehensive data synthesis pipeline to create training data for these tasks, including multiple sub-tasks like doodle editing and image fusion. 3. Designs a novel joint input scheme that feeds both original and scribbled images to the model, using color and shared encodings to precisely localize edit regions without relying on binary masks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces DreamOmni3, a unified model for scribble-based image editing and generation. It addresses the limitations of text-only instructions by allowing users to specify edits with freehand sketches, proposes a data creation pipeline and a joint input scheme for precise localization, and demonstrates outstanding performance on new benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DreamOmni3: Scribble-based Editing and Generation] --> B[核心问题/Problem: Text prompts fail to capture precise edit locations and fine-grained details.]
        A --> C[主要方法/Method: Proposes scribble-based tasks, a data synthesis pipeline, and a joint input scheme using colored scribbles.]
        A --> D[关键结果/Results: Achieves outstanding performance; establishes benchmarks; releases models and code.]
    ```

- **[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation**
  - **tags:** [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]
  - **authors:** Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22536
  - **contributions:** 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoAgent: Coherent Video Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Open-domain video generation lacks coherence and consistency/开放域视频生成缺乏连贯性和一致性]
        C --> C1[Plan-Synthesize-Verify-Edit Pipeline/计划-合成-验证-编辑流程]
        C1 --> C2[Storyboard Planner/故事板规划器]
        C1 --> C3[Global Context Manager/全局上下文管理器]
        C1 --> C4[Verifier Agent/验证智能体]
        C1 --> C5[Pacing-aware Editor/节奏感知编辑器]
        D --> D1[Improves coherence, consistency, narrative quality/提升连贯性、一致性、叙事质量]
    ```

- **[arXiv251230] VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models**
  - **tags:** [ai], [embodied ai / robot learning], [vision-language-action models, benchmark, generalization, robustness, structured task design]
  - **authors:** Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang
  - **institution:** Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22539
  - **code:** this https URL (from abstract)
  - **contributions:** 1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Difficult to quantitatively understand the limits and failure modes of VLAs]
        C[主要方法/Method: Structured benchmark with orthogonal difficulty axes (Task Structure, Language, Visual) and systematic perturbations]
        D[关键结果/Results: Revealed critical VLA limitations (memorization, asymmetric robustness); Provided open-source framework for research]
    ```

- **[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains**
  - **tags:** [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]
  - **authors:** Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang
  - **institution:** Sun Yat-sen University, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22545
  - **contributions:** 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-Rewarded Multimodal Coherent Reasoning<br>自我奖励多模态连贯推理] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --> P1[现有方法缺陷/Existing Method Flaws<br>Supervises only final answer]
        Method[主要方法/Method<br>SR-MCR Framework] --> M1[自我奖励/Self-Reward<br>Five intrinsic process cues]
        Method --> M2[优化目标/Optimization<br>GRPO with cooling mechanism]
        Results[关键结果/Results<br>Evaluation & Ablation] --> R1[性能提升/Performance Gain<br>SOTA accuracy (81.4%)]
        Results --> R2[消融研究/Ablation Study<br>Confirms contributions]
    ```

- **[arXiv251230] ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization**
  - **tags:** [cv], [medical image segmentation], [3D U-Net, Multi-scale Feature Fusion, Radiomics, Hybrid Upsampling, Residual Skip Mechanism]
  - **authors:** Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Arefin Ittesafun Abian, Yan Zhang, Mirjam Jonkman, Sami Azam
  - **institution:** United International University, Monash University, Charles Darwin University
  - **link:** https://arxiv.org/pdf/2512.22570
  - **contributions:** 1. Proposes ReFRM3D, a novel radiomics-enhanced fused residual multiparametric 3D network for brain tumor characterization, based on a 3D U-Net with multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. 2. Introduces a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented tumor regions. 3. Demonstrates state-of-the-art segmentation performance on multiple BraTS datasets (2019, 2020, 2021), achieving high Dice Similarity Coefficients for whole tumor, enhancing tumor, and tumor core.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses challenges in glioma segmentation and classification from multi-parametric MRI data by proposing ReFRM3D, a novel 3D network architecture enhanced with radiomics and multi-scale feature fusion. The method achieves superior segmentation performance on standard BraTS benchmarks, demonstrating its effectiveness for accurate brain tumor characterization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReFRM3D: Glioma Characterization] --> B[核心问题/Problem: Glioma diagnosis challenges: data variability, inefficient segmentation & classification]
        A --> C[主要方法/Method: ReFRM3D network: 3D U-Net, multi-scale fusion, hybrid upsampling, residual skip, radiomics classifier]
        A --> D[关键结果/Results: High DSC scores on BraTS2019/2020/2021 datasets for WT, ET, TC segmentation]
    ```

- **[arXiv251230] KV-Tracker: Real-Time Pose Tracking with Transformers**
  - **tags:** [cv], [pose tracking and reconstruction], [key-value caching, real-time tracking, multi-view geometry, transformer, online reconstruction]
  - **authors:** Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison
  - **institution:** Imperial College London
  - **link:** https://arxiv.org/pdf/2512.22581
  - **code:** https://marwan99.github.io/kv_tracker/
  - **contributions:** 1. A novel method for adapting slow, multi-view 3D geometry networks for real-time online use by caching key-value pairs from the transformer's self-attention block. 2. A model-agnostic caching strategy that can be applied to off-the-shelf multi-view networks without retraining, enabling significant inference speedup. 3. Demonstration of the system on challenging tasks like on-the-fly object tracking and reconstruction from monocular RGB video without depth or object priors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that powerful multi-view 3D geometry networks are too slow for real-time applications. It proposes KV-Tracker, which adapts these networks for online use by selecting keyframes, running a full multi-view model on them, and then caching the transformer's key-value pairs to serve as a compact scene representation for fast, real-time pose tracking. This approach achieves up to 15x speedup and high frame rates (e.g., ~27 FPS) on standard datasets while preventing drift and catastrophic forgetting.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("KV-Tracker: Real-Time Pose Tracking with Transformers") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("多视角3D网络速度慢/Multi-view 3D networks are slow")
        Problem --> P2("难以实时应用/Difficult for real-time use")
        Method --> M1("选择并管理关键帧/Select & manage keyframes")
        Method --> M2("缓存KV对作为场景表示/Cache KV pairs as scene representation")
        Method --> M3("模型无关的在线跟踪/Model-agnostic online tracking")
        Results --> R1("15倍推理加速/15x inference speedup")
        Results --> R2("高达27 FPS/Up to ~27 FPS")
        Results --> R3("防止漂移和遗忘/Prevents drift & forgetting")
    ```

- **[arXiv251230] PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment**
  - **tags:** [cv], [3D facial animation], [style disentanglement, modality alignment, Graph Attention Networks, cross-attention, contrastive learning]
  - **authors:** Bin Wang, Yang Xu, Huan Zhao, Hao Zhang, Zixing Zhang
  - **institution:** Hunan University, Central South University
  - **link:** https://arxiv.org/pdf/2512.22602
  - **contributions:** 1. A novel framework for personalized 3D talking head animation that preserves individual speaking styles through style disentanglement from audio and motion sequences. 2. A three-level modality alignment mechanism (spatial, temporal, feature) to enhance lip-synchronization accuracy between speech and 3D mesh. 3. Extensive experiments demonstrating superior performance in generating realistic, stylized animations compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes PTalker, a framework for personalized speech-driven 3D talking head animation. It addresses the lack of individual speaking style in existing methods by disentangling style and content from audio/motion and improves lip-sync via a three-level audio-mesh alignment mechanism. Experiments show PTalker outperforms state-of-the-art methods in generating realistic and stylized animations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PTalker: Personalized Speech-Driven 3D Talking Head Animation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法忽略个性化说话风格 / Existing methods overlook individual speaking styles]
        C --> C1[风格解耦 / Style Disentanglement]
        C --> C2[模态对齐 / Modality Alignment]
        C1 --> C1a[从音频和动作序列中分离风格与内容 / Separate style & content from audio & motion]
        C2 --> C2a[空间对齐 / Spatial Alignment: Graph Attention Networks]
        C2 --> C2b[时间对齐 / Temporal Alignment: Cross-Attention]
        C2 --> C2c[特征对齐 / Feature Alignment: Contrastive Loss & KL Divergence]
        D --> D1[生成逼真、个性化的3D说话头 / Generates realistic, personalized 3D talking heads]
        D --> D2[超越现有方法 / Outperforms state-of-the-art methods]
    ```

- **[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation**
  - **tags:** [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]
  - **authors:** Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song
  - **institution:** Zhejiang University, Hangzhou City University, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22605
  - **code:** https://anonymous.4open.science/r/M3ob-62EF
  - **contributions:** 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A["Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation"] --> B["核心问题/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap."]
    A --> C["主要方法/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment."]
    A --> D["关键结果/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios."]
    ```

- **[arXiv251230] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone**
  - **tags:** [mlsys], [multi-modal training], [diffusion language model, vision-language-action, parallel generation]
  - **authors:** Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong
  - **institution:** The University of Hong Kong, Huawei Technologies
  - **link:** https://arxiv.org/pdf/2512.22615
  - **contributions:** 1. Introduces Dream-VL, a state-of-the-art open diffusion-based Vision-Language Model (dVLM) that matches top AR-based VLMs on benchmarks and excels at visual planning. 2. Introduces Dream-VLA, a diffusion-based Vision-Language-Action model built upon Dream-VL, leveraging the bidirectional nature of diffusion for superior action chunking and faster fine-tuning convergence. 3. Demonstrates that diffusion-based VLMs/VLAs outperform autoregressive baselines on downstream tasks, achieving top-tier performance on robotic benchmarks like LIBERO, SimplerEnv-Bridge, and SimplerEnv-Fractal.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes building Vision-Language and Vision-Language-Action models on diffusion-based language models to overcome the limitations of autoregressive models in complex planning and control. The introduced models, Dream-VL and Dream-VLA, leverage the bidirectional, parallel generation nature of diffusion for superior performance in visual planning and robotic tasks, achieving state-of-the-art results on key benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Dream-VL & Dream-VLA<br>论文标题/Paper Title] --> B[AR模型在视觉规划与机器人控制中存在局限<br>核心问题/Problem]
        A --> C[基于扩散语言模型构建VLM和VLA模型<br>主要方法/Method]
        A --> D[在多个基准测试中取得SOTA性能<br>关键结果/Results]
    ```

- **[arXiv251230] Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer**
  - **tags:** [cv], [face clustering], [Sparse Differential Transformer, Top-K Jaccard similarity, noise resilience]
  - **authors:** Dafeng Zhang, Yongqi Song, Shizhuo Liu
  - **institution:** Samsung R&D Institute China-Beijing (SRC-B)
  - **link:** https://arxiv.org/pdf/2512.22612
  - **contributions:** 1. Proposed a prediction-driven Top-K Jaccard similarity coefficient to enhance neighbor purity and similarity measurement reliability. 2. Developed a Transformer-based model to predict relationships near the Top-K boundary for more accurate similarity estimation. 3. Introduced a Sparse Differential Transformer (SDT) to eliminate noise from irrelevant feature relationships and improve the model's anti-noise capability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of noise in face clustering caused by irrelevant nodes in Jaccard similarity measurements. The authors propose a Sparse Differential Transformer (SDT) to predict and refine Top-K Jaccard similarity, enhancing noise resilience. Experiments on datasets like MS-Celeb-1M show the method achieves state-of-the-art performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法引入过多无关节点，Jaccard系数判别力有限/Existing methods introduce too many irrelevant nodes, Jaccard coefficient has limited discriminative power]
        C --> C1[提出预测驱动的Top-K Jaccard相似度系数/Propose prediction-driven Top-K Jaccard similarity coefficient]
        C --> C2[开发基于Transformer的预测模型/Develop a Transformer-based prediction model]
        C --> C3[提出稀疏差分Transformer (SDT) 消除噪声/Propose Sparse Differential Transformer (SDT) to eliminate noise]
        D --> D1[在多个数据集上达到SOTA性能/Achieves SOTA performance on multiple datasets]
        D --> D2[为面部聚类提供更鲁棒的解决方案/Provides a more robust solution for face clustering]
    ```

- **[arXiv251230] Rethinking Memory Design in SAM-Based Visual Object Tracking**
  - **tags:** [cv], [visual object tracking], [Segment Anything Model, memory mechanism, hybrid memory, distractor-resolving, occlusion robustness]
  - **authors:** Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed
  - **institution:** Khalifa University
  - **link:** https://arxiv.org/pdf/2512.22624
  - **code:** https://github.com/HamadYA/SAM3_Tracking_Zoo
  - **contributions:** 1. Conducted a systematic analysis of memory design in SAM-based trackers, revealing that methods primarily differ in short-term memory frame selection. 2. Reimplemented and evaluated existing memory mechanisms within the SAM3 framework across ten benchmarks for a controlled analysis. 3. Proposed a unified hybrid memory framework that decomposes memory into short-term appearance and long-term distractor-resolving components, improving robustness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically studies memory design in SAM-based visual object tracking. It analyzes existing methods, reimplements their memory mechanisms in SAM3, and proposes a unified hybrid memory framework. The framework improves tracking robustness in challenging scenarios like occlusion and distractors for both SAM2 and SAM3 backbones.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Rethinking Memory Design in SAM-Based Visual Object Tracking] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有方法对内存的设计缺乏系统性理解/Lack of systematic understanding of memory design in SAM-based tracking]
        Problem --> P2[内存机制在更强基础模型上的迁移效果未知/Unclear how memory mechanisms transfer to next-gen models like SAM3]
        Method[主要方法/Method] --> M1[分析代表性SAM2跟踪器/Analyze representative SAM2-based trackers]
        Method --> M2[在SAM3中忠实复现内存机制/Faithfully reimplement memory mechanisms in SAM3]
        Method --> M3[提出统一的混合内存框架/Propose a unified hybrid memory framework]
        Results[关键结果/Results] --> R1[实现大规模基准评估/Conduct large-scale benchmark evaluations]
        Results --> R2[在遮挡、复杂运动等场景提升鲁棒性/Improve robustness under occlusion, complex motion, distractors]
    ```

- **[arXiv251230] Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion**
  - **tags:** [cv], [embodied visual planning], [video diffusion, goal-conditioned generation, embodied agents, visual imagination, first-and-last-frame-conditioned model]
  - **authors:** Yuming Gu, Yizhi Wang, Yining Hong, Yipeng Gao, Hao Jiang, Angtian Wang, Bo Liu, Nathaniel S. Dennler, Zhengfei Kuang, Hao Li, Gordon Wetzstein, Chongyang Ma
  - **institution:** University of Southern California, ByteDance, Stanford University, Massachusetts Institute of Technology, MBZUAI
  - **link:** https://arxiv.org/pdf/2512.22626
  - **code:** https://envision-paper.github.io/
  - **contributions:** 1. Proposes Envision, a two-stage diffusion framework for embodied visual planning that explicitly uses a goal image to constrain trajectory generation. 2. Introduces a Goal Imagery Model that synthesizes a coherent goal image by identifying task-relevant regions and performing region-aware cross attention. 3. Develops an Env-Goal Video Model based on a first-and-last-frame-conditioned video diffusion model (FL2V) to interpolate smooth, physically plausible video trajectories between start and goal states.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of spatial drift and goal misalignment in embodied visual planning by proposing Envision, a two-stage diffusion framework that first generates a goal image and then creates a video trajectory connecting the initial scene to that goal. This method enforces goal consistency and physical plausibility. Experiments show it outperforms baselines in goal alignment and spatial consistency, providing reliable visual plans for robotic control.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --> Problem["核心问题/Problem"]
        Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --> Method["主要方法/Method"]
        Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --> Results["关键结果/Results"]
        Problem --> P1["现有方法为前向预测/Existing approaches are forward predictive"]
        Problem --> P2["导致空间漂移和目标错位/Leading to spatial drift & goal misalignment"]
        Method --> M1["两阶段框架/Two-stage framework"]
        M1 --> M1_1["目标图像模型/Goal Imagery Model"]
        M1_1 --> M1_1a["合成目标图像/Synthesizes goal image"]
        M1 --> M1_2["环境-目标视频模型/Env-Goal Video Model"]
        M1_2 --> M1_2a["基于FL2V的插值/FL2V-based interpolation"]
        Results --> R1["目标对齐更优/Superior goal alignment"]
        Results --> R2["空间一致性更好/Better spatial consistency"]
        Results --> R3["支持机器人规划/Supports robotic planning"]
    ```

- **[arXiv251230] FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution**
  - **tags:** [cv], [image super-resolution], [reinforcement learning from human feedback (RLHF), reward hacking, perceptual quality, curriculum learning, fine-grained assessment]
  - **authors:** Yidi Liu, Zihao Fan, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Xueyang Fu, Zheng-Jun Zha
  - **institution:** University of Science and Technology of China, Shanghai AI Laboratory
  - **link:** https://arxiv.org/pdf/2512.22647
  - **contributions:** 1. Proposes a Fine-grained Perceptual Reward Model (FinPercep-RM) with an encoder-decoder architecture that outputs both a global quality score and a Perceptual Degradation Map to localize defects. 2. Introduces the FGR-30k dataset containing diverse and subtle distortions from real-world super-resolution models for training the reward model. 3. Designs a Co-evolutionary Curriculum Learning (CCL) mechanism that synchronizes the progressive training of the reward model and the ISR model to ensure stable training and suppress reward hacking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of reward hacking in RLHF-based Image Super-Resolution, where traditional Image Quality Assessment models are insensitive to local distortions. The authors propose a fine-grained reward model (FinPercep-RM) and a co-evolutionary curriculum learning strategy to provide localized feedback and stabilize training. Experiments show the method improves both global quality and local realism in generated images.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>传统IQA模型对局部失真不敏感，导致奖励欺骗/Reward Hacking]
        C[主要方法/Method<br>1. 细粒度感知奖励模型 (FinPercep-RM)<br>2. 协同进化课程学习 (CCL)]
        D[关键结果/Results<br>提升全局质量与局部真实感，实现稳定训练/Improves global quality & local realism, enables stable training]
    ```

- **[arXiv251230] Visual Autoregressive Modelling for Monocular Depth Estimation**
  - **tags:** [cv], [monocular depth estimation], [visual autoregressive modelling, classifier-free guidance, scale-wise conditional upsampling]
  - **authors:** Amir El-Ghoussani, André Kaup, Nassir Navab, Gustavo Carneiro, Vasileios Belagiannis
  - **institution:** Friedrich-Alexander University Erlangen-Nuremberg, Technical University of Munich, University of Surrey
  - **link:** https://arxiv.org/pdf/2512.22653
  - **code:** https://github.com/AmirMaEl/VAR-Depth
  - **contributions:** 1. Proposes a monocular depth estimation method based on visual autoregressive (VAR) priors as an alternative to diffusion models. 2. Introduces a scale-wise conditional upsampling mechanism with classifier-free guidance for the task. 3. Demonstrates the method's efficiency (10-stage inference, 74K fine-tuning samples) and strong performance, achieving state-of-the-art results on indoor benchmarks under constrained training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new method for monocular depth estimation that uses visual autoregressive (VAR) priors instead of diffusion models. The approach adapts a large text-to-image VAR model with a novel scale-wise upsampling mechanism and achieves competitive results with efficient fine-tuning. The work establishes autoregressive models as a complementary, data-scalable family of generative models for 3D vision tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Visual Autoregressive Modelling for Monocular Depth Estimation] --> B(核心问题/Problem: Monocular depth estimation as an ill-posed task)
        A --> C(主要方法/Method: Adapt VAR priors with scale-wise upsampling & classifier-free guidance)
        A --> D(关键结果/Results: SOTA indoor performance, strong outdoor results, efficient fine-tuning)
    ```

- **[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading**
  - **tags:** [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]
  - **authors:** Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni
  - **institution:** University Hospital Bonn, Technical University of Munich
  - **link:** https://arxiv.org/pdf/2512.22666
  - **contributions:** 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["INTERACT-CMIL: CMIL分级<br>INTERACT-CMIL: CMIL Grading"]
        Root --> Problem["核心问题/Problem<br>CMIL分级困难，主观性强<br>CMIL grading is difficult and subjective"]
        Root --> Method["主要方法/Method<br>多任务共享学习与任务间一致性<br>Multi-task Shared Learning & Inter-Task Consistency"]
        Root --> Results["关键结果/Results<br>性能显著提升，提供可解释预测<br>Significant performance gains, interpretable predictions"]
    ```

- **[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains**
  - **tags:** [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]
  - **authors:** Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang
  - **institution:** University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22664
  - **code:** https://github.com/qklee-lz/CLAdapter
  - **contributions:** 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[下游任务数据稀缺/Data-limited downstream tasks]
        C --> C1[提出CLAdapter/Propose CLAdapter]
        C1 --> C2[注意力与聚类中心/Attention & Cluster Centers]
        C2 --> C3[个性化特征增强/Personalized Feature Enhancement]
        D --> D1[10个数据集实验/Experiments on 10 datasets]
        D1 --> D2[实现SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos**
  - **tags:** [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]
  - **authors:** Shravan Saranyan, Pramit Saha
  - **institution:** Branham High School, University of Oxford
  - **link:** https://arxiv.org/pdf/2512.22657
  - **contributions:** 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]
        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]
        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]
        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]
        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]
    ```

- **[arXiv251230] CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation**
  - **tags:** [cv], [text-to-image generation], [inference-time refinement, semantic critique, spectral fusion, diffusion models, vision-language model]
  - **authors:** ZhenQi Chen, TsaiChing Ni, YuanFu Yang
  - **institution:** National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.22681
  - **contributions:** 1. Introduces CritiCore, a multimodal semantic critique module using VLM and LLMs to provide high-level feedback for better prompt alignment. 2. Proposes SpecFusion, a frequency-domain method to merge intermediate generation states, preserving high-frequency details while injecting structure. 3. Presents CritiFusion as a plug-in, training-free framework compatible with existing diffusion backbones, improving both semantic correspondence and visual quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of text-to-image diffusion models struggling with semantic alignment to complex prompts. It proposes CritiFusion, an inference-time framework that uses a semantic critique module and spectral fusion to refine generated images, improving faithfulness and detail without requiring additional training. The method achieves results competitive with state-of-the-art reward optimization approaches on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation] --> B(核心问题/Problem: Diffusion models struggle with semantic alignment to complex prompts.)
        A --> C(主要方法/Method: Inference-time framework with CritiCore for semantic critique and SpecFusion for spectral refinement.)
        A --> D(关键结果/Results: Improves text-image correspondence and visual quality; competitive with SOTA.)
    ```

- **[arXiv251230] Autoregressive Flow Matching for Motion Prediction**
  - **tags:** [cv], [motion prediction], [autoregressive flow matching, point tracks, probabilistic modeling, video conditioning]
  - **authors:** Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.22688
  - **code:** https://github.com/Johnathan-Xie/arfm-motion-prediction
  - **contributions:** 1. Proposes autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data. 2. Develops benchmarks for evaluating motion prediction models on human and robot motion. 3. Demonstrates that conditioning downstream tasks (robot action prediction, human motion prediction) on predicted future tracks improves performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of predicting future motion from videos, which requires understanding agent behavior and physics. It proposes Autoregressive Flow Matching (ARFM), a method trained on diverse video datasets to probabilistically generate future point track locations. The model shows the ability to predict complex motions and its predictions can significantly improve downstream task performance in robotics and human motion analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Autoregressive Flow Matching for Motion Prediction") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有模型泛化性差或运动建模不准 / Existing models lack generality or accurate motion modeling")
        Method --> M1("提出自回归流匹配 / Propose Autoregressive Flow Matching (ARFM)")
        Method --> M2("在大规模多样化视频数据上训练 / Train on large-scale diverse video datasets")
        Method --> M3("预测未来点轨迹位置 / Predict future point track locations")
        Results --> R1("能预测复杂运动 / Capable of predicting complex motions")
        Results --> R2("提升下游任务性能 / Improves downstream task performance")
    ```

- **[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors**
  - **tags:** [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]
  - **authors:** Salvador Rodriguez-Sanz, Monica Hernandez
  - **institution:** University of Zaragoza, Aragon Institute for Engineering Research (I3A)
  - **link:** https://arxiv.org/pdf/2512.22689
  - **contributions:** 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统算法假设强度相关，限于单模态/Traditional methods assume intensity correlation, limited to monomodal]
        B --> B2[学习模型需要大量数据，泛化性差/Learning-based models need large datasets, poor generalization]
        C --> C1[基于实例的框架/Instance-specific framework]
        C --> C2[使用神经ODE与结构描述符/Using Neural ODEs & Structural Descriptors]
        C --> C3[整合局部互信息/Integrating Local Mutual Information]
        D --> D1[超越SOTA结果/Surpassing SOTA results]
        D --> D2[对正则化鲁棒/Robust to varying regularization]
        D --> D3[高效且适用于不同尺度/Efficient & suitable for varying scales]
    ```

- **[arXiv251230] Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM**
  - **tags:** [other], [motion capture, wearable computing, human-computer interaction], [IMU, WebXR, SLAM, IoT, edge computing]
  - **authors:** Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidané, Tejaswi Gowda
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.22690
  - **contributions:** 1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5° joint-angle error) at approximately 5% of the cost, with low latency and high reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp
  - **Simple LLM Summary:** The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Motion capture is costly and complex, limiting accessibility] --> P1[昂贵且复杂/Expensive & Complex]
        Problem --> P2[局限于专业实验室/Limited to Specialized Labs]
        Method[主要方法/Method: Open-source, low-cost system using IoT sensors and web tech] --> M1[身体佩戴IMU传感器网络/Body-worn IMU Sensor Network]
        Method --> M2[智能手机WebXR SLAM定位/Smartphone WebXR SLAM for Positioning]
        Method --> M3[基于浏览器的应用/Web-Browser-Based Application]
        Results[关键结果/Results: Affordable, accurate, and real-time performance] --> R1[成本约为商业系统的5%/~5% Cost of Commercial System]
        Results --> R2[平均关节角度误差2-5度/Mean Joint-Angle Error 2-5°]
        Results --> R3[实时低延迟/Real-Time with Low Latency]
    ```

- **[arXiv251230] SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis**
  - **tags:** [cv], [novel view synthesis], [3D Gaussian Splatting, diffusion models, autonomous driving simulation]
  - **authors:** Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang
  - **institution:** University of Calgary
  - **link:** https://arxiv.org/pdf/2512.22706
  - **contributions:** 1. A unified framework (SCPainter) that jointly handles realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. 2. Integration of 3D Gaussian Splatting representations for assets with 3D scene point clouds and a diffusion model for high-quality image generation. 3. Demonstration of the framework's capability to create diverse and realistic driving data on the Waymo Open Dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SCPainter, a unified framework that combines 3D Gaussian Splatting for asset representation with diffusion models to jointly perform realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. The method projects 3D assets and scene point clouds into novel views and uses them to condition a diffusion model to generate high-quality images. Evaluation shows the framework can create diverse and realistic driving scenarios for training data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SCPainter: 统一框架 / Unified Framework] --> B[核心问题 / Problem: 现有方法孤立处理资产插入与NVS / Existing methods treat asset insertion and NVS in isolation]
        A --> C[主要方法 / Method: 集成3D高斯溅射与扩散模型 / Integrates 3D Gaussian Splatting and diffusion models]
        A --> D[关键结果 / Results: 在Waymo数据集上实现真实感生成 / Enables realistic generation on Waymo dataset]
    ```

- **[arXiv251230] Memento-II: Learning by Stateful Reflective Memory**
  - **tags:** [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]
  - **authors:** Jun Wang
  - **institution:** University College London (UCL)
  - **link:** https://arxiv.org/pdf/2512.22716
  - **contributions:** 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]
        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]
        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]
        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]
        C --> F[读取对应策略改进/Reading corresponds to policy improvement]
    ```

- **[arXiv251230] Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning**
  - **tags:** [cv], [medical image classification], [self-supervised learning, masked autoencoding, Score-CAM, prenatal ultrasound, cystic hygroma]
  - **authors:** Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken
  - **institution:** Carleton University, Ottawa Hospital Research Institute, University of Ottawa, Children's Hospital of Eastern Ontario Research Institute
  - **link:** https://arxiv.org/pdf/2512.22730
  - **contributions:** 1. Applied and fine-tuned a large-scale, ultrasound-specific self-supervised model (USF-MAE) for the task of cystic hygroma detection. 2. Demonstrated that this self-supervised pre-training approach significantly outperforms a supervised baseline (DenseNet-169) trained from scratch on a small labeled dataset. 3. Provided qualitative interpretability analysis using Score-CAM visualizations to show the model's focus on clinically relevant anatomical regions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated cystic hygroma detection in prenatal ultrasound, where labeled data is scarce. The authors propose fine-tuning a self-supervised model (USF-MAE) pre-trained on a large corpus of unlabeled ultrasound images. Their method significantly outperforms a supervised baseline, demonstrating that ultrasound-specific self-supervised learning enables accurate, robust, and data-efficient detection for early prenatal screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Improved cystic hygroma detection using ultrasound-specific self-supervised learning<br>基于超声特异性自监督学习的囊性水瘤检测改进"]
        Root --> Problem["核心问题/Problem<br>Supervised deep learning for cystic hygroma detection is limited by small labeled datasets.<br>监督式深度学习受限于小规模标注数据集。"]
        Root --> Method["主要方法/Method<br>Fine-tune USF-MAE, a self-supervised model pre-trained on 370k+ unlabeled ultrasound images.<br>微调在37万+无标签超声图像上预训练的USF-MAE自监督模型。"]
        Root --> Results["关键结果/Results<br>Outperformed DenseNet-169 baseline (Accuracy: 0.96 vs 0.93). Performance gains are statistically significant.<br>性能超越DenseNet-169基线，提升具有统计显著性。"]
    ```

- **[arXiv251230] Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation**
  - **tags:** [cv], [4D scene reconstruction], [Freetime FeatureGS, contrastive loss, streaming feature learning, Gaussian primitives, decomposed reconstruction]
  - **authors:** Yongzhen Hu, Yihui Yang, Haotong Lin, Yifan Wang, Junting Dong, Yifu Deng, Xinyu Zhu, Fan Jia, Hujun Bao, Xiaowei Zhou, Sida Peng
  - **institution:** Zhejiang University, Ant Group, Shanghai AI Lab
  - **link:** https://arxiv.org/pdf/2512.22745
  - **contributions:** 1. Proposes a novel decomposed 4D scene reconstruction method that eliminates the need for unstable video segmentation by relying only on per-image segmentation maps. 2. Introduces Freetime FeatureGS, a representation using Gaussian primitives with learnable features and linear motion, and a contrastive loss to enforce instance-level feature consistency. 3. Designs a temporally ordered streaming training strategy to propagate features over time, avoiding local minima and achieving accurate 4D segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses decomposed 4D scene reconstruction from multi-view videos by proposing a method that avoids reliance on video segmentation. The core idea is to represent the scene with a novel Freetime FeatureGS model and train it with a contrastive loss and a streaming strategy using only per-frame segmentation. Experiments show the method outperforms recent state-of-the-art approaches in reconstruction quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation] --> B[核心问题/Problem: 依赖视频分割的4D重建方法不稳定/Reliance on video segmentation leads to unstable 4D reconstruction]
        A --> C[主要方法/Method: 提出Freetime FeatureGS与流式特征学习/Propose Freetime FeatureGS and streaming feature learning]
        A --> D[关键结果/Results: 在多个数据集上超越现有方法/Outperforms recent methods on several datasets]
    ```

- **[arXiv251230] TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts**
  - **tags:** [mlsys], [multi-modal inference], [visual token pruning, long context, adaptive budget allocation, intra-image diversity, inter-image variation]
  - **authors:** Hao Zhang, Mengsi Lyu, Bo Huang, Yulong Ao, Yonghua Lin
  - **institution:** Beijing Academy of Artificial Intelligence (BAAI)
  - **link:** https://arxiv.org/pdf/2512.22748
  - **contributions:** 1. Identifies and analyzes the specific challenges of visual token pruning in long-context, multi-image scenarios. 2. Proposes a novel two-stage adaptive pruning method that decomposes redundancy into intra-image and inter-image components, guided by diversity and variation metrics. 3. Introduces a Pareto selection procedure in the inter-image stage to balance global token diversity with text alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high inference cost of Large Multimodal Models (LMMs) in long-context, multi-image settings by proposing TrimTokenator-LC, an adaptive visual token pruning method. The method uses a two-stage process to dynamically allocate token budgets based on intra-image diversity and inter-image variation, then selects tokens by balancing diversity and text relevance. Experiments show the approach significantly reduces the number of visual tokens while maintaining strong performance in long-context tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[视觉令牌冗余导致长上下文多图像推理成本高/High inference cost from redundant visual tokens in long-context multi-image settings]
        C --> C1[分解冗余为图像内和图像间/Decompose redundancy into intra-image and inter-image]
        C --> C2[两阶段自适应剪枝/Two-stage adaptive pruning]
        C2 --> C2_1[图像内阶段：内容感知预算与贪婪选择/Intra-image: content-aware budget & greedy selection]
        C2 --> C2_2[图像间阶段：全局多样性过滤与帕累托选择/Inter-image: global diversity filtering & Pareto selection]
        D --> D1[在长上下文中保持强性能/Maintains strong performance in long context]
        D --> D2[显著减少视觉令牌数量/Significantly cuts down visual tokens]
    ```

- **[arXiv251230] Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers**
  - **tags:** [cv], [vision transformer efficiency], [token reduction, Hilbert curve, neighbor-aware pruning, token merging, spatial continuity]
  - **authors:** Yunge Li, Lanyu Xu
  - **institution:** Oakland University
  - **link:** https://arxiv.org/pdf/2512.22760
  - **code:** https://github.com/Yunge6666/NAP-MAT
  - **contributions:** 1. Proposes a novel neighbor-aware token reduction framework for Vision Transformers that explicitly preserves spatial continuity and local context. 2. Introduces Neighbor-Aware Pruning (NAP), which incorporates the influence of neighboring tokens into importance scoring for selective retention. 3. Introduces Merging by Adjacent Token similarity (MAT), a localized token aggregation strategy that computes similarity and merges tokens only within adjacent regions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational inefficiency of Vision Transformers caused by redundant token representations. It proposes a novel token reduction method based on Hilbert curve reordering, which includes Neighbor-Aware Pruning (NAP) and Merging by Adjacent Token similarity (MAT) to preserve spatial neighbor structure. The approach achieves state-of-the-art accuracy-efficiency trade-offs, highlighting the importance of spatial continuity for ViT optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("ViT计算效率低/ViT Computational Inefficiency")
        Problem --> P2("现有方法忽略空间连续性/Existing Methods Overlook Spatial Continuity")
        Method --> M1("Hilbert曲线重排序/Hilbert Curve Reordering")
        Method --> M2("邻居感知剪枝(NAP)/Neighbor-Aware Pruning (NAP)")
        Method --> M3("相邻令牌合并(MAT)/Merging by Adjacent Token similarity (MAT)")
        Results --> R1("SOTA精度-效率权衡/SOTA Accuracy-Efficiency Trade-off")
        Results --> R2("强调空间连续性/Highlights Spatial Continuity")
    ```

- **[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting**
  - **tags:** [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]
  - **authors:** Yiqian Li, Wen Jiang, Kostas Daniilidis
  - **institution:** University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.22771
  - **contributions:** 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --> B(核心问题/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)
    A --> C(主要方法/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)
    A --> D(关键结果/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)
    ```

- **[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization**
  - **tags:** [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]
  - **authors:** Truong Son Nguyen
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.22774
  - **contributions:** 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Schrödinger AI: A Unified Spectral-Dynamical Framework<br>薛定谔AI: 统一谱-动力学框架"] --> Problem["核心问题/Problem<br>Limitations of conventional ML<br>传统机器学习的局限"]
        Root --> Method["主要方法/Method<br>Quantum-inspired framework<br>量子启发的框架"]
        Root --> Results["关键结果/Results<br>Empirical demonstrations<br>实证演示"]
        Problem --> P1["Struggle with uncertainty & adaptation<br>难以处理不确定性与适应性"]
        Problem --> P2["Brittle symbolic reasoning<br>脆弱的符号推理"]
        Method --> M1["Time-independent wave-energy solver<br>时间无关波能求解器"]
        Method --> M2["Time-dependent dynamical solver<br>时间相关动力学求解器"]
        Method --> M3["Low-rank operator calculus<br>低秩算子演算"]
        Results --> R1["Emergent semantic manifolds<br>涌现的语义流形"]
        Results --> R2["Dynamic reasoning adaptation<br>动态推理适应"]
        Results --> R3["Exact operator generalization<br>精确算子泛化"]
    ```

- **[arXiv251230] Plug In, Grade Right: Psychology-Inspired AGIQA**
  - **tags:** [cv], [image quality assessment], [AGIQA, Graded Response Model, semantic drift, quality grading, plug-and-play]
  - **authors:** Zhicheng Liao, Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Weisi Lin
  - **institution:** South China Normal University, City University of Hong Kong, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.22780
  - **contributions:** 1. Identifies and defines the "semantic drift" problem in existing AGIQA models where image embeddings show inconsistent similarity to multi-grade text descriptions. 2. Proposes a psychology-inspired, improved Graded Response Model (GRM) for AGIQA, framing quality as an image's "ability" to meet "difficulty" levels. 3. Designs a novel Arithmetic GRM based Quality Grading (AGQG) module that enforces a unimodal, interpretable quality distribution and demonstrates plug-and-play performance gains across various frameworks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the "semantic drift" issue in AI-generated image quality assessment (AGIQA), where inconsistent similarities between image and text embeddings degrade reliability. The authors propose a novel Arithmetic GRM based Quality Grading (AGQG) module, inspired by psychometrics, which models image quality as an ability to overcome graded difficulty levels. The plug-and-play module consistently improves state-of-the-art AGIQA models and generalizes to both natural and screen content images.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Plug In, Grade Right: Psychology-Inspired AGIQA"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["语义漂移/Semantic Drift"]
        Problem --> P2["多模态相似度分布/Multimodal Similarity"]
        Method --> M1["分级响应模型/Graded Response Model (GRM)"]
        Method --> M2["双分支模块/Two-branch Module"]
        Method --> M3["算术难度生成/Arithmetic Difficulty"]
        Results --> R1["即插即用提升/Plug-and-Play Improvement"]
        Results --> R2["泛化能力/Generalization"]
    ```

- **[arXiv251230] VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM**
  - **tags:** [cv], [object tracking], [vision-language tracking, multimodal large language model, visual prompt, global search, location-aware]
  - **authors:** Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng
  - **institution:** East China Normal University, Westlake University
  - **link:** https://arxiv.org/pdf/2512.22799
  - **code:** https://github.com/jcwang0602/VPTracker
  - **contributions:** 1. Introduces the first global vision-language tracking framework based on Multimodal Large Language Models (MLLMs)., 2. Proposes a location-aware visual prompting mechanism to incorporate spatial priors and suppress distractions., 3. Demonstrates enhanced tracking stability and target disambiguation in challenging scenarios, opening a new avenue for MLLM integration in visual tracking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VPTracker, a novel global vision-language tracking framework that leverages Multimodal Large Language Models (MLLMs) for robust target localization across the entire image. To address distractions from global search, it introduces a location-aware visual prompting mechanism that uses the target's previous location as a spatial prior. Experiments show the method significantly improves tracking stability and disambiguation under challenging conditions like occlusions and rapid motion.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VPTracker: Global Vision-Language Tracking] --> B[核心问题/Problem: 现有局部搜索方法在视角变化、遮挡、快速运动下易失效/Existing local search methods prone to failure under viewpoint changes, occlusions, rapid motion]
        A --> C[主要方法/Method: 基于MLLM的全局跟踪框架 + 位置感知视觉提示/Global tracking via MLLM + Location-aware visual prompt]
        A --> D[关键结果/Results: 显著提升跟踪稳定性和目标辨别能力/Significantly enhances tracking stability and target disambiguation]
    ```

- **[arXiv251230] Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation**
  - **tags:** [cv], [3D reconstruction], [3D Gaussian Representation, tri-plane representation, sparse reconstruction, semantic segmentation, medical image analysis]
  - **authors:** Bin Liu, Wenyan Tian, Huangxin Fu, Zizheng Li, Zhifen He, Bo Li
  - **institution:** Nanchang Hangkong University, Guilin University Of Electronic Technology
  - **link:** https://arxiv.org/pdf/2512.22800
  - **contributions:** 1. Proposes an efficient 3D reconstruction method for medical images by combining 3D Gaussian and tri-plane representations. 2. Enhances structural continuity and semantic consistency under sparse slice conditions, addressing a key limitation of traditional methods. 3. Demonstrates high-quality, anatomically coherent reconstruction on multimodal medical datasets (US, MRI) with improved efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of computationally expensive and detail-losing 3D reconstruction from sparse medical image slices. It proposes a novel method that integrates 3D Gaussian and tri-plane representations to efficiently generate high-quality, anatomically coherent 3D visualizations. Experiments on US and MRI data confirm the method's effectiveness in improving reconstruction quality and efficiency under sparse data conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation] --> B(核心问题/Problem: 稀疏切片下传统3D重建方法计算昂贵、结构不连续/Traditional 3D reconstruction is computationally expensive and structurally discontinuous with sparse slices)
        A --> C(主要方法/Method: 基于3D高斯表示与三平面表示的高效重建方法/Efficient reconstruction method based on 3D Gaussian and tri-plane representations)
        A --> D(关键结果/Results: 在稀疏数据下生成高质量、解剖一致且高效的3D医学图像/Generates high-quality, anatomically coherent, and efficient 3D medical images under sparse data)
    ```

- **[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion**
  - **tags:** [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]
  - **authors:** Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee
  - **institution:** Sharif University of Technology, Alan Turing Institute, London School of Economics
  - **link:** https://arxiv.org/pdf/2512.22802
  - **contributions:** 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReDiF: Reinforced Distillation for Few Step Diffusion] --> B(核心问题/Problem: Diffusion模型采样慢/Slow sampling in diffusion models)
        A --> C(主要方法/Method: 基于强化学习的蒸馏框架/Reinforcement learning based distillation framework)
        A --> D(关键结果/Results: 更少步骤，性能更优/Fewer steps, superior performance)
    ```

- **[arXiv251230] Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image**
  - **tags:** [cv], [object detection], [open-vocabulary object detection, low-quality image, image degradation, benchmark dataset, OWLv2]
  - **authors:** Po-Chih Wu
  - **institution:** National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.22801
  - **code:** https://github.com/gohakushi1118/Low-quality-image-dataset
  - **contributions:** 1. Constructed a new benchmark dataset simulating real-world low-quality images with four types of degradation (lossy compression, image intensity, noise, blur). 2. Evaluated six state-of-the-art open-vocabulary object detection models on this benchmark to assess their robustness. 3. Provided analysis showing varying model sensitivity to degradation types and levels, with OWLv2 demonstrating more consistent performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the robustness of open-vocabulary object detection models on low-quality images by creating a new benchmark dataset with various image degradations. The experiments show that while models are resilient to mild degradation, severe degradation causes significant performance drops, with OWLv2 models being the most robust.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Evaluating Open-Vocabulary Object Detection in Low-quality Image] --> B[核心问题/Problem: 真实世界低质量图像对开放词汇目标检测的影响/Impact of real-world low-quality images on open-vocabulary object detection]
    A --> C[主要方法/Method: 构建包含四种退化类型的低质量图像数据集/Build a low-quality image dataset with four degradation types]
    A --> D[关键结果/Results: 模型对严重退化敏感，OWLv2表现最稳健/Models are sensitive to severe degradation, OWLv2 is the most robust]
    ```

- **[arXiv251230] Parallel Diffusion Solver via Residual Dirichlet Policy Optimization**
  - **tags:** [mlsys], [diffusion models], [ODE solver, parallel gradient evaluation, reinforcement learning fine-tuning, low-latency sampling, Dirichlet policy]
  - **authors:** Ruoyu Wang, Ziyu Li, Beier Zhu, Liangyu Yuan, Hanwang Zhang, Xun Yang, Xiaojun Chang, Chi Zhang
  - **institution:** Westlake University, University of Illinois Urbana-Champaign, Nanyang Technological University, Shanghai Jiao Tong University, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.22796
  - **contributions:** 1. Proposes EPD-Solver, a novel ODE solver that uses multiple parallel gradient evaluations per step to reduce truncation errors while maintaining low latency. 2. Introduces a two-stage optimization framework, including a parameter-efficient RL fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy to avoid reward hacking. 3. Demonstrates the method's flexibility as a plugin (EPD-Plugin) to enhance existing ODE samplers and shows state-of-the-art performance in both unconditional and text-to-image generation benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high sampling latency of diffusion models by proposing EPD-Solver, a novel ODE solver that incorporates parallel gradient evaluations to reduce errors without increasing latency. The method uses a two-stage optimization, including RL fine-tuning with a Dirichlet policy, and can be used as a plugin. Experiments show it achieves superior image quality at low step counts and improves human preference scores in text-to-image generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Parallel Diffusion Solver via Residual Dirichlet Policy Optimization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[扩散模型采样延迟高 / High sampling latency of DMs]
        B --> B2[现有求解器在低步数下质量下降 / Existing solvers degrade quality at low NFEs]
        C --> C1[EPD-Solver: 集成并行方向求解器 / Ensemble Parallel Direction solver]
        C --> C2[两阶段优化: 蒸馏 + RL微调 / Two-stage optimization: Distillation + RL fine-tuning]
        C --> C3[作为插件提升现有求解器 / Plugin (EPD-Plugin) for existing samplers]
        D --> D1[低延迟下SOTA FID分数 / SOTA FID scores at low latency]
        D --> D2[在T2I任务中提升人类偏好分数 / Improved human preference scores in T2I]
    ```

- **[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation**
  - **tags:** [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]
  - **authors:** Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou
  - **institution:** THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT
  - **link:** https://arxiv.org/pdf/2512.22808
  - **contributions:** 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有数据空间不一致/Existing data spatial misalignment]
        B --> B2[因果生成与3D对齐的挑战/Causal generation & 3D alignment challenge]
        C --> C1[构建HRD数据集/Build HRD dataset]
        C --> C2[VQ-VAE压缩运动/VQ-VAE compresses motion]
        C --> C3[GPT自回归生成/GPT autoregressive generation]
        C --> C4[融入3D动态特征/Incorporate 3D dynamic features]
        D --> D1[更高的真实感与空间一致性/Higher realism & spatial consistency]
        D --> D2[实时生成效率/Real-time generation efficiency]
        D --> D3[保持严格因果性/Maintains strict causality]
    ```

- **[arXiv251230] 3D Scene Change Modeling With Consistent Multi-View Aggregation**
  - **tags:** [cv], [3D scene understanding / change detection], [3D Gaussian Splatting, signed-distance field, multi-view aggregation, continual reconstruction]
  - **authors:** Zirui Zhou, Junfeng Ni, Shujie Zhang, Yixin Chen, Siyuan Huang
  - **institution:** Tsinghua University, State Key Laboratory of General Artificial Intelligence (BIGAI)
  - **link:** https://arxiv.org/pdf/2512.22830
  - **code:** https://zr-zhou0o0.github.io/SCaR3D/
  - **contributions:** 1. Proposes SCaR-3D, a novel 3D scene change detection framework that uses a signed-distance-based 2D differencing module and multi-view aggregation with voting/pruning to robustly separate pre- and post-change states. 2. Develops a continual scene reconstruction strategy that selectively updates dynamic regions while preserving unchanged areas. 3. Contributes CCS3D, a challenging synthetic dataset for flexible and controlled evaluation of 3D change types.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of detecting object-level changes in 3D scenes from multi-view images, where existing methods suffer from spatial inconsistency and cannot separate pre- and post-change states. The proposed SCaR-3D framework leverages 3D Gaussian Splatting for consistent multi-view aggregation and includes a strategy for continual scene reconstruction. Experiments show the method outperforms existing approaches in both accuracy and efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[3D Scene Change Modeling With Consistent Multi-View Aggregation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有3D变化检测方法存在空间不一致性且无法分离变化前后状态/Existing 3D change detection methods exhibit spatial inconsistency and fail to separate pre- and post-change states]
        C --> C1[提出SCaR-3D框架: 基于符号距离的2D差分与多视图聚合投票剪枝/Propose SCaR-3D: Signed-distance-based 2D differencing and multi-view aggregation with voting & pruning]
        C --> C2[开发持续场景重建策略: 选择性更新动态区域/Develop continual reconstruction: Selectively update dynamic regions]
        C --> C3[贡献CCS3D合成数据集/Contribute CCS3D synthetic dataset]
        D --> D1[高精度与高效率/High accuracy and efficiency]
        D --> D2[优于现有方法/Outperforms existing methods]
    ```

- **[arXiv251230] KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution**
  - **tags:** [cv], [image super-resolution], [Kolmogorov-Arnold Neural Operator, B-spline functions, interpretability, spectral fitting, degradation modeling]
  - **authors:** Chenyu Li, Danfeng Hong, Bing Zhang, Zhaojie Pan, Jocelyn Chanussot
  - **institution:** Southeast University, Aerospace Information Research Institute (Chinese Academy of Sciences), Univ. Grenoble Alpes
  - **link:** https://arxiv.org/pdf/2512.22822
  - **contributions:** 1. Proposes the novel Kolmogorov-Arnold Neural Operator (KANO) for interpretable image super-resolution, inspired by the Kolmogorov-Arnold theorem. 2. Employs an additive structure of B-spline functions to model the degradation process transparently, capturing key spectral characteristics like local trends and peak-valley structures. 3. Provides a systematic comparative study between MLPs and KANs for complex sequence fitting, offering insights into interpretable SR model design.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of interpretability in single-image super-resolution by proposing a new model called KANO, which uses B-spline functions based on the Kolmogorov-Arnold theorem to transparently model the image degradation process. The method provides physically interpretable results by learning spectral characteristics. The authors demonstrate its effectiveness and compare it with other models across different image types.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Interpretable Super-Resolution] --> P1[SR是病态逆问题/SR is Ill-posed]
        Problem --> P2[现有方法黑盒/Existing Methods are Black-box]
        Method[主要方法/Method: KANO Model] --> M1[基于KAT定理/Based on KAT]
        Method --> M2[B样条函数拟合/B-spline Fitting]
        Method --> M3[学习形状参数/Learn Shape Parameters]
        Results[关键结果/Results] --> R1[透明降解表示/Transparent Degradation Representation]
        Results --> R2[物理可解释性/Physical Interpretability]
        Results --> R3[系统模型比较/Systematic Model Comparison]
    ```

- **[arXiv251230] Depth Anything in $360^$: Towards Scale Invariance in the Wild**
  - **tags:** [cv], [depth estimation], [panoramic depth estimation, scale invariance, zero-shot generalization, circular padding, ViT backbone]
  - **authors:** Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan
  - **institution:** Insta360 Research
  - **link:** https://arxiv.org/pdf/2512.22819
  - **code:** https://insta360-research-team.github.io/DA360
  - **contributions:** 1. Proposes DA360, a panoramic-adapted version of Depth Anything V2 that learns a shift parameter to transform scale- and shift-invariant output into scale-invariant disparity for direct 3D point cloud generation. 2. Integrates circular padding into the DPT decoder to eliminate seam artifacts and ensure spatially coherent depth maps respecting spherical continuity. 3. Introduces a new outdoor panoramic depth dataset, Metropolis, for evaluation and demonstrates state-of-the-art zero-shot performance on indoor and outdoor benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap in zero-shot generalization for panoramic depth estimation by proposing DA360, an adaptation of Depth Anything V2. The method learns a shift parameter for scale-invariant output and uses circular padding to prevent seam artifacts, enabling direct conversion to 3D point clouds. The results show significant error reduction compared to the base model and other panoramic methods, establishing new state-of-the-art performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Depth Anything in 360°: Towards Scale Invariance in the Wild"] --> Problem["核心问题/Problem: Panoramic depth estimation lags in zero-shot generalization compared to perspective images."]
        Root --> Method["主要方法/Method: Adapt Depth Anything V2 with learned shift parameter for scale invariance and circular padding for seam removal."]
        Root --> Results["关键结果/Results: Achieves >50% and >10% error reduction on indoor/outdoor benchmarks, outperforms PanDA by ~30%."]
    ```

- **[arXiv251230] A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences**
  - **tags:** [cv], [relative pose estimation], [affine correspondences, polynomial eigenvalue method, minimal solver, inertial measurement unit, focal length estimation]
  - **authors:** Zhenbao Yu, Shirong Ye, Ronghe Jin, Shunkun Liang, Zibin Liu, Huiyun Zhang, Banglei Guan
  - **institution:** National University of Defense Technology, Wuhan University, Henan University
  - **link:** https://arxiv.org/pdf/2512.22833
  - **contributions:** 1. Proposes a new minimal solver for estimating 3DOF relative pose and focal length from only two affine correspondences when the vertical direction is known from an IMU. 2. Derives a system of four constraint equations that reduce the problem to solving for only two parameters: focal length and relative rotation angle. 3. Utilizes the polynomial eigenvalue method to efficiently solve the derived equations, demonstrating superior performance over state-of-the-art solvers on synthetic and real-world datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new minimal solver for estimating the relative pose and focal length between two camera views. The method uses only two affine correspondences and known vertical direction (from an IMU) to formulate constraint equations, which are then solved using a polynomial eigenvalue approach. Experimental results show the proposed solver outperforms existing state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences<br>基于两个仿射对应的未知焦距相对位姿估计最小求解器"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Estimate relative pose & focal length from minimal data<br>从最少数据估计相对位姿和焦距"] --> P1["已知垂直方向/Known vertical direction (from IMU)<br>自由度从5DOF降至3DOF"]
        Problem --> P2["输入/Input<br>两个仿射对应/Two affine correspondences"]
        Method["主要方法/Method<br>建立约束方程并求解/Establish constraints and solve"] --> M1["建立方程/Establish equations from two ACs"]
        Method --> M2["推导系统/Derive 4 equations for 2 parameters<br>焦距和旋转角/focal length & rotation angle"]
        Method --> M3["求解方法/Solution Method<br>多项式特征值方法/Polynomial eigenvalue method"]
        Results["关键结果/Results<br>评估与比较/Evaluation & Comparison"] --> R1["合成与真实数据/Synthetic & real-world datasets"]
        Results --> R2["性能更好/Performs better than state-of-the-art solvers"]
    ```

- **[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning**
  - **tags:** [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]
  - **authors:** Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang
  - **institution:** University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.22854
  - **code:** https://neutrinoliu.github.io/byteloom/
  - **contributions:** 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法缺乏多视图信息注入机制/Existing methods lack multi-view injection]
        B --> B2[严重依赖手部网格标注/Heavy reliance on hand mesh annotations]
        C --> C1[提出RCM-cache机制/Propose RCM-cache mechanism]
        C --> C2[设计渐进式课程学习/Design progressive curriculum learning]
        D --> D1[保持物体几何一致性/Preserves object geometry consistency]
        D --> D2[生成平滑运动视频/Generates smooth motion videos]
    ```

- **[arXiv251230] MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments**
  - **tags:** [ai], [embodied navigation], [socially compliant navigation, multimodal dataset, chain-of-thought, vision language models, benchmark]
  - **authors:** Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao
  - **institution:** Hokkaido University, George Mason University
  - **link:** https://arxiv.org/pdf/2512.22867
  - **contributions:** 1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MUSON: 面向推理的多模态城市社会合规导航数据集] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有数据集缺乏显式推理监督/Lack explicit reasoning supervision]
        Problem --> P2[动作分布高度长尾/Highly long-tailed action distribution]
        Method[主要方法/Method] --> M1[引入MUSON数据集/Introduce MUSON dataset]
        M1 --> M1_Sub1[五步思维链标注/Five-step Chain-of-Thought annotation]
        M1 --> M1_Sub2[平衡的离散动作空间/Balanced discrete action space]
        Results[关键结果/Results] --> R1[Qwen2.5-VL-3B取得最高精度/Qwen2.5-VL-3B achieves highest accuracy]
        Results --> R2[数据集作为有效基准/Dataset serves as effective benchmark]
    ```

- **[arXiv251230] Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs**
  - **tags:** [cv], [self-supervised learning], [self-supervised learning, foundation model, chest radiographs, anatomical consistency, multi-perspective learning]
  - **authors:** Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang
  - **institution:** Shanghai Jiao Tong University, Arizona State University, University of Bern, Mayo Clinic
  - **link:** https://arxiv.org/pdf/2512.22872
  - **code:** GitHub.com/JLiangLab/Lamps
  - **contributions:** 1. Proposes a novel self-supervised learning framework (Lamps) that explicitly leverages the consistency, coherence, and hierarchy of human anatomy as supervision signals. 2. Demonstrates superior performance and robustness across 10 chest X-ray datasets compared to 10 baseline models. 3. Releases code and pre-trained models to facilitate research in anatomy-aware foundation models for medical imaging.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of existing self-supervised learning methods in medical imaging, which often overlook anatomical structures. It proposes Lamps, a method that learns from multiple anatomical perspectives (consistency, coherence, hierarchy) using self-supervision on chest radiographs. The results show that Lamps achieves superior robustness and transferability, offering a promising foundation model aligned with human anatomy.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Lamps: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs"] --> Problem["核心问题/Problem: Existing SSL methods overlook anatomical consistency, coherence, and hierarchy in medical images."]
        Root --> Method["主要方法/Method: Lamps uses anatomical consistency, coherence, and hierarchy as self-supervision signals for pre-training on chest X-rays."]
        Root --> Results["关键结果/Results: Superior robustness and transferability across 10 datasets vs. 10 baselines; code and models released."]
    ```

- **[arXiv251230] Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples**
  - **tags:** [ai], [debiasing], [spurious correlation, feature transformation, worst group accuracy, bias-invariant representation, empirical risk minimization]
  - **authors:** Weiwei Li, Junzhuo Liu, Yuanyuan Ren, Yuchen Zheng, Yahao Liu, Wen Li
  - **institution:** University of Electronic Science and Technology of China, Shihezi University
  - **link:** https://arxiv.org/pdf/2512.22874
  - **code:** https://github.com/davelee-uestc/nsf_debiasing
  - **contributions:** 1. Proposes a data-oriented pipeline for mitigating spurious correlation without requiring prior annotation of bias attributes. 2. Introduces a method to identify spurious features by observing the dispersed distribution of biased samples in the feature space. 3. Develops a feature transformation learning process to align with a bias-invariant representation, leading to an unbiased classifier.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of deep learning models learning spurious correlations from biased training data. It proposes a method that identifies, neutralizes, and eliminates spurious features by exploiting the clusteredness of samples and aligning feature transformations with a bias-invariant representation. The approach significantly improves worst-group accuracy by over 20% compared to standard training on image and NLP benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Let Samples Speak: Mitigating Spurious Correlation<br>让样本说话：利用样本聚类性缓解伪相关] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Deep models learn spurious features<br>深度模型学习伪相关特征]
        C --> C1[Identify spurious features via sample distribution<br>通过样本分布识别伪特征]
        C --> C2[Neutralize & eliminate via feature transformation<br>通过特征变换中和与消除]
        C --> C3[Update classifier<br>更新分类器]
        D --> D1[>20% worst-group accuracy improvement vs ERM<br>相比ERM最差组准确率提升>20%]
    ```

- **[arXiv251230] Hash Grid Feature Pruning**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [hash grid, Gaussian splatting, feature pruning, rate-distortion, implicit neural field]
  - **authors:** Yangzhi Ma, Bojun Liu, Jie Li, Li Li, Dong Liu
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.22882
  - **contributions:** 1. Identifies the problem of invalid features in hash grids due to the non-uniform distribution of Gaussian splats, leading to storage and transmission redundancy. 2. Proposes a hash grid feature pruning method that identifies and removes invalid features based on input Gaussian splat coordinates before encoding. 3. Demonstrates improved rate-distortion performance with an average 8% bitrate reduction in standardized tests, without compromising model reconstruction quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of redundant storage in hash grids used for Gaussian splatting compression, caused by invalid features in sparse 3D regions. It proposes a pruning method that removes these invalid features before encoding, reducing bitrate. The method achieves an average 8% bitrate reduction without affecting model performance, improving rate-distortion efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hash Grid Feature Pruning] --> B[核心问题/Problem: Hash grid中存在大量无效特征，导致存储和传输冗余]
        A --> C[主要方法/Method: 基于高斯泼溅坐标识别并剪枝无效特征，仅编码有效特征]
        A --> D[关键结果/Results: 在保持性能的同时，平均比特率降低8%]
    ```

- **[arXiv251230] Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance**
  - **tags:** [mlsys], [diffusion models], [Guided Path Sampling, Classifier-Free Guidance, iterative refinement, off-manifold, path stability]
  - **authors:** Haosen Li, Wenshuo Chen, Shaofeng Liang, Lei Wang, Haozhe Jia, Yutao Yue
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Griffith University, Data61/CSIRO
  - **link:** https://arxiv.org/pdf/2512.22881
  - **contributions:** 1. Identifies a fundamental limitation where Classifier-Free Guidance (CFG) causes iterative refinement methods to diverge by pushing the sampling path off the data manifold. 2. Proposes Guided Path Sampling (GPS), a new paradigm that replaces CFG's extrapolation with a principled, manifold-constrained interpolation to ensure path stability. 3. Devises an optimal scheduling strategy to dynamically adjust guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that standard Classifier-Free Guidance (CFG) causes instability in iterative refinement for diffusion models by pushing the sampling path off the data manifold. To solve this, the authors propose Guided Path Sampling (GPS), a method that uses manifold-constrained interpolation to keep the path stable and includes an optimal guidance schedule. Experiments show GPS improves image quality and prompt adherence, establishing path stability as key for effective refinement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Guided Path Sampling] --> B[核心问题/Problem: CFG导致采样路径偏离数据流形/CFG pushes sampling path off data manifold]
        A --> C[主要方法/Method: 提出GPS，使用流形约束插值/Propose GPS with manifold-constrained interpolation]
        A --> D[关键结果/Results: 提升图像质量与提示对齐/Improves image quality & prompt adherence]
    ```

- **[arXiv251230] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models**
  - **tags:** [mlsys], [diffusion models], [concept erasure, multimodal evaluation, inference-time robustness, cross-attention, latent perturbation]
  - **authors:** Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen
  - **institution:** National Taiwan University, Academia Sinica (Research Center for Information Technology Innovation)
  - **link:** https://arxiv.org/pdf/2512.22877
  - **contributions:** 1. Introduces M-ErasureBench, the first comprehensive multimodal benchmark for evaluating concept erasure methods across text prompts, learned embeddings, and inverted latents under white-box and black-box settings. 2. Identifies a critical vulnerability where existing erasure methods fail against non-textual input modalities, with concept reproduction rates exceeding 90%. 3. Proposes IRECE, a plug-and-play inference-time module that enhances robustness by localizing concepts via cross-attention and perturbing associated latents, reducing CRR by up to 40% while preserving image quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that existing concept erasure methods for diffusion models are vulnerable to attacks using non-textual inputs like learned embeddings or inverted latents. To address this, the authors propose a new multimodal benchmark (M-ErasureBench) and a plug-and-play defense module (IRECE) that perturbs latents during inference. Experiments show IRECE significantly reduces concept reproduction rates under challenging attacks while maintaining visual quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有方法仅针对文本提示/Existing methods only target text prompts]
        Problem --> P2[其他模态成为攻击面/Other modalities become attack surfaces]
        Method[主要方法/Method] --> M1[提出多模态基准M-ErasureBench/Propose multimodal benchmark M-ErasureBench]
        Method --> M2[提出推理时增强模块IRECE/Propose inference-time module IRECE]
        Results[关键结果/Results] --> R1[现有方法在非文本模态下失败/Existing methods fail under non-text modalities]
        Results --> R2[IRECE将CRR降低达40%/IRECE reduces CRR by up to 40%]
    ```

- **[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]
  - **authors:** Hasan Faraz Khan, Noor Fatima, Muzammil Behzad
  - **institution:** King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence
  - **link:** https://arxiv.org/pdf/2512.22878
  - **contributions:** 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --> Problem_Sub[问题细节/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]
        Method[主要方法/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --> Method_Sub[方法细节/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]
        Results[关键结果/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --> Results_Sub[结果细节/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]
    ```

- **[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery**
  - **tags:** [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]
  - **authors:** Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.22899
  - **contributions:** 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]
        C[主要方法/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]
        D[关键结果/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]
    ```

- **[arXiv251230] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation**
  - **tags:** [cv], [multimodal learning], [audio-video fusion, instruction tuning, diffusion transformer]
  - **authors:** Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei, Tat-Seng Chua
  - **institution:** National University of Singapore (NUS), Zhejiang University (ZJU), Renmin University of China (RUC), University of Rochester (UR), Hong Kong University of Science and Technology (Guangzhou) (HKUST(GZ))
  - **link:** https://arxiv.org/pdf/2512.22905
  - **code:** https://JavisVerse.github.io/JavisGPT-page
  - **contributions:** 1. Proposes JavisGPT, the first unified multimodal LLM for joint audio-video comprehension and generation. 2. Introduces a novel architecture with a SyncFusion module and synchrony-aware queries to bridge a pretrained JAV-DiT generator for coherent output. 3. Constructs a large-scale, high-quality instruction dataset (JavisInst-Omni) with over 200K GPT-4o-curated dialogues for training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces JavisGPT, a unified multimodal LLM designed for joint audio-video understanding and generation. It features a novel encoder-LLM-decoder architecture with a SyncFusion module and is trained using a three-stage pipeline on a newly created large-scale instruction dataset. Experiments show it outperforms existing models, especially in complex, temporally synchronized tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[缺乏统一的音频-视频理解与生成模型/Lack of unified audio-video comprehension and generation model]
        Method[主要方法/Method] --> M1[简洁的编码器-LLM-解码器架构/Concise encoder-LLM-decoder architecture]
        M1 --> M1_1[SyncFusion模块用于时空融合/SyncFusion module for spatio-temporal fusion]
        M1 --> M1_2[同步感知可学习查询/Synchrony-aware learnable queries]
        Method --> M2[三阶段训练流程/Three-stage training pipeline]
        M2 --> M2_1[多模态预训练/Multimodal pretraining]
        M2 --> M2_2[音频-视频微调/Audio-video fine-tuning]
        M2 --> M2_3[大规模指令调优/Large-scale instruction-tuning]
        Method --> M3[构建JavisInst-Omni指令数据集/Construct JavisInst-Omni instruction dataset]
        Results[关键结果/Results] --> R1[在音频-视频理解与生成基准测试中表现优异/Outperforms existing MLLMs on benchmarks]
        R1 --> R1_1[在复杂和时间同步场景中表现突出/Particularly strong in complex and temporally synchronized settings]
    ```

- **[arXiv251230] ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving**
  - **tags:** [cv], [autonomous driving], [vision-language-action, latent reasoning, hierarchical parallel planning, trajectory generation, cognitive reasoning]
  - **authors:** Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li
  - **institution:** Tsinghua University, CUHK MMLab, Voyager Research (Didi Chuxing)
  - **link:** https://arxiv.org/pdf/2512.22939
  - **contributions:** 1. A unified vision-language-action framework that transfers reasoning from discrete text to a continuous latent space, bridging the gap between VLM reasoning and control. 2. A Cognitive Latent Reasoner that compresses scene understanding into decision-oriented meta-action embeddings efficiently using only two VLM forward passes. 3. A Hierarchical Parallel Planner that generates multi-scale, causality-consistent trajectories in a single forward pass, enabling real-time performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ColaVLA, a framework for autonomous driving that uses cognitive latent reasoning to efficiently translate vision-language understanding into continuous control actions, coupled with a hierarchical parallel planner for trajectory generation. It addresses key challenges in VLM-based planning, such as latency and the text-control mismatch. Experiments on nuScenes show state-of-the-art performance in both open-loop and closed-loop settings with improved efficiency and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ColaVLA] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[VLM规划器挑战: 文本与连续控制不匹配, 高延迟, 非因果/VLM Planner Challenges: Text-Control Mismatch, High Latency, Non-causal]
        C --> C1[认知潜在推理器: 将场景理解压缩为元动作嵌入/Cognitive Latent Reasoner: Compresses scene to meta-action embeds]
        C --> C2[分层并行规划器: 单次前向生成多尺度轨迹/Hierarchical Parallel Planner: Single-pass multi-scale trajectory gen]
        D --> D1[SOTA性能: 在nuScenes开环和闭环测试中/SOTA Performance: On nuScenes open & closed-loop]
        D --> D2[高效与鲁棒: 有利的效率与鲁棒性/Favorable Efficiency & Robustness]
    ```

- **[arXiv251230] Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects**
  - **tags:** [cv], [object detection], [density map, dense tiny objects, remote sensing, cross-attention, feature fusion]
  - **authors:** Zhicheng Zhao, Xuanang Fan, Lingma Sun, Chenglong Li, Jin Tang
  - **institution:** Anhui University, Hefei University, 38th Research Institute of China Electronics Technology Group Corporation
  - **link:** https://arxiv.org/pdf/2512.22949
  - **contributions:** 1. Proposed a Density Generation Branch (DGB) to model object distribution and provide spatial priors for focusing on dense regions. 2. Designed a Dense Area Focusing Module (DAFM) that uses density maps to efficiently focus computational resources on dense areas for local-global feature interaction. 3. Introduced a Dual Filter Fusion Module (DFFM) that uses discrete cosine transform and density-guided cross-attention to disentangle and fuse multi-scale features, enhancing complementarity and suppressing background.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting dense, tiny objects in remote sensing imagery by proposing DRMNet, a network that uses density maps to guide adaptive feature learning. The method focuses computational resources on dense regions and fuses multi-scale features effectively. Experiments on AI-TOD and DTOD datasets show it outperforms existing methods, especially in high-density and occluded scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects<br>论文标题"] --> Problem["高分辨率遥感图像中密集微小物体检测的挑战<br>Problem: Detecting dense tiny objects in high-resolution remote sensing imagery"]
        Root --> Method["提出DRMNet，利用密度图指导自适应特征学习<br>Method: Propose DRMNet using density maps to guide adaptive feature learning"]
        Root --> Results["在AI-TOD和DTOD数据集上超越SOTA<br>Results: Outperforms SOTA on AI-TOD and DTOD datasets"]
        Problem --> P1["相互遮挡严重，像素足迹有限<br>Severe mutual occlusion, limited pixel footprints"]
        Problem --> P2["现有方法资源分配均匀，无法聚焦密集区域<br>Existing methods allocate resources uniformly, failing to focus on dense regions"]
        Method --> M1["密度生成分支 (DGB)<br>Density Generation Branch (DGB)"]
        Method --> M2["密集区域聚焦模块 (DAFM)<br>Dense Area Focusing Module (DAFM)"]
        Method --> M3["双滤波器融合模块 (DFFM)<br>Dual Filter Fusion Module (DFFM)"]
        Results --> R1["在复杂高密度和遮挡场景中表现优异<br>Excels in complex high-density and occlusion scenarios"]
    ```

- **[arXiv251230] Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection**
  - **tags:** [cv], [3D object detection], [4D radar-camera fusion, wavelet attention, geometry-guided progressive fusion]
  - **authors:** Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong
  - **institution:** Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.22972
  - **contributions:** 1. Proposes WRCFormer, a novel framework for fusing raw 4D radar tensors with camera data using multi-view representations. 2. Designs a Wavelet Attention Module within a wavelet-based Feature Pyramid Network to enhance sparse signal representation. 3. Introduces a two-stage query-based, modality-agnostic Geometry-guided Progressive Fusion mechanism for efficient multi-view feature integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of sparse data and high computational cost in 4D radar-camera fusion for 3D object detection by proposing WRCFormer. The method fuses raw radar tensors and camera inputs using a wavelet-based feature enhancement module and a geometry-guided progressive fusion mechanism. Experiments on the K-Radar benchmark show state-of-the-art performance, particularly highlighting robustness in adverse weather conditions like sleet.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Sparsity & computational cost of 4D radar"] --> P1["信息损失/Information loss in point-cloud processing"]
        Problem --> P2["计算成本高/High computational cost of raw data"]
        Method["主要方法/Method<br>WRCFormer framework"] --> M1["小波注意力模块/Wavelet Attention Module"]
        Method --> M2["几何引导渐进融合/Geometry-guided Progressive Fusion"]
        Results["关键结果/Results<br>SOTA on K-Radar"] --> R1["整体性能提升 +2.4%/Overall +2.4%"]
        Results --> R2["恶劣天气鲁棒性/Robust in sleet +1.6%"]
    ```

- **[arXiv251230] RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance**
  - **tags:** [cv], [camouflaged object detection], [camouflaged image generation, out-painting, layout control, textual-visual guidance, distribution divergence metric]
  - **authors:** Chunyuan Chen, Yunuo Cai, Shujuan Li, Weiyun Liang, Bin Wang, Jing Xu
  - **institution:** Nankai University, Fudan University
  - **link:** https://arxiv.org/pdf/2512.22974
  - **contributions:** 1. Proposes a unified out-painting framework with explicit layout controls to improve semantic coherence between foreground objects and generated backgrounds. 2. Constructs a multi-modal textual-visual condition combining fine-grained textual descriptions with texture-oriented background retrieval to enhance visual fidelity. 3. Introduces a background-foreground distribution divergence metric for the quantitative assessment of camouflage quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap between generated and real camouflaged images by proposing RealCamo, a framework that uses layout controls and multi-modal textual-visual guidance for realistic out-painting. The method improves both visual similarity and semantic consistency in generated backgrounds. Experiments demonstrate its effectiveness in producing high-quality camouflaged images.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RealCamo: Boosting Real Camouflage Synthesis<br>RealCamo: 提升真实伪装合成] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing CIG methods have a gap to real imagery.<br>现有CIG方法与真实图像存在差距。]
        C[主要方法/Method<br>Proposes RealCamo with layout controls & textual-visual guidance.<br>提出带布局控制和文本-视觉引导的RealCamo。]
        D[关键结果/Results<br>Improves camouflage quality & realism.<br>提升了伪装质量和真实感。]
    ```

- **[arXiv251230] CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision**
  - **tags:** [cv], [object detection], [CLIP, contrastive learning, joint training, learnable text embeddings, InfoNCE loss]
  - **authors:** Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki
  - **institution:** Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.22969
  - **contributions:** 1. Proposes CLIP-Joint-Detect, a detector-agnostic framework for end-to-end joint training of object detectors with CLIP-style contrastive vision-language supervision. 2. Introduces a lightweight parallel head that aligns visual features with learnable class-specific text embeddings using a combination of InfoNCE contrastive loss and an auxiliary cross-entropy term. 3. Demonstrates consistent performance improvements on standard benchmarks (Pascal VOC, MS COCO) with both two-stage (Faster R-CNN) and one-stage (YOLO) architectures while preserving real-time inference speed.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the vulnerability of conventional object detectors to class imbalance and label noise by proposing CLIP-Joint-Detect. This framework integrates contrastive vision-language supervision into object detectors via a lightweight parallel head and joint training, aligning visual features with learnable text embeddings. The method improves detection performance across different architectures and datasets without sacrificing inference speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("CLIP-Joint-Detect") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("传统检测器依赖交叉熵分类/Traditional detectors rely on cross-entropy classification")
        Problem --> P2("易受类别不平衡和标签噪声影响/Vulnerable to class imbalance & label noise")
        Method --> M1("轻量级并行头/Lightweight parallel head")
        Method --> M2("将特征投影到CLIP空间/Projects features to CLIP space")
        Method --> M3("与可学习的文本嵌入对齐/Aligns with learnable text embeddings")
        Method --> M4("使用InfoNCE和交叉熵损失/Uses InfoNCE & cross-entropy loss")
        Method --> M5("端到端联合训练/End-to-end joint training")
        Results --> R1("在Pascal VOC和MS COCO上性能提升/Performance gains on Pascal VOC & MS COCO")
        Results --> R2("适用于两阶段和一阶段检测器/Works for two-stage & one-stage detectors")
        Results --> R3("保持实时推理速度/Preserves real-time inference speed")
    ```

- **[arXiv251230] Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [symmetric optimal transport, spatial-aware guidance, multimodal alignment]
  - **authors:** Linglin Liao, Qichuan Geng, Yu Liu
  - **institution:** Capital Normal University
  - **link:** https://arxiv.org/pdf/2512.22981
  - **contributions:** 1. Proposes a Spatial-aware Symmetric Alignment (SSA) framework for handling hybrid medical texts containing locational, descriptive, and diagnostic information. 2. Introduces a Dual-granularity Symmetric Optimal Transport (DSOT) alignment algorithm to establish bi-directional fine-grained multimodal correspondences. 3. Devises a composite directional guidance strategy that explicitly introduces spatial constraints by constructing region-level guidance masks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing text-guided medical image segmentation methods, which struggle with hybrid texts and spatial constraints. It proposes the Spatial-aware Symmetric Alignment (SSA) framework, which uses a symmetric optimal transport mechanism and spatial guidance strategy to improve alignment. Experiments show SSA achieves state-of-the-art performance, especially for lesions with spatial relational constraints.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题 / Paper Title: Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation] --> B(核心问题 / Problem)
        A --> C(主要方法 / Method)
        A --> D(关键结果 / Results)
        B --> B1[现有方法瓶颈 / Bottlenecks of Existing Methods]
        B1 --> B2[难以处理混合文本 / Struggles with Hybrid Texts]
        B1 --> B3[忽略空间约束 / Ignores Spatial Constraints]
        C --> C1[SSA框架 / SSA Framework]
        C1 --> C2[对称最优传输对齐 / Symmetric Optimal Transport Alignment]
        C1 --> C3[复合方向性引导策略 / Composite Directional Guidance Strategy]
        D --> D1[SOTA性能 / SOTA Performance]
        D --> D2[准确分割空间约束病灶 / Accurate Segmentation of Spatially Constrained Lesions]
    ```

- **[arXiv251230] PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects**
  - **tags:** [cv], [pose estimation], [6DoF pose estimation, multi-modal fusion, event camera, high-speed tracking, unseen objects]
  - **authors:** Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng
  - **institution:** Renmin University of China
  - **link:** https://arxiv.org/pdf/2512.22979
  - **contributions:** 1. Proposes PoseStreamer, a multi-modal framework integrating an Adaptive Pose Memory Queue, Object-centric 2D Tracker, and Ray Pose Filter for robust 6DoF pose estimation in high-speed scenarios. 2. Introduces a novel multi-modal dataset, MoCapCube6D, for benchmarking performance under rapid motion. 3. Demonstrates superior accuracy and generalizability as a template-free framework for unseen moving objects.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of 6DoF pose estimation for unseen objects in high-speed, low-light scenarios where RGB cameras suffer from motion blur. It proposes PoseStreamer, a multi-modal framework that fuses RGB and event camera data with three novel components for temporal consistency, 2D tracking priors, and geometric refinement. Experiments show the framework achieves superior accuracy in high-speed scenarios and strong generalizability for unseen objects.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[PoseStreamer: 多模态6DoF姿态估计框架] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[RGB相机运动模糊/Motion Blur in RGB]
        Problem --> P2[高速场景性能不佳/Poor High-speed Performance]
        Method --> M1[自适应姿态记忆队列/Adaptive Pose Memory Queue]
        Method --> M2[以对象为中心的2D跟踪器/Object-centric 2D Tracker]
        Method --> M3[射线姿态滤波器/Ray Pose Filter]
        Method --> M4[多模态数据集/Multi-modal Dataset (MoCapCube6D)]
        Results --> R1[高速场景精度更高/Superior High-speed Accuracy]
        Results --> R2[对未见物体泛化性强/Strong Generalizability for Unseen Objects]
    ```

- **[arXiv251230] YOLO-IOD: Towards Real Time Incremental Object Detection**
  - **tags:** [cv], [object detection], [incremental learning, catastrophic forgetting, knowledge distillation, YOLO, pseudo-labeling]
  - **authors:** Shizhou Zhang, Xueqiang Lv, Yinghui Xing, Qirui Wu, Di Xu, Chen Zhao, Yanning Zhang
  - **institution:** Northwestern Polytechnical University, Huawei
  - **link:** https://arxiv.org/pdf/2512.22973
  - **contributions:** 1) Identifies three key knowledge conflicts (foreground-background confusion, parameter interference, misaligned knowledge distillation) causing forgetting in YOLO-based incremental detectors. 2) Proposes the YOLO-IOD framework with three novel components (CPR, IKS, CAKD) to address these conflicts. 3) Introduces a new benchmark, LoCo COCO, designed to prevent data leakage for more realistic incremental learning evaluation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of catastrophic forgetting in real-time incremental object detection (IOD) for YOLO detectors. It proposes YOLO-IOD, a framework built on YOLO-World that uses pseudo-label refinement, kernel selection, and asymmetric knowledge distillation to mitigate forgetting. The method shows superior performance with minimal forgetting on both conventional and a newly introduced, more realistic benchmark.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[YOLO-IOD: Towards Real Time Incremental Object Detection] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有IOD方法不适用于实时YOLO框架 / Existing IOD methods do not accommodate real-time YOLO]
        B --> B2[YOLO增量检测存在知识冲突导致灾难性遗忘 / Knowledge conflicts cause catastrophic forgetting in YOLO-based IOD]
        C --> C1[基于YOLO-World的阶段式参数高效微调 / Stage-wise parameter-efficient fine-tuning on YOLO-World]
        C --> C2[三个核心组件 / Three Core Components]
        C2 --> C2_1[CPR: 冲突感知伪标签细化 / Conflict-Aware Pseudo-Label Refinement]
        C2 --> C2_2[IKS: 基于重要性的核选择 / Importance-based Kernel Selection]
        C2 --> C2_3[CAKD: 跨阶段非对称知识蒸馏 / Cross-Stage Asymmetric Knowledge Distillation]
        C --> C3[引入新基准LoCo COCO / Introduce new benchmark LoCo COCO]
        D --> D1[在传统和LoCo COCO基准上性能优越 / Superior performance on conventional and LoCo COCO benchmarks]
        D --> D2[实现最小的遗忘 / Achieves minimal forgetting]
    ```

- **[arXiv251230] Reverse Personalization**
  - **tags:** [cv], [face anonymization], [diffusion inversion, identity-guided conditioning, attribute-controllable anonymization]
  - **authors:** Han-Wei Kung, Tuomas Varanka, Nicu Sebe
  - **institution:** University of Trento, University of Oulu
  - **link:** https://arxiv.org/pdf/2512.22984
  - **code:** https://github.com/hanweikung/reverse-personalization
  - **contributions:** 1. Introduces a reverse personalization framework for face anonymization using conditional diffusion inversion, eliminating the need for text prompts or subject-specific fine-tuning. 2. Incorporates an identity-guided conditioning branch to generalize anonymization to subjects not present in the model's pre-training data. 3. Enables attribute-controllable anonymization, allowing users to preserve or modify specific facial attributes while removing identity, a capability lacking in prior methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of removing identity-specific features from facial images while preserving other attributes and scene context. It proposes a reverse personalization method based on conditional diffusion inversion and an identity-guided conditioning branch, which allows for direct image manipulation without text prompts. The method achieves state-of-the-art performance in balancing identity removal, attribute preservation, and image quality, while also offering user control over retained attributes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Reverse Personalization] --> Problem[核心问题/Problem: Removing identity from faces while preserving attributes]
        Root --> Method[主要方法/Method: Conditional diffusion inversion with identity-guided conditioning]
        Root --> Results[关键结果/Results: State-of-the-art balance of identity removal, attribute preservation, and quality]
    ```

- **[arXiv251230] A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection**
  - **tags:** [cv], [object detection], [UAV, YOLOv8, ResNet50, VGG16, Edge Inference]
  - **authors:** Soham Dutta, Soham Banerjee, Sneha Mahata, Anindya Sen, Sayantani Datta
  - **institution:** Heritage Institute of Technology, Kolkata
  - **link:** https://arxiv.org/pdf/2512.22990
  - **contributions:** 1. A unified UAV-based pipeline integrating leaf disease detection, apple freshness classification, and fruit localization. 2. A cost-effective hardware architecture using ESP32-CAM and Raspberry Pi for offline, on-site inference. 3. High-performance results using only RGB sensors as a low-cost alternative to multispectral solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a low-cost, integrated UAV pipeline using RGB cameras and deep learning models (ResNet50, VGG16, YOLOv8) to perform apple leaf disease diagnosis, fruit freshness assessment, and detection in orchards. The system runs offline on edge devices like Raspberry Pi and ESP32-CAM. Experiments show high accuracy, providing a practical and affordable alternative to expensive multispectral systems for precision agriculture.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["A Low-Cost UAV Deep Learning Pipeline<br>低成本无人机深度学习流程"] --> B["核心问题/Problem<br>Orchard tasks isolated & costly<br>果园任务孤立且成本高"]
        A --> C["主要方法/Method<br>Unified RGB-only UAV pipeline<br>统一的仅RGB无人机流程"]
        C --> D["Models: ResNet50, VGG16, YOLOv8<br>模型"]
        C --> E["Hardware: ESP32-CAM, Raspberry Pi<br>硬件"]
        A --> F["关键结果/Results<br>High accuracy & offline inference<br>高精度与离线推理"]
        F --> G["Disease: 98.9%<br>疾病检测"]
        F --> H["Freshness: 97.4%<br>新鲜度评估"]
        F --> I["Detection F1: 0.857<br>果实检测"]
    ```

- **[arXiv251230] With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs**
  - **tags:** [cv], [object recognition], [Geo-Semantic Contextual Graph, graph-based classifier, contextual reasoning, panoptic segmentation, metric depth]
  - **authors:** Ciprian Constantinescu, Marius Leordeanu
  - **institution:** National University of Science and Technology POLITEHNICA Bucharest
  - **link:** https://arxiv.org/pdf/2512.23024
  - **contributions:** 1. A novel method for constructing a Geo-Semantic Contextual Graph (GSCG) from a monocular image by fusing metric depth with panoptic and material segmentation. 2. A specialized graph-based classifier that aggregates features from a target object, its neighbors, and the global scene context for classification. 3. Demonstrating that the explicit, structured, and interpretable context of the GSCG significantly outperforms context-agnostic and strong baseline models on object recognition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel framework for contextual object classification by first constructing a Geo-Semantic Contextual Graph (GSCG) from a single image to represent objects and their relationships, and then using a graph-based classifier to leverage this context. The method achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic models and strong baselines like fine-tuned ResNets and a multimodal LLM, highlighting the power of structured, interpretable scene context.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 传统物体识别系统忽略关键上下文信息/Traditional object recognition systems ignore vital contextual information.]
        C[主要方法/Method: 构建地理语义上下文图并使用图分类器/Build Geo-Semantic Contextual Graph and use a graph-based classifier.]
        D[关键结果/Results: 上下文感知模型准确率73.4%，显著超越基线/Context-aware model achieves 73.4% accuracy, significantly surpassing baselines.]
    ```

- **[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding**
  - **tags:** [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]
  - **authors:** Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang
  - **institution:** Nanjing University, China Mobile Zijin Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.23020
  - **contributions:** 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖预定义对象表，无法处理未定义目标/Existing methods rely on pre-defined OLT, limiting open-world application]
        C --> C1[提出OpenGround框架与主动认知推理模块/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]
        C1 --> C2[通过认知任务链和动态更新的OLT增强VLM认知/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]
        D --> D1[Nr3D上表现有竞争力，ScanRefer上达到SOTA/Competitive on Nr3D, SOTA on ScanRefer]
        D --> D2[在OpenTarget数据集上提升17.6%/17.6% improvement on OpenTarget dataset]
    ```

- **[arXiv251230] Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI**
  - **tags:** [cv], [medical image classification], [MobResTaNet, Explainable AI (XAI), lightweight model, ultrasound diagnosis, web-mobile platform]
  - **authors:** Fuyad Hasan Bhoyan, Prashanta Sarker, Parsia Noor Ethila, Md. Emon Hossain, Md Kaviul Hossain, Md Humaion Kabir Mehedi
  - **institution:** University of Liberal Arts Bangladesh, BRAC University
  - **link:** https://arxiv.org/pdf/2512.23033
  - **contributions:** 1. Proposed a hybrid deep learning model (MobResTaNet) for classifying ten gallbladder conditions from ultrasound images with high accuracy (99.85%) and low parameter count (2.24M). 2. Developed an interpretable diagnostic system with real-time Explainable AI (XAI) visualizations to support transparent clinical decision-making. 3. Deployed the system as an efficient and accessible web-mobile software platform using technologies like HTML, CSS, JavaScript, Bootstrap, and Flutter for point-of-care use.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of interpreting gallbladder ultrasound images by developing an AI-driven diagnostic software. The core method is a lightweight hybrid deep learning model called MobResTaNet, which classifies diseases and provides real-time, interpretable predictions via XAI. The main conclusion is that the system achieves high accuracy with a small model size and is successfully deployed as accessible web and mobile applications for clinical support.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Interpretable Gallbladder Ultrasound Diagnosis] --> B[核心问题/Problem: Challenging ultrasound interpretation for gallbladder diseases]
        A --> C[主要方法/Method: AI software with lightweight MobResTaNet model & real-time XAI]
        A --> D[关键结果/Results: 99.85% accuracy, 2.24M parameters, deployed web-mobile platform]
    ```

- **[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project**
  - **tags:** [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]
  - **authors:** Thomson Tong, Diba Darooneh
  - **institution:** None
  - **link:** https://arxiv.org/pdf/2512.23028
  - **code:** BodyLanguageDetection repository [1]
  - **contributions:** 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp
  - **Simple LLM Summary:** This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Architecture-Led Hybrid Report on Body Language Detection Project] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[如何基于VLM架构构建可靠的应用系统/How to build reliable application systems based on VLM architecture]
        C --> C1[分析两种VLM架构并映射到视频处理流程/Analyze two VLM architectures and map to a video processing pipeline]
        C --> C2[系统采样视频帧，提示VLM生成结构化输出/System samples video frames, prompts VLM for structured output]
        C --> C3[使用预定义模式验证输出结构/Validate output structure with predefined schema]
        D --> D1[结构化输出可能语法正确但语义错误/Structured outputs can be syntactically valid but semantically incorrect]
        D --> D2[模式验证是结构性的，非几何正确性/Schema validation is structural, not geometric]
        D --> D3[理解架构对设计稳健接口和评估至关重要/Understanding architecture is critical for robust interface design and evaluation]
    ```

- **[arXiv251230] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion**
  - **tags:** [cv], [semantic segmentation], [semi-supervised learning, pseudo-label drift, vision-language model, self-supervised model, dual-student architecture]
  - **authors:** Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi
  - **institution:** Qinghai University, Beijing Jiaotong University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.23035
  - **code:** https://xavierjiezou.github.io/Co2S/
  - **contributions:** 1. Proposed Co2S, a stable semi-supervised RS segmentation framework that fuses priors from vision-language and self-supervised models. 2. Introduced a heterogeneous dual-student architecture with ViT models initialized by CLIP and DINOv3 to mitigate error accumulation. 3. Developed an explicit-implicit semantic co-guidance mechanism and a global-local feature collaborative fusion strategy to enhance semantic consistency and segmentation precision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of pseudo-label drift in semi-supervised remote sensing image segmentation. It proposes the Co2S framework, which uses a dual-student architecture combining CLIP and DINOv3 models, along with co-guidance and co-fusion mechanisms, to improve stability and accuracy. Experiments on six datasets show the method achieves leading performance across various scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("伪标签漂移/Pseudo-label Drift")
        Problem --> P2("标注负担/Annotation Burden")
        Method --> M1("异构双学生架构/Heterogeneous Dual-Student")
        Method --> M2("显式-隐式语义协同引导/Explicit-Implicit Co-Guidance")
        Method --> M3("全局-局部特征融合/Global-Local Feature Fusion")
        M1 --> M1_1("CLIP ViT")
        M1 --> M1_2("DINOv3 ViT")
        Results --> R1("六个数据集领先性能/Leading Performance on Six Datasets")
        Results --> R2("多种划分协议鲁棒/Robust Across Partition Protocols")
    ```

- **[arXiv251230] 3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds**
  - **tags:** [cv], [3D scene understanding], [self-supervised learning, point clouds, video reconstruction, geometric regularization, indoor segmentation]
  - **authors:** Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano
  - **institution:** AIST, University of Technology Nuremberg, INRIA (Valeo.ai)
  - **link:** https://arxiv.org/pdf/2512.23042
  - **contributions:** 1. Proposes LAM3C, a self-supervised framework for learning 3D representations from video-generated point clouds without real 3D scans. 2. Introduces RoomTours, a large-scale dataset of 49,219 video-generated point cloud scenes from web videos. 3. Designs a noise-regularized loss to enforce local geometric smoothness and stabilize feature learning on noisy point clouds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high cost of collecting 3D scans by proposing a method to learn 3D representations from unlabeled videos. The core method, LAM3C, learns from video-generated point clouds using a novel noise-regularized loss and a new dataset called RoomTours. The results show that this approach outperforms previous self-supervised methods on indoor segmentation tasks, demonstrating that videos are a viable and abundant source for 3D pre-training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("大规模3D扫描昂贵且耗时/Large-scale 3D scan collection is expensive and labor-intensive")
        Method --> M1("提出LAM3C框架/Propose LAM3C framework")
        Method --> M2("引入RoomTours数据集/Introduce RoomTours dataset")
        Method --> M3("使用噪声正则化损失/Use noise-regularized loss")
        Results --> R1("性能优于现有自监督方法/Outperforms previous self-supervised methods")
        Results --> R2("证明视频是丰富的3D数据源/Demonstrates videos as an abundant 3D data source")
    ```

- **[arXiv251230] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web**
  - **tags:** [cv], [video understanding], [agentic video research, open-web benchmark, temporal visual evidence, video browsing, multimodal reasoning]
  - **authors:** Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao
  - **institution:** Singapore Management University, University of Trento, Beijing Academy of Artificial Intelligence, Beijing University of Posts and Telecommunications, Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.23044
  - **code:** https://liang-zhengyang.github.io/video-browsecomp/
  - **contributions:** 1. Introduces Video-BrowseComp, the first benchmark for open-web agentic video reasoning, comprising 210 questions that require active navigation of video timelines. 2. Enforces a mandatory dependency on temporal visual evidence, ensuring answers cannot be derived from text search alone, thus evaluating true video grounding. 3. Reveals a critical performance bottleneck in state-of-the-art models, showing they rely heavily on textual proxies and fail in metadata-sparse, dynamic video domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a gap in evaluating proactive, agentic video research on the open web. To address this, it introduces the Video-BrowseComp benchmark, which requires models to actively navigate and reason over video timelines to answer questions. The evaluation shows that even advanced models perform poorly, highlighting a critical reliance on text and a failure in visually-grounded video understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Video-BrowseComp: Benchmarking Agentic Video Research on Open Web] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有基准测试仅评估被动视频感知/Existing benchmarks only evaluate passive video perception]
        B --> B2[缺乏对主动、开放式网络视频研究的评估/Lack of evaluation for agentic, open-web video research]
        C --> C1[提出Video-BrowseComp基准/Propose Video-BrowseComp benchmark]
        C --> C2[强制依赖时序视觉证据/Enforce mandatory dependency on temporal visual evidence]
        C --> C3[包含210个挑战性问题/Comprises 210 challenging questions]
        D --> D1[最先进模型准确率低/State-of-the-art models achieve low accuracy (e.g., 15.24%)]
        D --> D2[模型过度依赖文本代理/Models heavily rely on textual proxies]
        D --> D3[在元数据稀疏的动态环境中失败/Models fail in metadata-sparse, dynamic environments]
    ```

- **[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models**
  - **tags:** [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]
  - **authors:** Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu
  - **institution:** Northeastern University
  - **link:** https://arxiv.org/pdf/2512.23073
  - **code:** https://github.com/Ming-K9/MFT-VLM
  - **contributions:** 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model's existing knowledge, not just from updating weights.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp
  - **Simple LLM Summary:** This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models<br>重新思考微调：解锁视觉语言模型的隐藏能力"]
        Root --> Problem["Problem: Traditional fine-tuning overlooks underutilized structures in pre-trained VLMs.<br>核心问题：传统微调忽略了预训练VLM中未充分利用的结构。"]
        Root --> Method["Method: Apply Mask Fine-Tuning (MFT) to VLMs for structural reparameterization.<br>主要方法：将掩码微调（MFT）应用于VLM进行结构重参数化。"]
        Root --> Results["Results: MFT surpasses LoRA and full fine-tuning without altering the backbone.<br>关键结果：MFT超越了LoRA和全参数微调，且不改变主干网络。"]
    ```

- **[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification**
  - **tags:** [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]
  - **authors:** Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang
  - **institution:** Missouri State University
  - **link:** https://arxiv.org/pdf/2512.23089
  - **contributions:** 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedSAM-based lung masking for multi-label chest X-ray classification] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Automated CXR interpretation is challenging<br>自动CXR解读具有挑战性]
        C --> C1[Fine-tune MedSAM for lung segmentation<br>微调MedSAM进行肺部分割]
        C --> C2[Use masks to guide multi-label classification<br>使用掩码指导多标签分类]
        D --> D1[Masking effect is task/architecture dependent<br>掩码效果依赖于任务和架构]
        D --> D2[Trade-off: abnormality vs. normal screening<br>权衡：异常检测与正常筛查]
        D --> D3[Masking is a controllable spatial prior<br>掩码是一种可控的空间先验]
    ```

- **[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion**
  - **tags:** [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]
  - **authors:** Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu
  - **institution:** Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.23130
  - **contributions:** 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: 成像-病理MRI合成"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>生成模型中的特征纠缠导致解剖结构损坏"] --> P1["现有范式/Existing Paradigms<br>Global pixel domain or binary masks<br>全局像素域或二进制掩码"]
        Method["主要方法/Method<br>Disentangled Deviation Diffusion<br>解耦偏差扩散"] --> M1["分解任务/Decompose Task<br>1. Deterministic anatomical reconstruction<br>确定性解剖重建<br>2. Stochastic deviation modeling<br>随机偏差建模"]
        Method --> M2["核心模型/Core Model<br>Deviation-Space Diffusion Model<br>偏差空间扩散模型<br>Learns pathological residuals<br>学习病理残差"]
        Method --> M3["融合与稳定/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>接缝感知融合与推理时稳定模块"]
        Results["关键结果/Results<br>Outperforms baselines<br>超越基线模型"] --> R1["评估/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>肿瘤基准上的定量与定性评估"]
        Results --> R2["优势/Advantages<br>Higher perceptual realism & anatomical fidelity<br>更高的感知真实性与解剖保真度"]
    ```

- **[arXiv251230] Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations**
  - **tags:** [cv], [image registration], [deformable registration, domain shift, local features, UniReg, cross-modal]
  - **authors:** Mingzhen Shao, Sarang Joshi
  - **institution:** University of Utah
  - **link:** https://arxiv.org/pdf/2512.23142
  - **contributions:** 1. Demonstrates that domain-shift immunity is an inherent property of deep deformable registration models, stemming from their reliance on local feature representations. 2. Introduces UniReg, a universal registration framework that decouples feature extraction from deformation estimation to validate this mechanism. 3. Reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the robustness of deep learning models for deformable image registration under domain shift. It proposes UniReg, a framework using fixed feature extractors and a UNet, to show that reliance on local features provides inherent domain-shift immunity. The findings indicate that local feature consistency is key to robustness, and early CNN layers are the source of failure in conventional models.
  - **Mindmap:**

    ```mermaid
    graph TB
    A["Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations"] --> B["核心问题/Problem: Are deep deformable registration models robust to domain shift?"]
    A --> C["主要方法/Method: Propose UniReg, a framework decoupling feature extraction and deformation estimation."]
    A --> D["关键结果/Results: Models have inherent domain-shift immunity due to local features; UniReg shows robust cross-domain performance."]
    ```

- **[arXiv251230] GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection**
  - **tags:** [cv], [3D object detection], [semi-supervised learning, teacher-student framework, geometric relation supervision, voxel-wise augmentation, pseudo-labeling]
  - **authors:** Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang
  - **institution:** Fudan University, Shanghai Innovation Institute, Tongji University, The University of Hong Kong, Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23147
  - **code:** https://github.com/SII-Whaleice/GeoTeacher
  - **contributions:** 1. A keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student model. 2. A voxel-wise data augmentation strategy with a distance-decay mechanism to increase the diversity of object geometries while preserving distant object integrity. 3. A flexible framework that can be combined with different semi-supervised 3D object detection methods to further improve their performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes GeoTeacher, a geometry-guided semi-supervised 3D object detection method. It introduces a geometric relation supervision module and a novel voxel-wise augmentation strategy to enhance the student model's ability to capture object geometries when labeled data is limited. Experiments on ONCE and Waymo datasets show the method achieves state-of-the-art performance and demonstrates good generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法忽视几何信息/Existing methods overlook geometric information]
        C --> C1[关键点几何关系监督/Keypoint-based geometric relation supervision]
        C --> C2[体素数据增强与距离衰减/Voxel-wise augmentation with distance-decay]
        D --> D1[在ONCE和Waymo上SOTA/SOTA on ONCE and Waymo]
        D --> D2[方法具有良好的泛化性/Method shows good generalization]
    ```

- **[arXiv251230] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling**
  - **tags:** [ai], [robotic imitation learning], [world model, vision-language-action (VLA) model, inverse dynamics model, surgical robotics, synthetic data generation]
  - **authors:** Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu
  - **institution:** NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital
  - **link:** https://arxiv.org/pdf/2512.23162
  - **contributions:** 1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling] --> B[核心问题/Problem: Data scarcity for paired video-action data in surgical robotics]
        A --> C[主要方法/Method: Build SurgWorld world model to generate synthetic videos; Use inverse dynamics to infer pseudo-kinematics]
        A --> D[关键结果/Results: Surgical VLA policy trained with augmented data outperforms policy trained only on real data]
    ```

- **[arXiv251230] REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation**
  - **tags:** [cv], [text-image alignment evaluation], [reinforcement learning, multimodal large language models, visual reasoning, element-level grounding, group relative policy optimization]
  - **authors:** Fulin Shi, Wenyi Xiao, Bin Chen, Liang Din, Leilei Gan
  - **institution:** Zhejiang University, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.23169
  - **contributions:** 1. Proposes REVEALER, a unified framework for element-level text-image alignment evaluation based on a structured "grounding-reasoning-conclusion" visual reasoning paradigm. 2. Introduces a reinforcement learning optimization method using Group Relative Policy Optimization (GRPO) with a composite reward function to enhance judgment quality. 3. Demonstrates state-of-the-art performance and superior inference efficiency across multiple benchmarks compared to existing methods and proprietary models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of coarse-grained and non-interpretable evaluation of text-to-image model outputs. It proposes REVEALER, a framework that uses reinforcement-guided visual reasoning with MLLMs to perform fine-grained, element-level alignment assessment. Experiments show REVEALER achieves state-of-the-art performance and is more efficient than existing iterative reasoning methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有评估方法缺乏细粒度可解释性/Existing evaluation lacks fine-grained interpretability]
        C --> C1[基于强化学习的结构化视觉推理/Reinforcement-guided structured visual reasoning]
        C --> C2["范式: 定位-推理-结论/Paradigm: grounding-reasoning-conclusion"]
        D --> D1[在多个基准测试中达到SOTA/Achieves SOTA on multiple benchmarks]
        D --> D2[优于专有模型和基线/Superior to proprietary models & baselines]
        D --> D3[更高的推理效率/Higher inference efficiency]
    ```

- **[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR**
  - **tags:** [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]
  - **authors:** Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry
  - **institution:** Milwaukee School of Engineering, Emory University
  - **link:** https://arxiv.org/pdf/2512.23177
  - **contributions:** 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Project VIPR: Machine Learning-Assisted Vocal Cord Ultrasound Examination] --> B[核心问题/Problem: VCUS accuracy is operator-dependent]
    A --> C[主要方法/Method: Use ML models for vocal cord segmentation and VCP classification]
    A --> D[关键结果/Results: Segmentation accuracy 96%, VIPRnet classification accuracy 99%]
    ```

- **[arXiv251230] GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection**
  - **tags:** [cv], [3D object detection], [Gaussian Splatting, Voxel Representation, Multi-View Learning, Synergistic Learning, Geometry Extraction]
  - **authors:** Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau
  - **institution:** The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.23176
  - **contributions:** 1. Proposes a novel synergistic framework (GVSynergy-Det) that integrates continuous Gaussian and discrete voxel representations for complementary geometric feature learning in 3D object detection. 2. Adapts generalizable Gaussian Splatting to extract geometric features for detection and develops a cross-representation enhancement mechanism to enrich voxel features with Gaussian-derived details. 3. Achieves state-of-the-art performance on major indoor benchmarks (ScanNetV2, ARKitScenes) without requiring dense 3D supervision (e.g., depth or point clouds).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GVSynergy-Det, a novel image-based 3D object detection framework that synergistically combines Gaussian and voxel representations to capture complementary geometric information without dense 3D supervision. The method integrates features from both representations through a learnable mechanism, enabling more accurate object localization. Experiments show it achieves state-of-the-art results on indoor benchmarks, outperforming existing methods while maintaining a compact model size.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[基于图像的3D检测挑战/Image-based 3D Detection Challenges]
        P1 --> P2[高精度需密集3D监督/High accuracy requires dense 3D supervision]
        P1 --> P3[无监督难以提取准确几何/Unsupervised struggles with geometry extraction]
        Method[主要方法/Method] --> M1[双表征协同学习/Dual-Representation Synergistic Learning]
        M1 --> M2[高斯溅射提取几何特征/Gaussian Splatting for geometric features]
        M1 --> M3[体素提供空间上下文/Voxels provide spatial context]
        M1 --> M4[跨表征增强机制/Cross-representation enhancement mechanism]
        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art performance]
        R1 --> R2[ScanNetV2 & ARKitScenes数据集/ScanNetV2 & ARKitScenes datasets]
        Results --> R3[无需密集3D监督/No dense 3D supervision required]
        Results --> R4[紧凑模型尺寸/Compact model size]
    ```

- **[arXiv251230] GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation**
  - **tags:** [cv], [autonomous driving, 3D scene understanding], [3D Gaussian Splatting, Driving World Model, Multi-Modal Generation, Vision-Language Model, Scene Representation]
  - **authors:** Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang
  - **institution:** Shanghai Jiao Tong University, Tsinghua University, MEGVII Technology, Mach Drive
  - **link:** https://arxiv.org/pdf/2512.23180
  - **code:** this https URL
  - **contributions:** 1. Proposes a unified Driving World Model framework based on 3D Gaussian scene representation for joint 3D scene understanding and multi-modal generation. 2. Introduces early modality alignment by embedding linguistic features into 3D Gaussian primitives and a task-aware language-guided sampling strategy to inject compact 3D tokens into an LLM. 3. Designs a dual-condition multi-modal generation model that uses high-level language conditions and low-level image conditions to guide the generation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes GaussianDWM, a novel Driving World Model framework that uses 3D Gaussians as a scene representation to unify 3D scene understanding and multi-modal generation for autonomous driving. It aligns text with the 3D scene via Gaussian primitives and uses a dual-condition model for generation. The method achieves state-of-the-art performance on nuScenes and NuInteract datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GaussianDWM: 3D Gaussian Driving World Model] --> B1(核心问题/Problem: Existing DWMs lack 3D understanding and precise text-scene alignment)
        A --> B2(主要方法/Method: Unified framework using 3D Gaussians for representation, with early modality alignment and dual-condition generation)
        A --> B3(关键结果/Results: Achieves SOTA performance on driving datasets)
    ```

- **[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis**
  - **tags:** [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]
  - **authors:** Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik
  - **institution:** BRAC University, Charles Sturt University, University of Technology Sydney
  - **link:** https://arxiv.org/pdf/2512.23196
  - **contributions:** 1. Proposes "ForCM", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ForCM: Forest Cover Mapping] --> B[核心问题/Problem: Accurate forest cover mapping for environmental monitoring]
        A --> C[主要方法/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]
        A --> D[关键结果/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]
    ```

- **[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection**
  - **tags:** [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]
  - **authors:** Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1
  - **link:** https://arxiv.org/pdf/2512.23208
  - **contributions:** 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏军事目标数据集/Lack of military target datasets]
        B --> B2[SAR数据成本高/High cost of SAR data]
        B --> B3[跨域适应挑战/Cross-domain adaptation challenge]
        C --> C1[使用Unreal Engine生成合成RGB数据/Generate synthetic RGB data using Unreal Engine]
        C --> C2[合成到真实域适应实验/Synthetic-to-real domain adaptation experiments]
        C --> C3[基准测试SOTA方法/Benchmark SOTA DA methods]
        D --> D1[最小监督方法表现最佳/Minimal supervision methods perform best]
        D --> D2[识别当前挑战/Identify current challenges]
    ```

- **[arXiv251230] Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks**
  - **tags:** [cv], [diffusion models], [diffusion timestep selection, few-shot learning, dense prediction, Taskonomy, parameter-efficient fine-tuning]
  - **authors:** Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon
  - **institution:** KAIST (Korea Advanced Institute of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.23210
  - **contributions:** 1. Proposes a Task-aware Timestep Selection (TTS) module to adaptively select ideal diffusion timesteps for a given task based on losses and similarity scores. 2. Introduces a Timestep Feature Consolidation (TFC) module to consolidate the selected timestep features to improve dense prediction performance. 3. Presents a framework that, with a parameter-efficient fine-tuning adapter, achieves superior few-shot dense prediction performance on unseen tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the sub-optimal performance of diffusion models in few-shot dense prediction tasks due to heuristic timestep feature selection. It proposes a framework with learnable modules (TTS and TFC) to adaptively select and consolidate diffusion timestep features, which is validated on the Taskonomy dataset, showing superior performance in universal few-shot learning scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks<br>面向通用少样本密集任务的面向任务可学习扩散时间步"] --> Problem
        Root --> Method
        Root --> Results
        Problem["Heuristic diffusion timestep selection leads to sub-optimal, task-biased performance.<br>启发式扩散时间步选择导致次优的、偏向特定任务的性能。"]
        Method["Proposes TTS for adaptive timestep selection and TFC for feature consolidation.<br>提出TTS用于自适应时间步选择和TFC用于特征整合。"]
        Results["Achieves superior few-shot dense prediction on the Taskonomy dataset.<br>在Taskonomy数据集上实现了优越的少样本密集预测性能。"]
    ```

- **[arXiv251230] AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding**
  - **tags:** [cv], [object detection], [autonomous driving, adverse conditions, semantic segmentation, depth estimation, multi-task learning]
  - **authors:** Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon
  - **institution:** KAIST (Korea Advanced Institute of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.23215
  - **contributions:** 1. Introduces the AVOID dataset, a new simulated driving dataset focused on unexpected small road obstacles captured under diverse adverse weather and time conditions. 2. Provides rich multi-modal annotations per image, including semantic and depth maps, LiDAR data (raw and semantic), and waypoints to support various perception tasks. 3. Benchmarks real-time obstacle detection networks and proposes ablation studies using a comprehensive multi-task network for semantic segmentation, depth, and waypoint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the AVOID dataset to address the lack of driving datasets containing small, unexpected road obstacles under varied adverse conditions. The dataset, collected in simulation, provides rich multi-modal annotations and is used to benchmark real-time obstacle detection and multi-task perception models. The work aims to improve the robustness of visual perception for autonomous driving in challenging scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AVOID Dataset] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有数据集缺乏不利条件下的意外小障碍物/Existing datasets lack small unexpected obstacles under adverse conditions]
        Method[主要方法/Method] --> M1[提出AVOID模拟驾驶数据集/Propose AVOID simulated driving dataset]
        M1 --> M2[包含多模态标注: 语义/深度/LiDAR/路径点/Contains multi-modal annotations: semantic/depth/LiDAR/waypoints]
        Results[关键结果/Results] --> R1[为实时障碍物检测提供基准/Benchmark for real-time obstacle detection]
        Results --> R2[提出并评估多任务网络/Propose and evaluate multi-task network]
    ```

- **[arXiv251230] MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?**
  - **tags:** [cv], [multimodal evaluation], [multimodal large language models, unmanned aerial vehicles, benchmark, low-altitude scenarios, spatial bias]
  - **authors:** Shiqi Dai, Zizhi Ma, Zhicong Luo, Xuesong Yang, Yibin Huang, Wanyue Zhang, Chi Chen, Zonghao Guo, Wang Xu, Yufei Sun, Maosong Sun
  - **institution:** Tsinghua University, Nankai University, Northwest Polytechnical University, Chinese Academy of Sciences, Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23219
  - **code:** https://github.com/MM-UAV/MM-UAVBench
  - **contributions:** 1. Introduces MM-UAVBench, a comprehensive benchmark for evaluating MLLMs in low-altitude UAV scenarios across perception, cognition, and planning dimensions. 2. Constructs a dataset of over 5.7K manually annotated questions derived from real-world UAV data, covering 19 sub-tasks. 3. Through extensive experiments on 16 MLLMs, identifies critical bottlenecks like spatial bias and multi-view understanding that hinder model performance in these scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MM-UAVBench, a new benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in low-altitude Unmanned Aerial Vehicle (UAV) scenarios. The benchmark systematically tests models on perception, cognition, and planning using over 5.7K annotated questions from real UAV data. The evaluation reveals that current MLLMs struggle with the complex demands of low-altitude environments, highlighting key bottlenecks like spatial bias.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MM-UAVBench: How Well Do MLLMs See, Think, and Plan in Low-Altitude UAV Scenarios?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有基准未覆盖低空无人机场景的独特挑战 / Existing benchmarks lack coverage of unique low-altitude UAV challenges]
        C --> C1[提出多维度评估基准MM-UAVBench / Propose multi-dimensional benchmark MM-UAVBench]
        C --> C2[包含19个子任务与5.7K+人工标注问题 / Contains 19 sub-tasks and 5.7K+ manually annotated questions]
        D --> D1[当前MLLMs难以适应低空复杂需求 / Current MLLMs struggle to adapt to complex low-altitude demands]
        D --> D2[发现空间偏见等关键瓶颈 / Uncover critical bottlenecks like spatial bias]
    ```

- **[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information**
  - **tags:** [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]
  - **authors:** Youngchae Kwon, Jinyoung Choi, Injung Kim
  - **institution:** Handong Global University
  - **link:** https://arxiv.org/pdf/2512.23221
  - **contributions:** 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: 整体时尚物品检测] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>时尚物品检测因外观多样和子类别相似而具有挑战性。]
        C[主要方法/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>提出Holi-DETR，利用共现、空间布局和身体关键点三种上下文线索的整体检测器。]
        D[关键结果/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>性能超越原始DETR (+3.6pp AP) 和 Co-DETR (+1.1pp AP)。]
    ```

- **[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images**
  - **tags:** [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]
  - **authors:** Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi
  - **institution:** Sungkyunkwan University, The University of Texas MD Anderson Cancer Center
  - **link:** https://arxiv.org/pdf/2512.23227
  - **contributions:** 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Anomaly Detection by Effectively Leveraging Synthetic Images] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 真实缺陷图像稀缺，现有合成方法在成本与质量间难以权衡/Scarcity of real defect images, trade-off between cost and quality in synthesis]
        C[主要方法/Method: 1. 使用预训练图像翻译与检索模型高效生成缺陷图/Use pre-trained image-to-image & retrieval models for generation. 2. 两阶段训练策略：先预训练再微调/Two-stage training: pre-train then fine-tune]
        D[关键结果/Results: 在MVTec AD数据集上验证有效，降低成本并提升性能/Validated on MVTec AD, reduces cost and improves performance]
    ```

- **[arXiv251230] SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems**
  - **tags:** [cv], [diffusion models], [diffusion-based inverse problems, Stein's Unbiased Risk Estimate (SURE), posterior sampling, trajectory correction, PCA-based noise estimation]
  - **authors:** Minwoo Kim, Hongki Lim
  - **institution:** Inha University
  - **link:** https://arxiv.org/pdf/2512.23232
  - **contributions:** 1. Proposes SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations in diffusion-based inverse problem solvers. 2. Introduces the use of Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA-based noise estimation to mitigate noise-induced errors during early and middle sampling stages. 3. Demonstrates that SGPS maintains high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs), outperforming existing methods at low NFE counts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of error accumulation in diffusion-based inverse problem solvers, which typically require many steps for high-quality reconstruction. The authors propose SURE Guided Posterior Sampling (SGPS), a method that corrects the sampling trajectory using SURE gradient updates and PCA-based noise estimation. The method reduces error accumulation, enabling high-quality reconstructions in fewer than 100 steps and outperforming existing approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SURE Guided Posterior Sampling<br>SURE引导后验采样"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["扩散逆问题求解<br>Diffusion-Based Inverse Problems"]
        Problem --> P2["误差累积需大量步数<br>Error Accumulation Requires Many Steps"]
        Method --> M1["SURE引导轨迹校正<br>SURE Guided Trajectory Correction"]
        Method --> M2["PCA噪声估计<br>PCA-Based Noise Estimation"]
        Results --> R1["少于100次NFE<br>Fewer than 100 NFEs"]
        Results --> R2["高质量重建<br>High-Quality Reconstruction"]
        Results --> R3["优于现有方法<br>Outperforms Existing Methods"]
    ```

- **[arXiv251230] Bridging Your Imagination with Audio-Video Generation via a Unified Director**
  - **tags:** [cv], [video generation], [unified director model, Mixture-of-Transformers, interleaved concept learning, disentangled expert learning]
  - **authors:** Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen
  - **institution:** ByteDance Intelligent Creation, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.23222
  - **code:** https://kebii.github.io/UniMAGE
  - **contributions:** 1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a "first interleaving, then disentangling" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel "first interleaving, then disentangling" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有系统将脚本起草与关键镜头设计视为独立任务/Existing systems treat script drafting and key-shot design as disjoint tasks]
        C --> C1[提出统一导演模型UniMAGE/Propose unified director model UniMAGE]
        C --> C2[采用混合Transformer架构/Employ Mixture-of-Transformers architecture]
        C --> C3[引入"先交错，后解耦"训练范式/Introduce "first interleaving, then disentangling" training paradigm]
        D --> D1[在开源模型中达到SOTA性能/Achieves SOTA performance among open-source models]
        D --> D2[生成逻辑连贯的脚本和视觉一致的图像/Generates logically coherent scripts and visually consistent keyframes]
    ```

- **[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network**
  - **tags:** [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]
  - **authors:** Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao
  - **institution:** (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)
  - **link:** https://arxiv.org/pdf/2512.23234
  - **contributions:** 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --> B(核心问题/Problem: 红外气体泄漏检测困难/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)
        A --> C(主要方法/Method: 提出PEG-DRNet/Propose PEG-DRNet)
        C --> C1(气体块建模气体传输/Gas Block models gas transport)
        C --> C2(自适应梯度相位边缘算子/Adaptive Gradient and Phase Edge Operator (AGPEO))
        C --> C3(内容自适应稀疏路由聚合网络/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))
        A --> D(关键结果/Results: 在IIG和LangGas数据集上性能优越/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)
    ```

- **[arXiv251230] RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models**
  - **tags:** [cv], [diffusion models], [data pruning, remote sensing, diffusion foundation models, entropy-based selection, scene-aware clustering]
  - **authors:** Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu
  - **institution:** Tsinghua University, Sun Yat-sen University, National Supercomputing Center in Shenzhen, Tsinghua Shenzhen International Graduate School, The University of Hong Kong, Peking University
  - **link:** https://arxiv.org/pdf/2512.23239
  - **contributions:** 1. A novel, training-free, two-stage data pruning method for remote sensing diffusion models that jointly considers local information content and global scene-level diversity. 2. A reference-guided, scene-aware clustering strategy that leverages existing classification datasets to efficiently select a high-quality subset from large-scale unlabeled data. 3. Demonstrated effectiveness under high pruning ratios (e.g., 85%), significantly improving model convergence and generation quality, and achieving state-of-the-art performance on downstream tasks like super-resolution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of training diffusion-based remote sensing foundation models on large, redundant datasets. It proposes RS-Prune, a training-free, two-stage data pruning method that first removes low-information samples and then performs scene-aware clustering for fine-grained selection. The method enables rapid model convergence even after pruning 85% of data and achieves superior performance on downstream generation tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RS-Prune: Training-Free Data Pruning for RS Diffusion Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[数据冗余与噪声/Data Redundancy & Noise]
        B --> B2[训练效率低/Low Training Efficiency]
        C --> C1[两阶段剪枝/Two-Stage Pruning]
        C1 --> C1a[基于熵的筛选/Entropy-Based Filtering]
        C1 --> C1b[场景感知聚类/Scene-Aware Clustering]
        D --> D1[高剪枝率有效/Effective at High Ratios (85%)]
        D --> D2[提升收敛与质量/Improved Convergence & Quality]
        D --> D3[下游任务SOTA/SOTA on Downstream Tasks]
    ```

- **[arXiv251230] Contour Information Aware 2D Gaussian Splatting for Image Representation**
  - **tags:** [cv], [image representation], [2D Gaussian Splatting, contour awareness, segmentation priors, warm-up scheme]
  - **authors:** Masaya Takabe, Hiroshi Watanabe, Sujun Hong, Tomohiro Ikai, Zheming Fan, Ryo Ishimoto, Kakeru Sugimoto, Ruri Imichi
  - **institution:** Waseda University, Sharp Corporation
  - **link:** https://arxiv.org/pdf/2512.23255
  - **contributions:** 1. A Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors to preserve edge structures. 2. A region-constrained rasterization method that prevents cross-boundary blending of Gaussians. 3. A warm-up training scheme to stabilize optimization and improve convergence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of blurry object boundaries in highly compressed 2D Gaussian Splatting (2DGS) for image representation. The proposed method integrates segmentation priors to constrain Gaussians to specific object regions during rasterization, preventing blending across edges. Experiments show it achieves higher reconstruction quality around edges with very few Gaussians while maintaining fast rendering and low memory usage.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Contour Information Aware 2D Gaussian Splatting<br/>轮廓信息感知的2D高斯泼溅] --> B
        A --> C
        A --> D
        B[Problem: Blurry boundaries in compressed 2DGS<br/>问题：压缩2DGS中的模糊边界]
        C[Method: Region-constrained rasterization with segmentation priors<br/>方法：结合分割先验的区域约束栅格化]
        D[Results: Better edge quality with few Gaussians<br/>结果：用少量高斯实现更好的边缘质量]
    ```

- **[arXiv251230] Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism**
  - **tags:** [cv], [multimodal fusion], [Dynamic Resolution Input Strategy (DRIS), Multi-scale Vision-language Alignment Mechanism (MS-VLAM), Vision-language Model (VLM), remote sensing image captioning, cross-modal retrieval]
  - **authors:** Siyu Zhang, Ying Chen, Lianlei Shan, Runhe Qiu
  - **institution:** Sanda University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.23243
  - **contributions:** 1. Proposes a Dynamic Resolution Input Strategy (DRIS) that adaptively allocates computational resources based on image content complexity to balance detail and efficiency. 2. Introduces a Multi-scale Vision-language Alignment Mechanism (MS-VLAM) with object, local-region, and global-level alignment to capture cross-modal semantic consistency. 3. Presents an integrated VLM framework that demonstrates superior performance in remote sensing image captioning and cross-modal retrieval tasks on the RS-GPT4V dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of fixed-resolution inputs and the lack of semantic hierarchy in single-scale alignment for multimodal remote sensing interpretation. It proposes a novel Vision-language Model framework featuring a Dynamic Resolution Input Strategy and a Multi-scale Vision-language Alignment Mechanism. The framework significantly improves both semantic understanding accuracy and computational efficiency in tasks like image captioning and cross-modal retrieval, as validated on the RS-GPT4V dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multimodal Interpretation of Remote Sensing Images<br>多模态遥感图像解译] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[Fixed resolution fails to balance efficiency and detail<br>固定分辨率无法平衡效率与细节]
        B1 --> C2[Single-scale alignment lacks semantic hierarchy<br>单尺度对齐缺乏语义层次]
        B2 --> D1[Dynamic Resolution Input Strategy (DRIS)<br>动态分辨率输入策略]
        B2 --> D2[Multi-scale Vision-language Alignment Mechanism (MS-VLAM)<br>多尺度视觉-语言对齐机制]
        B3 --> E1[Improves accuracy in captioning & retrieval<br>提升描述与检索精度]
        B3 --> E2[Enhances computational efficiency<br>提升计算效率]
    ```

- **[arXiv251230] ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation**
  - **tags:** [cv], [text-to-image generation], [identity-consistent generation, text embedding modification, padding embeddings, adaptive feature-sharing, Consistency Quality Score (CQS)]
  - **authors:** Shin seong Kim, Minjung Shin, Hyunin Cho, Youngjung Uh
  - **institution:** Yonsei University
  - **link:** https://arxiv.org/pdf/2512.23245
  - **code:** https://minjung-s.github.io/asemconsist
  - **contributions:** 1. A novel framework (ASemConsist) for identity-consistent generation using selective text embedding modification and a semantic control strategy that repurposes padding embeddings as semantic containers. 2. An adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to ambiguous identity prompts. 3. A unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and prompt alignment into a single metric to capture performance imbalances.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating a sequence of images with consistent character identity while maintaining alignment with diverse per-image prompts. The proposed ASemConsist framework modifies text embeddings to control identity semantics, uses padding embeddings as semantic containers, and adaptively applies constraints based on prompt ambiguity. It achieves state-of-the-art performance by overcoming the trade-off between identity consistency and prompt alignment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ASemConsist: Adaptive Semantic Feature Control] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[身份一致性生成中的权衡/Trade-off in identity-consistent generation]
        B1 --> B2[保持身份一致性与确保每张图像提示对齐之间的冲突/Conflict between identity preservation and per-image prompt alignment]
        C --> C1[选择性文本嵌入修改/Selective text embedding modification]
        C --> C2[将填充嵌入重新用作语义容器/Repurposing padding embeddings as semantic containers]
        C --> C3[自适应特征共享策略/Adaptive feature-sharing strategy]
        D --> D1[提出统一评估协议CQS/Proposed unified evaluation protocol CQS]
        D --> D2[实现最先进的性能/Achieved state-of-the-art performance]
        D --> D3[克服了先前的权衡/Overcame prior trade-offs]
    ```

- **[arXiv251230] Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization**
  - **tags:** [mlsys], [diffusion models], [diffusion transformer, inference acceleration, caching, error minimization, dynamic programming]
  - **authors:** Tong Shao, Yusen Fu, Guoying Sun, Jingde Kong, Zhuotao Tian, Jingyong Su
  - **institution:** Harbin Institute of Technology, Shenzhen
  - **link:** https://arxiv.org/pdf/2512.23258
  - **contributions:** 1. Proposes CEM, a novel plugin for optimizing caching strategies in DiT acceleration via cumulative error minimization. 2. Introduces a dynamic programming algorithm guided by a predefined error prior to adaptively minimize caching error. 3. Demonstrates the method's model-agnostic nature, seamless integration into existing frameworks, and significant fidelity improvements across multiple models and tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow inference of Diffusion Transformers (DiTs) by proposing CEM, a plug-and-play fidelity optimization plugin. CEM minimizes cumulative caching error via a dynamic programming algorithm, adapting to error variations during denoising. The method is training-free, model-agnostic, and significantly improves generation fidelity when integrated with existing acceleration techniques.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization"] --> Problem["核心问题/Problem: DiT推理慢，现有缓存加速方法存在固定策略导致的累积误差/DiT inference is slow; existing caching-based acceleration suffers from fixed-strategy cumulative error"]
        Root --> Method["主要方法/Method: 提出CEM插件，通过累积误差最小化的动态规划优化缓存策略/Propose CEM plugin, optimizing caching strategy via cumulative error minimization with dynamic programming"]
        Root --> Results["关键结果/Results: 显著提升生成保真度，模型无关，可无缝集成/Significantly improves generation fidelity, model-agnostic, seamlessly integrable"]
    ```

- **[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing**
  - **tags:** [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]
  - **authors:** Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang
  - **institution:** Fudan University, Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.23244
  - **contributions:** 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ViLaCD-R1: 遥感语义变化检测的视觉语言框架] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1[传统方法语义理解不足/Traditional methods lack semantic understanding]
        B1 --> C2[现有VLM方法定位不准确/Existing VLM methods have inaccurate localization]
        B2 --> D1[两阶段框架/Two-stage framework]
        D1 --> E1[多图像推理器/Multi-Image Reasoner]
        E1 --> F1[SFT与RL训练/SFT and RL training]
        E1 --> F2[生成粗变化掩码/Generate coarse change mask]
        D1 --> E2[掩码引导解码器/Mask-Guided Decoder]
        E2 --> F3[融合特征与掩码/Fuse features and mask]
        E2 --> F4[预测精细变化图/Predict precise change map]
        B3 --> G1[提升语义变化识别/Improves semantic change recognition]
        B3 --> G2[抑制非语义变化/Suppresses non-semantic variations]
        B3 --> G3[达到SOTA性能/Achieves SOTA performance]
    ```

- **[arXiv251230] YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection**
  - **tags:** [cv], [object detection], [Mixture-of-Experts, adaptive computation, real-time detection, dynamic routing, YOLO]
  - **authors:** Xu Lin, Jinlong Peng, Zhenye Gan, Jiawen Zhu, Jun Liu
  - **institution:** Tencent Youtu Lab, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.23273
  - **code:** https://github.com/isLinXu/YOLO-Master
  - **contributions:** 1. Proposes YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for real-time object detection. 2. Designs an Efficient Sparse Mixture-of-Experts (ES-MoE) block to dynamically allocate computational resources based on scene complexity. 3. Introduces a lightweight dynamic routing network with a diversity-enhancing objective to encourage complementary expert specialization and efficient inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that existing YOLO models use static computation, leading to inefficiency on simple scenes and poor performance on complex ones. To solve this, it proposes YOLO-Master, which uses a Mixture-of-Experts block and a dynamic router to adaptively allocate computation per input. Experiments show it achieves higher accuracy and faster speed than baselines, especially on dense scenes, while maintaining real-time performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[YOLO-Master] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[静态计算导致资源错配/Static computation misallocates resources]
        C --> C1[高效稀疏专家混合块/ES-MoE Block]
        C --> C2[动态路由网络/Dynamic Routing Network]
        D --> D1[更高精度与速度/Higher Accuracy & Speed]
        D --> D2[在密集场景提升显著/Better on Dense Scenes]
    ```

- **[arXiv251230] Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition**
  - **tags:** [cv], [multimodal emotion recognition], [micro-gesture recognition, behavior-based emotion prediction, multimodal fusion, Cross-Modal Token Fusion, InterFusion]
  - **authors:** Arman Martirosyan, Shahane Tigranyan, Maria Razzhivina, Artak Aslanyan, Nazgul Salikhova, Ilya Makarov, Andrey Savchenko, Aram Avetisyan
  - **institution:** Russian-Armenian University, ISP RAS, HSE University, Innopolis University, AIRI, Sber AI Lab
  - **link:** https://arxiv.org/pdf/2512.23291
  - **contributions:** 1. Proposed a multimodal framework for micro-gesture classification that fuses RGB (MViTv2-S) and 3D pose (2s-AGCN) embeddings via a Cross-Modal Token Fusion module. 2. Developed a separate multimodal framework for behavior-based emotion prediction that fuses facial (SwinFace) and contextual (MViTv2-S) embeddings via an InterFusion module. 3. Demonstrated robust performance on the iMiGUE dataset, achieving 2nd place in the behavior-based emotion prediction task of the MiGA 2025 Challenge.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp
  - **Simple LLM Summary:** This paper presents two multimodal frameworks to tackle micro-gesture recognition and behavior-based emotion prediction on the iMiGUE dataset. The first framework fuses video and skeletal pose data, while the second fuses facial and contextual embeddings for emotion classification. The method demonstrated strong performance, securing 2nd place in the emotion prediction task of the MiGA 2025 Challenge.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Track Multimodal Learning on iMiGUE] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1(微手势识别/Micro-gesture Recognition)
        B1 --> C2(行为情绪预测/Behavior-based Emotion Prediction)
        B2 --> D1(多模态框架/Multimodal Framework)
        D1 --> E1(微手势: RGB+姿态融合/Micro-gesture: RGB+Pose Fusion)
        D1 --> E2(情绪: 面部+上下文融合/Emotion: Facial+Context Fusion)
        E1 --> F1(使用MViTv2-S和2s-AGCN/Use MViTv2-S & 2s-AGCN)
        E1 --> F2(跨模态令牌融合/Cross-Modal Token Fusion)
        E2 --> F3(使用SwinFace和MViTv2-S/Use SwinFace & MViTv2-S)
        E2 --> F4(内部融合模块/InterFusion Module)
        B3 --> G1(在iMiGUE数据集上评估/Evaluated on iMiGUE Dataset)
        B3 --> G2(MiGA 2025挑战赛第二名/2nd Place in MiGA 2025 Challenge)
    ```

- **[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images**
  - **tags:** [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]
  - **authors:** Md. Sazzadul Islam Prottasha, Nabil Walid Rafi
  - **institution:** Bangladesh University of Professionals
  - **link:** https://arxiv.org/pdf/2512.23304
  - **contributions:** 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp
  - **Simple LLM Summary:** This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MedGemma vs GPT-4: 医学图像零样本疾病分类] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 比较开源与闭源多模态LLM在医学图像诊断中的性能]
        C[主要方法/Method: 使用LoRA微调的MedGemma与未调优的GPT-4进行零样本分类对比]
        D[关键结果/Results: MedGemma准确率(80.37%)和敏感性更高，领域微调对减少幻觉至关重要]
    ```

- **[arXiv251230] PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering**
  - **tags:** [cv], [visual SLAM], [ORB-SLAM3, YOLOv8, dynamic object filtering, point cloud refinement, CUDA acceleration]
  - **authors:** Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin
  - **institution:** Yuan Ze University
  - **link:** https://arxiv.org/pdf/2512.23318
  - **contributions:** 1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3's robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: vSLAM accuracy compromised by dynamic objects]
        Method[主要方法/Method: ORB-SLAM3 + YOLOv8 segmentation + multi-stage point cloud filtering]
        Results[关键结果/Results: Mixed performance, notable improvement in specific sequences (e.g., Seq04)]
    ```

- **[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations**
  - **tags:** [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]
  - **authors:** Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang
  - **institution:** Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23328
  - **contributions:** 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LLM智能体缺乏物理世界部署所需的稳健空间心智模型/LLM agents lack robust spatial mental models for physical-world deployment]
        C --> C1[提出基于魔方的三层诊断基准/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]
        C --> C2[从完整符号状态到部分视觉状态逐步评估/Progressive evaluation from full symbolic to partial visual state]
        D --> D1[领先LLM在长视野任务上通过率为0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]
        D --> D2[揭示了长期规划和主动探索的根本性失败/Exposes fundamental failure in long-term planning and active exploration]
    ```

- **[arXiv251230] CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation**
  - **tags:** [ai], [reinforcement learning], [CAD code generation, multi-expert reinforcement learning, Chain-of-Thought, CADExpert benchmark, CADQuery]
  - **authors:** Ke Niu, Haiyang Yu, Zhuofan Chen, Zhengtao Yao, Weitao Jia, Xiaodong Ge, Jingqun Tang, Benlei Cui, Bin Li, Xiangyang Xue
  - **institution:** Fudan University, ByteDance Inc.
  - **link:** https://arxiv.org/pdf/2512.23333
  - **contributions:** 1. Proposes a novel Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm for generating precise and editable CAD models., 2. Introduces a two-stage training process: Multi-Expert Fine-Tuning (MEFT) and Multi-Expert Reinforcement Learning (MERL)., 3. Presents CADExpert, an open-source benchmark with 17,299 instances including orthographic projections, CoT processes, CADQuery code, and 3D models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automating the generation of high-precision, editable CAD models from sketches, which existing methods struggle with. It proposes a new training paradigm called CME-CAD, which uses a two-stage process of Multi-Expert Fine-Tuning and Reinforcement Learning to collaboratively improve model performance. The approach aims to generate accurate, constraint-compatible CAD code and is supported by a new open-source benchmark called CADExpert.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CME-CAD: CAD代码生成] --> B1
        A --> B2
        A --> B3
        B1[核心问题/Problem<br>现有方法生成模型不可编辑、不精确<br>依赖文本/图像输入，标注成本高]
        B2[主要方法/Method<br>异构协作多专家强化学习<br>两阶段训练: MEFT + MERL]
        B3[关键结果/Results<br>生成精确、可编辑的CAD模型<br>发布CADExpert基准数据集]
    ```

- **[arXiv251230] Visual Language Hypothesis**
  - **tags:** [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]
  - **authors:** Xiu Li
  - **institution:** Bytedance
  - **link:** https://arxiv.org/pdf/2512.23335
  - **contributions:** 1. Proposes the "Visual Language Hypothesis," framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an "expand-and-snap" process to achieve the necessary topology change for semantic abstraction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the "Visual Language Hypothesis," which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific "expand-and-snap" process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Visual Language Hypothesis] --> B[核心问题/Problem: What structural properties enable semantic abstraction in vision?]
        A --> C[主要方法/Method: Propose hypothesis of discrete semantic language, derive geometric (fiber bundle) structure]
        A --> D[关键结果/Results: Semantic invariance needs discriminative target; Model needs "expand-and-snap" for topology change]
    ```

- **[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents**
  - **tags:** [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]
  - **authors:** Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin
  - **institution:** Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.23343
  - **code:** https://github.com/AgentMemory/Huaman-Agent-Memory
  - **contributions:** 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp
  - **Simple LLM Summary:** This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Meets Brain: Memory Systems / AI与大脑：记忆系统] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[Interdisciplinary Gap / 跨学科鸿沟]
        P1 --> P2[Existing works struggle to assimilate human memory essence / 现有工作难以吸收人类记忆机制精髓]
    
        Method[主要方法/Method] --> M1[Systematic Synthesis / 系统综述]
        M1 --> M2[Comparative Analysis / 对比分析]
        M2 --> M3[Review Benchmarks & Security / 回顾基准与安全]
    
        Results[关键结果/Results] --> R1[Unified Memory Framework / 统一的记忆框架]
        R1 --> R2[Future Directions / 未来方向]
        R2 --> R3[Multimodal Memory & Skill Acquisition / 多模态记忆与技能获取]
    ```

- **[arXiv251230] CountGD++: Generalized Prompting for Open-World Counting**
  - **tags:** [cv], [object counting], [open-world counting, negative prompting, pseudo-exemplars]
  - **authors:** Niki Amini-Naieni, Andrew Zisserman
  - **institution:** University of Oxford (Visual Geometry Group)
  - **link:** https://arxiv.org/pdf/2512.23351
  - **code:** https://github.com/niki-amini-naieni/CountGDPlusPlus
  - **contributions:** 1. Extended the counting prompt to include negative specifications (what not to count) via text and/or visual examples. 2. Introduced 'pseudo-exemplars' to automate the annotation of visual examples at inference time. 3. Enabled counting models to accept visual examples from both natural and synthetic external images, and integrated the model as a vision expert agent for an LLM.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses limitations in open-world object counting by introducing CountGD++, a model that significantly expands prompt flexibility. It allows specifying what not to count, automates visual example annotation via pseudo-exemplars, and accepts external visual examples, leading to improved accuracy and generalization across datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CountGD++: Generalized Prompting for Open-World Counting] --> B[核心问题/Problem: Limited prompt flexibility in object counting models]
        A --> C[主要方法/Method: Negative prompts, pseudo-exemplars, external visual examples]
        A --> D[关键结果/Results: Improved accuracy, efficiency, and generalization]
    ```

- **[arXiv251230] SpatialMosaic: A Multiview VLM Dataset for Partial Visibility**
  - **tags:** [cv], [multi-view vision-language reasoning], [spatial reasoning, multi-view images, partial visibility, instruction-tuning, 3D reconstruction]
  - **authors:** Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park
  - **institution:** Seoul National University, University College London, POSTECH
  - **link:** https://arxiv.org/pdf/2512.23365
  - **contributions:** 1. A scalable multi-view data generation and annotation pipeline for realistic spatial reasoning QA pairs. 2. SpatialMosaic, a comprehensive 2M QA pair instruction-tuning dataset, and SpatialMosaic-Bench, a 1M QA pair benchmark for evaluating multi-view spatial reasoning under challenging conditions. 3. SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of enabling Vision-Language Models (VLMs) to perform 3D spatial reasoning directly from multi-view images under realistic conditions like partial visibility and occlusion. It proposes a data generation pipeline to create the SpatialMosaic dataset and benchmark, and introduces a hybrid VLM framework that incorporates 3D reconstruction models. Experiments show the approach effectively enhances spatial reasoning in challenging multi-view scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpatialMosaic: A Multiview VLM Dataset for Partial Visibility] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖3D重建，限制可扩展性/Existing methods rely on 3D reconstruction, limiting scalability]
        B --> B2[真实场景的碎片化视觉线索未被充分探索/Real-world fragmented visual cues are under-explored]
        C --> C1[可扩展的多视图数据生成与标注流水线/Scalable multi-view data generation & annotation pipeline]
        C --> C2[构建SpatialMosaic数据集与基准/Build SpatialMosaic dataset & benchmark]
        C --> C3[提出集成3D重建模型的混合VLM框架/Propose hybrid VLM framework with 3D reconstruction models]
        D --> D1[生成2M QA对用于指令微调/Generated 2M QA pairs for instruction-tuning]
        D --> D2[构建1M QA对的评估基准/Built 1M QA pair evaluation benchmark]
        D --> D3[有效提升挑战性条件下的空间推理能力/Effectively enhances spatial reasoning under challenging conditions]
    ```

- **[arXiv251230] MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning**
  - **tags:** [cv], [two-view correspondence / outlier rejection], [geometric attention, graph neural network, cross-stage consensus, outlier rejection, camera pose estimation]
  - **authors:** Shuyuan Lin, Mengtin Lo, Haosheng Chen, Yanjie Liang, Qiangqiang Wu
  - **institution:** Jinan University, Chongqing University of Posts and Telecommunications, Peng Cheng Laboratory, City University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.23369
  - **code:** http://www.linshuyuan.com
  - **contributions:** 1. Proposed a Contextual Geometric Attention (CGA) module that dynamically integrates spatial and feature information to capture local and global geometric relationships. 2. Introduced a Cross-Stage Multi-Graph Consensus (CSMGC) module to ensure geometric consistency across different network stages via a sparse graph network. 3. Demonstrated state-of-the-art performance on the YFCC100M and SUN3D datasets for outlier rejection and camera pose estimation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of robust two-view correspondence learning for tasks like camera pose estimation. It proposes MGCA-Net, a novel network featuring a Contextual Geometric Attention module and a Cross-Stage Multi-Graph Consensus module to better model geometric constraints and ensure information consistency. Experiments show it outperforms existing methods on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MGCA-Net: 多图上下文注意力网络 / Multi-Graph Contextual Attention Network] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[现有方法在局部几何建模和跨阶段信息优化上存在局限 / Limitations in local geometric modeling and cross-stage information optimization]
        B --> B2[难以准确捕获匹配对的几何约束，降低模型鲁棒性 / Hard to capture geometric constraints, reducing robustness]
        C --> C1[上下文几何注意力模块 / Contextual Geometric Attention (CGA) Module]
        C --> C2[跨阶段多图共识模块 / Cross-Stage Multi-Graph Consensus (CSMGC) Module]
        C1 --> C1a[动态整合空间位置和特征信息 / Dynamically integrates spatial position and feature information]
        C1 --> C1b[增强捕获局部和全局几何关系的能力 / Enhances capability to capture local and global geometric relationships]
        C2 --> C2a[通过跨阶段稀疏图网络建立几何共识 / Establishes geometric consensus via cross-stage sparse graph network]
        C2 --> C2b[确保不同阶段间几何信息的一致性 / Ensures consistency of geometric information across stages]
        D --> D1[在YFCC100M和SUN3D数据集上显著优于SOTA方法 / Significantly outperforms SOTA on YFCC100M and SUN3D datasets]
        D --> D2[在离群点剔除和相机姿态估计任务中表现优异 / Excels in outlier rejection and camera pose estimation tasks]
    ```

- **[arXiv251230] NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization**
  - **tags:** [cv], [image forensics], [image manipulation detection, generalization benchmark, cross-dimension evaluation, AIGC-based manipulation]
  - **authors:** Yifei Li, Haoyuan He, Yu Zheng, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jie Zhou, Jiwen Lu
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.23374
  - **code:** https://github.com/JoeLeelyf/NeXT-IMDL
  - **contributions:** 1. Proposes NeXT-IMDL, a large-scale diagnostic benchmark for Image Manipulation Detection and Localization (IMDL) designed to systematically probe the generalization boundaries of detectors. 2. Categorizes AI-generated content (AIGC) manipulations along four fundamental axes (editing models, manipulation types, content semantics, forgery granularity) and implements five rigorous cross-dimension evaluation protocols. 3. Through extensive experiments on 11 representative models, reveals that current models exhibit systemic failures and significant performance degradation under the proposed protocols, challenging the perceived progress in the field.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a "benchmark illusion" in Image Manipulation Detection and Localization (IMDL), where current cross-dataset evaluations overestimate model robustness. To address this, the authors propose NeXT-IMDL, a diagnostic benchmark that systematically categorizes manipulations and introduces rigorous cross-dimension evaluation protocols. Experiments show that 11 state-of-the-art models suffer significant performance drops under these new protocols, highlighting their fragility and the need for more robust next-generation IMDL models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[NeXT-IMDL: Next-Generation Image Manipulation Detection & Localization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有评估方法高估模型鲁棒性/Current evaluations overestimate model robustness]
        C --> C1[构建诊断性基准 NeXT-IMDL/Build diagnostic benchmark NeXT-IMDL]
        C1 --> C2[四维操纵分类/Four-axis manipulation categorization]
        C1 --> C3[五维交叉评估协议/Five cross-dimension evaluation protocols]
        D --> D1[11个模型出现系统性失败/11 models exhibit systemic failures]
        D --> D2[性能显著下降/Significant performance degradation]
    ```

- **[arXiv251230] SoulX-LiveTalk Technical Report**
  - **tags:** [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]
  - **authors:** Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu
  - **institution:** Soul AI Lab, Donghua University
  - **link:** https://arxiv.org/pdf/2512.23379
  - **code:** https://soul-ailab.github.io/soulx-livetalk/
  - **contributions:** 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SoulX-LiveTalk] --> B[核心问题/Problem: 实时无限时长音频驱动化身生成中计算负载与低延迟的冲突]
        A --> C[主要方法/Method: 自校正双向蒸馏与多步回顾自校正机制]
        A --> D[关键结果/Results: 0.87秒启动延迟，32 FPS实时吞吐]
    ```

- **[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers**
  - **tags:** [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]
  - **authors:** Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee
  - **institution:** Urmia University of Technology
  - **link:** https://arxiv.org/pdf/2512.23380
  - **code:** https://github.com/your-repo/CoLog (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)
  - **contributions:** 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --> Problem["核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]
        Root --> Method["主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]
        Root --> Results["关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]
    ```

- **[arXiv251230] Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment**
  - **tags:** [cv], [image aesthetics assessment], [aesthetic description, multimodal learning, large language model, hierarchical learning, entropy minimization]
  - **authors:** Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji
  - **institution:** Tsinghua University, Kuaishou Technology, Pengcheng Laboratory, National Cheng Kung University, E Fund Management Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.23413
  - **contributions:** 1. Introduces the Refined Aesthetic Description (RAD) dataset, a large-scale, multi-dimensional structured dataset generated via an iterative pipeline to address data scarcity and imbalance in artistic aesthetics. 2. Proposes ArtQuant, an aesthetics assessment framework that couples isolated aesthetic dimensions through joint description generation and utilizes LLM decoders to better model long-text semantics. 3. Provides a theoretical analysis showing that the semantic adequacy of the RAD dataset and the generation paradigm of ArtQuant collectively minimize prediction entropy, offering mathematical grounding for the framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges in artistic image aesthetics assessment by introducing a large-scale dataset (RAD) and a novel framework (ArtQuant) that uses hierarchical description learning with LLMs. The method achieves state-of-the-art performance on several benchmarks while requiring significantly fewer training epochs, effectively narrowing the cognitive gap between images and human aesthetic judgment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[数据稀缺与模型碎片化 / Data Scarcity & Model Fragmentation]
        C --> C1[提出RAD数据集 / Propose RAD Dataset]
        C --> C2[提出ArtQuant框架 / Propose ArtQuant Framework]
        C2 --> C2_1[联合描述生成 / Joint Description Generation]
        C2 --> C2_2[LLM解码器 / LLM Decoder]
        D --> D1[SOTA性能 / SOTA Performance]
        D --> D2[减少训练成本 / Reduced Training Cost]
        D --> D3[缩小认知差距 / Narrow Cognitive Gap]
    ```

- **[arXiv251230] DriveLaW:Unifying Planning and Video Generation in a Latent Driving World**
  - **tags:** [cv], [autonomous driving], [world model, video generation, diffusion planner, latent representation, unified planning]
  - **authors:** Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, Haiyang Sun, Bing Wang, Kun Ma, Hangjun Ye, Wenyu Liu, Xinggang Wang
  - **institution:** Huazhong University of Science and Technology, Xiaomi EV
  - **link:** https://arxiv.org/pdf/2512.23421
  - **contributions:** 1. Proposes DriveLaW, a novel paradigm that unifies video generation and motion planning for autonomous driving by directly injecting latent representations from the video generator into the planner. 2. Introduces a three-stage progressive training strategy to jointly optimize the video generation component (DriveLaW-Video) and the diffusion planning component (DriveLaW-Act). 3. Achieves state-of-the-art performance on both video prediction and motion planning benchmarks, significantly surpassing previous methods in metrics like FID, FVD, and NAVSIM.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the decoupling of world prediction and motion planning in current autonomous driving world models. It proposes DriveLaW, a unified framework that connects a video generator and a diffusion planner via latent representations, ensuring consistency between future scene generation and trajectory planning. The method achieves new state-of-the-art results on both video forecasting and planning benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DriveLaW: Unifying Planning and Video Generation] --> B[核心问题/Problem: Decoupled world prediction and planning in autonomous driving]
        A --> C[主要方法/Method: Unified paradigm with latent injection from video generator to diffusion planner]
        A --> D[关键结果/Results: SOTA in video prediction (FID, FVD) and planning (NAVSIM)]
    ```

- **[arXiv251230] SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation**
  - **tags:** [cv], [medical image segmentation], [2D-3D fusion, instance segmentation, order-aware matching, semantic feature injection, center-guided refinement]
  - **authors:** Xiaolan Li, Wanquan Liu, Pengcheng Li, Pengyu Jie, Chenqiang Gao
  - **institution:** Sun Yat-sen University, Hainan University
  - **link:** https://arxiv.org/pdf/2512.23411
  - **contributions:** 1. A point-wise residual gating module that injects frozen 2D SAM embeddings into 3D point features to refine boundaries without 2D mask supervision. 2. A center-guided mask refinement mechanism that regularizes consistency between instance masks and geometric centroids to reduce center drift. 3. An order-aware Hungarian matching strategy that integrates anatomical tooth order and center distance for coherent instance labeling, especially for cases with missing or crowded teeth.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in 3D tooth instance segmentation, such as boundary leakage and inconsistent labeling, by proposing SOFTooth, a framework that fuses 2D semantic features from SAM with 3D geometric data. The method introduces modules for boundary refinement, center stabilization, and order-aware instance assignment. It achieves state-of-the-art performance on a benchmark dataset, demonstrating effective transfer of 2D semantics to 3D segmentation without fine-tuning the 2D model.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SOFTooth: 语义增强的顺序感知融合用于牙齿实例分割] --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1[拥挤牙弓与模糊边界/Crowded arches & ambiguous boundaries]
        Problem --> P2[中心漂移与身份不一致/Center drift & inconsistent identities]
        Problem --> P3[稀有第三磨牙/Rare third molars]
        Method --> M1[点级残差门控注入2D语义/Point-wise residual gating for 2D semantics]
        Method --> M2[中心引导的掩码细化/Center-guided mask refinement]
        Method --> M3[顺序感知匈牙利匹配/Order-aware Hungarian matching]
        Results --> R1[在3DTeethSeg'22上达到SOTA/SOTA on 3DTeethSeg'22]
        Results --> R2[在涉及第三磨牙的案例上提升明显/Clear gains on third molar cases]
        Results --> R3[无需2D微调的有效2D-3D迁移/Effective 2D-3D transfer without fine-tuning]
    ```

- **[arXiv251230] Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision**
  - **tags:** [mlsys], [diffusion models], [preference optimization, diffusion models, score-space supervision, text-to-image synthesis, denoising trajectory]
  - **authors:** Dohyun Kim, Seungwoo Lyu, Seung Wook Kim, Paul Hongsuck Seo
  - **institution:** Korea University, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.23426
  - **code:** this https URL
  - **contributions:** 1. Proposes DDSPO, a method for direct score-space preference optimization in diffusion models that provides per-timestep supervision from contrastive policy pairs. 2. Introduces a practical strategy to automatically generate preference signals using a pretrained model and semantically degraded prompts, avoiding costly human-labeled data. 3. Demonstrates improved text-image alignment and visual quality, outperforming or matching existing methods with significantly less supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Direct Diffusion Score Preference Optimization (DDSPO), a method that optimizes diffusion models by applying preference supervision directly in the score space at each denoising timestep, using automatically generated signals from a reference model. This approach provides dense, stepwise learning signals and reduces reliance on expensive human-labeled data. Empirical results show DDSPO improves text-image alignment and visual quality, matching or outperforming prior preference-based methods with less supervision.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Direct Diffusion Score Preference Optimization <br/> 直接扩散分数偏好优化] --> B
        A --> C
        A --> D
        B[Problem: Diffusion models struggle with user intent alignment & aesthetic consistency <br/> 问题: 扩散模型难以对齐用户意图和保持美学一致性]
        C[Method: DDSPO provides stepwise score-space supervision via contrastive policy pairs <br/> 方法: DDSPO通过对比策略对提供逐步分数空间监督]
        D[Results: Improves alignment & quality, reduces supervision need <br/> 结果: 改善对齐与质量，减少监督需求]
    ```

- **[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation**
  - **tags:** [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]
  - **authors:** Jesse Brouwers, Xiaoyan Xing, Alexander Timans
  - **institution:** UvA-Bosch Delta Lab, University of Amsterdam
  - **link:** https://arxiv.org/pdf/2512.23427
  - **code:** https://github.com/JesseBrouw/UncertSAM
  - **contributions:** 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Integrating Uncertainty for Domain-Agnostic Segmentation] --> B[核心问题/Problem: SAM在域偏移或知识有限场景下脆弱/SAM vulnerable in shifted or limited-knowledge domains]
        A --> C[主要方法/Method: 构建UncertSAM基准，评估后验不确定性方法，尝试不确定性引导优化/Build UncertSAM benchmark, evaluate post-hoc UQ methods, attempt uncertainty-guided refinement]
        A --> D[关键结果/Results: 拉普拉斯近似不确定性估计与误差相关，初步验证不确定性整合潜力/Laplace approximation yields correlated uncertainty, preliminary potential of integrating UQ]
    ```

- **[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification**
  - **tags:** [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]
  - **authors:** Mustafa Demetgul, Sanja Lazarova Molnar
  - **institution:** Karlsruhe Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23436
  - **contributions:** 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --> B[核心问题/Problem: Classical road monitoring is expensive and unsystematic.]
    A --> C[主要方法/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]
    A --> D[关键结果/Results: Over 95% classification accuracy achieved.]
    ```

- **[arXiv251230] RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction**
  - **tags:** [cv], [3D reconstruction], [benchmark, multi-view, physical degradation, neural radiance field, Gaussian splatting]
  - **authors:** Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada
  - **institution:** The University of Tokyo, NII, Tohoku University, University of Würzburg, RIKEN
  - **link:** https://arxiv.org/pdf/2512.23437
  - **contributions:** 1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RealX3D: A Physically-Degraded 3D Benchmark] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[真实世界退化影响3D重建/Real-world degradations hinder 3D reconstruction]
        C --> C1[构建真实捕获基准/Build real-capture benchmark]
        C --> C2[四类退化, 多严重级别/Four corruption families, multiple severity levels]
        C --> C3[提供对齐的LQ/GT视图, RAW数据, 激光扫描/Provide aligned LQ/GT views, RAW, laser scans]
        D --> D1[当前方法质量显著下降/Current methods show substantial quality degradation]
        D --> D2[突出现实挑战性/Underscores fragility in challenging real environments]
    ```

- **[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images**
  - **tags:** [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]
  - **authors:** Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović
  - **institution:** Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton
  - **link:** https://arxiv.org/pdf/2512.23441
  - **contributions:** 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer's Disease.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Stochastic Siamese MAE Pretraining for Longitudinal Medical Images] --> B[核心问题/Problem: MAE lacks temporal awareness for longitudinal medical data.]
        A --> C[主要方法/Method: STAMP - Stochastic Siamese MAE using conditional variational inference.]
        A --> D[关键结果/Results: Outperforms existing methods on AMD and AD progression prediction.]
    ```

- **[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]
  - **authors:** Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang
  - **institution:** Lenovo (PCIE), University of Minnesota (UMN)
  - **link:** https://arxiv.org/pdf/2512.23453
  - **code:** https://github.com/AI-Researcher-Team/CoFi-Dec
  - **contributions:** 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CoFi-Dec: Hallucination-Resistant Decoding] --> B[核心问题/Problem: LVLMs产生与视觉输入不一致的幻觉内容]
        A --> C[主要方法/Method: 基于粗到细视觉条件的生成式自反馈与Wasserstein融合]
        A --> D[关键结果/Results: 在六个基准测试中显著减少幻觉，无需训练，模型无关]
    ```

- **[arXiv251230] Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin**
  - **tags:** [cv], [object detection], [YOLOv8-Pose, multimodal LLMs, waterline detection, scale gap estimation, geometric calibration]
  - **authors:** Kayathri Vigneswaran, Hugo Retief, Jai Clifford Holmes, Mariangel Garcia Andarcia, Hansaka Tennakoon
  - **institution:** International Water Management Institute (IWMI), Association for Water and Rural Development (AWARD)
  - **link:** https://arxiv.org/pdf/2512.23454
  - **contributions:** 1. Proposes a novel hybrid framework combining vision-based waterline detection, YOLOv8-Pose for scale extraction, and multimodal LLMs for automated river gauge reading. 2. Demonstrates that incorporating geometric metadata (scale gap) significantly improves the predictive accuracy of LLMs for water level estimation. 3. Provides a scalable and efficient solution for automated hydrological monitoring, highlighting its sensitivity to image quality and potential for real-time digitization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated river water level monitoring by proposing a hybrid framework that integrates computer vision (waterline detection and YOLOv8-Pose) with multimodal large language models (GPT-4o and Gemini 2.0 Flash) to read gauge plates. The method uses geometric calibration from scale gap detection to enhance LLM performance, achieving high accuracy under optimal conditions. The study concludes that combining geometric metadata with multimodal AI offers a robust, scalable solution for real-time hydrological monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated River Gauge Plate Reading<br/>自动化河流水位计读数] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Manual gauge reading errors & environmental constraints<br/>人工读数误差与环境限制]
        C --> C1[Hybrid Object Detection & Generative AI Framework<br/>混合目标检测与生成式AI框架]
        C1 --> C2[Vision-based waterline detection<br/>基于视觉的水位线检测]
        C1 --> C3[YOLOv8-Pose scale extraction<br/>YOLOv8-Pose尺度提取]
        C1 --> C4[Multimodal LLMs (GPT-4o, Gemini) for reading<br/>多模态大语言模型读数]
        D --> D1[High precision waterline detection (94.24%)<br/>高精度水位线检测]
        D --> D2[Improved LLM accuracy with scale gap metadata<br/>尺度间隙元数据提升LLM精度]
        D --> D3[Gemini Stage 2: MAE=5.43cm, RMSE=8.58cm<br/>Gemini Stage 2最佳性能]
    ```

- **[arXiv251230] Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators**
  - **tags:** [cv], [image-to-image translation], [Brownian bridge, deterministic translation, denoising diffusion, dual approximators, super-resolution]
  - **authors:** Bohan Xiao, Peiyong Wang, Qisheng He, Ming Dong
  - **institution:** Wayne State University
  - **link:** https://arxiv.org/pdf/2512.23463
  - **code:** https://github.com/bohan95/dual-app-bridge
  - **contributions:** 1. Proposes a novel denoising Brownian bridge model with dual neural network approximators for deterministic I2I translation., 2. Introduces a method that guarantees consistent, predictable outputs with high fidelity to ground truth, addressing the limitations of stochastic models., 3. Demonstrates superior performance in tasks like super-resolution compared to both stochastic and deterministic baseline models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Dual-approx Bridge, a novel generative model for deterministic image-to-image translation. It uses Brownian bridge dynamics with two neural approximators to produce high-fidelity, low-variance outputs. Experiments show it outperforms existing baselines in image quality and faithfulness to ground truth.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[确定性图像转换需求 / Need for deterministic I2I translation (e.g., super-resolution)]
        C --> C1[双近似器去噪布朗桥模型 / Denoising Brownian Bridge Model with Dual Approximators]
        D --> D1[输出一致且质量高 / Consistent, high-quality output with high fidelity to GT]
        D --> D2[优于随机与确定性基线 / Superior to stochastic & deterministic baselines]
    ```

- **[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation**
  - **tags:** [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]
  - **authors:** Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao
  - **institution:** Tencent Hunyuan
  - **link:** https://arxiv.org/pdf/2512.23464
  - **code:** https://github.com/Tencent-Hunyuan/HY-Motion-1.0
  - **contributions:** 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"]
        Root --> Problem["核心问题/Problem: Generating high-quality, text-aligned 3D human motions"]
        Root --> Method["主要方法/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline"]
        Root --> Results["关键结果/Results: SOTA performance, Extensive motion coverage, Open-source release"]
    ```

- **[arXiv251230] MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration**
  - **tags:** [cv], [point cloud registration], [graph neural network, multi-domain context, dynamic inlier selection, feature aggregation]
  - **authors:** Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng
  - **institution:** Jinan University, Qingdao University of Science and Technology, Shenzhen University
  - **link:** https://arxiv.org/pdf/2512.23472
  - **code:** http://www.linshuyuan.com
  - **contributions:** 1. A graph neighborhood aggregation module that constructs a global graph to capture overall structural relationships in point clouds. 2. A progressive context interaction module that enhances feature discriminability through intra-domain decoupling and inter-domain interaction. 3. A dynamic inlier selection method that optimizes inlier weights using residual information from iterative pose estimation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes MCI-Net, a novel network for point cloud registration that improves feature learning by integrating contextual cues from multiple domains. The method introduces modules for graph-based neighborhood aggregation, progressive context interaction, and dynamic inlier selection. Experiments show it achieves state-of-the-art performance, including a 96.4% registration recall on the 3DMatch dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MCI-Net: Point Cloud Registration] --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(现有方法依赖欧氏邻域/Existing methods rely on Euclidean neighborhoods)
        Problem --> P2(难以捕捉隐式语义和结构一致性/Struggle to capture implicit semantics and structural consistency)
        Method --> M1(图邻域聚合模块/Graph Neighborhood Aggregation Module)
        Method --> M2(渐进式上下文交互模块/Progressive Context Interaction Module)
        Method --> M3(动态内点选择方法/Dynamic Inlier Selection Method)
        Results --> R1(在室内外数据集上性能优越/Superior performance on indoor and outdoor datasets)
        Results --> R2(在3DMatch上达到96.4%的召回率/Achieves 96.4% recall on 3DMatch)
    ```

- **[arXiv251230] SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context**
  - **tags:** [cv], [correspondence learning], [adaptive focused regularization, bilateral field adjustment, position-aware recovery]
  - **authors:** Shuyuan Lin, Hailiang Liao, Qiang Qi, Junjie Huang, Taotao Lai, Jian Weng
  - **institution:** Jinan University, Qingdao University of Science and Technology, Minjiang University
  - **link:** https://arxiv.org/pdf/2512.23473
  - **code:** http://www.linshuyuan.com
  - **contributions:** 1. Proposed the Adaptive Focused Regularization (AFR) module to enhance position-awareness and robustness against spurious motion samples. 2. Proposed the Bilateral Field Adjustment (BFA) module to refine motion fields by modeling long-range relationships across spatial and channel dimensions. 3. Proposed a Position-Aware Recovery (PAR) module to ensure consistent and precise recovery of motion vectors from the refined field.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of CNN-based two-view correspondence learning failing to aggregate global context and oversmoothing motion fields. It proposes SC-Net, a novel network that integrates spatial and cross-channel context using three key modules (AFR, BFA, PAR). Experiments show SC-Net outperforms state-of-the-art methods on pose estimation and outlier removal tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SC-Net] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[CNN无法有效聚合全局上下文/CNN fails to aggregate global context]
        B --> B2[大视差场景中的运动场过平滑/Oversmoothing in large disparity scenes]
        C --> C1[自适应聚焦正则化模块/AFR Module]
        C --> C2[双边场调整模块/BFA Module]
        C --> C3[位置感知恢复模块/PAR Module]
        D --> D1[在YFCC100M和SUN3D上超越SOTA/Outperforms SOTA on YFCC100M & SUN3D]
        D --> D2[提升姿态估计和离群点去除/Improves pose estimation & outlier removal]
    ```

- **[arXiv251230] TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [temporal-aware retrieval, entropy-weighted sampling, training-free framework]
  - **authors:** Zongsheng Cao, Yangfan He, Anran Liu, Feng Chen, Zepeng Wang, Jun Xie
  - **institution:** Lenovo (PCIE), University of Minnesota (UMN)
  - **link:** https://arxiv.org/pdf/2512.23483
  - **code:** https://github.com/AI-Researcher-Team/TV-RAG
  - **contributions:** 1. A time-decay retrieval module that injects temporal offsets into similarity computation to rank queries by their true multimedia context. 2. An entropy-weighted key-frame sampler that selects information-dense frames to reduce redundancy while preserving representativeness. 3. A lightweight, training-free architecture that can be grafted onto any Large Video Language Model (LVLM) for improved long-video reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TV-RAG, a training-free framework to enhance long video understanding by Large Video Language Models (LVLMs). It introduces a temporal-aware retrieval module and an entropy-weighted frame sampler to better capture semantic shifts and temporal dependencies in long videos. The system outperforms leading baselines on multiple benchmarks without requiring model retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[LVLMs struggle with long videos: narrow temporal windows, miss fine-grained semantic shifts/LVLMs处理长视频困难：时间窗口窄，忽略细粒度语义变化]
        B --> B2[Text-based retrieval ignores temporal interdependence among multimodal channels/基于文本的检索忽略了多模态通道间的时间依赖性]
        C --> C1[Time-decay retrieval module/时间衰减检索模块]
        C --> C2[Entropy-weighted key-frame sampler/熵加权关键帧采样器]
        D --> D1[Surpasses leading baselines on Video-MME, MLVU, LongVideoBench/在Video-MME, MLVU, LongVideoBench上超越主流基线]
        D --> D2[Lightweight, training-free, graftable onto any LVLM/轻量级、无需训练、可适配任何LVLM]
    ```

- **[arXiv251230] Multi-label Classification with Panoptic Context Aggregation Networks**
  - **tags:** [cv], [multi-label classification], [context modeling, cross-scale aggregation, attention mechanism, geometric relationships, Hilbert space]
  - **authors:** Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu
  - **institution:** Zhengzhou University, Sorbonne University, Xiamen University
  - **link:** https://arxiv.org/pdf/2512.23486
  - **contributions:** 1. Proposes a novel Deep Panoptic Context Aggregation Network (PanCAN) that hierarchically integrates multi-order geometric contexts. 2. Introduces a method combining random walks with attention to learn multi-order neighborhood relationships in a high-dimensional Hilbert space. 3. Demonstrates effective cross-scale modeling by cascading modules and dynamically fusing salient anchor features, significantly improving complex scene understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing multi-label classification methods in modeling cross-scale contextual interactions. It proposes the Panoptic Context Aggregation Network (PanCAN), which hierarchically aggregates multi-order geometric contexts using attention and random walks in a Hilbert space. Experiments on standard benchmarks show PanCAN outperforms state-of-the-art methods, substantially improving classification performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-label Classification with Panoptic Context Aggregation Networks] --> B[核心问题/Problem: 现有方法忽略跨尺度上下文交互/Existing methods neglect cross-scale contextual interactions]
        A --> C[主要方法/Method: 提出PanCAN，在希尔伯特空间中通过注意力和随机游走进行跨尺度多阶上下文聚合/Proposes PanCAN, aggregates cross-scale multi-order contexts via attention & random walks in Hilbert space]
        A --> D[关键结果/Results: 在多个基准测试中优于SOTA，提升多标签分类性能/Outperforms SOTA on multiple benchmarks, improves multi-label classification performance]
    ```

- **[arXiv251230] IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation**
  - **tags:** [cv], [image generation], [identity-preserving generation, iterative identity discovery, re-denoising identity injection, human-centric story generation, character consistency]
  - **authors:** Donghao Zhou, Jingyu Lin, Guibao Shen, Quande Liu, Jialin Gao, Lihao Liu, Lan Du, Cunjian Chen, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng
  - **institution:** The Chinese University of Hong Kong, Monash University, The Hong Kong University of Science and Technology (Guangzhou), Kuaishou Technology, Amazon, South China University of Technology
  - **link:** https://arxiv.org/pdf/2512.23519
  - **code:** https://correr-zhou.github.io/IdentityStory
  - **contributions:** 1. Proposes IdentityStory, a framework for human-centric story generation that ensures consistent character identity across sequential images. 2. Introduces Iterative Identity Discovery to extract cohesive character identities. 3. Presents Re-denoising Identity Injection to inject identities into images while preserving the desired context.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of generating a series of images with consistent human characters from text prompts, a task known as human-centric story generation. It proposes the IdentityStory framework, which uses Iterative Identity Discovery and Re-denoising Identity Injection to tame identity-preserving generators. Experiments show it outperforms existing methods in maintaining face consistency and supports multi-character combinations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[IdentityStory: 人类中心故事生成 / Human-Centric Story Generation] --> B[核心问题 / Problem]
        A --> C[主要方法 / Method]
        A --> D[关键结果 / Results]
        B --> B1[保持角色身份一致性 / Maintaining Character Identity Consistency]
        B --> B2[协调多角色 / Coordinating Multiple Characters]
        C --> C1[迭代身份发现 / Iterative Identity Discovery]
        C --> C2[重去噪身份注入 / Re-denoising Identity Injection]
        D --> D1[优于现有方法 / Outperforms Existing Methods]
        D --> D2[支持多角色组合 / Supports Multi-Character Combinations]
    ```

- **[arXiv251230] Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution**
  - **tags:** [cv], [image super-resolution], [diffusion models, inference-time scaling, iterative refinement, frequency steering, training-free]
  - **authors:** Hexin Zhang, Dong Li, Jie Huang, Bingzhou Wang, Xueyang Fu, Zhengjun Zha
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.23532
  - **contributions:** 1. Proposes IAFS, a training-free framework that combines iterative refinement and frequency-aware particle fusion for diffusion-based super-resolution. 2. Introduces adaptive frequency steering to balance high-frequency perceptual quality and low-frequency structural fidelity. 3. Demonstrates through extensive experiments that IAFS effectively resolves the perception-fidelity conflict and outperforms existing inference-time scaling methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of balancing perceptual quality and structural fidelity in diffusion-based image super-resolution. It proposes IAFS, a training-free framework that uses iterative inference-time scaling with adaptive frequency steering to progressively refine images. Experiments show IAFS outperforms existing methods in achieving better detail and accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("感知质量与结构保真度冲突/Perception-Fidelity Conflict")
        Method --> M1("迭代推理时缩放/Iterative Inference-time Scaling")
        Method --> M2("自适应频率引导/Adaptive Frequency Steering")
        M1 --> M1_1("迭代细化/Iterative Refinement")
        M2 --> M2_1("频率感知粒子融合/Frequency-aware Particle Fusion")
        Results --> R1("解决感知-保真度冲突/Resolves Perception-Fidelity Conflict")
        Results --> R2("超越现有方法/Outperforms Existing Methods")
    ```

- **[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization**
  - **tags:** [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]
  - **authors:** Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang
  - **institution:** Zhejiang University, HKUST (The Hong Kong University of Science and Technology)
  - **link:** https://arxiv.org/pdf/2512.23537
  - **contributions:** 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AnyMS: 布局引导免训练多主体定制] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[多主体定制中文本对齐、身份保持与布局控制的平衡问题/Balancing text alignment, identity preservation, and layout control in multi-subject customization]
        C --> C1[提出免训练框架AnyMS/Proposes training-free framework AnyMS]
        C1 --> C2[引入自底向上双级注意力解耦机制/Introduces bottom-up dual-level attention decoupling]
        C2 --> C3[全局解耦确保文本对齐/Global decoupling ensures text alignment]
        C2 --> C4[局部解耦防止主体冲突/Local decoupling prevents subject conflicts]
        C --> C5[使用预训练图像适配器提取特征/Uses pre-trained image adapters for feature extraction]
        D --> D1[实现SOTA性能/Achieves SOTA performance]
        D --> D2[支持复杂组合与更多主体/Supports complex compositions and scales to more subjects]
    ```

- **[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis**
  - **tags:** [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]
  - **authors:** Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang
  - **institution:** Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University
  - **link:** https://arxiv.org/pdf/2512.23545
  - **contributions:** 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PathFound: Agentic Multimodal Model] --> B[核心问题/Problem: Static inference vs. clinical workflow]
        A --> C[主要方法/Method: Agentic model with VFM, VLM, RL]
        A --> D[关键结果/Results: SOTA accuracy, discovers subtle details]
        B --> B1[静态推理范式/Static inference paradigm]
        B --> B2[缺乏证据再获取/Lacks reassessment & evidence acquisition]
        C --> C1[多阶段诊断/Multi-stage diagnosis]
        C --> C2[主动信息获取/Proactive information acquisition]
        D --> D1[诊断准确性提升/Improved diagnostic accuracy]
        D --> D2[发现细微特征/Discover subtle pathological features]
    ```

- **[arXiv251230] PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation**
  - **tags:** [mlsys], [diffusion models], [text-to-image generation, prompt purification, semantic distance, null space projection, training-free safety]
  - **authors:** Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang
  - **institution:** University of Minnesota, Lenovo PCIE
  - **link:** https://arxiv.org/pdf/2512.23546
  - **code:** https://github.com/AI-Researcher-Team/PurifyGen
  - **contributions:** 1. Introduces a novel, training-free, dual-stage strategy for safe text-to-image generation that retains the original model weights. 2. Proposes a fine-grained risk discrimination method using complementary semantic distance to classify prompt tokens without keyword matching. 3. Develops a dual-space transformation for semantic purification, projecting risky embeddings into the null space of toxic concepts and the range space of clean concepts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes PurifyGen, a training-free method to make text-to-image generation safer. It works by first identifying risky tokens in a prompt using semantic distances and then purifying them by removing harmful semantic components while reinforcing safe ones. Experiments show it effectively reduces unsafe content across multiple datasets and competes with training-dependent approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PurifyGen: Safe Text-to-Image Generation] --> B[核心问题/Problem: Diffusion models risk generating unsafe content]
        A --> C[主要方法/Method: Dual-stage prompt purification via risk discrimination & semantic transformation]
        A --> D[关键结果/Results: Outperforms current methods, competes with training-dependent approaches]
    ```

- **[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature**
  - **tags:** [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]
  - **authors:** Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke
  - **institution:** DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University
  - **link:** https://arxiv.org/pdf/2512.23565
  - **contributions:** 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]
        Method[主要方法/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]
        Results[关键结果/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]
    ```

- **[arXiv251230] Instruction-Following Evaluation of Large Vision-Language Models**
  - **tags:** [ai], [instruction-following evaluation], [large vision-language models, visual instruction tuning, output format specification]
  - **authors:** Daiki Shiono, Shumpei Miyawaki, Ryota Tanaka, Jun Suzuki
  - **institution:** Tohoku University, NTT Corporation
  - **link:** https://arxiv.org/pdf/2512.23572
  - **contributions:** 1. Quantitatively demonstrates the decline in instruction-following ability of LVLMs after visual instruction fine-tuning. 2. Constructs new training datasets that highlight whether the output format is specified. 3. Shows that explicitly indicating the output format during fine-tuning helps LVLMs follow instructions more accurately.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies and quantifies a problem where Large Vision-Language Models (LVLMs) lose their instruction-following ability after visual instruction tuning. The authors propose constructing datasets that explicitly specify the output format and find that training with such data mitigates the performance decline. The main conclusion is that including instructions on output format during fine-tuning can help preserve LVLMs' instruction-following capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Instruction-Following Evaluation of Large Vision-Language Models] --> B(核心问题/Problem: LVLMs lose instruction-following ability after fine-tuning)
        A --> C(主要方法/Method: Construct datasets highlighting output format specification)
        A --> D(关键结果/Results: Explicit output format instructions improve instruction-following)
    ```

- **[arXiv251230] ThinkGen: Generalized Thinking for Visual Generation**
  - **tags:** [cv], [text-to-image generation], [Chain-of-Thought (CoT), Multimodal Large Language Model (MLLM), Diffusion Transformer (DiT), reinforcement learning, SepGRPO]
  - **authors:** Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei
  - **institution:** Beijing Jiaotong University, Bytedance
  - **link:** https://arxiv.org/pdf/2512.23568
  - **code:** https://github.com/jiaosiyuu/ThinkGen
  - **contributions:** 1. Proposes ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning for various generation tasks. 2. Introduces a decoupled architecture using a pretrained MLLM to generate instructions and a DiT for image synthesis. 3. Proposes a separable GRPO-based training paradigm (SepGRPO) for alternating reinforcement learning between modules, enabling joint training across diverse datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ThinkGen, a framework that integrates Chain-of-Thought reasoning from Multimodal LLMs with a Diffusion Transformer for visual generation. It uses a decoupled architecture and a novel separable reinforcement learning training method to generalize across diverse generation scenarios. Experiments show it achieves state-of-the-art performance on multiple benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ThinkGen: Generalized Thinking for Visual Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[CoT推理在生成任务中应用有限/CoT for generation is nascent and scenario-specific]
        C --> C1[解耦架构: MLLM + DiT/Decoupled architecture: MLLM + DiT]
        C --> C2[可分离GRPO训练范式/SepGRPO training paradigm]
        D --> D1[在多个基准上实现SOTA/Achieves SOTA across multiple benchmarks]
    ```

- **[arXiv251230] ProGuard: Towards Proactive Multimodal Safeguard**
  - **tags:** [ai], [multimodal safety], [proactive guard, out-of-distribution (OOD) detection, reinforcement learning (RL), multimodal safety taxonomy, synonym-bank reward]
  - **authors:** Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao
  - **institution:** Shanghai Artificial Intelligence Laboratory, Nanjing University, Beihang University
  - **link:** https://arxiv.org/pdf/2512.23573
  - **code:** https://yushaohan.github.io/ProGuard
  - **contributions:** 1. Introduces ProGuard, a vision-language proactive guard model that identifies and describes out-of-distribution safety risks without requiring model adjustments. 2. Constructs a modality-balanced dataset of 87K samples with binary safety labels and hierarchical risk categories to mitigate modality bias. 3. Trains the model purely via reinforcement learning augmented with a synonym-bank-based similarity reward to enhance OOD risk inference and description.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ProGuard, a proactive multimodal safeguard that uses reinforcement learning on a balanced dataset to detect and describe unseen safety risks. It achieves performance comparable to closed-source models on safety classification and significantly improves OOD risk detection and description by over 50%.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ProGuard: Towards Proactive Multimodal Safeguard] --> B[核心问题/Problem: 生成模型快速发展带来持续的多模态安全风险，现有防御方法存在局限。]
        A --> C[主要方法/Method: 提出ProGuard，基于强化学习训练视觉语言基础模型，引入OOD类别推断任务和同义词库奖励。]
        A --> D[关键结果/Results: 在二元安全分类上媲美闭源大模型，OOD风险检测提升52.6%，描述提升64.8%。]
    ```

- **[arXiv251230] Image Denoising Using Global and Local Circulant Representation**
  - **tags:** [cv], [image denoising], [circulant representation, tensor-SVD, Haar transform]
  - **authors:** Zhaoming Kong, Xiaowei Yang, Jiahuan Zhang
  - **institution:** South China University of Technology, Guangdong Provincial People's Hospital, Southern Medical University
  - **link:** https://arxiv.org/pdf/2512.23569
  - **code:** https://github.com/ZhaomingKong/Haar-tSVD
  - **contributions:** 1. Established a theoretical connection between PCA and the Haar transform under circulant representation. 2. Proposed a computationally simple, one-step plug-and-play denoiser (Haar-tSVD) that balances speed and performance by capturing global and local correlations. 3. Introduced an adaptive noise estimation scheme and integrated deep neural networks to enhance robustness under severe noise conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new image denoising method called Haar-tSVD, which combines tensor singular value decomposition with the Haar transform to efficiently remove noise. It is designed as a fast, parallelizable algorithm that does not require learning and can be integrated with deep networks for better performance. Experiments show the method is effective and efficient for noise removal across various datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Image Denoising Using Global and Local Circulant Representation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: High demand for efficient and effective image denoising] --> P1[挑战/Challenge: Noise degrades image quality]
        Method[主要方法/Method: Haar-tSVD] --> M1[理论/Theoretical: Connect PCA and Haar transform]
        Method --> M2[算法/Algorithm: Unified t-SVD projection with Haar transform]
        Method --> M3[增强/Enhancement: Adaptive noise estimation & DNN integration]
        Results[关键结果/Results: Efficient and effective noise removal] --> R1[验证/Validation: Experiments on various datasets]
    ```

- **[arXiv251230] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation**
  - **tags:** [mlsys], [multi-modal inference], [on-policy distillation, real-time video diffusion, multimodal conditioning, Anchor-Heavy Identity Sinks, autoregressive generation]
  - **authors:** Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu
  - **institution:** SII, SJTU, GAIR
  - **link:** https://arxiv.org/pdf/2512.23576
  - **code:** /githubCode
  - **contributions:** 1. Proposes an improved on-policy distillation recipe for real-time multimodal video diffusion, addressing issues like flickering and quality degradation in prior methods. 2. Develops LiveTalk, a real-time interactive avatar system integrating the distilled model with audio language models and long-form video inference techniques. 3. Demonstrates 20x reduction in inference cost/latency while matching baseline quality, and outperforms SOTA in multi-turn coherence and content quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of real-time interactive video generation by improving on-policy distillation for multimodal-conditioned diffusion models. The proposed method enhances training stability and output quality, enabling a 20x speedup. The resulting LiveTalk system achieves real-time, coherent multi-turn avatar interactions, significantly outperforming existing models in latency and quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 扩散模型双向注意力迭代过程阻碍实时交互 / Diffusion model's iterative bidirectional attention prevents real-time interaction]
        C[主要方法/Method: 改进的按策略蒸馏，强调条件输入质量与优化计划 / Improved on-policy distillation, emphasizing condition input quality & optimization schedule]
        D[关键结果/Results: 20倍加速，实时延迟，多轮交互质量超越SOTA / 20x acceleration, real-time latency, multi-turn quality surpasses SOTA]
    ```

- **[arXiv251230] Same or Not? Enhancing Visual Perception in Vision-Language Models**
  - **tags:** [cv], [vision-language models], [fine-grained visual understanding, dataset, benchmark, visual perception, image-pair queries]
  - **authors:** Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari
  - **institution:** California Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.23592
  - **contributions:** 1. Introduced TWIN, a large-scale dataset of 561,000 image-pair queries designed to train VLMs on fine-grained visual perception by determining if two similar images depict the same object. 2. Introduced FGVQA, a benchmark suite of 12,000 queries to evaluate fine-grained VQA capabilities across multiple domains. 3. Demonstrated that fine-tuning VLMs on TWIN significantly improves their fine-grained recognition on FGVQA (up to 19.3%) without harming general VQA performance, and showed the importance of dataset scale.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of Vision-Language Models (VLMs) in fine-grained visual perception. It proposes a new training dataset (TWIN) and a benchmark (FGVQA) to enhance VLMs' ability to notice subtle visual details. The results show that fine-tuning on TWIN significantly improves fine-grained recognition on unseen domains without compromising general performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Same or Not? Enhancing Visual Perception in Vision-Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLMs lack fine-grained perception, miss subtle details 视觉语言模型缺乏细粒度感知，忽略细微差别]
        C[主要方法/Method: Introduce TWIN dataset & FGVQA benchmark 引入TWIN数据集和FGVQA基准]
        D[关键结果/Results: Fine-tuning on TWIN improves fine-grained recognition by up to 19.3% 在TWIN上微调将细粒度识别提升高达19.3%]
    ```

- **[arXiv251230] Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging**
  - **tags:** [cv], [medical image analysis], [Scalable Residual Feature Aggregation (SRFA), Hybrid Metaheuristic Optimization (HHO-BA), Vision Transformer (ViT) with EfficientNet-B3]
  - **authors:** Janani Annur Thiruvengadam, Kiran Mayee Nabigaru, Anusha Kovi
  - **institution:** Amazon.com Services LLC
  - **link:** https://arxiv.org/pdf/2512.23597
  - **contributions:** 1. Proposes a Scalable Residual Feature Aggregation (SRFA) framework integrating MAGRes-UNet for segmentation and DenseNet-121 for hierarchical feature extraction. 2. Introduces a hybrid HHO-BA metaheuristic feature selection strategy to refine the optimal feature subset. 3. Develops a novel hybrid classifier combining Vision Transformer (ViT) and EfficientNet-B3, fine-tuned using a dual SSA-GWO optimization mechanism for robust classification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of early pancreatic neoplasm detection in multimodal CT imaging by proposing a Scalable Residual Feature Aggregation (SRFA) framework. The method combines advanced segmentation, feature extraction with residual storage, hybrid metaheuristic feature selection, and a novel ViT-EfficientNet-B3 classifier optimized with SSA and GWO. The proposed system achieves high performance (96.23% accuracy), demonstrating significant improvement over traditional and contemporary models for robust early detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 胰腺肿瘤早期检测困难/Early pancreatic neoplasm detection is difficult due to subtle, low-contrast lesions and high patient variability in CT scans.]
        C[主要方法/Method: SRFA框架整合MAGRes-UNet分割、DenseNet-121特征提取、HHO-BA特征选择、ViT-EfficientNet-B3混合分类器，并使用SSA-GWO优化/SRFA framework integrates MAGRes-UNet segmentation, DenseNet-121 feature extraction, HHO-BA feature selection, ViT-EfficientNet-B3 hybrid classifier, optimized with SSA-GWO.]
        D[关键结果/Results: 模型达到96.23%准确率，性能显著优于传统CNN和当前基于Transformer的模型/Model achieves 96.23% accuracy, significantly outperforming traditional CNNs and contemporary transformer-based models.]
    ```

- **[arXiv251230] Detection Fire in Camera RGB-NIR**
  - **tags:** [cv], [object detection], [YOLOv11, EfficientNetV2, two-stage detection, NIR dataset, Patched-YOLO]
  - **authors:** Nguyen Truong Khai, Luong Duc Vinh
  - **institution:** Viettel (inferred from email domain vinhld@viettel.com)
  - **link:** https://arxiv.org/pdf/2512.23594
  - **contributions:** 1. An additional NIR dataset with various data augmentation strategies to address data scarcity. 2. A two-stage detection pipeline combining YOLOv11 and EfficientNetV2-B0 to improve night-time fire detection accuracy and reduce false positives from artificial lights. 3. Patched-YOLO, a patch-based processing method to enhance detection of small and distant fire objects in RGB images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of improving fire detection accuracy, especially at night using NIR cameras and for small objects in RGB images. It proposes a two-stage model (YOLOv11 + EfficientNetV2-B0) for NIR detection and a Patched-YOLO method for RGB, alongside an augmented NIR dataset. The proposed approaches aim to achieve higher accuracy and reduce false positives compared to previous methods.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Detection Fire in Camera RGB-NIR] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[夜间红外火焰检测准确性低 / Low accuracy in nighttime NIR fire detection]
    B --> B2[误将亮光识别为火焰 / Misclassification of bright lights as fire]
    B --> B3[RGB图像中小型/远距离火焰检测难 / Difficulty detecting small/distant fire in RGB]
    C --> C1[新增NIR数据集 / Additional NIR Dataset]
    C --> C2[两阶段检测模型 / Two-Stage Detection Model]
    C --> C3[Patched-YOLO / Patched-YOLO]
    D --> D1[检测精度超越先前模型 / Detection accuracy surpasses previous models]
    ```

- **[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study**
  - **tags:** [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]
  - **authors:** Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu
  - **institution:** Princeton University, Harvard University
  - **link:** https://arxiv.org/pdf/2512.23628
  - **code:** github.com/zlab-princeton/3d_mem
  - **contributions:** 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Memorization in 3D Shape Generation: An Empirical Study] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(3D生成模型是否记忆训练数据?/Do 3D generative models memorize training data?)
        C --> C1(设计量化框架/Design evaluation framework)
        C --> C2(使用Vecset扩散模型进行控制实验/Use Vecset diffusion model for controlled experiments)
        D --> D1(数据多样性和细粒度条件增加记忆/Data diversity & fine-grained conditioning increase memorization)
        D --> D2(适度引导规模峰值记忆/Moderate guidance scale peaks memorization)
        D --> D3(更长Vecsets和旋转增强可缓解记忆/Longer Vecsets & rotation augmentation mitigate memorization)
    ```

- **[arXiv251230] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding**
  - **tags:** [mlsys], [agent system], [active perception, audio-guided, tool orchestration, coarse-to-fine perception, multimodal alignment]
  - **authors:** Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang
  - **institution:** Zhejiang University, Westlake University, Ant Group
  - **link:** https://arxiv.org/pdf/2512.23646
  - **code:** https://kd-tao.github.io/OmniAgent
  - **contributions:** 1. Introduces OmniAgent, an audio-guided active perception agent that shifts from passive response to active multimodal inquiry. 2. Proposes a novel coarse-to-fine audio-guided perception paradigm that uses audio cues to localize events and guide reasoning. 3. Demonstrates state-of-the-art performance on audio-video benchmarks, outperforming leading models by 10%-20% accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of fine-grained cross-modal understanding in omnimodal LLMs by proposing OmniAgent, an active perception agent that dynamically orchestrates tools using audio cues to guide video analysis. It achieves superior performance on audio-video understanding benchmarks, significantly outperforming existing models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OmniAgent: Audio-Guided Active Perception Agent] --> B[核心问题/Problem: Omnimodal LLMs lack fine-grained cross-modal understanding and multimodal alignment]
        A --> C[主要方法/Method: Audio-guided active perception agent with dynamic tool orchestration and coarse-to-fine perception]
        A --> D[关键结果/Results: Achieves SOTA, outperforms leading models by 10%-20% accuracy]
    ```

- **[arXiv251230] Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception**
  - **tags:** [cv], [3D object detection and tracking], [spatio-temporal alignment, multi-hypothesis decoding, motion modeling, end-to-end perception, autonomous driving]
  - **authors:** Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun
  - **institution:** Harbin Institute of Technology, IROOTECH
  - **link:** https://arxiv.org/pdf/2512.23635
  - **code:** https://github.com/lixiaoyu2000/HAT
  - **contributions:** 1. Proposes HAT, a novel spatio-temporal alignment module that adaptively decodes optimal alignment from multiple motion hypotheses without direct supervision. 2. Integrates both explicit motion models and semantic cues to address suboptimal alignment caused by varying object motion states and features. 3. Demonstrates consistent improvements in 3D perception and tracking performance across diverse baselines and enhances robustness in end-to-end autonomous driving systems, especially under semantic corruption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that existing spatio-temporal alignment methods in end-to-end 3D perception for autonomous driving are suboptimal due to simplified motion models. It proposes HAT, a module that generates multiple motion-aware feature proposals and adaptively selects the best alignment using semantic and motion cues. HAT improves detection and tracking performance on benchmarks and enhances the robustness of end-to-end autonomous driving systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>现有对齐方法因简化运动模型而次优<br>Existing alignment is suboptimal due to simplified motion models] --> P1[变化导致挑战/Variations in motion states and features pose challenges]
        Method[主要方法/Method<br>提出HAT模块/Propose HAT module] --> M1[多假设生成/Multi-hypothesis generation using explicit models]
        Method --> M2[自适应解码/Adaptive decoding with semantic & motion cues]
        Results[关键结果/Results<br>性能提升与鲁棒性增强/Performance improvement & enhanced robustness] --> R1[SOTA跟踪结果/SOTA tracking results (46.0% AMOTA)]
        Results --> R2[提升端到端系统/Improves E2E AD perception and reduces collisions]
    ```

- **[arXiv251230] RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion**
  - **tags:** [ai], [robot learning], [video-to-locomotion, visual motion intent, diffusion policy]
  - **authors:** Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang
  - **institution:** Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23649
  - **contributions:** 1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework's effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RoboMirror: Video to Humanoid Locomotion] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[视觉理解与控制存在鸿沟/Gap between visual understanding and control]
        B --> B2[现有方法缺乏真正的视觉理解/Existing methods lack genuine visual understanding]
        C --> C1[利用VLM提取视觉运动意图/Use VLMs to distill visual motion intents]
        C --> C2[基于扩散的策略生成动作/Diffusion-based policy generates actions]
        C --> C3[无需姿态重建或重定向/No explicit pose reconstruction or retargeting]
        D --> D1[支持第一与第三人称视频控制/Supports egocentric & third-person control]
        D --> D2[降低80%控制延迟/Reduces control latency by 80%]
        D --> D3[任务成功率提升3.7%/3.7% higher task success rate]
    ```

- **[arXiv251230] IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition**
  - **tags:** [cv], [intrinsic image decomposition], [multi-view consistency, transformer, physically grounded model, feed-forward inference, specular shading]
  - **authors:** Kang Du, Yirui Guan, Zeyu Wang
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Tencent
  - **link:** https://arxiv.org/pdf/2512.23667
  - **contributions:** 1. Proposes IDT, a feed-forward transformer framework for multi-view intrinsic decomposition that directly outputs view-consistent factors without iterative sampling. 2. Introduces a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. 3. Demonstrates superior performance in producing cleaner decompositions and better multi-view consistency compared to prior methods on synthetic and real-world datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of view inconsistency in multi-view intrinsic image decomposition. It proposes IDT, a feed-forward transformer that jointly processes multiple views using a physically grounded model to decompose images into reflectance and shading components. Experiments show IDT achieves more consistent and interpretable decompositions than previous methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition") --> Problem("核心问题/Problem: Multi-view intrinsic decomposition suffers from view inconsistency")
        Root --> Method("主要方法/Method: Feed-forward transformer with a physically grounded model for joint multi-view reasoning")
        Root --> Results("关键结果/Results: Achieves view-consistent, cleaner decompositions (diffuse reflectance, diffuse shading, specular shading)")
    ```

- **[arXiv251230] Web World Models**
  - **tags:** [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]
  - **authors:** Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang
  - **institution:** Princeton University, University of California, Los Angeles, University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.23676
  - **code:** https://princeton-ai2-lab.github.io/Web-World-Models/
  - **contributions:** 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world "physics" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Web World Models] --> B["核心问题/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models)."]
        A --> C["主要方法/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top."]
        A --> D["关键结果/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs."]
    ```

- **[arXiv251230] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation**
  - **tags:** [cv], [depth estimation], [video diffusion, transparent objects, LoRA, depth estimation, normal estimation]
  - **authors:** Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao
  - **institution:** Beijing Academy of Artificial Intelligence, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.23705
  - **code:** https://daniellli.github.io/projects/DKT/
  - **contributions:** 1. Introduces TransPhy3D, a large-scale synthetic video dataset for transparent/reflective scenes with RGB, depth, and normal maps. 2. Proposes DKT, a method that repurposes a pre-trained video diffusion model via lightweight LoRA adapters for temporally consistent depth and normal estimation from videos. 3. Demonstrates state-of-the-art zero-shot performance on real and synthetic benchmarks and shows practical improvement in robotic grasping tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenging problem of depth and normal estimation for transparent and reflective objects in videos. The proposed method, DKT, repurposes a pre-trained video diffusion model using LoRA adapters and a novel synthetic dataset (TransPhy3D) to achieve temporally consistent predictions. The results show state-of-the-art zero-shot performance on benchmarks and improved robotic manipulation, supporting the claim that generative video priors effectively capture the physics of transparency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Transparent/reflective objects break assumptions of traditional depth sensing, causing holes and instability."]
        Method["主要方法/Method<br>Repurpose video diffusion model with LoRA adapters, trained on new synthetic dataset TransPhy3D."]
        Results["关键结果/Results<br>Achieves SOTA zero-shot performance on benchmarks and improves robotic grasping success."]
    ```

- **[arXiv251230] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion**
  - **tags:** [cv], [video super-resolution], [diffusion models, online processing, low-latency, auto-regressive, temporal guidance]
  - **authors:** Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu
  - **institution:** National Yang Ming Chiao Tung University, Shanda AI Research Tokyo, MediaTek Inc.
  - **link:** https://arxiv.org/pdf/2512.23709
  - **code:** https://jamichss.github.io/stream-diffvsr-project-page/
  - **contributions:** 1. A causally conditioned diffusion framework for online VSR that operates strictly on past frames, enabling low-latency deployment. 2. An Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising to enhance temporal coherence. 3. A lightweight temporal-aware decoder with a Temporal Processor Module (TPM) and a four-step distilled denoiser, achieving fast inference (0.328s per 720p frame) while maintaining high perceptual quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the impracticality of diffusion-based video super-resolution (VSR) for low-latency applications by proposing Stream-DiffVSR, an online framework that uses causal conditioning, a distilled denoiser, and novel temporal modules. The method significantly reduces latency to 0.328 seconds per frame while improving perceptual quality, making it the first diffusion VSR approach suitable for real-time online deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Stream-DiffVSR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[扩散VSR延迟高/Diffusion VSR has high latency]
        B --> B2[依赖未来帧/Relies on future frames]
        C --> C1[因果条件扩散/Causally Conditioned Diffusion]
        C --> C2[自回归时序引导/Auto-regressive Temporal Guidance (ARTG)]
        C --> C3[四步蒸馏去噪器/4-step Distilled Denoiser]
        D --> D1[延迟0.328秒/Latency 0.328s per frame]
        D --> D2[感知质量提升/Improved perceptual quality (LPIPS)]
        D --> D3[首个在线扩散VSR/First online diffusion VSR]
    ```

- **[arXiv251230] AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings**
  - **tags:** [cv], [medical image classification], [MobileNetV2, radiomics, Grad-CAM, RandomForest, feature fusion]
  - **authors:** Areeb Ehsan
  - **institution:** Georgia State University
  - **link:** https://arxiv.org/pdf/2512.22184
  - **contributions:** 1. Proposes a hybrid "virtual biopsy" pipeline combining a lightweight CNN (MobileNetV2) with handcrafted radiomics features for brain tumor classification. 2. Employs a late fusion strategy using a RandomForest classifier on the concatenated CNN embeddings and radiomics features to improve performance. 3. Provides model explainability through Grad-CAM visualizations and radiomics feature importance analysis, and evaluates robustness under low-resolution and noisy imaging conditions relevant to low-resource settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of brain tumor diagnosis in low-resource settings by proposing a virtual biopsy pipeline. The method combines a lightweight CNN with interpretable radiomics features and fuses them using a RandomForest classifier. Experiments show the fusion approach improves classification performance and the analysis highlights its sensitivity to image quality issues common in resource-constrained environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings"] --> Problem["核心问题/Problem: Brain tumor diagnosis is difficult in low-resource settings due to limited expert access, hardware, and invasive biopsies."]
        Root --> Method["主要方法/Method: Uses a hybrid pipeline with a lightweight CNN (MobileNetV2) and radiomics features, fused via RandomForest."]
        Root --> Results["关键结果/Results: Fusion improves performance; robustness tests reveal sensitivity to low-quality images relevant to low-resource environments."]
    ```

- **[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images**
  - **tags:** [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]
  - **authors:** Haozhe Jia
  - **institution:** Boston University
  - **link:** https://arxiv.org/pdf/2512.22209
  - **contributions:** 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis."]
        Method["主要方法/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images."]
        Results["关键结果/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy."]
    ```

- **[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging**
  - **tags:** [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]
  - **authors:** Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger
  - **institution:** Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology
  - **link:** https://arxiv.org/pdf/2512.22176
  - **contributions:** 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>磁共振成像深度学习分析中场强依赖的性能变异性") --> Problem
        Root --> Method
        Root --> Results
    
        Problem("核心问题/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI场强对深度学习模型性能与泛化能力的影响")
        Method("主要方法/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>在1.5T、3.0T及混合数据上训练/评估nnU-Net模型；使用UMAP和影像组学分析")
        Results("关键结果/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>训练数据中的场强显著影响性能，尤其对软组织")
    ```

- **[arXiv251230] Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction**
  - **tags:** [cv], [medical image reconstruction], [Swin Transformer, complex-valued network, k-space undersampling, super-resolution, Parkinson's disease]
  - **authors:** Muhammad Usman, Sung-Min Gho
  - **institution:** Stanford University, DeepNoid Inc.
  - **link:** https://arxiv.org/pdf/2512.22202
  - **contributions:** 1. Proposed a novel complex-valued Swin Transformer network for super-resolution reconstruction of multi-echo MRI data. 2. Demonstrated high-quality SMWI reconstruction from low-resolution/undersampled k-space data, significantly reducing required scan time. 3. Validated the method's ability to preserve critical diagnostic features for Parkinson's Disease, enhancing clinical applicability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the long scan time problem of Susceptibility Map Weighted Imaging (SMWI) for Parkinson's disease diagnosis. The authors propose a complex-valued Swin Transformer network to reconstruct high-quality SMWI images from reduced k-space data. Experimental results show the method achieves high reconstruction quality (SSIM 0.9116) while preserving diagnostic details, enabling faster clinical scans.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>SMWI full scan time too long] --> P1[关键限制/Key Limitation<br>Long scan time limits clinical use]
        Method[主要方法/Method<br>Complex Swin Transformer Network] --> M1[技术核心/Technical Core<br>Super-resolve multi-echo MRI data]
        Results[关键结果/Results<br>High-quality reconstruction from reduced data] --> R1[量化指标/Quantitative Metrics<br>SSIM: 0.9116, MSE: 0.076]
        Results --> R2[临床价值/Clinical Value<br>Preserves diagnostic features, faster scan]
    ```

- **[arXiv251230] MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression**
  - **tags:** [cv], [point cloud compression], [Mamba, end-to-end learning, joint geometry-attribute compression, entropy model, rate-distortion optimization]
  - **authors:** Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang
  - **institution:** National Chung Cheng University, National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.22463
  - **contributions:** 1. Proposes MEGA-PCC, a fully end-to-end learning-based framework for joint point cloud geometry and attribute compression that eliminates the need for post-hoc recoloring and manual bitrate tuning. 2. Introduces a Mamba-based Entropy Model (MEM) that captures spatial and channel-wise correlations to improve probability estimation for entropy coding. 3. Employs a shared encoder with dual decoders built on the Mamba architecture to model long-range dependencies, enabling data-driven bitrate allocation and superior rate-distortion performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing point cloud compression methods, which rely on complex post-processing and manual bitrate allocation. The authors propose MEGA-PCC, an end-to-end framework using a Mamba-based shared encoder and a specialized entropy model for joint geometry and attribute compression. Experiments show the method outperforms traditional and learning-based baselines in both performance and efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法依赖后处理与手动码率分配/Existing methods rely on recoloring & manual bit allocation]
        C --> C1[端到端联合压缩框架/End-to-end joint compression framework]
        C1 --> C2[共享编码器与双解码器/Shared encoder & dual decoders]
        C1 --> C3[Mamba熵模型/Mamba-based Entropy Model (MEM)]
        D --> D1[优异的率失真性能/Superior rate-distortion performance]
        D --> D2[更高的运行时效率/Higher runtime efficiency]
    ```

- **[arXiv251230] Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction**
  - **tags:** [cv], [medical image reconstruction], [semantic contrastive learning, orthogonal CT, U-Net, GAN, sparse-view reconstruction]
  - **authors:** Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao
  - **institution:** Beihang University
  - **link:** https://arxiv.org/pdf/2512.22674
  - **contributions:** 1. Proposes a novel semantic feature contrastive learning loss function for CT reconstruction, 2. Introduces a three-stage U-Net-based architecture for coarse reconstruction, detail refinement, and semantic similarity measurement, 3. Demonstrates superior reconstruction quality and faster processing on a chest dataset with orthogonal projections.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of streak artifacts in sparse-view X-ray CT reconstruction by proposing a novel semantic contrastive learning loss and a three-stage U-Net architecture. The method improves image quality and processing speed compared to other algorithms, offering a practical solution for orthogonal CT reconstruction.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction<br/>基于语义对比学习的正交X射线CT重建"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br/>Sparse-view CT leads to streak artifacts<br/>稀疏视图CT导致条纹伪影"] --> P1["Ill-posed reconstruction<br/>病态重建"]
        Method["主要方法/Method<br/>Semantic contrastive learning & three-stage U-Net<br/>语义对比学习与三阶段U-Net"] --> M1["Novel loss function<br/>新颖损失函数"]
        Method --> M2["Coarse & detail refinement<br/>粗重建与细节优化"]
        Results["关键结果/Results<br/>Superior quality & faster processing<br/>更优质量与更快处理"] --> R1["Improved image quality<br/>提升图像质量"]
        Results --> R2["Low computational complexity<br/>低计算复杂度"]
    ```

- **[arXiv251230] JParc: Joint cortical surface parcellation with registration**
  - **tags:** [cv], [medical image segmentation], [cortical parcellation, surface registration, atlas propagation, deep learning, geometric features]
  - **authors:** Jian Li, Karthik Gopinath, Brian L. Edlow, Adrian V. Dalca, Bruce Fischl
  - **institution:** Athinoula A. Martinos Center for Biomedical Imaging (MGH & HMS), MIT Computer Science and Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.22485
  - **contributions:** 1. Proposes JParc, a novel joint framework that integrates cortical surface registration and parcellation into a single learning-based model. 2. Demonstrates that the performance improvement is primarily due to accurate registration and a learned parcellation atlas, providing an explanation for the success of learning-based methods. 3. Achieves state-of-the-art parcellation accuracy (Dice &gt;90%) on the Mindboggle dataset using only basic geometric features like sulcal depth and curvature.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces JParc, a joint framework for cortical surface registration and parcellation. It shows that combining these tasks and learning an atlas leads to superior performance, achieving over 90% Dice score on a standard dataset using simple geometric features. This accuracy can enhance brain mapping studies and clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[JParc: Joint cortical surface parcellation with registration] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[学习与注册分离/Learning vs. Registration Gap]
        Problem --> P2[需要自动精准分割/Need for Accurate Automated Parcellation]
        Method --> M1[联合框架/Joint Registration & Parcellation Framework]
        Method --> M2[使用浅层子网络微调/Shallow Subnetwork for Fine-tuning]
        Method --> M3[仅用几何特征/Using Basic Geometric Features]
        Results --> R1[Dice分数 >90%/Dice Score >90%]
        Results --> R2[性能归因于注册与图谱/Performance Attributed to Registration & Atlas]
        Results --> R3[提升下游应用/Enhances Downstream Applications]
    ```

- **[arXiv251230] SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction**
  - **tags:** [cv], [medical imaging reconstruction], [Compton camera, Swin Transformer, end-to-end reconstruction, list-mode data, transposed convolution]
  - **authors:** Minghao Dong, Xinyang Luo, Xujian Ouyang, Yongshun Xiao
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.22766
  - **contributions:** 1. Proposes SwinCCIR, a novel end-to-end deep learning framework for Compton camera imaging that directly maps list-mode events to source distribution, bypassing traditional back-projection. 2. Introduces the use of Swin Transformer blocks to model the complex relationships in the data, combined with a transposed convolution-based image generation module. 3. Demonstrates the method's effectiveness on both simulated and practical datasets, showing it overcomes artifacts and deformations inherent in conventional reconstruction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes SwinCCIR, an end-to-end deep learning model using Swin Transformer blocks and transposed convolutions to directly reconstruct images from Compton camera list-mode data. The method bypasses the problematic back-projection step of traditional approaches. Experiments on simulated and real data show it effectively reduces artifacts and deformations, improving image quality for practical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SwinCCIR: 康普顿相机成像重建网络] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统反投影导致严重伪影和变形/Back-projection causes severe artifacts & deformation]
        B --> B2[系统误差难以校准/Systematic errors hard to calibrate]
        C --> C1[采用Swin Transformer块/Adopts Swin Transformer blocks]
        C --> C2[转置卷积图像生成模块/Transposed convolution generation module]
        C --> C3[建立列表模式事件到源分布的映射/Establishes mapping from list-mode events to source distribution]
        D --> D1[在仿真和真实数据集上验证/Validated on simulated & practical dataset]
        D --> D2[有效克服传统成像问题/Effectively overcomes conventional imaging problems]
    ```

- **[arXiv251230] A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard**
  - **tags:** [cv], [remote sensing image segmentation], [GeoSAM, glacier delineation, multi-temporal mapping, Sentinel-2, spectral-index]
  - **authors:** Alexandru Hegyi
  - **institution:** University of Oslo
  - **link:** https://arxiv.org/pdf/2512.22855
  - **contributions:** 1. Proposes a semi-automatic workflow combining GeoSAM with spectral-index pre-processing for rapid glacier mapping. 2. Demonstrates the method's effectiveness for generating temporally consistent glacier outlines in a challenging Arctic environment (Svalbard). 3. Highlights the workflow's flexibility and transferability to other optical datasets due to its reliance on derived RGB imagery.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp
  - **Simple LLM Summary:** This report presents a semi-automatic workflow for rapidly delineating glaciers from satellite imagery. The method uses GeoSAM guided by spectral-index prompts on Sentinel-2 data to create annual glacier outlines. The results show the approach is fast and produces consistent maps for major glaciers, though it requires some user inspection for refinement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard] --> B[核心问题/Problem: Consistent glacier boundary delineation is essential but difficult to scale across time and environments.]
        A --> C[主要方法/Method: Combines image compositing, spectral-index pre-processing, prompt-guided GeoSAM segmentation, and post-processing.]
        A --> D[关键结果/Results: Produces spatially coherent and temporally consistent outlines; errors are associated with small, complex features; method is fast and transferable.]
    ```

- **[arXiv251230] Deep Learning for Art Market Valuation**
  - **tags:** [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]
  - **authors:** Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang
  - **institution:** Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy
  - **link:** https://arxiv.org/pdf/2512.23078
  - **contributions:** 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]
        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]
        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]
    ```

- **[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation**
  - **tags:** [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]
  - **authors:** Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu
  - **institution:** University of Science and Technology of China, PAII Inc., Beijing University of Technology
  - **link:** https://arxiv.org/pdf/2512.23185
  - **contributions:** 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[EIR: Enhanced Image Representations for Medical Report Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[报告生成耗时耗力/Report generation is time-consuming]
        Problem --> P2[信息不对称与领域鸿沟/Information asymmetry & domain gap]
        Method[主要方法/Method] --> M1[跨模态Transformer融合元数据/Cross-modal transformer for metadata fusion]
        Method --> M2[医学领域预训练模型/Medical domain pre-trained model]
        Results[关键结果/Results] --> R1[在MIMIC和Open-I数据集上验证/Validated on MIMIC & Open-I datasets]
    ```

## 2026-01-01

- **[arXiv260101] Break Out the Silverware -- Semantic Understanding of Stored Household Items**
  - **tags:** [ai], [commonsense reasoning], [benchmark dataset, vision-language model, hybrid agent pipeline, storage location prediction, semantic understanding]
  - **authors:** Michaela Levi-Richter, Reuth Mirsky, Oren Glickman
  - **institution:** Bar Ilan University, Tufts University
  - **link:** https://arxiv.org/pdf/2512.23739
  - **contributions:** 1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]
        Method[主要方法/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]
        Results[关键结果/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]
    ```

- **[arXiv260101] A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit**
  - **tags:** [ai], [subspace clustering], [Schubert Variety, Grassmann Manifold, Linde-Buzo-Grey (LBG), Subspace Clustering, Geometric Learning]
  - **authors:** Karim Salta, Michael Kirby, Chris Peterson
  - **institution:** Colorado State University
  - **link:** https://arxiv.org/pdf/2512.23766
  - **contributions:** 1. Introduces the concept of a trainable prototype called a Schubert Variety of Best Fit (SVBF) for representing clusters of subspaces. 2. Integrates the SVBF prototype into the Linde-Buzo-Grey (LBG) clustering pipeline to create the SVBF-LBG algorithm. 3. Demonstrates improved cluster purity on synthetic, image, spectral, and video action data compared to methods using subspace means.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a73e8fbe969f250567092a96ad55fca5bd7133b8ca34f30feedb918976b1faf5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new subspace clustering method that uses a geometric prototype called a Schubert Variety of Best Fit (SVBF) instead of a simple subspace mean. The SVBF is integrated into the Linde-Buzo-Grey algorithm, resulting in an SVBF-LBG framework that shows improved clustering performance on various data types while preserving mathematical structure for analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Need for geometric representatives in subspace clustering"] --> Problem_Sub["子问题/Sub-Problem<br>Subspace means on Grassmann manifold may not be optimal"]
        Method["主要方法/Method<br>Propose SVBF-LBG algorithm"] --> Method_Sub1["方法组件/Component 1<br>Schubert Variety of Best Fit (SVBF) prototype"]
        Method --> Method_Sub2["方法组件/Component 2<br>Integration into Linde-Buzo-Grey (LBG) pipeline"]
        Results["关键结果/Results<br>Improved cluster purity"] --> Results_Sub1["结果细节/Detail 1<br>Tested on synthetic, image, spectral, video data"]
        Results --> Results_Sub2["结果细节/Detail 2<br>Retains mathematical structure for downstream analysis"]
    ```

- **[arXiv260101] Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments**
  - **tags:** [cv], [monocular depth estimation], [Depth Anything V2, DV-LORA, synthetic-to-real adaptation, SCARED dataset]
  - **authors:** Ankan Aich, Yangming Lee
  - **institution:** Rochester Institute of Technology (RIT)
  - **link:** https://arxiv.org/pdf/2512.23786
  - **contributions:** 1. Leveraged the high-fidelity synthetic priors of the Depth Anything V2 model to capture precise geometric details of thin structures in surgical scenes. 2. Efficiently adapted these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA) to minimize parameters and bridge the synthetic-to-real gap. 3. Introduced a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dd5c633cc3115f9e880a9737b5b76f77a1dcd45eb6b882e43363394c4566463_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of monocular depth estimation in specular, fluid-filled surgical environments by adapting the synthetic-prior-rich Depth Anything V2 model to the medical domain using DV-LORA. It also proposes a new stratified evaluation protocol for high-specularity scenes. The method achieves state-of-the-art results on the SCARED dataset, demonstrating superior robustness in adverse lighting conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Leveraging Synthetic Priors for Monocular Depth Estimation<br>利用合成先验进行单目深度估计] --> B
        A --> C
        A --> D
        B[Problem: MDE fragile in specular surgical scenes<br>问题：镜面手术场景中MDE脆弱]
        C[Method: Adapt Depth Anything V2 with DV-LORA<br>方法：使用DV-LORA适配Depth Anything V2]
        D[Results: SOTA on SCARED, 98.1% accuracy<br>结果：在SCARED上达到SOTA，98.1%准确率]
    ```

- **[arXiv260101] Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments**
  - **tags:** [cv], [human pose estimation and action analysis], [video-based assessment, 2D skeleton extraction, Cognitive Task Analysis (CTA), performance metrics, synthetic training environments]
  - **authors:** Surya Rayala, Marcos Quinones-Grueiro, Naveeduddin Mohammed, Ashwin T S, Benjamin Goldberg, Randall Spain, Paige Lawton, Gautam Biswas
  - **institution:** Vanderbilt University, US Army DEVCOM Soldier Center
  - **link:** https://arxiv.org/pdf/2512.23819
  - **contributions:** 1. A video-based assessment pipeline that extracts performance analytics (2D skeletons, gaze vectors, trajectories) from training videos without extra hardware. 2. Development of task-specific metrics for psychomotor fluency, situational awareness, and team coordination, integrated into an extended Cognitive Task Analysis hierarchy. 3. Demonstration of the approach via a case study on real-world Enter and Clear the Room drills and discussion of its integration into After Action Review systems like Gamemaster and GIFT.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33d390da3c0f68240f7cda6efac9e31e941e99ab611d61cd2837f0352453d12c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automatically and objectively assessing soldier performance in synthetic training environments. It proposes a video-based pipeline using computer vision to extract movement and gaze data, from which it derives specific performance metrics for cognitive and teamwork skills. The method is demonstrated on real-world drills and shows potential for scalable, hardware-free evaluation to support training feedback.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Video-Based Performance Evaluation for ECR Drills") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("传统评估方法受限/Traditional assessment limited")
        P1 --> P1_1("依赖昂贵传感器/Relies on costly sensors")
        P1 --> P1_2("主观人为观察/Subjective human observation")
        Method --> M1("视频分析管道/Video-based pipeline")
        M1 --> M1_1("提取2D骨架、视线、轨迹/Extract 2D skeletons, gaze, trajectories")
        M1 --> M1_2("开发任务特定指标/Develop task-specific metrics")
        M1 --> M1_3("扩展认知任务分析/Extended Cognitive Task Analysis")
        Results --> R1("案例研究验证/Case study validation")
        Results --> R2("支持行动后评估/Supports After Action Reviews")
        Results --> R3("未来: 3D分析/Future: 3D analysis")
    ```

- **[arXiv260101] Pretraining Frame Preservation in Autoregressive Video Memory Compression**
  - **tags:** [cv], [video generation], [autoregressive video models, memory compression, frame preservation, context length, pretraining]
  - **authors:** Lvmin Zhang, Shengqu Cai, Muyang Li, Chong Zeng, Beijia Lu, Anyi Rao, Song Han, Gordon Wetzstein, Maneesh Agrawala
  - **institution:** Stanford University, MIT, Carnegie Mellon University, HKUST
  - **link:** https://arxiv.org/pdf/2512.23851
  - **contributions:** 1. Proposes PFP, a novel neural network structure designed to compress long videos into short contexts while preserving high-frequency details of individual frames. 2. Introduces an explicit pretraining objective focused on preserving the perceptual appearance of frames at arbitrary temporal positions within the compressed context. 3. Demonstrates that pretrained PFP models can be effectively fine-tuned as memory encoders for autoregressive video models, enabling long-range history memory with low context cost and minimal fidelity loss.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd63197bd634062ab10ea1d91296e2bdbae09b94e054d1cc001ce4fbedb7bfcd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of managing long video history in autoregressive video generation models by proposing PFP, a pretraining method for compressing videos into short contexts while preserving frame details. The method allows a 20-second video to be compressed into a ~5k length context, enabling efficient long-term memory for video generation with low fidelity loss. The framework is evaluated through ablative studies, discussing trade-offs in neural architecture design.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Pretraining Frame Preservation in Autoregressive Video Memory Compression") --> Problem("核心问题/Problem: Long video history leads to high context cost in autoregressive models")
        Root --> Method("主要方法/Method: PFP pretraining to compress video into short context while preserving frame details")
        Root --> Results("关键结果/Results: 20s video compressed to ~5k context; enables long history memory with low fidelity loss")
    ```

- **[arXiv260101] Lifelong Domain Adaptive 3D Human Pose Estimation**
  - **tags:** [cv], [human pose estimation], [lifelong domain adaptation, catastrophic forgetting, generative adversarial network, pose-aware knowledge, temporal-aware knowledge]
  - **authors:** Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen
  - **institution:** University of Central Florida, University of North Carolina at Charlotte
  - **link:** https://arxiv.org/pdf/2512.23860
  - **contributions:** 1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Lifelong Domain Adaptive 3D Human Pose Estimation] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[3D HPE泛化挑战/3D HPE Generalization Challenge]
    B --> B2[非平稳目标域/Non-stationary Target Domains]
    B --> B3[灾难性遗忘/Catastrophic Forgetting]
    C --> C1[终身域适应任务/Lifelong DA Task]
    C --> C2[GAN框架/GAN Framework]
    C --> C3[3D姿态生成器/3D Pose Generator]
    C2 --> C2a[3D姿态生成器/3D Pose Generators]
    C2 --> C2b[2D姿态判别器/2D Pose Discriminator]
    C2 --> C2c[3D姿态估计器/3D Pose Estimator]
    C3 --> C3a[姿态感知/Pose-aware]
    C3 --> C3b[时序感知/Temporal-aware]
    C3 --> C3c[域感知/Domain-aware]
    D --> D1[缓解域偏移/Mitigates Domain Shifts]
    D --> D2[对齐姿态/Aligns Poses]
    D --> D3[卓越性能/Superior Performance]
    ```

- **[arXiv260101] Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation**
  - **tags:** [ai], [robotic manipulation], [tactile sensing, vision-language-action models, hierarchical perception, world model, contact-rich manipulation]
  - **authors:** Guo Ye, Zexi Zhang, Xu Zhao, Shang Wu, Haoran Lu, Shihan Lu, Han Liu
  - **institution:** Northwestern University
  - **link:** https://arxiv.org/pdf/2512.23864
  - **contributions:** 1. Introduces DreamTacVLA, a framework that grounds VLA models in contact physics using a hierarchical perception scheme with high-resolution tactile images, wrist-camera vision, and third-person vision. 2. Proposes a Hierarchical Spatial Alignment (HSA) loss to align tactile tokens with their spatial counterparts in visual views, creating a unified representation. 3. Finetunes the system with a tactile world model that predicts future tactile signals, enabling the agent to condition actions on anticipated contact dynamics, and constructs a hybrid large-scale dataset from digital twin and real-world sources to overcome data scarcity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a7e7908fa684e60efce68108a909c179516c57fb8ccd7e68353665652c969a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of Vision-Language-Action (VLA) models in contact-rich manipulation tasks by introducing DreamTacVLA. The method integrates high-resolution tactile sensing with vision using hierarchical spatial alignment and a tactile prediction world model, trained on a hybrid dataset. It demonstrates superior performance, achieving up to 95% success, highlighting the importance of touch-aware reasoning for robust robotic agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation] --> B[核心问题/Problem: VLA models are blind to physical contact, struggling with contact-rich tasks.]
        A --> C[主要方法/Method: Hierarchical perception with tactile, wrist, and third-person vision, aligned via HSA loss and finetuned with a tactile world model.]
        A --> D[关键结果/Results: Outperforms VLA baselines, achieving up to 95% success on contact-rich manipulation tasks.]
    ```

- **[arXiv260101] MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework**
  - **tags:** [cv], [medical image synthesis], [variational autoencoder, synthetic CT, cranial suture segmentation, pediatric neuroimaging, MRI-to-CT translation]
  - **authors:** Krithika Iyer, Austin Tapp, Athelia Paulli, Gabrielle Dickerson, Syed Muhammad Anwar, Natasha Lepore, Marius George Linguraru
  - **institution:** Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Hospital; CIBORG Lab, Children's Hospital Los Angeles; University of Southern California
  - **link:** https://arxiv.org/pdf/2512.23894
  - **contributions:** 1. Proposed a deep learning pipeline to generate synthetic CTs (sCTs) from pediatric T1-weighted MRIs, enabling bone and suture visualization from radiation-free scans. 2. Introduced a method to predict detailed cranial bone segmentation and generate suture probability heatmaps, from which direct suture segmentations are derived. 3. Demonstrated, for the first time, a pediatric cranial CT synthesis framework capable of accurate suture segmentation on MRI-derived sCTs, validated with high structural similarity and Dice scores.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1440b1c25db399b2255913b18d4389f7c541299e16cbe4a1083b532fb23ded2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of visualizing cranial bones and sutures in pediatric patients without using ionizing radiation. The authors propose a pipeline based on variational autoencoders to synthesize CT scans from routine MRI, which also predicts bone and suture segmentations. The results show that the synthetic CTs are perceptually indistinguishable from real CTs and enable accurate, non-invasive cranial evaluation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MRI-to-CT Synthesis With Cranial Suture Segmentations<br>MRI-to-CT合成与颅缝分割] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CT有辐射, MRI无法显示骨骼/CT has radiation, MRI cannot show bone]
        C --> C1[使用变分自编码器生成合成CT/Use VAE to generate synthetic CT]
        C --> C2[预测骨骼分割和颅缝概率图/Predict bone segmentation & suture heatmaps]
        D --> D1[合成CT与真实CT高度相似/sCT highly similar to real CT]
        D --> D2[骨骼和颅缝分割准确/Bone & suture segmentation accurate]
    ```

- **[arXiv260101] Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale**
  - **tags:** [cv], [remote sensing], [vision transformer, scaling laws, data-limited regime, electro-optical, foundation models]
  - **authors:** Charith Wickrema, Eliza Mace, Hunter Brown, Heidys Cabrera, Nick Krall, Matthew O'Neill, Shivangi Sarkar, Lowell Weissman, Eric Hughes, Guido Zarrella
  - **institution:** The MITRE Corporation
  - **link:** https://arxiv.org/pdf/2512.23903
  - **contributions:** 1. Conducted a large-scale empirical study on scaling vision transformer (ViT) models for remote sensing using over a quadrillion pixels of commercial satellite data, significantly exceeding prior scales. 2. Identified and reported success and failure modes observed during petascale training of remote sensing foundation models, providing practical insights for the community. 3. Demonstrated that performance in this domain, even at petascale, remains in a data-limited regime rather than a model parameter-limited one, challenging assumptions from natural-image domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/940589a24e7270959f66022b367ddfb910f57d2b706f38db894b2c98d2493dd3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the scaling behavior of foundation models for remote sensing by training progressively larger vision transformer backbones on a massive dataset of over a quadrillion satellite pixels. The key finding is that, unlike in natural-image domains, remote sensing model performance at this scale is still limited by data availability rather than model capacity. These insights aim to guide future data collection, compute budgeting, and optimization for frontier-scale remote sensing AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("远程感知领域缺乏扩展规律/Lack of scaling laws in remote sensing")
        Problem --> P2("需要领域专用编码器/Need for domain-specialized encoders")
        Method --> M1("使用超万亿像素卫星数据/Train on quadrillion-pixel EO data")
        Method --> M2("训练渐进式更大的ViT/Train progressively larger ViT backbones")
        Results --> R1("观察到数据限制而非模型限制/Observed data-limited regime")
        Results --> R2("报告成功与失败模式/Report success and failure modes")
    ```

- **[arXiv260101] Learning to learn skill assessment for fetal ultrasound scanning**
  - **tags:** [cv], [medical image analysis], [bi-level optimisation, meta learning, skill assessment, fetal ultrasound, task predictor]
  - **authors:** Yipei Wang, Qianye Yang, Lior Drukker, Aris T. Papageorghiou, Yipeng Hu, J. Alison Noble
  - **institution:** University of Oxford, University College London
  - **link:** https://arxiv.org/pdf/2512.23920
  - **contributions:** 1. Proposes a novel bi-level optimisation framework for skill assessment that does not rely on manually predefined skill ratings. 2. Introduces a method that quantifies skill by how well a clinical task is performed on the acquired images, using a joint clinical task predictor and skill predictor. 3. Validates the feasibility of the proposed framework on real-world clinical ultrasound videos for scanning the fetal head.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18a809a751f481712110929a049f34570c336104353fc4936c1ea3360376670c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the subjective and time-intensive nature of traditional ultrasound skill assessment by proposing a novel bi-level optimisation framework. The method assesses a sonographer's skill by how well a clinical task is performed on the acquired fetal ultrasound images, without needing predefined skill labels. The results demonstrate the framework's feasibility in predicting ultrasound skills by quantifying optimized task performance as a skill indicator.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning to learn skill assessment for fetal ultrasound scanning] --> B(核心问题/Problem: 传统超声技能评估主观且耗时/Traditional ultrasound skill assessment is subjective and time-intensive)
        A --> C(主要方法/Method: 提出双层优化框架/Propose a bi-level optimisation framework)
        A --> D(关键结果/Results: 验证了通过优化任务表现量化技能的可行性/Validated feasibility of quantifying skill via optimised task performance)
    ```

- **[arXiv260101] MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation**
  - **tags:** [cv], [medical image segmentation], [incomplete multimodal learning, meta-parameterized fusion, consistency regularization]
  - **authors:** Yulong Zou, Bo Liu, Cun-Jing Zheng, Yuan-ming Geng, Siyue Li, Qiankun Zuo, Shuihua Wang, Yudong Zhang, Jin Hong
  - **institution:** Nanchang University
  - **link:** https://arxiv.org/pdf/2512.23936
  - **code:** https://github.com/worldlikerr/MGML
  - **contributions:** 1. Proposes a plug-and-play Meta-Guided Multi-Modal Learning (MGML) framework for incomplete multimodal brain tumor segmentation. 2. Introduces a meta-parameterized adaptive modality fusion (Meta-AMF) module to generate adaptive supervision for coherent multimodal fusion. 3. Incorporates a consistency regularization module to enhance segmentation performance and improve model robustness and generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d47f46d185c2662482a0c33ab269c7e876415553d0649cca8e73ee3645e65e6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of incomplete multimodal MRI data in brain tumor segmentation by proposing a plug-and-play MGML framework. The framework uses meta-parameterized adaptive fusion and consistency regularization to effectively utilize available modalities and improve segmentation performance. Experiments on BraTS datasets show the method outperforms state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework] --> B(核心问题/Problem: Incomplete Multimodal MRI Data for Brain Tumor Segmentation)
    A --> C(主要方法/Method: Meta-parameterized Adaptive Modality Fusion & Consistency Regularization)
    A --> D(关键结果/Results: Superior Performance on BraTS2020 & BraTS2023 Datasets)
    ```

- **[arXiv260101] Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation**
  - **tags:** [cv], [cross-view geo-localisation], [DINOv2, Mixture-of-Experts (MoE), multi-scale channel reallocation, convolution adapter, parameter-efficient fine-tuning]
  - **authors:** Hualin Ye, Bingxi Liu, Jixiang Du, Yu Qin, Ziyi Chen, Hong Zhang
  - **institution:** Huaqiao University, Southern University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23938
  - **contributions:** 1. Leveraging the DINOv2 backbone with a convolution adapter for parameter-efficient fine-tuning to enhance adaptability to cross-view variations. 2. Proposing a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. 3. Introducing an improved aggregation module that integrates Mixture-of-Experts (MoE) routing into a cross-attention framework for adaptive feature processing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/deb51d7736f1851644b47ea6627c5e39017ca1a64961ed88b6527ed2c0fb7b5c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of cross-view geo-localisation, where significant viewpoint discrepancies hinder image matching. The proposed method introduces a novel system featuring a DINOv2 backbone with a convolution adapter, a multi-scale channel reallocation module, and an MoE-enhanced aggregation module for adaptive feature processing. Experiments show the method achieves competitive performance with fewer trained parameters.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[跨视角差异大/Cross-view discrepancies hinder feature alignment]
        C --> C1[使用带卷积适配器的DINOv2/Use DINOv2 with convolution adapter]
        C --> C2[多尺度通道重分配模块/Multi-scale channel reallocation module]
        C --> C3[KV路由的MoE聚合模块/MoE aggregation with KV routing]
        D --> D1[在University-1652和SUES-200上性能优异/Competitive performance on University-1652 & SUES-200]
        D --> D2[参数更少/Fewer trained parameters]
    ```

- **[arXiv260101] Kinematic-Based Assessment of Surgical Actions in Microanastomosis**
  - **tags:** [cv], [surgical video analysis], [action segmentation, self-similarity matrix, YOLO-DeepSORT, surgical skill assessment, edge computing]
  - **authors:** Yan Meng, Daniel Donoho, Marcelle Altshuler, Omar Arnaout
  - **institution:** Children's National Hospital, Brigham and Women's Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2512.23942
  - **contributions:** 1. Proposed a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed for edge computing platforms. 2. Introduced a system with three key modules: instrument tip tracking (YOLO & DeepSORT), action segmentation via self-similarity matrix and unsupervised clustering, and a supervised classification module for skill evaluation. 3. Demonstrated high effectiveness on a dataset of 58 expert-rated videos, achieving 92.4% frame-level action segmentation accuracy and 85.5% skill classification accuracy, validating its potential for objective, real-time surgical training feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/454fd192bf8243137cb3f65f7d10780b527e77fd15d64c451fe6d42970107350_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an AI framework to automate the assessment of surgical skill in microanastomosis procedures. The method combines instrument tracking, action segmentation using self-similarity matrices, and supervised classification to evaluate performance. Experimental results show high accuracy in segmenting actions and classifying skill, demonstrating its potential for providing objective, real-time feedback in surgical training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Kinematic-Based Assessment of Surgical Actions in Microanastomosis] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[主观评估耗时耗力/Subjective, time-consuming expert assessment]
        C --> C1[目标追踪/YOLO-DeepSORT tracking]
        C --> C2[动作分割/Self-similarity matrix segmentation]
        C --> C3[技能分类/Supervised skill classification]
        D --> D1[92.4% 分割准确率/92.4% segmentation accuracy]
        D --> D2[85.5% 分类准确率/85.5% classification accuracy]
    ```

- **[arXiv260101] T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models**
  - **tags:** [sec], [adversarial machine learning], [adversarial attack, text-to-video, diffusion models, prompt perturbation, semantic-temporal evaluation]
  - **authors:** Changzhen Li, Yuecong Min, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen
  - **institution:** Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)
  - **link:** https://arxiv.org/pdf/2512.23953
  - **contributions:** 1. Introduces T2VAttack, the first comprehensive study of adversarial attacks on Text-to-Video (T2V) diffusion models from both semantic and temporal perspectives. 2. Proposes two novel attack methods: T2VAttack-S (synonym replacement via greedy search) and T2VAttack-I (iterative word insertion). 3. Conducts a comprehensive evaluation revealing critical vulnerabilities in state-of-the-art T2V models, showing that minor prompt modifications can cause substantial degradation in output quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8069e3c162b4bb54472f5b4383bd4f61d383e9d40b2077dc42f8a12561729b3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the adversarial robustness of Text-to-Video (T2V) diffusion models. It proposes T2VAttack, a framework with two attack methods (synonym replacement and word insertion) targeting both semantic and temporal video quality. The experiments show that even small, subtle changes to the input prompt can significantly degrade the output of several leading T2V models, exposing a critical security vulnerability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models"] --> Problem["核心问题/Problem: Vulnerability of T2V models to adversarial attacks"]
        Root --> Method["主要方法/Method: T2VAttack-S (synonym replacement) & T2VAttack-I (word insertion)"]
        Root --> Results["关键结果/Results: Minor prompt changes cause substantial degradation in SOTA models"]
    ```

- **[arXiv260101] U-Net-Like Spiking Neural Networks for Single Image Dehazing**
  - **tags:** [cv], [image dehazing], [Spiking Neural Networks, U-Net, Leaky-Integrate-and-Fire, computational efficiency]
  - **authors:** Huibin Li, Haoran Liu, Mingzhe Liu, Yulong Xiao, Peng Li, Guibin Zan
  - **institution:** Chengdu University of Technology, Wenzhou University of Technology, Politecnico di Milano, Sigray, Inc.
  - **link:** https://arxiv.org/pdf/2512.23950
  - **code:** https://github.com/HaoranLiu507/DehazeSNN
  - **contributions:** 1. Proposes DehazeSNN, a novel architecture integrating a U-Net-like design with Spiking Neural Networks (SNNs) for single image dehazing. 2. Introduces the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) to enhance cross-channel communication. 3. Demonstrates competitive performance with state-of-the-art methods while achieving a smaller model size and reduced computational operations (MACs).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b4280687a6989b7dc46c7fdadaa4358338cd9373eaf17fdd4a397e16e7d6732_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of single image dehazing by proposing DehazeSNN, a U-Net-like architecture built with Spiking Neural Networks. The method introduces a novel OLIFBlock to improve feature processing and aims to overcome the limitations of CNNs and Transformers regarding long-range dependencies and computational cost. Experiments show DehazeSNN delivers high-quality results with improved efficiency on benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[U-Net-Like Spiking Neural Networks for Single Image Dehazing] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>图像去雾/Image Dehazing] --> B1[CNN难以处理长程依赖/CNNs struggle with long-range dependencies]
        B --> B2[Transformer计算成本高/Transformers demand high computational resources]
        C[主要方法/Method<br>DehazeSNN] --> C1[U-Net-like SNN架构/U-Net-like SNN Architecture]
        C --> C2[正交LIF块/Orthogonal LIF Block (OLIFBlock)]
        D[关键结果/Results] --> D1[性能有竞争力/Competitive performance]
        D --> D2[模型小，计算量少/Smaller model size, fewer MACs]
    ```

- **[arXiv260101] DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation**
  - **tags:** [cv], [novel view synthesis], [4D Gaussian Splatting, view extrapolation, video diffusion model, progressive restoration, point cloud estimation]
  - **authors:** Yuang Jia, Jinlong Wang, Jiayi Zhao, Chunlam Li, Shunzhou Wang, Wei Gao
  - **institution:** Peking University (Shenzhen Graduate School)
  - **link:** https://arxiv.org/pdf/2512.23983
  - **contributions:** 1. Proposes an images-only, decoupled 4D reconstruction method that estimates and fuses static and dynamic point clouds without relying on LiDAR or manual annotations. 2. Introduces a progressive restoration pipeline that iteratively refines 4D Gaussian renderings using a video diffusion model to enhance quality for large viewpoint extrapolation. 3. Demonstrates a method for driving view extrapolation that outperforms baselines in generating high-quality images at novel, shifted viewpoints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ed84d0d40dcaf8960a53c9af7a3ee129d60f54e15eeb76c74bc49c1cdd997a5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating high-quality images from novel viewpoints in autonomous driving scenarios without relying on expensive sensors or labels. The proposed method, DriveExplorer, first reconstructs a 4D scene using a deformable Gaussian framework from images only, then iteratively refines the renderings for extrapolated views using a video diffusion model in a progressive restoration loop. The results show that this approach produces higher-quality extrapolated views compared to existing baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DriveExplorer: Images-Only Decoupled 4D Reconstruction] --> B[核心问题/Problem: 依赖昂贵先验的驾驶视图外推<br>Problem: View extrapolation relies on expensive priors (LiDAR, labels)]
        A --> C[主要方法/Method: 渐进式恢复的4D高斯与扩散模型<br>Method: Progressive restoration with 4D Gaussian & diffusion model]
        A --> D[关键结果/Results: 生成更高质量的外推视图图像<br>Results: Produces higher-quality extrapolated view images]
    ```

- **[arXiv260101] Anomaly detection in satellite imagery through temporal inpainting**
  - **tags:** [cv], [anomaly detection], [temporal inpainting, SATLAS foundation model, Sentinel-2, change detection, deep learning]
  - **authors:** Bertrand Rouet-Leduc, Claudia Hulbert
  - **institution:** Kyoto University, Geolabe
  - **link:** https://arxiv.org/pdf/2512.23986
  - **contributions:** 1. Proposes a novel deep learning method for satellite anomaly detection using temporal inpainting to predict the expected state of the surface. 2. Leverages the SATLAS foundation model, fine-tuned on global Sentinel-2 time series data across diverse environments. 3. Demonstrates superior sensitivity and specificity in detecting surface ruptures from an earthquake, with detection thresholds approximately three times lower than traditional baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50fd4012191b7a3ba947d1bc5a8c1c43be7c391abfdcc5dc5f22c18480ef540f_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of detecting surface changes in satellite imagery by using a deep learning inpainting model to predict what a location should look like based on its past appearance. The discrepancy between this prediction and the actual observation highlights anomalies. The method, validated on earthquake ruptures, shows significantly higher sensitivity than traditional approaches, enabling more precise global monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Anomaly detection in satellite imagery through temporal inpainting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>Detecting surface changes is hard due to noise and seasonal variations.] --> P1[挑战/Challenges<br>Atmospheric noise, seasonal changes, sensor artifacts]
        Method[主要方法/Method<br>Deep learning temporal inpainting model] --> M1[模型基础/Model Foundation<br>Built upon SATLAS foundation model]
        Method --> M2[训练数据/Training Data<br>Global Sentinel-2 time series across diverse climates]
        Method --> M3[检测原理/Detection Principle<br>Discrepancy between prediction and observation reveals anomalies]
        Results[关键结果/Results<br>Validated on 2023 Turkey-Syria earthquake surface ruptures] --> R1[性能/Performance<br>Higher sensitivity & specificity than baselines (temporal median, Reed-Xiaoli)]
        Results --> R2[灵敏度提升/Sensitivity Gain<br>Detection thresholds ~3x lower than baseline approaches]
    ```

- **[arXiv260101] GCA-ResUNet: Medical Image Segmentation Using Grouped Coordinate Attention**
  - **tags:** [cv], [medical image segmentation], [Grouped Coordinate Attention, GCA-ResUNet, coordinate attention, multi-organ segmentation, CNN-Transformer hybrid]
  - **authors:** Jun Ding, Shang Gao
  - **institution:** Not explicitly stated in provided content. Affiliation/email domain not found.
  - **link:** https://arxiv.org/pdf/2512.23990
  - **contributions:** 1. Identified a limitation of unified attention mechanisms in CNNs for handling channel-wise semantic heterogeneity in multi-organ and low-contrast medical image segmentation. 2. Proposed a lightweight, plug-and-play Grouped Coordinate Attention (GCA) module that decouples channel context modeling into groups and integrates direction-aware coordinate encoding. 3. Developed an effective integration strategy embedding GCA into a ResNet50 backbone, demonstrating consistent performance improvements over CNN and Transformer baselines on public benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dece05c6757ff0ac5c42368fc1f0f87e90e69b1db30883b770662cbd8f580481_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes GCA-ResUNet, a medical image segmentation framework that enhances a CNN backbone with a novel Grouped Coordinate Attention module to better capture long-range dependencies and channel heterogeneity. The method achieves state-of-the-art results on Synapse and ACDC datasets, offering a favorable trade-off between accuracy and computational efficiency for clinical deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GCA-ResUNet: Medical Image Segmentation Using Grouped Coordinate Attention] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[CNN的局部性和统一注意力机制限制了长程上下文建模 / CNN locality & unified attention limit long-range context modeling]
        B --> B2[Transformer计算成本高，需要大量数据 / Transformer is computationally expensive and data-hungry]
        C --> C1[提出分组坐标注意力模块 / Propose Grouped Coordinate Attention (GCA) module]
        C1 --> C1a[通道分组建模语义异质性 / Channel grouping for semantic heterogeneity]
        C1 --> C1b[方向感知坐标编码捕获空间依赖 / Direction-aware coordinate encoding for spatial dependencies]
        C --> C2[将GCA集成到ResNet50骨干网络中 / Integrate GCA into ResNet50 backbone]
        D --> D1[在Synapse和ACDC数据集上达到SOTA / Achieves SOTA on Synapse & ACDC benchmarks]
        D --> D2[在复杂边界和小结构分割上表现更好 / Better at delineating complex boundaries & small structures]
        D --> D3[提供精度与效率的良好权衡 / Provides favorable accuracy-efficiency trade-off]
    ```

- **[arXiv260101] Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge**
  - **tags:** [cv], [3D reconstruction], [3D Gaussian Splatting, Rendezvous and Proximity Operations, Photometric Optimization, Shadow Splatting, Spacecraft Pose Estimation]
  - **authors:** Tae Ha Park, Simone D'Amico
  - **institution:** Nara Space Technology Inc., Stanford University
  - **link:** https://arxiv.org/pdf/2512.23998
  - **contributions:** 1. Proposes a novel pipeline to recover the 3D structure of an unknown spacecraft using 3D Gaussian Splatting (3DGS) from monocular image sequences captured during RPO., 2. Incorporates prior knowledge of the Sun's position (estimated by the servicer spacecraft) into the 3DGS training pipeline to handle dynamic space illumination and improve photometric rendering quality., 3. Demonstrates that the enhanced 3DGS model can adapt to rapidly changing lighting, reflect global shadows and self-occlusion, and is crucial for improving downstream camera pose estimation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e32ab5204fb3cfe9ec9f948cf84dff85038750fa587faef643915e0d8fb067b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of reconstructing the 3D structure of an unknown spacecraft from images taken during space rendezvous, where dynamic lighting violates the static scene assumption of standard 3D Gaussian Splatting. The proposed method integrates known Sun position information into the training process to significantly improve the photometric accuracy of the rendered model. Experiments show this approach enables the model to adapt to space illumination changes, producing better geometry and appearance for downstream pose estimation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge") --> Problem("核心问题/Problem: 3DGS训练需要静态场景，与太空动态光照条件冲突")
        Root --> Method("主要方法/Method: 将已知的太阳位置先验知识整合到3DGS训练流程中")
        Root --> Results("关键结果/Results: 模型适应快速变化的光照，反映全局阴影和自遮挡")
    ```

- **[arXiv260101] Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation**
  - **tags:** [cv], [semantic segmentation], [self-supervised learning, topological features, cross-modal alignment, adversarial augmentation, differentiable box-counting]
  - **authors:** Haotang Li, Zhenyu Qi, Hao Qin, Huanrui Yang, Sen He, Kebin Peng
  - **institution:** The University of Arizona, East Carolina University
  - **link:** https://arxiv.org/pdf/2512.23997
  - **contributions:** 1. Proposed GASeg, a novel self-supervised segmentation framework that bridges geometry and appearance using topological information. 2. Introduced the Differentiable Box-Counting (DBC) module to quantify multi-scale topological statistics from geometric and appearance feature streams. 3. Designed Topological Augmentation (TopoAug), an adversarial strategy using morphological operators, and GALoss, a multi-objective loss for cross-modal feature alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d58e5a3637c9b516e1fe8659ad85bc4de8415691d3af5e82026319ad6445e50_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of self-supervised semantic segmentation failing under appearance ambiguities. It proposes GASeg, a framework that uses topological features to bridge geometry and appearance, featuring a differentiable box-counting module, topological augmentation, and a cross-modal alignment loss. The method achieves state-of-the-art performance on benchmarks like COCO-Stuff and Cityscapes, demonstrating the effectiveness of integrating topological information for robust segmentation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GASeg: Bridging Geometry and Appearance via Topology] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Self-supervised segmentation fails under appearance ambiguities 自监督分割在表观歧义下失败]
        C --> C1[Proposed GASeg framework 提出GASeg框架]
        C1 --> C2[Differentiable Box-Counting (DBC) module 可微盒计数模块]
        C1 --> C3[Topological Augmentation (TopoAug) 拓扑增强]
        C1 --> C4[Multi-objective GALoss 多目标GALoss]
        D --> D1[Achieves SOTA on COCO-Stuff, Cityscapes, PASCAL 在多个基准上达到SOTA]
    ```

- **[arXiv260101] FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing**
  - **tags:** [cv], [vision-language models], [multi-feature fusion, recurrent visual injection, remote sensing vision-language model]
  - **authors:** Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao
  - **institution:** Nanjing University
  - **link:** https://arxiv.org/pdf/2512.24022
  - **code:** https://github.com/Yunkaidang/RSVLM
  - **contributions:** 1. Proposes a Multi-Feature Fusion Remote Sensing Vision-Language Model (MF-RSVLM) that extracts and fuses multi-scale visual features to better capture small and complex structures in remote sensing scenes., 2. Introduces a recurrent visual feature injection scheme to keep the language model grounded in visual evidence and mitigate visual forgetting during text generation., 3. Demonstrates state-of-the-art or highly competitive performance on diverse remote sensing benchmarks, including classification, image captioning, and VQA tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51db66b7af59969350cb2c0f2ca84f598178ab4f9ea4040dfc08f46586ef7fe0_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of applying general vision-language models to remote sensing data by proposing MF-RSVLM, a model that fuses multi-scale visual features and uses recurrent visual injection to reduce forgetting. It achieves strong results on remote sensing classification, captioning, and VQA tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLMs struggle with fine-grained features and visual forgetting in remote sensing]
        C[主要方法/Method: Multi-feature fusion and recurrent visual injection]
        D[关键结果/Results: SOTA/competitive performance on RS benchmarks]
    ```

- **[arXiv260101] Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis**
  - **tags:** [cv], [medical image segmentation], [Hilbert Curve, Mamba SSM, Cross-Attention, Multi-Modal Fusion, Visual Language Model]
  - **authors:** Hao Wu, Hui Li, Yiyun Su
  - **institution:** Southern University of Science and Technology, Xiamen University, Rutgers University
  - **link:** https://arxiv.org/pdf/2512.24013
  - **contributions:** 1. A systematic redesign of SAM2 incorporating Hilbert space-filling curves into the Mamba SSM scanning mechanism to preserve spatial locality in 3D medical data. 2. Introduction of a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. 3. A two-stage "segmentation-first, fusion-prompt" framework that unifies segmentation masks and textual attributes into an enhanced prompt to guide VLM-based disease classification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21f004815031e896e86d16784cfd665cc86890eb1d3ec08d8faf6d34527ab04f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of applying general-purpose VLMs to 3D multimodal medical image analysis by proposing Hilbert-VLM, a novel two-stage framework. It first uses a redesigned SAM2 model with Hilbert-Mamba for precise lesion segmentation and then fuses the visual masks with textual attributes to create enhanced prompts for a VLM classifier. The model demonstrates improved performance on the BraTS2021 benchmark, showing potential for more accurate and reliable medical diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bridging the Perception-Cognition Gap<br>Paper Title: Bridging the Perception-Cognition Gap] --> B(核心问题: 3D多模态医学图像分析<br>Problem: 3D Multi-modal Medical Image Analysis)
        A --> C(主要方法: Hilbert-VLM两阶段框架<br>Method: Hilbert-VLM Two-Stage Framework)
        A --> D(关键结果: BraTS2021基准测试结果<br>Results: BraTS2021 Benchmark Results)
        B --> B1[挑战: 信息融合与细节忽略<br>Challenges: Fusion & Detail Oversight]
        C --> C1[阶段1: HilbertMed-SAM分割<br>Stage1: HilbertMed-SAM Segmentation]
        C --> C2[阶段2: 增强提示引导VLM分类<br>Stage2: Enhanced Prompt for VLM Classification]
        D --> D1[Dice分数: 82.35%<br>Dice Score: 82.35%]
        D --> D2[分类准确率: 78.85%<br>Classification ACC: 78.85%]
    ```

- **[arXiv260101] RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations**
  - **tags:** [cv], [text-guided segmentation], [agentic MLLM, multi-turn tool invocation, iterative mask refinement]
  - **authors:** Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang
  - **institution:** Fudan University, Shanghai Jiao Tong University School of Medicine
  - **link:** https://arxiv.org/pdf/2512.24023
  - **contributions:** 1. Proposes RSAgent, an agentic MLLM that interleaves reasoning and action for segmentation via multi-turn tool invocations, enabling iterative refinement. 2. Builds a data pipeline to synthesize multi-turn reasoning segmentation trajectories for training. 3. Introduces a two-stage training framework combining cold-start supervised fine-tuning with agentic reinforcement learning using fine-grained, task-specific rewards.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492283683a8b1ec7673cb4aa97997d0e1ab70ed4a074c17cfa6a9a5e87c60998_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of one-shot methods in text-guided segmentation, where initial errors cannot be corrected. It proposes RSAgent, an agentic multimodal LLM that iteratively uses a segmentation toolbox, observes feedback, and refines its spatial hypotheses over multiple turns. Experiments show RSAgent achieves state-of-the-art performance on benchmarks like ReasonSeg and RefCOCOg.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RSAgent: Learning to Reason and Act for Text-Guided Segmentation] --> B[核心问题/Problem: One-shot grounding methods lack verification and refinement capabilities]
        A --> C[主要方法/Method: Agentic MLLM with multi-turn tool invocation for iterative reasoning and mask refinement]
        A --> D[关键结果/Results: Achieves SOTA on benchmarks (66.5% gIoU on ReasonSeg, 81.5% cIoU on RefCOCOg)]
    ```

- **[arXiv260101] Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression**
  - **tags:** [cv], [image representation and compression], [2D Gaussian Splatting, rate-distortion optimization, adaptive quantization, structure-guided initialization, geometry-consistent regularization]
  - **authors:** Huanxiong Liang, Yunuo Chen, Yicheng Pan, Sixian Wang, Jincheng Dai, Guo Lu, Wenjun Zhang
  - **institution:** Shanghai Jiao Tong University, Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.24018
  - **contributions:** 1. A structure-guided initialization method that assigns 2D Gaussians based on spatial structural priors for a localized and semantically meaningful distribution. 2. An adaptive bitwidth quantization scheme for covariance parameters during fine-tuning, granting higher precision to complex regions for RD-aware optimization. 3. A geometry-consistent regularization that aligns Gaussian orientations with local gradient directions to better preserve structural details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45f543cadfd5e91c5a71253cd986e9ef8c5270af87a338c12cf69f1ce15135a6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of existing 2D Gaussian Splatting (2DGS) methods for image compression at low bitrates, which allocate representation capacity without considering image structure. The authors propose a structure-guided allocation principle that couples image structure with capacity and quantization precision through three techniques: structure-guided initialization, adaptive bitwidth quantization, and geometry-consistent regularization. The method significantly improves rate-distortion performance while maintaining over 1000 FPS decoding speed, achieving large BD-rate reductions compared to the baseline.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Structure-Guided Allocation of 2D Gaussians<br>2D高斯结构引导分配"] --> B["核心问题/Problem<br>Existing 2DGS allocates capacity oblivious to structure, limiting RD efficiency."]
        A --> C["主要方法/Method<br>1. Structure-Guided Initialization<br>2. Adaptive Bitwidth Quantization<br>3. Geometry-Consistent Regularization"]
        A --> D["关键结果/Results<br>Improves RD performance, maintains 1000+ FPS, reduces BD-rate significantly."]
    ```

- **[arXiv260101] FitControler: Toward Fit-Aware Virtual Try-On**
  - **tags:** [cv], [virtual try-on], [garment fit, layout generation, diffusion models]
  - **authors:** Lu Yang, Yicheng Liu, Yanan Li, Xiang Bai, Hao Lu
  - **institution:** Huazhong University of Science and Technology, Wuhan Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.24016
  - **contributions:** 1. Introducing the novel task of fit-aware virtual try-on and proposing FitControler, a learnable plug-in module for diffusion-based VTON models to enable customized fit control. 2. Constructing a new dataset, Fit4Men, containing 13,000 body-garment pairs with diverse fits, poses, and camera distances. 3. Proposing two new metrics to quantitatively assess the fit consistency of generated try-on images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/161fe70b2650b65573a6e86bb49034407bfe06a8eeb57afd8037d4fc5ba5658f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of controlling garment fit in virtual try-on, which is often neglected by prior work. The authors propose FitControler, a plug-in module that uses a fit-aware layout generator and a multi-scale fit injector to enable layout-driven, fit-controlled image generation. Experiments show the method works with various VTON models and achieves accurate fit control.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FitControler: Toward Fit-Aware Virtual Try-On] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有VTON忽略服装合身性/Existing VTON neglects garment fit]
        C --> C1[可学习的插件模块/Learnable plug-in module]
        C1 --> C2[合身感知布局生成器/Fit-aware layout generator]
        C1 --> C3[多尺度合身性注入器/Multi-scale fit injector]
        D --> D1[构建数据集Fit4Men/Built dataset Fit4Men]
        D --> D2[提出合身性评估指标/Proposed fit consistency metrics]
        D --> D3[实现精准的合身性控制/Achieved accurate fit control]
    ```

- **[arXiv260101] PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing**
  - **tags:** [mlsys], [diffusion models], [DDIM inversion, motion analysis, pipelined scheduling, frame interpolation, long-form video editing]
  - **authors:** Mustafa Munir, Md Mostafijur Rahman, Kartikeya Bhardwaj, Paul Whatmough, Radu Marculescu
  - **institution:** The University of Texas at Austin, Qualcomm AI Research
  - **link:** https://arxiv.org/pdf/2512.24026
  - **contributions:** 1. A motion-aware frame selection method using SSIM and Optical Flow to skip editing of low-motion frames. 2. A pipelined task scheduling algorithm that splits videos into segments for parallel DDIM inversion and joint editing based on GPU memory. 3. A neural network-based interpolation technique to smooth border frames and interpolate skipped frames.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3787a383f3a5b9871e80fb8c9aaad731151ef891a01b7640701576d610e23860_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost of long-form video editing with diffusion models. It proposes PipeFlow, a method that uses motion analysis to skip frames, parallel pipelined processing, and interpolation to achieve linear scaling with video length. The method achieves significant speedups (up to 31.7x) over prior work while maintaining quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing] --> B[核心问题/Problem: Long-form video editing is computationally expensive due to DDIM inversion and joint editing.]
        A --> C[主要方法/Method: 1. Motion-aware frame skipping. 2. Pipelined parallel processing. 3. Neural interpolation.]
        A --> D[关键结果/Results: Achieves linear scaling, up to 31.7x speedup over baselines.]
    ```

- **[arXiv260101] On Exact Editing of Flow-Based Diffusion Models**
  - **tags:** [cv], [image editing], [flow-based diffusion models, velocity correction, distribution transformation, Tweedie correction, Empirical Bayes Inference]
  - **authors:** Zixiang Li, Yue Song, Jianing Peng, Ting Liu, Jun Huang, Xiaochao Qu, Luoqi Liu, Wei Wang, Yao Zhao, Yunchao Wei
  - **institution:** Beijing Jiaotong University, Meitu Inc, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.24015
  - **contributions:** 1. Proposes Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior., 2. Introduces a dual-perspective velocity conversion mechanism that decomposes latent evolution into structure-preserving and semantically-guided branches for controlled editing., 3. Applies a posterior-consistent update derived from Empirical Bayes Inference and Tweedie correction to the conditional velocity field to compensate for error and ensure stable, interpretable latent dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dfdb63e56ff58967475e42a4e69f692d60678dab6f6b842bd2ac85a1db6d86e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of accumulated velocity errors and semantic inconsistency in flow-based diffusion model editing. It proposes Conditioned Velocity Correction (CVC), a framework that uses a dual-perspective velocity conversion and a posterior-consistent update based on Empirical Bayes to achieve stable and faithful image editing. The method demonstrates superior fidelity and semantic alignment across diverse editing tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[On Exact Editing of Flow-Based Diffusion Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[累积速度误差导致语义不一致/Accumulated velocity errors cause semantic inconsistency]
        B --> B2[结构保真度损失/Loss of structural fidelity]
        C --> C1[条件速度校正框架/Conditioned Velocity Correction (CVC) framework]
        C1 --> C2[双视角速度转换机制/Dual-perspective velocity conversion]
        C2 --> C3[结构保持分支/Structure-preserving branch]
        C2 --> C4[语义引导分支/Semantically-guided branch]
        C1 --> C5[后验一致更新/Posterior-consistent update]
        C5 --> C6[经验贝叶斯与Tweedie校正/Empirical Bayes & Tweedie correction]
        D --> D1[稳定的潜在动力学/Stable latent dynamics]
        D --> D2[忠实重建与平滑语义转换/Faithful reconstruction & smooth semantic conversion]
        D --> D3[卓越的保真度与语义对齐/Superior fidelity & semantic alignment]
    ```

- **[arXiv260101] Reinforced Diffusion: Learning to Push the Limits of Anisotropic Diffusion for Image Denoising**
  - **tags:** [cv], [image denoising], [anisotropic diffusion, reinforcement learning, deep Q-learning, stochastic diffusion]
  - **authors:** Xinran Qin, Yuhui Quan, Ruotao Xu, Hui Ji
  - **institution:** South China University of Technology, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24035
  - **contributions:** 1. A trainable anisotropic diffusion framework for image denoising based on reinforcement learning. 2. Modeling the denoising process as a series of diffusion actions with order learned by deep Q-learning, forming a stochastic anisotropic diffusion process. 3. Demonstrating that the proposed method outperforms existing diffusion-based methods and competes with deep CNN-based methods on multiple noise types.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be25ff96e1d5d489676fdfbae564029c066808ffe1f445710f3d58c023ed964_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a novel image denoising method called Reinforced Diffusion, which uses deep reinforcement learning (deep Q-learning) to learn the optimal sequence of naive diffusion actions, forming an adaptive stochastic anisotropic diffusion process. This approach overcomes the limitations of traditional fixed diffusion operators. Experimental results show it outperforms other diffusion-based methods and is competitive with state-of-the-art deep CNN-based denoisers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforced Diffusion: Image Denoising] --> B(核心问题/Problem: Traditional anisotropic diffusion has limited performance due to non-adaptive operators.)
        A --> C(主要方法/Method: Trainable diffusion framework using Deep Q-Learning to learn action sequences.)
        A --> D(关键结果/Results: Outperforms diffusion-based methods, competes with deep CNN methods.)
    ```

- **[arXiv260101] Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval**
  - **tags:** [ai], [cross-modal retrieval], [noisy labels, instance refining, margin preserving, neighborhood consensus]
  - **authors:** Yizhi Liu, Ruitao Pu, Shilin Xu, Yingke Chen, Quan-Hui Liu, Yuan Sun
  - **institution:** Sichuan University, State Key Laboratory of AI Safety, Northumbria University
  - **link:** https://arxiv.org/pdf/2512.24064
  - **code:** https://github.com/perquisite/NIRNL
  - **contributions:** 1. Proposes a novel robust cross-modal learning framework called Neighbor-aware Instance Refining with Noisy Labels (NIRNL). 2. Introduces Cross-modal Margin Preserving (CMP) to enhance discrimination between sample pairs by adjusting relative distances. 3. Designs Neighbor-aware Instance Refining (NIR) to identify and categorize instances into pure, hard, and noisy subsets for tailored optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b11b5fe9b39647355b1db0a9a978e8e0bd13a99b38678ec9490a28b9024434_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of noisy labels in cross-modal retrieval, which degrades model performance. The authors propose the NIRNL framework, which uses Cross-modal Margin Preserving and Neighbor-aware Instance Refining to better utilize all data and mitigate error propagation. Experiments show the method achieves state-of-the-art performance with strong robustness, especially under high noise rates.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Noisy labels degrade cross-modal retrieval performance] --> P1[标注噪声/Label Noise]
        Method[主要方法/Method: NIRNL Framework] --> M1[Cross-modal Margin Preserving (CMP)]
        Method --> M2[Neighbor-aware Instance Refining (NIR)]
        M2 --> M2_1[识别子集/Identify Subsets: pure, hard, noisy]
        Results[关键结果/Results: State-of-the-art performance] --> R1[高噪声下鲁棒/Robust under high noise]
    ```

- **[arXiv260101] Pathology Context Recalibration Network for Ocular Disease Recognition**
  - **tags:** [cv], [medical image analysis], [Pathology Recalibration Module, expert prior Guidance Adapter, Integrated Loss]
  - **authors:** Zunjie Xiao, Xiaoqing Zhang, Risa Higashita, Jiang Liu
  - **institution:** Southern University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24066
  - **contributions:** 1. Proposed a novel Pathology Recalibration Module (PRM) to leverage pathology context prior via pixel-wise context compression and pathology distribution concentration. 2. Introduced an expert prior Guidance Adapter (EPGA) to highlight significant pixel-wise regions by mining expert experience prior. 3. Designed an Integrated Loss (IL) to boost performance by considering sample-wise loss distributions and training label frequencies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2286b84bd9cbd7682cb726e99010af22979d096fd6391215bb5d0212bb1875c5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes PCRNet, a network for ocular disease recognition that incorporates a Pathology Recalibration Module and an expert prior Guidance Adapter to integrate clinical pathology context and expert experience priors into a DNN. An Integrated Loss is also introduced to handle sample and label imbalances. Experiments on three datasets show PCRNet's superiority over state-of-the-art methods, and visualizations explain its decision-making process.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Pathology Context Recalibration Network for Ocular Disease Recognition] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: DNNs ignore pathology context & expert experience priors for ocular disease recognition] --> P1[问题1/Sub-Problem: Lack of pathology context utilization]
        Problem --> P2[问题2/Sub-Problem: Lack of expert experience integration]
        Method[主要方法/Method: PCRNet] --> M1[模块1/Module: Pathology Recalibration Module (PRM)]
        Method --> M2[模块2/Module: expert prior Guidance Adapter (EPGA)]
        Method --> M3[组件/Component: Integrated Loss (IL)]
        Results[关键结果/Results] --> R1[结果1/Result: Superior performance on three datasets]
        Results --> R2[结果2/Result: Better than SOTA attention networks & loss methods]
        Results --> R3[结果3/Result: Visualization explains decision-making]
    ```

- **[arXiv260101] Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images**
  - **tags:** [cv], [object detection], [hierarchical contrastive learning, decoupled queries, DETR, fine-grained detection, remote sensing]
  - **authors:** Jingzhou Chen, Dexin Chen, Fengchao Xiong, Yuntao Qian, Liang Xiao
  - **institution:** Nanjing University of Science and Technology, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.24074
  - **contributions:** 1. Proposes a balanced hierarchical contrastive loss that uses learnable class prototypes and equilibrates gradients to address data imbalance across hierarchical levels. 2. Introduces a decoupled learning strategy that separates DETR's object queries into classification and localization sets for task-specific optimization. 3. Demonstrates superior performance on three fine-grained remote sensing datasets with hierarchical annotations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ead6df5aaaf078f6a443e544143fe4909df82deba2e44cdcb26ff94cd6a5a3d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses challenges in fine-grained object detection for remote sensing images with hierarchical labels, where imbalanced data and conflicting learning objectives hinder performance. The authors propose a balanced hierarchical contrastive loss and a decoupled query strategy within the DETR framework to ensure balanced learning and separate feature optimization for classification and localization. Experiments show the method outperforms state-of-the-art approaches on three datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["数据分布不平衡/Imbalanced Data Distribution"]
        Problem --> P2["分类与定位任务冲突/Classification-Localization Conflict"]
        Method --> M1["平衡层次对比损失/Balanced Hierarchical Contrastive Loss"]
        Method --> M2["解耦查询学习策略/Decoupled Query Learning Strategy"]
        M1 --> M1_Sub["可学习原型与梯度均衡/Learnable Prototypes & Gradient Equalization"]
        M2 --> M2_Sub["分类查询与定位查询分离/Separate Classification & Localization Queries"]
        Results --> R1["在三个数据集上超越SOTA/Outperforms SOTA on Three Datasets"]
    ```

- **[arXiv260101] RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention**
  - **tags:** [mlsys], [diffusion models], [sparse attention, hardware-efficient, block-wise mean, spatiotemporal-aware permutation, first-frame sink]
  - **authors:** Aiyue Chen, Yaofu Liu, Junjian Huang, Guang Lian, Yiwu Yao, Wangli Lan, Jing Lin, Zhixin Ma, Tingting Zhou, Harry Yang
  - **institution:** Huawei Technologies Co., Ltd, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24086
  - **contributions:** 1. Proposes using block-wise mean values as representative tokens for low-overhead sparse mask prediction. 2. Implements spatiotemporal-aware token permutation to enhance the effectiveness of the sparse attention pattern. 3. Introduces a first-frame sink mechanism specifically optimized for video generation scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a00bea8b8b0ba5148f027ed70530ca8414368fe966ff2da38fd4f95487de2b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes RainFusion2.0, a hardware-efficient and adaptive sparse attention mechanism to reduce the high computational cost of Diffusion Transformers in video and image generation. The method uses block-wise mean tokens for mask prediction and introduces spatiotemporal-aware permutation and a first-frame sink mechanism. Experiments show it achieves 80% sparsity with 1.5-1.8x speedup without quality loss, and generalizes across models and hardware.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RainFusion2.0] --> B[核心问题/Problem: DiT模型注意力计算成本高，现有稀疏注意力方法开销大且硬件通用性差]
        A --> C[主要方法/Method: 块均值代表令牌预测，时空感知令牌重排，首帧下沉机制]
        A --> D[关键结果/Results: 80%稀疏度，1.5~1.8倍端到端加速，跨模型和硬件有效]
    ```

- **[arXiv260101] Factorized Learning for Temporally Grounded Video-Language Models**
  - **tags:** [cv], [video-language models], [temporal grounding, factorized learning, preference optimization, evidence tokens, video understanding]
  - **authors:** Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng
  - **institution:** National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24097
  - **code:** https://github.com/nusnlp/d2vlm
  - **contributions:** 1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a "grounding then answering with evidence referencing" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Factorized Learning for Temporally Grounded Video-Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Existing models struggle with accurate temporal grounding for event-level perception. 现有模型在事件级感知的精确时间定位上存在困难。]
        C[主要方法/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). 提出D2VLM框架和因子化偏好优化算法。]
        D[关键结果/Results: Demonstrates clear advantage on various tasks. 在多种任务上展现出明显优势。]
    ```

- **[arXiv260101] Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation**
  - **tags:** [ai], [text-to-motion generation], [Latent Motion Reasoning, Dual-Granularity Tokenizer, Semantic-Kinematic Impedance Mismatch, Hierarchical Motor Control, Autoregressive Reasoning]
  - **authors:** Yijie Qian, Juncheng Wang, Yuxiang Feng, Chao Xu, Wang Lu, Yang Liu, Baigui Sun, Yiqiang Chen, Yong Liu, Shujun Wang
  - **institution:** Zhejiang University, Hong Kong Polytechnic University, Institute of Computing Technology, Chinese Academy of Sciences, IROOTECH TECHNOLOGY, King's College London
  - **link:** https://arxiv.org/pdf/2512.24100
  - **code:** https://chenhaoqcdyq.github.io/LMR/
  - **contributions:** 1. Identifies the Semantic-Kinematic Impedance Mismatch as a fundamental bottleneck in direct text-to-motion translation. 2. Proposes Latent Motion Reasoning (LMR), a novel two-stage "Think-then-Act" generation framework inspired by hierarchical motor control. 3. Introduces a Dual-Granularity Tokenizer that disentangles motion into separate Reasoning and Execution latent spaces for planning and physical fidelity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce14b37e6dbbad8a248665b68bdf378f0c30148636495248943d2552d0ca994c_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a core problem in text-to-motion generation where directly mapping language to motion (a "System 1" approach) leads to a semantic-kinematic mismatch. To solve this, it proposes Latent Motion Reasoning (LMR), a two-stage method that first reasons in a high-level latent space before generating detailed motion, improving both semantic alignment and physical plausibility. The results validate that motion planning is more effective in a learned motion-aligned concept space than in natural language.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Semantic-Kinematic Impedance Mismatch]
        C[主要方法/Method: Latent Motion Reasoning (LMR) with Dual-Granularity Tokenizer]
        D[关键结果/Results: Improved semantic alignment and physical plausibility]
    ```

- **[arXiv260101] Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks**
  - **tags:** [cv], [adversarial attack], [diffusion models, monocular depth estimation, physical adversarial attack, Jacobian Vector Product Guidance, Salient Region Selection]
  - **authors:** Yongtao Chen, Yanbo Wang, Wentao Zhao, Guole Shen, Tianchen Deng, Jingchuan Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.24111
  - **contributions:** 1. Proposes a training-free, diffusion-based generative framework for creating naturalistic adversarial objects to attack Monocular Depth Estimation models. 2. Introduces a Salient Region Selection module to identify MDE-critical areas and a Jacobian Vector Product Guidance mechanism to align adversarial gradients with the diffusion model's capabilities. 3. Demonstrates through extensive experiments that the generated adversarial objects are more effective, stealthy, and physically deployable than prior texture-based attacks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/866f9f80528d36a9ff26173e3b4cfe5a8d9aaa0c968fa621b3f8bc4fe8d6694d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the vulnerability of Monocular Depth Estimation (MDE) in autonomous driving by proposing a novel adversarial attack framework. It uses a guided diffusion model to generate scene-consistent, physically plausible adversarial objects, which are shown to be more effective and stealthy than existing patch-based attacks in both digital and physical experiments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("MDE易受攻击/MDE is vulnerable")
        Problem --> P2("现有攻击不现实/Existing attacks lack realism")
        Method --> M1("无训练生成框架/Training-free generative framework")
        Method --> M2("显著区域选择/Salient Region Selection")
        Method --> M3("JVP引导/Jacobian Vector Product Guidance")
        Results --> R1("高攻击有效性/High attack effectiveness")
        Results --> R2("高隐蔽性/High stealthiness")
        Results --> R3("强物理可部署性/Strong physical deployability")
    ```

- **[arXiv260101] Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design**
  - **tags:** [mlsys], [llm training], [Neural Architecture Search, Few-Shot Prompting, Code Deduplication, Automated Architecture Design, Lightweight Validation]
  - **authors:** Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte
  - **institution:** Computer Vision Lab, CAIDAS & IFI, University of Würzburg
  - **link:** https://arxiv.org/pdf/2512.24120
  - **contributions:** 1. Few-Shot Architecture Prompting (FSAP), a systematic study determining n=3 examples as optimal for LLM-based architecture generation in vision tasks. 2. Whitespace-Normalized Hash Validation, a lightweight (&lt;1ms) deduplication method providing 100x speedup over AST parsing. 3. A dataset-balanced evaluation methodology for comparing architectures across heterogeneous vision benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c601e530777a60f08e6c3607d74352f9046689f8debe4c26a2d893b33b09df97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses challenges in using LLMs for automated neural network architecture design in computer vision. It introduces a systematic few-shot prompting strategy (FSAP) and a fast deduplication method to prevent redundant training. The main conclusion is that using three examples in prompts best balances diversity and focus, and the lightweight validation enables efficient large-scale generation of unique architectures.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Enhancing LLM-Based Neural Network Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("自动化架构设计的挑战 / Challenges in Automated Architecture Design")
        Problem --> P2("LLM提示与验证策略未系统研究 / LLM Prompting & Validation Not Systematically Studied")
        Method --> M1("少样本架构提示 / Few-Shot Architecture Prompting (FSAP)")
        Method --> M2("空白标准化哈希验证 / Whitespace-Normalized Hash Validation")
        Results --> R1("n=3示例为最优 / n=3 Examples is Optimal")
        Results --> R2("验证速度提升100倍 / 100x Speedup in Validation")
        Results --> R3("生成1900个独特架构 / Generated 1,900 Unique Architectures")
    ```

- **[arXiv260101] GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation**
  - **tags:** [ai], [multimodal reasoning], [geometric problem solving, hierarchical evaluation, vision-language models, reasoning benchmark, theorem application]
  - **authors:** Yuan Feng, Yue Yang, Xiaohan He, Jiatong Zhao, Jianlong Chen, Zijun Chen, Daocheng Fu, Qi Liu, Renqiu Xia, Bo Zhang, Junchi Yan
  - **institution:** Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Fudan University, The Chinese University of Hong Kong, Shenzhen
  - **link:** https://arxiv.org/pdf/2512.24119
  - **code:** https://github.com/FrontierX-Lab/GeoBench
  - **contributions:** 1. Proposes GeoBench, a novel hierarchical benchmark for geometric problem-solving with four reasoning levels (Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, Self-Reflective Backtracking). 2. Introduces six formally verified tasks generated via TrustGeoGen to systematically assess multimodal reasoning capabilities. 3. Provides key empirical findings, such as the critical influence of sub-goal decomposition and the unexpected degradation from Chain-of-Thought prompting in certain tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1e39e6ff4463092b188cd464fc8a691dd432dcd024fb86c0073997019d94d82_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GeoBench, a hierarchical benchmark to address limitations in evaluating geometric reasoning in vision-language models, such as test data contamination and lack of diagnostic granularity. It systematically assesses models across four reasoning levels using six formally verified tasks. Key findings show that while reasoning models outperform general MLLMs, performance drops with complexity, and sub-goal decomposition is crucial, whereas Chain-of-Thought prompting can sometimes harm performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有评估的局限性/Limitations of Current Evaluations]
        B1 --> B2[测试数据污染/Test Data Contamination]
        B1 --> B3[重答案轻推理/Overemphasis on Final Answers]
        B1 --> B4[诊断粒度不足/Insufficient Diagnostic Granularity]
        C --> C1[分层基准/Hierarchical Benchmark]
        C1 --> C2[四个推理层级/Four Reasoning Levels]
        C2 --> C3[视觉感知/Visual Perception]
        C2 --> C4[目标导向规划/Goal-Oriented Planning]
        C2 --> C5[严谨定理应用/Rigorous Theorem Application]
        C2 --> C6[自反思回溯/Self-Reflective Backtracking]
        C --> C7[六个验证任务/Six Verified Tasks]
        C7 --> C8[通过TrustGeoGen生成/Generated via TrustGeoGen]
        D --> D1[推理模型表现更好/Reasoning Models Outperform General MLLMs]
        D --> D2[性能随复杂度下降/Performance Declines with Complexity]
        D --> D3[子目标分解关键/Sub-goal Decomposition Critical]
        D --> D4[CoT提示可能有害/Chain-of-Thought Can Degrade Performance]
    ```

- **[arXiv260101] GARDO: Reinforcing Diffusion Models without Reward Hacking**
  - **tags:** [ai], [reinforcement learning], [reward hacking, diffusion models, regularization, mode collapse, online RL]
  - **authors:** Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan
  - **institution:** Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.24138
  - **code:** https://tinnerhrhe.github.io/gardo_project
  - **contributions:** 1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GARDO: Reinforcing Diffusion Models without Reward Hacking] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Reward Hacking in RL for Diffusion Models/扩散模型RL中的奖励破解]
        B --> B2[Proxy Reward Mismatch & Mode Collapse/代理奖励不匹配与模式崩溃]
        C --> C1[Gated & Adaptive Regularization/门控自适应正则化]
        C --> C2[Diversity-aware Reward Optimization/多样性感知奖励优化]
        D --> D1[Mitigates Reward Hacking/缓解奖励破解]
        D --> D2[Enhances Diversity & Maintains Efficiency/提升多样性并保持效率]
    ```

- **[arXiv260101] Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning**
  - **tags:** [cv], [diffusion models], [Preference Mode Collapse, Reinforcement Learning from Human Feedback, reward hacking, generative diversity, directional correction]
  - **authors:** Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu, Xiu Li
  - **institution:** Tsinghua University, AMAP, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.24146
  - **contributions:** 1. Introduces and quantifies the phenomenon of Preference Mode Collapse (PMC) in diffusion model alignment. 2. Proposes DivGenBench, a novel benchmark to measure the extent of PMC. 3. Proposes the Directional Decoupling Alignment (D²-Align) framework to mitigate PMC by directionally correcting the reward signal.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f62594e4cc99762132258ccba4873b41aea26a85dbeb402b03da55458d0e32bf_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies Preference Mode Collapse (PMC), where diffusion models over-optimize for reward scores and lose generative diversity. To address this, the authors propose D²-Align, a framework that learns a directional correction to the reward signal to prevent collapse. The method achieves better alignment with human preference while preserving output diversity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题/Paper Title: Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Preference Mode Collapse (PMC)导致生成多样性下降/PMC degrades generative diversity]
        C[主要方法/Method: 提出D²-Align框架，方向性校正奖励信号/Proposes D²-Align framework to directionally correct reward signal]
        D[关键结果/Results: 在保持多样性的同时实现更好的人类偏好对齐/Achieves better human preference alignment while preserving diversity]
    ```

- **[arXiv260101] Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset**
  - **tags:** [cv], [multimodal learning], [industrial defect dataset, vision-language foundation model, diffusion model, open-vocabulary understanding, data-efficient adaptation]
  - **authors:** TsaiChing Ni, ZhenQi Chen, YuanFu Yang
  - **institution:** Institute of Intelligent Systems, National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.24160
  - **contributions:** 1. Introduces IMDD-1M, the first large-scale multimodal industrial defect dataset with 1M aligned image-text pairs spanning 60+ materials and 400+ defect types. 2. Trains a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. 3. Demonstrates the model's data-efficient adaptation capability, achieving comparable performance with &lt;5% of task-specific data compared to dedicated expert models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a4859b01599d7daa2377f92cee5fef8f3e4a9889e10c4f5a81f7747139b84b0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces IMDD-1M, a large-scale multimodal dataset for industrial defect analysis, and uses it to train a specialized diffusion-based vision-language foundation model. The model serves as a generalizable base that can be efficiently adapted to specific tasks with minimal data, showing strong performance for industrial inspection and generation tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Open-Vocabulary Industrial Defect Understanding<br>开放词汇工业缺陷理解] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏大规模工业多模态数据集<br>Lack of large-scale industrial multimodal dataset]
        C --> C1[构建IMDD-1M数据集<br>Build IMDD-1M Dataset]
        C --> C2[训练扩散式视觉语言基础模型<br>Train Diffusion-based VLM]
        D --> D1[实现数据高效适应<br>Achieves Data-Efficient Adaptation]
        D --> D2[性能媲美专家模型<br>Performance Comparable to Expert Models]
    ```

- **[arXiv260101] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models**
  - **tags:** [cv], [multimodal reasoning], [diffusion models, generative reasoning, image-to-image generation, vision-centric tasks, multimodal large language models]
  - **authors:** Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng
  - **institution:** Shanghai AI Laboratory, Nanjing University, The Chinese University of Hong Kong, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.24165
  - **code:** https://diffthinker-project.github.io
  - **contributions:** 1. Establishes a novel Generative Multimodal Reasoning paradigm that reformulates reasoning as a native image-to-image generation task. 2. Introduces DiffThinker, a diffusion-based reasoning framework designed for superior logical consistency and spatial precision in vision-centric tasks. 3. Provides the first systematic investigation into the properties of this paradigm, identifying core characteristics like efficiency, controllability, native parallelism, and collaboration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4fc496e91826df74d41f1bf12c921b51dc427d1e0de947424de30a6dd0a5ca4_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that current Multimodal Large Language Models (MLLMs) have text-centric reasoning processes that underperform on complex vision-centric tasks. To address this, it proposes DiffThinker, a new framework that uses diffusion models to perform multimodal reasoning by directly generating solution images. Extensive experiments show that DiffThinker significantly outperforms leading models like GPT-5 and Gemini-3-Flash, demonstrating the promise of generative, image-based reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["DiffThinker: 生成式多模态推理 / Generative Multimodal Reasoning"] --> Problem["MLLMs推理以文本为中心，视觉任务性能不佳 / MLLMs' reasoning is text-centric, suboptimal for vision tasks"]
        Root --> Method["提出扩散模型框架，将推理重构为图像生成任务 / Proposes diffusion framework, reformulates reasoning as image-to-image task"]
        Root --> Results["显著超越GPT-5等模型，展现新范式的潜力 / Significantly outperforms GPT-5 etc., shows promise of new paradigm"]
    ```

- **[arXiv260101] Bayesian Self-Distillation for Image Classification**
  - **tags:** [cv], [image classification], [self-distillation, Bayesian inference, calibration, robustness, label noise]
  - **authors:** Anton Adelöw, Matteo Gamba, Atsuto Maki
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.24162
  - **code:** https://github.com/antonadelow/BSD
  - **contributions:** 1. Proposes Bayesian Self-Distillation (BSD), a novel method for constructing sample-specific target distributions via Bayesian inference using the model's own predictions, eliminating reliance on hard targets after initialization. 2. Demonstrates that BSD consistently improves test accuracy and significantly reduces Expected Calibration Error (ECE) compared to existing self-distillation methods across various architectures and datasets. 3. Shows that BSD enhances model robustness against data corruptions, perturbations, and label noise, achieving state-of-the-art robustness under label noise when combined with a contrastive loss.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddb506b17a44678d283fc42cb840d215e54f2897aec39dd9d9f698c270430ac8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of using hard targets in training deep neural networks for image classification, which leads to overconfidence and poor calibration. The authors propose Bayesian Self-Distillation (BSD), a method that uses Bayesian inference on the model's own predictions to create sample-specific soft targets, avoiding hard targets after initialization. The method is shown to improve accuracy, calibration, and robustness across several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bayesian Self-Distillation for Image Classification] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[硬标签导致过度自信与校准差/Hard targets cause overconfidence & poor calibration]
        C --> C1[贝叶斯自蒸馏构建样本特定目标/Bayesian Self-Distillation constructs sample-specific targets]
        C --> C2[不依赖初始化后的硬标签/No reliance on hard targets after initialization]
        D --> D1[提升准确率与校准/Improves accuracy & calibration]
        D --> D2[增强对噪声与扰动的鲁棒性/Enhances robustness to noise & corruptions]
    ```

- **[arXiv260101] Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges**
  - **tags:** [cv], [hyperspectral image segmentation], [deep global clustering, memory-efficient segmentation, unsupervised disease detection, multi-objective loss balancing]
  - **authors:** Yu-Tang Chang, Pin-Wei Chen, Shih-Fang Chen
  - **institution:** National Taiwan University
  - **link:** https://arxiv.org/pdf/2512.24172
  - **code:** https://github.com/b05611038/HSI_global_clustering
  - **contributions:** 1. Proposed Deep Global Clustering (DGC), a conceptual framework for memory-efficient hyperspectral image segmentation that learns global clustering from local patches without pre-training., 2. Demonstrated the framework's ability to achieve background-tissue separation and unsupervised disease detection on a leaf disease dataset with high efficiency (training &lt;30 min on consumer hardware)., 3. Identified and analyzed the key challenge of optimization instability due to multi-objective loss balancing, positioning the work as intellectual scaffolding for future principled solutions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5ca8fa4968e4f11184d8faa275b2f2edacdd6e6374072e519e722e16016a06_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational bottleneck in hyperspectral image (HSI) analysis by proposing Deep Global Clustering (DGC), a memory-efficient framework that learns global segmentation from local patches without pre-training. It successfully demonstrates background-tissue separation and unsupervised disease detection on agricultural data. However, the main conclusion is that while the design philosophy is promising, the framework suffers from optimization instability due to loss balancing, requiring more principled solutions for stable implementation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Global Clustering for Hyperspectral Image Segmentation<br>高光谱图像分割的深度全局聚类] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Computational bottlenecks in HSI analysis<br>Foundation models fail in domain-specific transfer]
        C[主要方法/Method<br>Deep Global Clustering (DGC)<br>Memory-efficient, learns from local patches]
        D[关键结果/Results<br>Achieves background-tissue separation<br>Shows unsupervised disease detection<br>Suffers from optimization instability]
    ```

- **[arXiv260101] Guiding a Diffusion Transformer with the Internal Dynamics of Itself**
  - **tags:** [mlsys], [diffusion models], [Internal Guidance, Diffusion Transformer, Classifier-Free Guidance, Sampling Guidance, Denoising Diffusion]
  - **authors:** Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu
  - **institution:** University of Electronic Science and Technology of China, National University of Singapore, Sun Yat-sen University, North China Institute of Computer Systems Engineering
  - **link:** https://arxiv.org/pdf/2512.24176
  - **contributions:** 1. Proposes Internal Guidance (IG), a novel sampling guidance strategy that uses intermediate-layer outputs within a Diffusion Transformer to improve generation quality. 2. Introduces an auxiliary supervisory signal at an intermediate layer during training and extrapolates outputs during sampling, requiring no extra training, degradation strategies, or additional sampling steps. 3. Demonstrates state-of-the-art performance on ImageNet 256x256, achieving an FID of 1.19 when combined with CFG, and shows significant improvements in training efficiency and generation quality across various baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/605c83917fcbd68353ebfb53e0da152ab446f909b59cdfd81ee3bd6c6023a44f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the issue of standard classifier-free guidance (CFG) causing over-simplified or distorted samples in diffusion models. It proposes Internal Guidance (IG), a simple method that adds auxiliary supervision to an intermediate layer during training and extrapolates outputs during sampling. The method significantly improves generation quality and efficiency, achieving state-of-the-art FID scores on ImageNet.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Guiding a Diffusion Transformer with the Internal Dynamics of Itself] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[标准CFG导致样本过简或失真/Standard CFG leads to over-simplified or distorted samples]
    B --> B2[现有替代方法需要额外训练或步骤/Existing alternatives require extra training or steps]
    C --> C1[提出内部引导/Propose Internal Guidance (IG)]
    C --> C2[训练时中间层辅助监督/Auxiliary supervision on intermediate layer during training]
    C --> C3[采样时输出外推/Extrapolate outputs during sampling]
    D --> D1[显著提升训练效率和生成质量/Significant improvements in training efficiency and generation quality]
    D --> D2[在ImageNet上达到SOTA FID=1.19/Achieves SOTA FID=1.19 on ImageNet]
    ```

- **[arXiv260101] PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds**
  - **tags:** [cv], [3D point cloud regression], [PointRAFT, partial point clouds, object height embedding, PointNet++, RGB-D]
  - **authors:** Pieter M. Blok, Haozhou Wang, Hyun Kwon Suh, Peicheng Wang, James Burridge, Wei Guo
  - **institution:** The University of Tokyo, Sejong University
  - **link:** https://arxiv.org/pdf/2512.24193
  - **code:** https://github.com/pieterblok/pointraft.git
  - **contributions:** 1. Proposed PointRAFT, a high-throughput point cloud regression network for directly predicting continuous 3D shape properties from partial point clouds. 2. Introduced a novel object height embedding as an architectural component to incorporate tuber height, improving regression performance under occlusion. 3. Demonstrated superior performance and real-time capability on a large-scale agricultural dataset, achieving high accuracy for potato tuber weight prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c71915dfc1ce9e8203a646c9242d84788697d59124a23181fa869b682fd4cdf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of systematically underestimating potato tuber weight from incomplete 3D point clouds captured on harvesters. It proposes PointRAFT, a deep learning network that directly regresses weight from partial point clouds using a novel object height embedding. The method significantly outperforms baselines and achieves real-time processing speeds suitable for commercial harvesters.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds"] --> Problem["核心问题/Problem: Incomplete point clouds from RGB-D lead to weight underestimation"]
        Root --> Method["主要方法/Method: PointRAFT network with object height embedding for direct regression"]
        Root --> Results["关键结果/Results: Low error (MAE 12.0g), high speed (150 tubers/sec)"]
    ```

- **[arXiv260101] CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers**
  - **tags:** [mlsys], [diffusion models], [diffusion transformer, inference acceleration, interval caching, cross-attention, training-free]
  - **authors:** Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim
  - **institution:** Korea University, KAIST AI
  - **link:** https://arxiv.org/pdf/2512.24195
  - **code:** https://casl-ku.github.io/CorGi
  - **contributions:** 1. Proposes CorGi, a training-free framework that accelerates DiT inference by selectively caching and reusing outputs of low-contribution transformer blocks across denoising steps. 2. Introduces CorGi+, an extension for text-to-image tasks that uses cross-attention maps to identify salient tokens and applies partial attention updates to protect important details. 3. Demonstrates significant speedup (up to 2.0x on average) on state-of-the-art DiT models while preserving high generation quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92c567dd9feea285991c6dbbdd5d85a85129420ce1925c585625dca252b806c4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high inference cost of Diffusion Transformers (DiT) by proposing CorGi, a training-free acceleration framework that reduces redundant computation through contribution-guided, block-wise interval caching. For text-to-image tasks, CorGi+ further refines the approach using cross-attention maps for partial updates. Evaluations show the methods achieve up to 2.0x speedup while maintaining image quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CorGi: Contribution-Guided Block-Wise Interval Caching<br>CorGi: 贡献引导的块级间隔缓存] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DiT推理成本高<br>High DiT Inference Cost]
        B1 --> B2[去噪步骤间存在冗余计算<br>Redundant Computation Across Steps]
        C --> C1[CorGi: 缓存低贡献块<br>Cache Low-Contribution Blocks]
        C1 --> C2[CorGi+: 使用交叉注意力图<br>Use Cross-Attention Maps]
        C2 --> C3[部分注意力更新<br>Partial Attention Updates]
        D --> D1[加速高达2.0倍<br>Up to 2.0x Speedup]
        D --> D2[保持生成质量<br>Preserve Generation Quality]
    ```

- **[arXiv260101] RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation**
  - **tags:** [cv], [semantic navigation], [zero-shot navigation, monocular camera, in-context learning, 3D foundation models, open-vocabulary]
  - **authors:** Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu
  - **institution:** Beihang University, Beijing Academy of Artificial Intelligence (BAAI), Institute of Automation, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.24212
  - **contributions:** 1. Proposes RANGER, a zero-shot semantic navigation framework that operates using only a monocular camera, eliminating the dependency on depth and pose sensors. 2. Introduces strong in-context learning capability, allowing the system to quickly adapt to new environments by observing a short video without architectural changes or fine-tuning. 3. Integrates key components like 3D reconstruction, semantic point cloud generation, and VLM-driven exploration into a cohesive framework, validated on benchmarks and real-world tests.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec071dfb8e64cc84b9968fc8e7e295d190c6ad5e4a190eaac70aeb44778d22a5_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes RANGER, a monocular zero-shot semantic navigation framework that uses 3D foundation models to operate without depth or pose data and can quickly adapt to new environments via in-context learning from short videos. Experiments show it achieves competitive navigation performance and superior adaptability without prior 3D mapping.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RANGER: A Monocular Zero-Shot Semantic Navigation Framework] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[依赖深度与位姿/Depth & Pose Dependency]
        B --> B2[缺乏情境学习能力/Lack of ICL Capability]
        C --> C1[仅单目相机/Monocular Camera Only]
        C --> C2[3D基础模型/3D Foundation Models]
        C --> C3[情境学习适应/In-Context Learning Adaptation]
        D --> D1[竞争性导航性能/Competitive Navigation Performance]
        D --> D2[优越的情境学习适应性/Superior ICL Adaptability]
    ```

- **[arXiv260101] Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19**
  - **tags:** [cv], [medical image classification], [imbalanced data, progressive generative adversarial network (ProGAN), slime mould algorithm (SMA), ResNet, synthetic data generation]
  - **authors:** Sina Jahromi, Farshid Hajati, Alireza Rezaee, Javaher Nourian
  - **institution:** University of Tehran, University of New England
  - **link:** https://arxiv.org/pdf/2512.24214
  - **contributions:** 1. Proposes a Progressive GAN (ProGAN) to generate synthetic medical images to address data imbalance. 2. Introduces a weighted approach for combining synthetic and real data before classification. 3. Employs the Slime Mould Algorithm (SMA), a multi-objective meta-heuristic, to optimize the hyper-parameters of the ResNet classifier.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95bbc93b1f6719a8c5ffdfe98638f4844e331a5a71715b6698a65f669e17f927_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of imbalanced medical image data, particularly for COVID-19 detection from chest X-rays. The proposed method uses a Progressive GAN to generate synthetic data and a weighted combination of real and synthetic data, with a ResNet classifier optimized by the Slime Mould Algorithm. The model achieved high accuracy (95.5% for 4-class, 98.5% for 2-class) on an imbalanced dataset, demonstrating its effectiveness for pandemic-related medical image classification.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19] --> B(核心问题/Problem: 医学图像数据不平衡/Imbalanced Medical Image Data)
    A --> C(主要方法/Method: 使用ProGAN生成数据并用SMA优化ResNet/Use ProGAN for Data Generation and SMA to Optimize ResNet)
    A --> D(关键结果/Results: 在COVID-19胸部X光数据集上取得高准确率/High Accuracy on COVID-19 Chest X-ray Dataset)
    B --> B1(挑战: 疫情中数据集更不平衡/Challenge: Increased Imbalance During Pandemics)
    C --> C1(步骤1: 用ProGAN生成合成数据/Step 1: Generate Synthetic Data with ProGAN)
    C --> C2(步骤2: 加权结合真实与合成数据/Step 2: Weighted Combination of Real and Synthetic Data)
    C --> C3(步骤3: 用SMA优化ResNet超参数/Step 3: Optimize ResNet Hyper-parameters with SMA)
    D --> D1(4分类准确率: 95.5%/4-Class Accuracy: 95.5%)
    D --> D2(2分类准确率: 98.5%/2-Class Accuracy: 98.5%)
    ```

- **[arXiv260101] Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes**
  - **tags:** [cv], [video editing], [video diffusion, temporal coherence, 3D causal VAE, asset editing, two-stage alignment]
  - **authors:** Shuyun Wang, Haiyang Sun, Bing Wang, Hangjun Ye, Xin Yu
  - **institution:** The University of Queensland, Xiaomi EV
  - **link:** https://arxiv.org/pdf/2512.24227
  - **code:** https://github.com/wm-research/mirage
  - **contributions:** 1. Proposes Mirage, a one-step video diffusion model for photorealistic and temporally coherent asset editing in driving scenes. 2. Introduces a method to inject temporally agnostic latents from a pretrained 2D encoder into a 3D decoder to restore spatial detail while preserving temporal causality. 3. Presents a two-stage data alignment strategy combining coarse 3D and fine 2D refinement to mitigate pose misalignment between scene objects and inserted assets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3ac6637a44c226026b70a284fe5a6fad57f59339114a7deea74b0da4d2aebb_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Mirage, a one-step video diffusion model for editing objects in driving scene videos. It addresses challenges in maintaining visual fidelity and temporal coherence by injecting 2D encoder features into a 3D decoder and using a two-stage alignment strategy. Experiments show the method achieves high realism and consistency, and it can generalize to other video-to-video tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mirage: 驾驶场景中的逼真连贯资产编辑 / Mirage: Photorealistic Coherent Asset Editing in Driving Scenes] --> B
        A --> C
        A --> D
        B[核心问题 / Problem] --> B1[现有方法难以同时保证高视觉保真度和时间连贯性 / Existing methods struggle with high fidelity and temporal coherence]
        C[主要方法 / Method] --> C1[基于文本到视频扩散先验 / Builds on text-to-video diffusion prior]
        C --> C2[注入2D编码器潜在特征到3D解码器 / Injects 2D encoder latents into 3D decoder]
        C --> C3[两阶段数据对齐策略 / Two-stage data alignment strategy]
        D[关键结果 / Results] --> D1[实现高真实感和时间一致性 / Achieves high realism and temporal consistency]
        D --> D2[可泛化到其他视频到视频任务 / Generalizes to other video-to-video tasks]
    ```

- **[arXiv260101] MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model**
  - **tags:** [cv], [facial expression recognition], [facial emotion recognition, generalization, foundation model, masked autoencoder, downstream task]
  - **authors:** Rahul Medicharla, Alper Yilmaz
  - **institution:** The Ohio State University
  - **link:** https://arxiv.org/pdf/2512.24231
  - **code:** https://github.com/OSUPCVLab/EmotionFromFaceImages
  - **contributions:** 1. Proposes MotivNet, a novel FER model built on the Meta-Sapiens foundation model to achieve strong generalization without cross-domain training. 2. Defines and applies three criteria (benchmark performance, model similarity, data similarity) to validate a new downstream task for the Sapiens foundation model. 3. Demonstrates that the proposed approach achieves competitive performance across diverse datasets, making FER more viable for real-world, in-the-wild applications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81d3ca85dac4626b7644912941beba01e188b0d41eefb51c00fdb16cd74177b8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces MotivNet, a facial emotion recognition model that uses the Meta-Sapiens foundation model as a backbone to achieve strong generalization across datasets without requiring cross-domain training. The authors validate MotivNet as a suitable downstream task for Sapiens using specific criteria and show it achieves competitive performance. The work aims to make FER more robust and applicable in real-world scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[FER模型泛化能力弱 / Poor generalization of FER models]
        P1 --> P2[跨域训练不切实际 / Cross-domain training is impractical]
        Method[主要方法/Method] --> M1[使用Sapiens作为主干 / Use Sapiens as backbone]
        M1 --> M2[定义下游任务评估标准 / Define downstream task evaluation criteria]
        Results[关键结果/Results] --> R1[跨数据集具有竞争力 / Competitive across datasets]
        R1 --> R2[验证为有效的下游任务 / Validated as a viable downstream task]
    ```

- **[arXiv260101] ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation**
  - **tags:** [cv], [semantic segmentation], [open-vocabulary segmentation, attention refinement, training-free, CLIP, plug-and-play]
  - **authors:** Ziquan Liu, Zhewei Zhu, Xuyang Shi
  - **institution:** Southwest University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24224
  - **contributions:** 1. Proposes a lightweight, learnable Attention Refinement Module (ARM) that adaptively fuses hierarchical CLIP features to refine coarse spatial details. 2. Introduces a "train once, use anywhere" paradigm where ARM, trained once on a general dataset, acts as a universal plug-and-play post-processor for diverse training-free frameworks. 3. Demonstrates consistent performance improvements across multiple benchmarks with negligible inference overhead, establishing an efficient paradigm for training-free OVSS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f320a432567db109650fb122082758ebd1181c40fae33b7e17d4556ae80c600_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of coarse, image-level CLIP representations for open-vocabulary semantic segmentation. It proposes the Attention Refinement Module (ARM), a learnable module that adaptively fuses hierarchical CLIP features to refine pixel-level details. Experiments show ARM consistently boosts baseline performance with minimal overhead, offering an efficient plug-and-play solution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[ARM: CLIP-based Open-vocabulary Semantic Segmentation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: CLIP特征粗糙，缺乏像素级细节] --> P1[现有方法/Existing Methods]
        P1 --> P1_1[依赖外部模型，计算昂贵/Relies on external models, expensive]
        P1 --> P1_2[静态启发式方法，效果次优/Static heuristics, sub-optimal]
        Method[主要方法/Method: 注意力精炼模块 (ARM)] --> M1[轻量可学习模块/Lightweight Learnable Module]
        M1 --> M2[语义引导的交叉注意力/Semantically-guided Cross-Attention]
        M2 --> M3[自适应融合层次特征/Adaptively fuses hierarchical features]
        Method --> M4[训练一次，随处使用/Train once, use anywhere]
        Results[关键结果/Results] --> R1[一致提升基线性能/Consistently boosts baseline performance]
        Results --> R2[推理开销可忽略/Negligible inference overhead]
        Results --> R3[高效有效的训练免费范式/Efficient & effective training-free paradigm]
    ```

- **[arXiv260101] MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation**
  - **tags:** [cv], [semantic segmentation], [Mamba, multimodal fusion, event camera, dual-branch, temporal modeling]
  - **authors:** Fuqiang Gu, Yuanke Li, Xianlei Long, Kangping Ji, Chao Chen, Qingyi Gu, Zhenliang Ni
  - **institution:** Chongqing University, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.24243
  - **code:** https://github.com/CQU-UISC/MambaSeg
  - **contributions:** 1. Proposes MambaSeg, a novel dual-branch semantic segmentation framework using parallel Mamba encoders for efficient RGB and event stream modeling. 2. Introduces the Dual-Dimensional Interaction Module (DDIM) with Cross-Spatial (CSIM) and Cross-Temporal (CTIM) modules for fine-grained multimodal fusion along spatial and temporal dimensions. 3. Achieves state-of-the-art segmentation performance on DDD17 and DSEC datasets while significantly reducing computational cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69734d37c2ca74823c81a268f32960cc1bdcd9f8ca90e9ad27d6167cfe5d04fe_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of RGB-based semantic segmentation in challenging conditions by proposing MambaSeg, a framework that fuses RGB images and event camera data using Mamba encoders and a novel dual-dimensional fusion module. The method effectively leverages the complementary strengths of both modalities, achieving superior accuracy and efficiency compared to existing approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MambaSeg: 图像-事件语义分割 / Image-Event Semantic Segmentation"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["RGB方法在动态/弱光下失效 / RGB methods fail under fast motion/low-light"]
        Problem --> P2["事件流缺乏颜色纹理 / Event streams lack color & texture"]
        Problem --> P3["现有融合方法计算昂贵且忽略时序 / Existing fusion is costly & neglects temporal dynamics"]
        Method --> M1["双分支Mamba编码器 / Dual-branch Mamba encoders"]
        Method --> M2["双维度交互模块 (DDIM) / Dual-Dimensional Interaction Module (DDIM)"]
        M2 --> M2a["跨空间交互模块 (CSIM) / Cross-Spatial Interaction Module (CSIM)"]
        M2 --> M2b["跨时间交互模块 (CTIM) / Cross-Temporal Interaction Module (CTIM)"]
        Results --> R1["在DDD17/DSEC上SOTA性能 / SOTA performance on DDD17 & DSEC"]
        Results --> R2["显著降低计算成本 / Significantly reduces computational cost"]
    ```

- **[arXiv260101] Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT**
  - **tags:** [cv], [medical image reconstruction], [metal artifact reduction, denoising diffusion models, manifold projection, physics simulation, foundation models]
  - **authors:** Zhi Li, Yaqi Wang, Bingtao Ma, Yifan Zhang, Huiyu Zhou, Shuai Wang
  - **institution:** Hangzhou Dianzi University, University of Leicester, Hangzhou Dental Hospital Group
  - **link:** https://arxiv.org/pdf/2512.24260
  - **code:** https://github.com/ (URL from abstract)
  - **contributions:** 1. An Anatomically-Adaptive Physics Simulation (AAPS) pipeline for synthesizing high-fidelity training data, bridging the synthetic-to-real gap. 2. A DMP-Former model that reformulates restoration as a deterministic manifold projection, enabling artifact removal in a single forward pass and eliminating the need for slow iterative sampling. 3. A Semantic-Structural Alignment (SSA) module that leverages priors from medical foundation models (MedDINOv3) to ensure the clinical plausibility of the reconstructed images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd16a1a110bf902fb100678cfdd04e4141b492d81205b2bcb44cce6e47345d9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the Physically-Grounded Manifold Projection (PGMP) framework to reduce metal artifacts in Dental CBCT scans. The method uses a physics-based data synthesis pipeline, a deterministic transformer for fast reconstruction, and a module that aligns outputs with anatomical priors from a foundation model. Experiments show it outperforms existing methods in efficiency and diagnostic reliability on both synthetic and clinical datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Metal artifacts in CBCT hinder diagnosis] --> B1[监督方法有频谱模糊/Supervised: spectral blurring]
        B --> B2[无监督方法有结构幻觉/Unsupervised: structural hallucinations]
        B --> B3[扩散模型采样慢/Diffusion models: slow sampling]
        C[主要方法/Method<br>PGMP Framework] --> C1[AAPS: 物理模拟合成数据/AAPS: Physics-based data synthesis]
        C --> C2[DMP-Former: 确定性流形投影/DMP-Former: Deterministic manifold projection]
        C --> C3[SSA: 基础模型先验对齐/SSA: Foundation model prior alignment]
        D[关键结果/Results<br>Outperforms SOTA] --> D1[效率高/High efficiency (single pass)]
        D --> D2[诊断可靠性高/High diagnostic reliability]
        D --> D3[在合成与临床数据上验证/Validated on synthetic & clinical data]
    ```

- **[arXiv260101] One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training**
  - **tags:** [cv], [medical image synthesis], [one-shot synthesis, language-guided concept disentanglement, data augmentation, rare disease, generative framework]
  - **authors:** Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Zhihua Wang, Fei Wu, Quanlin Li, Pinghong Zhou, Shuo Wang, Xian Yang
  - **institution:** Zhejiang University, Fudan University, Imperial College London, The University of Manchester
  - **link:** https://arxiv.org/pdf/2512.24278
  - **contributions:** 1. Proposed EndoRare, a one-shot, retraining-free generative framework for synthesizing diverse, high-fidelity images of rare gastrointestinal lesions from a single reference image., 2. Introduced a language-guided concept disentanglement method to separate pathognomonic lesion features from non-diagnostic attributes, ensuring diversity while preserving diagnostic fidelity., 3. Demonstrated that synthetic images improve both AI classifier performance (true positive rate) and novice clinician diagnostic accuracy (recall and precision) for rare pathologies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf643227c51e8cd7c3969dfe80d336673878c555397635a5c9831a5997ed5bd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the data scarcity problem for rare gastrointestinal lesions in AI development and clinical training. It proposes EndoRare, a one-shot generative framework that uses language-guided concept disentanglement to synthesize diverse and clinically plausible lesion images from a single example. The results show that these synthetic images significantly enhance the performance of AI classifiers and improve the diagnostic accuracy of novice endoscopists.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[One-shot synthesis of rare gastrointestinal lesions] --> B[核心问题/Problem: Rare lesions are infrequent, limiting AI model data and clinician training.]
        A --> C[主要方法/Method: EndoRare framework uses one-shot, language-guided concept disentanglement to generate diverse, high-fidelity synthetic images.]
        A --> D[关键结果/Results: Improves AI classifier true positive rate and novice clinician recall & precision.]
    ```

- **[arXiv260101] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation**
  - **tags:** [mlsys], [multi-modal training], [counterfactual video generation, visual hallucination, diffusion-based video editing, advantage normalization, contrastive training]
  - **authors:** Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang
  - **institution:** Tsinghua University, Beihang University, AMAP (Alibaba Group)
  - **link:** https://arxiv.org/pdf/2512.24271
  - **code:** https://amap-ml.github.io/Taming-Hallucinations/
  - **contributions:** 1. Introduces DualityForge, a framework for automatically synthesizing counterfactual video QA data using controllable diffusion-based video editing. 2. Presents DualityVidQA, a large-scale video dataset built using DualityForge to mitigate MLLM hallucinations. 3. Proposes DNA-Train, a two-stage SFT-RL training regime with pair-wise advantage normalization for stable and efficient policy optimization on contrastive data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa10c3c7d7f412fdbab92df4afa93ab6b3653346b89fc3443bf25d8615391b81_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of visual hallucinations in Multimodal Large Language Models (MLLMs) when processing counterfactual videos. The proposed solution, DualityForge, synthesizes counterfactual video QA data for training, and a novel training method, DNA-Train, leverages this data to improve grounding. Experiments show the method significantly reduces hallucinations and improves performance on both hallucination and general benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Taming Hallucinations: Boosting MLLMs' Video Understanding<br>驯服幻觉：通过反事实视频生成提升MLLM视频理解"] --> B["核心问题/Problem<br>MLLMs over-rely on language priors, causing visual hallucinations on counterfactual videos."]
        A --> C["主要方法/Method<br>DualityForge: Counterfactual video & QA synthesis.<br>DNA-Train: Contrastive SFT-RL training."]
        A --> D["关键结果/Results<br>24.0% hallucination reduction.<br>Strong generalization on benchmarks."]
    ```

- **[arXiv260101] LiftProj: Space Lifting and Projection-Based Panorama Stitching**
  - **tags:** [cv], [image stitching], [3D lifting, panoramic stitching, point cloud fusion, cylindrical projection, hole filling]
  - **authors:** Yuan Jia, Ruimin Wu, Rui Song, Jiaojiao Li, Bin Song
  - **institution:** The affiliations indicate the authors are members of IEEE. The specific institution is not explicitly stated on the first page, but the acknowledgment mentions support from the "National Nature Science Foundation of China," suggesting a Chinese research institution.
  - **link:** https://arxiv.org/pdf/2512.24276
  - **contributions:** 1. Proposes a novel panoramic stitching framework that shifts from 2D warping to a 3D consistency paradigm by first lifting images into a dense 3D point representation. 2. Introduces a global cross-view fusion process in a unified 3D coordinate system, augmented by confidence metrics, followed by a unified projection to create a geometrically consistent 360° layout. 3. Designs the framework to be modular, allowing flexible integration of various 3D lifting and completion modules to address unknown regions via hole filling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa42233e64d5c4ec1cce0b3482d738a3664cd44807fdde7db2fc77e9f5b1a58b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of ghosting and distortion in traditional 2D image stitching for complex 3D scenes with parallax. It proposes LiftProj, a method that lifts images to 3D point clouds for fusion and then projects them onto a panoramic manifold. The approach significantly reduces geometric artifacts and produces more natural panoramas in challenging scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LiftProj: Space Lifting and Projection-Based Panorama Stitching] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 2D传统拼接在复杂3D场景中产生重影和扭曲/Ghosting & distortion in 3D scenes with parallax using 2D methods]
        C[主要方法/Method: 3D空间提升与融合后投影/3D lifting, fusion, and cylindrical projection]
        D[关键结果/Results: 减少几何失真，生成更自然的全景图/Reduces geometric artifacts, yields more natural panoramas]
    ```

- **[arXiv260101] UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots**
  - **tags:** [ai], [embodied ai / robot learning], [multimodal motion generation, action streaming, discrete codebook, FSQ (Finite Scalar Quantization), zero-shot tracking]
  - **authors:** Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang
  - **institution:** Peking University, Beijing Institute for General Artificial Intelligence (BIGAI), Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.24321
  - **code:** https://jnnan.github.io/uniact/
  - **contributions:** 1. A unified two-stage framework (UniAct) that integrates a fine-tuned MLLM with a causal streaming pipeline for low-latency (&lt;500ms) multimodal instruction execution. 2. The use of a shared discrete codebook via FSQ to unify heterogeneous inputs (language, music, trajectory, motion) and constrain motions to a physically grounded manifold. 3. Introduction of the UniMoCap benchmark and demonstration of robust generalization, including a 19% improvement in zero-shot tracking success rate.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d71f14808bc6cc1375b432715e3356f7f970e4a056f6960261c0aed51ae956aa_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes UniAct, a unified framework for generating and streaming motions to humanoid robots from diverse multimodal instructions like language and music. It uses a fine-tuned MLLM and a shared discrete codebook to translate instructions into actions with low latency. The method shows improved zero-shot motion tracking and robust generalization on a new benchmark.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Bridging high-level multimodal perception with whole-body execution for humanoid robots]
        C[主要方法/Method: Two-stage framework with fine-tuned MLLM and causal streaming pipeline, using shared discrete codebook (FSQ)]
        D[关键结果/Results: Sub-500 ms latency, 19% improvement in zero-shot tracking, robust generalization on UniMoCap benchmark]
    ```

- **[arXiv260101] Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction**
  - **tags:** [cv], [medical image analysis], [CT preprocessing, quality control, foundation models, lung cancer screening, validation]
  - **authors:** Md. Enamul Hoq, Linda Larson-Prior, Fred Prior
  - **institution:** University of Arkansas for Medical Sciences
  - **link:** https://arxiv.org/pdf/2512.24294
  - **contributions:** 1. Development and validation of Virtual-Eyes, a novel, anatomically targeted 16-bit CT quality-control pipeline for lung cancer screening. 2. Quantitative demonstration that such preprocessing significantly improves the performance and calibration of generalist foundation models (e.g., RAD-DINO) for cancer risk prediction. 3. Discovery that specialist models (e.g., Sybil) can degrade with the same preprocessing, revealing their potential reliance on contextual shortcuts in raw clinical data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6f230cd5094d46243a5c6fe260cf946e5540833e1a82b86423bbe80a292b76c_w640_q70.webp
  - **Simple LLM Summary:** This paper develops Virtual-Eyes, a quality-control pipeline for lung CT scans that standardizes resolution and extracts lung regions. The study finds that this preprocessing significantly boosts the cancer risk prediction performance of generalist foundation models but can harm specialist models that have adapted to raw, unprocessed clinical data.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Virtual-Eyes: 肺癌CT质量控制流程验证] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>LDCT预处理影响未量化] --> P1[缺乏量化/Lack of Quantification]
        Problem --> P2[通用vs专用模型差异/Generalist vs. Specialist]
        Method[主要方法/Method<br>Virtual-Eyes Pipeline] --> M1[质量控制/Quality Control<br>512x512, 过滤系列]
        Method --> M2[肺块提取/Lung Block Extraction<br>HU过滤, 覆盖评分]
        Method --> M3[模型评估/Model Evaluation<br>RAD-DINO, Sybil等]
        Results[关键结果/Results<br>预处理效果不同] --> R1[提升通用模型/Improves Generalist FMs<br>RAD-DINO AUC↑]
        Results --> R2[损害专用模型/Harms Specialist Models<br>Sybil AUC↓]
        Results --> R3[揭示捷径学习/Reveals Shortcut Learning<br>上下文依赖]
    ```

- **[arXiv260101] Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention**
  - **tags:** [cv], [video object segmentation], [causal intervention, backdoor adjustment, front-door adjustment, egocentric vision, referring segmentation]
  - **authors:** Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.24323
  - **contributions:** 1. Proposes CERES, a plug-in causal framework to adapt pre-trained RVOS models to the egocentric domain. 2. Introduces dual-modal causal intervention, using backdoor adjustment to mitigate language bias and front-door adjustment to address visual confounding factors. 3. Achieves state-of-the-art performance on Ego-RVOS benchmarks, demonstrating the effectiveness of causal reasoning for robust egocentric video understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/183bcfa950e9779617d770bdebe6c5435404af301a891a12dde386054365b0da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robust Egocentric Referring Video Object Segmentation (Ego-RVOS), which suffers from dataset biases and visual confounders like motion and occlusion. The authors propose CERES, a causal framework that applies backdoor and front-door adjustments to language and visual features, respectively, to build more robust representations. Experiments show that CERES achieves state-of-the-art results, highlighting the value of causal reasoning for reliable egocentric video models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Ambiguities in egocentric videos and training data biases lead to spurious correlations.]
        C[主要方法/Method: CERES framework with dual-modal causal intervention (backdoor & front-door adjustment).]
        D[关键结果/Results: Achieves state-of-the-art performance on Ego-RVOS benchmarks.]
    ```

- **[arXiv260101] SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning**
  - **tags:** [mlsys], [agent system], [multimodal agent, reinforcement learning, tool-use, policy optimization, benchmark]
  - **authors:** Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu
  - **institution:** SenseTime Research, Tsinghua University, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.24330
  - **code:** https://github.com/OpenSenseNova/SenseNova-MARS
  - **contributions:** 1. Introduces SenseNova-MARS, a novel framework that empowers Vision-Language Models (VLMs) with interleaved visual reasoning and tool-use capabilities via Reinforcement Learning (RL). 2. Proposes the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve RL training stability and enhance the model's ability to invoke tools and reason effectively. 3. Introduces the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions, for evaluating agentic VLMs on complex visual tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a82c0f2ba917fb46c8e884b39cb5c9d4a0a7e1d4671707b44bf5a18d77fa8e73_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SenseNova-MARS, a framework that uses reinforcement learning to enable Vision-Language Models to dynamically interleave reasoning with external tools like search and image cropping. The proposed BN-GSPO algorithm improves training stability and tool-use capability. Experiments show the model achieves state-of-the-art performance on search and fine-grained image understanding benchmarks, even surpassing some proprietary models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SenseNova-MARS] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[VLMs缺乏动态工具调用与推理交织的能力/VLMs lack dynamic interleaving of tool-use and reasoning]
        C --> C1[提出BN-GSPO强化学习算法/Propose BN-GSPO RL algorithm]
        C --> C2[集成图像搜索、文本搜索、图像裁剪工具/Integrate image search, text search, image crop tools]
        D --> D1[在MMSearch和HR-MMSearch上SOTA/Achieves SOTA on MMSearch and HR-MMSearch]
        D --> D2[超越Gemini-3-Flash和GPT-5/Surpasses proprietary models like Gemini-3-Flash and GPT-5]
    ```

- **[arXiv260101] Spatial-aware Vision Language Model for Autonomous Driving**
  - **tags:** [cv], [autonomous driving], [Vision-Language Model, LiDAR, 3D spatial reasoning, Gradual Fusion Q-Former, spatial-aware QA]
  - **authors:** Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong
  - **institution:** Motional, University of Amsterdam
  - **link:** https://arxiv.org/pdf/2512.24331
  - **contributions:** 1. Proposes LVLDrive, a novel framework that upgrades VLMs for autonomous driving by incorporating LiDAR point clouds for robust 3D metric spatial understanding., 2. Introduces a Gradual Fusion Q-Former to incrementally inject LiDAR features, mitigating catastrophic disturbance to pre-trained VLMs and preserving their knowledge., 3. Develops a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e07c9f7613087f7d69052670bb8235b51f29f857774c6631936664f17a18b77_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that current Vision-Language Models (VLMs) for autonomous driving lack accurate 3D spatial understanding due to their reliance on 2D images. To solve this, it proposes LVLDrive, a framework that integrates LiDAR point clouds using a Gradual Fusion Q-Former and trains the model with a spatial-aware QA dataset. The results show that LVLDrive outperforms vision-only methods in scene understanding, spatial perception, and driving decision-making, demonstrating the necessity of explicit 3D data for trustworthy autonomous systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LVLDrive<br>空间感知视觉语言模型] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[2D图像VLM缺乏3D度量空间理解<br>2D VLMs lack 3D metric spatial understanding]
        C --> C1[融合LiDAR点云与视觉<br>Fuse LiDAR point cloud with vision]
        C --> C2[渐进融合Q-Former<br>Gradual Fusion Q-Former]
        C --> C3[空间感知QA数据集<br>Spatial-aware QA dataset]
        D --> D1[性能优于纯视觉方法<br>Outperforms vision-only counterparts]
        D --> D2[证明3D数据对可信系统的必要性<br>Demonstrates necessity of 3D data for trustworthy systems]
    ```

- **[arXiv260101] The Mechanics of CNN Filtering with Rectification**
  - **tags:** [cv], [cnn theory], [convolutional filtering, rectification, even-odd decomposition, discrete cosine transform, energy-momentum analogy]
  - **authors:** Liam Frija-Altrac, Matthew Toews
  - **institution:** École de Technologie Supérieure
  - **link:** https://arxiv.org/pdf/2512.24338
  - **contributions:** 1. Proposes "elementary information mechanics" as a new theoretical model for understanding CNN operations, drawing analogies to physical concepts like energy and momentum. 2. Demonstrates that small CNN filters are dominated by low-frequency DCT components (DC and gradients), which correspond to fundamental information propagation modes (diffusion, vibration, translation). 3. Shows a linear relationship between the speed of information displacement and the ratio of odd to total kernel energy, linking CNN information processing to the relativistic energy-momentum relation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e07a90b40ec2be4040a8a82b963be5a6a8e15890031f07ced02c5665f6381c87_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new theoretical framework called "elementary information mechanics" to model convolutional filtering with ReLU, drawing analogies from physics. It decomposes kernels into even and odd components, linking them to diffusion and directional displacement of information, and shows that low-frequency DCT components dominate filter behavior. The main conclusion is that fundamental physical principles, like the energy-momentum relation, can explain core information propagation mechanisms in generic CNNs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["The Mechanics of CNN Filtering with Rectification<br/>CNN滤波与整流机制"] --> B["核心问题/Problem<br/>What fundamental processes govern information propagation in CNNs?"]
        A --> C["主要方法/Method<br/>Propose elementary information mechanics model; Analyze kernel even-odd decomposition & DCT spectrum"]
        A --> D["关键结果/Results<br/>Low-frequency DCT components dominate; Speed of info displacement linked to odd/total energy ratio; Connection to energy-momentum relation"]
    ```

- **[arXiv260101] DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images**
  - **tags:** [cv], [medical image analysis], [visual question answering, lesion segmentation, multimodal models]
  - **authors:** Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy
  - **institution:** Microsoft, University of Washington, Stanford University, Hospital Clinic of Barcelona
  - **link:** https://arxiv.org/pdf/2512.24340
  - **code:** https://osf.io/72rp3
  - **contributions:** 1. Introduction of the Dermatology Assessment Schema (DAS), a novel expert-developed framework for structured dermatological feature assessment. 2. Release of DermaVQA-DAS, an extended dataset supporting closed-ended question answering and lesion segmentation on patient-generated images. 3. Comprehensive benchmarking of state-of-the-art multimodal models on the new tasks, analyzing the impact of prompt design on segmentation performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of patient-centered benchmarks in dermatology by introducing DermaVQA-DAS, a dataset extension built upon a novel expert-developed assessment schema (DAS) for structured feature annotation. It supports two tasks—closed-ended visual question answering and lesion segmentation—on patient-generated images and queries. The study benchmarks modern multimodal models, finding strong QA performance and demonstrating that prompt design significantly impacts segmentation results.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DermaVQA-DAS] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem] --> P1[现有数据集缺乏患者视角/Existing datasets lack patient perspective]
        P1 --> P2[限制以患者为中心的护理应用/Limits patient-centered care applications]
    
        Method[主要方法/Method] --> M1[提出皮肤病评估框架(DAS)/Propose Dermatology Assessment Schema (DAS)]
        M1 --> M2[扩展DermaVQA数据集/Extend DermaVQA dataset]
        M2 --> M3[支持两项任务:封闭式问答与分割/Support two tasks: closed QA & segmentation]
    
        Results[关键结果/Results] --> R1[提示设计影响分割性能/Prompt design impacts segmentation performance]
        R1 --> R2[模型在QA上表现强劲/Models perform strongly on QA]
        R2 --> R3[公开数据集与评估协议/Publicly release dataset & evaluation protocols]
    ```

- **[arXiv260101] Geometric Multi-Session Map Merging with Learned Local Descriptors**
  - **tags:** [cv], [SLAM (Simultaneous Localization and Mapping)], [map merging, learned local descriptors, geometric transformer, factor-graph optimization, loop closure detection]
  - **authors:** Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez
  - **institution:** University of California, Los Angeles (UCLA), DEVCOM Army Research Laboratory (ARL)
  - **link:** https://arxiv.org/pdf/2512.24384
  - **contributions:** 1. Proposes GMLD, a learning-based local descriptor framework for multi-session point cloud map merging. 2. Introduces a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for robust loop closure detection and pose estimation. 3. Incorporates inter-session scan matching cost factors into factor-graph optimization to enhance global map consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c330f606af1c67ed97c26c40d98ecfd5f18fd1d10de890504cf057e102199d9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of merging point cloud maps from multiple sessions or agents in large-scale environments. The proposed GMLD framework uses learned local descriptors and a geometric transformer for feature extraction, combined with factor-graph optimization for global consistency. Experimental results on public and self-collected datasets demonstrate accurate and robust map merging performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometric Multi-Session Map Merging with Learned Local Descriptors] --> B(核心问题/Problem: Multi-session map merging in large-scale environments)
        A --> C(主要方法/Method: GMLD framework with learned descriptors & geometric transformer)
        A --> D(关键结果/Results: Accurate and robust map merging with low error)
    ```

- **[arXiv260101] Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems**
  - **tags:** [cv], [multi-modal perception], [spatial intelligence, multi-modal pre-training, 3D object detection, semantic occupancy prediction, foundation models]
  - **authors:** Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi
  - **institution:** Zhejiang University, National University of Singapore, Nanjing University of Aeronautics and Astronautics, Alibaba Group, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.24385
  - **code:** https://github.com/worldbench/awesome-spatial-intelligence
  - **contributions:** 1. Formulates a unified taxonomy for multi-modal pre-training paradigms, from single-modality to unified frameworks. 2. Investigates the integration of textual inputs and occupancy representations for open-world perception and planning. 3. Identifies critical bottlenecks (e.g., computational efficiency) and proposes a roadmap towards general-purpose multi-modal foundation models for robust Spatial Intelligence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d3c33bdbff1adb3e8892d63c2303634ba223ae7e14138e8573b929718fecafc_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of integrating diverse sensor data (e.g., cameras, LiDAR) to achieve Spatial Intelligence for autonomous systems. It proposes a comprehensive framework and taxonomy for multi-modal pre-training, analyzing techniques for unified representation learning and identifying future research directions. The main conclusion is a roadmap towards building general-purpose multi-modal foundation models capable of robust real-world perception and planning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems<br>打造空间智能：自主系统多模态数据预训练路线图"] --> Problem["核心问题/Problem<br>Integrating multi-modal sensor data for unified Spatial Intelligence<br>融合多模态传感器数据以实现统一的空间智能"]
        Root --> Method["主要方法/Method<br>Comprehensive framework & unified taxonomy for multi-modal pre-training<br>全面的多模态预训练框架与统一分类法"]
        Root --> Results["关键结果/Results<br>Roadmap for general-purpose multi-modal foundation models<br>通用多模态基础模型的路线图"]
    ```

- **[arXiv260101] RedunCut: Measurement-Driven Sampling and Accuracy Performance Modeling for Low-Cost Live Video Analytics**
  - **tags:** [mlsys], [others], [dynamic model size selection, live video analytics, measurement-driven sampling, accuracy performance modeling, cost-accuracy tradeoff]
  - **authors:** Gur-Eyal Sela, Kumar Krishna Agrawal, Bharathan Balaji, Joseph Gonzalez, Ion Stoica
  - **institution:** UC Berkeley, Amazon
  - **link:** https://arxiv.org/pdf/2512.24386
  - **contributions:** 1. A measurement-driven planner that estimates the cost-benefit tradeoff of sampling to avoid inefficient sampling. 2. A lightweight, data-driven performance model to improve per-segment accuracy prediction. 3. A new DMSS system (RedunCut) that reduces compute cost by 14-62% at fixed accuracy across diverse video workloads and remains robust to limited data and drift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b99d771bb4464484bc20a905a41cf7ba47990b34219a4a1e3c0b5dc5c0c242bb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high inference cost in live video analytics by proposing RedunCut, a dynamic model size selection system. It introduces a measurement-driven planner to optimize sampling and a data-driven model to predict accuracy, reducing compute costs by 14-62% while maintaining target accuracy across diverse video types. The system demonstrates robustness to limited historical data and concept drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RedunCut: Low-Cost Live Video Analytics] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[高推理成本/High Inference Cost]
        Problem --> P2[现有方法泛化性差/Prior Methods Fail to Generalize]
        P2 --> P2_1[采样效率低/Inefficient Sampling]
        P2 --> P2_2[精度预测不准/Inaccurate Accuracy Prediction]
        Method[主要方法/Method] --> M1[测量驱动的规划器/Measurement-Driven Planner]
        Method --> M2[轻量级性能模型/Lightweight Performance Model]
        Results[关键结果/Results] --> R1[成本降低 14-62%/Cost Reduction 14-62%]
        Results --> R2[保持目标精度/Maintains Target Accuracy]
        Results --> R3[对数据漂移鲁棒/Robust to Drift]
    ```

- **[arXiv260101] Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning**
  - **tags:** [cv], [cross-view geo-localization], [visual reasoning, reinforcement learning, contrastive learning, cross-view alignment, visual planning]
  - **authors:** Soham Pahari, M. Srinivas
  - **institution:** UPES (School of Computer Science), NIT Warangal (Department of CS&E)
  - **link:** https://arxiv.org/pdf/2512.24404
  - **contributions:** 1. Proposes a novel visual reasoning paradigm called Geo-Consistent Visual Planning and a framework named ViReLoc for planning and localization using only visual representations. 2. Introduces a method that learns spatial and geometric dependencies through step-by-step visual inference optimized with reinforcement learning objectives. 3. Integrates contrastive learning and adaptive feature interaction to align ground and aerial perspectives and reduce viewpoint differences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4164fa0c19f567cf79df4a59a8129e5b520a00f0d6456c7d4f5649d4a580da_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of text-based reasoning in spatial tasks by proposing ViReLoc, a visual reasoning framework for ground-to-aerial localization and route planning. The method uses reinforcement learning and contrastive learning to perform inference directly in the visual domain without relying on GPS. Experiments show improved spatial reasoning and cross-view retrieval, establishing visual reasoning as a secure complementary approach for navigation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[文本推理在空间任务中的局限性<br/>Limitations of Text-Based Reasoning in Spatial Tasks]
        C --> C1[视觉推理框架ViReLoc<br/>Visual Reasoning Framework ViReLoc]
        C1 --> C2[Geo-Consistent Visual Planning<br/>Geo-Consistent Visual Planning]
        C1 --> C3[强化学习与对比学习<br/>Reinforcement & Contrastive Learning]
        D --> D1[空间推理与跨视图检索性能提升<br/>Improved Spatial Reasoning & Cross-View Retrieval]
        D --> D2[无需GPS的安全导航方案<br/>Secure Navigation Without GPS]
    ```

- **[arXiv260101] DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model**
  - **tags:** [cv], [talking head generation], [flow matching, autoregressive model, low latency, lip synchronization, causal encoder]
  - **authors:** Bohong Chen, Haiyang Liu
  - **institution:** Zhejiang University, The University of Tokyo
  - **link:** https://arxiv.org/pdf/2512.24408
  - **code:** https://robinwitch.github.io/DyStream-Page
  - **contributions:** 1. Proposed DyStream, a flow matching-based autoregressive model for real-time, streaming dyadic talking head video generation. 2. Introduced a causal encoder enhanced with a short lookahead module to incorporate minimal future context for quality improvement while maintaining ultra-low latency. 3. Demonstrated state-of-the-art lip-sync quality and a per-frame generation time of 34 ms, keeping total system latency under 100 ms.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60897d46da28b4d10e44efe7e6ee02b27fe594926b04b3ae834d8f1d5a78f416_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high latency problem in generating realistic listener responses for dyadic talking head videos. It proposes DyStream, a flow matching-based autoregressive model with a causal lookahead encoder, which enables real-time, frame-by-frame generation from streaming dual-track audio. The method achieves ultra-low latency (under 100 ms) and state-of-the-art lip-sync quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DyStream Paper] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法延迟高/High Latency in Existing Methods]
        B --> B2[影响倾听者实时反馈/Affects Real-time Listener Feedback]
        C --> C1[流式自回归框架/Stream-friendly Autoregressive Framework]
        C --> C2[基于流匹配的头部/Flow-Matching Heads]
        C --> C3[带前瞻模块的因果编码器/Causal Encoder with Lookahead]
        D --> D1[每帧34ms生成/34 ms per Frame Generation]
        D --> D2[总延迟<100ms/Total Latency < 100 ms]
        D --> D3[领先的唇同步质量/State-of-the-art LipSync Quality]
    ```

- **[arXiv260101] AI-Driven Evaluation of Surgical Skill via Action Recognition**
  - **tags:** [cv], [action recognition], [video transformer, hierarchical temporal attention, YOLO-based tracking, microanastomosis, surgical skill assessment]
  - **authors:** Yan Meng, Daniel A. Donoho, Marcelle Altshuler, Omar Arnaout
  - **institution:** Children's National Hospital, Brigham and Women's Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2512.24411
  - **contributions:** 1. A novel AI-driven framework for automated assessment of microanastomosis surgical skill. 2. Integration of an improved video transformer (TimeSformer with hierarchical temporal and weighted spatial attention) for action recognition. 3. Extraction and analysis of fine-grained motion features using YOLO-based object detection and tracking for detailed instrument kinematics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b060680d765ea8d59b727d7a45b5d8ddec7501ea571a866eb276f7cbbe079811_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an AI system to automate the evaluation of surgical skill in microanastomosis procedures. The method uses an enhanced video transformer for action recognition and YOLO-based tracking for motion analysis, achieving high accuracy in segmenting actions and classifying skill levels. The results demonstrate the system's potential to provide objective and scalable feedback for surgical training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI-Driven Evaluation of Surgical Skill via Action Recognition] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[主观且耗时的传统评估方法/Subjective & time-consuming traditional assessment]
        Problem --> P2[评估方法难以规模化/Assessment methods are hard to scale]
        Method[主要方法/Method] --> M1[基于改进TimeSformer的视频动作识别/Video action recognition with improved TimeSformer]
        Method --> M2[基于YOLO的器械运动特征提取/YOLO-based instrument motion feature extraction]
        Results[关键结果/Results] --> R1[动作分割准确率93.62%/Action segmentation accuracy 93.62%]
        Results --> R2[技能分类平均准确率76%/Average skill classification accuracy 76%]
    ```

- **[arXiv260101] Exploring Compositionality in Vision Transformers using Wavelet Representations**
  - **tags:** [cv], [vision transformer interpretability], [Vision Transformer (ViT), compositionality, Discrete Wavelet Transform (DWT), representation learning, encoder analysis]
  - **authors:** Akshad Shyam Purushottamdas, Pranav K Nayak, Divya Mehul Rajparia, Deekshith Patel, Yashmitha Gogineni, Konda Reddy Mopuri, Sumohana S. Channappayya
  - **institution:** IIT Hyderabad
  - **link:** https://arxiv.org/pdf/2512.24438
  - **contributions:** 1. A framework for testing compositionality in ViT encoder representations, analogous to prior work in representation learning. 2. The novel use of the Discrete Wavelet Transform (DWT) to generate input-specific primitives (basis sets) for images to analyze ViTs. 3. Empirical results demonstrating that ViT encoder representations exhibit approximate compositionality when using primitives from a one-level DWT decomposition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ed3d61f79d7e2fec6558aa9fe2df11bfd9e9c1d70312c79be40b99f44455a4_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether Vision Transformer (ViT) encoders learn compositional representations. The authors propose a framework that uses the Discrete Wavelet Transform (DWT) to decompose images into primitives and then tests if the ViT's representation of the whole image can be composed from the representations of these primitives. Their findings show that ViT encoder representations do exhibit approximate compositionality, offering a new perspective on how ViTs structure visual information.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Exploring Compositionality in Vision Transformers using Wavelet Representations") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("ViT表征是否具有组合性?/Do ViT representations exhibit compositionality?")
        Method --> M1("使用DWT获取图像基元/Use DWT to get image primitives")
        Method --> M2("测试组合表征的还原能力/Test recomposition ability of representations")
        Results --> R1("DWT基元产生近似可组合的表征/DWT primitives produce approximately compositional representations")
    ```

- **[arXiv260101] Spectral and Spatial Graph Learning for Multispectral Solar Image Compression**
  - **tags:** [cv], [image compression], [graph neural network, multispectral image compression, spectral graph embedding, spatial graph attention, learned image compression]
  - **authors:** Prasiddha Siwakoti, Atefeh Khoshkhahtinat, Piyush M. Mehta, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva
  - **institution:** West Virginia University, NASA Goddard Space Flight Center
  - **link:** https://arxiv.org/pdf/2512.24463
  - **code:** https://github.com/agyat4/sgraph
  - **contributions:** 1. Proposed an Inter-Spectral Windowed Graph Embedding (iSWGE) module to model inter-band relationships by representing spectral channels as graph nodes with learned edges. 2. Introduced a Windowed Spatial Graph Attention and Convolutional Block Attention (WSGA-C) module to reduce spatial redundancy and emphasize fine-scale structures. 3. Developed a learned image compression framework tailored for multispectral solar imagery, achieving improved spectral fidelity and reconstruction quality on the SDOML dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22141561baf730c3986a8299115f064f9a3f82996264d04ebeefffad89a18be1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of compressing high-volume multispectral solar imagery for space missions. It proposes a learned compression framework that uses two novel graph-based modules to model spectral and spatial dependencies. The method demonstrates improved performance in preserving spectral information and reconstruction quality compared to strong baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Spectral and Spatial Graph Learning for Multispectral Solar Image Compression] --> B(核心问题/Problem: High-fidelity compression of multispectral solar imagery with limited bandwidth)
        A --> C(主要方法/Method: Two complementary graph learning modules: iSWGE for spectral, WSGA-C for spatial dependencies)
        A --> D(关键结果/Results: 20.15% MSID reduction, up to 1.09% PSNR gain, 1.62% MS-SSIM gain)
    ```

- **[arXiv260101] F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model**
  - **tags:** [cv], [image super-resolution], [diffusion models, DINOv2, feature conditioning, hallucination control, real-world images]
  - **authors:** Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh
  - **institution:** MPI Lab, Samsung Research America
  - **link:** https://arxiv.org/pdf/2512.24473
  - **contributions:** 1. Proposes F2IDiff, a new Feature-to-Image Diffusion Foundation Model for SISR that uses lower-level DINOv2 features for conditioning instead of text. 2. Demonstrates that this approach provides stricter and richer conditioning for small patches, enabling controlled generation and higher fidelity, especially for high-fidelity smartphone LR images. 3. Shows that the model can be trained effectively with a much smaller dataset (38K images) and a smaller U-Net than large text-to-image models like SD2.1, while achieving superior performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccded9f21c9c6de980a50bfd5c308052e1cce34fc21b16a1a1b0cbd3a8c8c57d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of undesirable hallucinations in generative super-resolution for high-fidelity smartphone images. It proposes F2IDiff, a diffusion foundation model conditioned on DINOv2 features instead of text, which allows for stricter control and richer description of image patches. The method achieves better fidelity and performance than text-conditioned models while requiring significantly less data and a smaller network.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[F2IDiff: Real-world Image Super-resolution] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[文本特征对细节描述不足<br/>Text features lack detail]
        B --> B2[智能手机高分辨率LR图像需要无幻觉生成<br/>Smartphone LR needs hallucination-free generation]
        C --> C1[使用DINOv2特征进行条件控制<br/>Use DINOv2 features for conditioning]
        C --> C2[构建特征到图像扩散基础模型(F2IDiff)<br/>Build Feature-to-Image Diffusion FM (F2IDiff)]
        D --> D1[比基于文本的模型保真度更高<br/>Higher fidelity than text-based models]
        D --> D2[使用更小的数据集和网络实现<br/>Achieved with smaller dataset & network]
    ```

- **[arXiv260101] Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways**
  - **tags:** [sec], [steganography defense], [adversarial sanitization, diffusion models, pretrained denoiser, quaternion update, stegomalware]
  - **authors:** Vladimir Frants, Sos Agaian
  - **institution:** University of Texas at San Antonio (inferred from author "Sos Agaian" affiliation)
  - **link:** https://arxiv.org/pdf/2512.24499
  - **contributions:** 1. Proposes Adversarial Diffusion Sanitization (ADS), a training-free defense that neutralizes hidden payloads in diffusion-based steganography instead of detecting them. 2. Introduces a color-aware, quaternion-coupled update rule to minimize perceptual artifacts under strict distortion constraints. 3. Demonstrates effectiveness by driving decoder success rates of the state-of-the-art method Pulsar to near zero with minimal impact on image utility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c855da6cedcd1ea0c4dd38e35e187e7080351881b3a1515b6079801e94495a6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the threat of diffusion model-based steganography used for covert malware delivery. It proposes Adversarial Diffusion Sanitization (ADS), a training-free method that uses a pretrained denoiser and a color-aware update rule to subtly perturb images and neutralize hidden payloads. The results show ADS effectively disrupts steganographic decoding with minimal visual distortion, offering a practical defense for security gateways.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Training-Free Color-Aware Adversarial Diffusion Sanitization<br>免训练颜色感知对抗扩散净化] --> B
    A --> C
    A --> D
    B[核心问题/Problem<br>Diffusion steganography enables covert stegomalware delivery<br>扩散隐写术实现隐蔽的隐写恶意软件传递]
    C[主要方法/Method<br>ADS: Training-free sanitization using pretrained denoiser & color-aware quaternion update<br>ADS: 使用预训练去噪器和颜色感知四元数更新的免训练净化]
    D[关键结果/Results<br>Neutralizes Pulsar payloads with near-zero success rate & minimal distortion<br>以接近零的成功率和最小失真中和Pulsar有效载荷]
    ```

- **[arXiv260101] Using Large Language Models To Translate Machine Results To Human Results**
  - **tags:** [cv], [object detection], [YOLOv5, YOLOv8, GPT-4, automated report generation, cosine similarity]
  - **authors:** Trishna Niraula, Jonathan Stubblefield
  - **institution:** Arkansas State University
  - **link:** https://arxiv.org/pdf/2512.24518
  - **contributions:** 1. Introduced a novel pipeline integrating YOLO-based object detection models with a large language model (GPT-4) to generate natural-language radiology reports from chest X-ray images. 2. Conducted a comparative evaluation of YOLOv5 and YOLOv8 in terms of detection accuracy, inference latency, and the quality of the generated text reports. 3. Performed human evaluation to assess the clarity and natural writing flow of the AI-generated reports, revealing a trade-off between clinical accuracy and stylistic authenticity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843beb25a0a5b1b5d3f0617e07b26982372c49b0980f8c9a7193a05521b3d256_w640_q70.webp
  - **Simple LLM Summary:** This study addresses the gap between structured AI predictions in medical imaging and the narrative reports used by clinicians. It proposes a pipeline that uses YOLO models for anomaly detection in chest X-rays and GPT-4 to translate these detections into descriptive radiology reports. The results show the system achieves strong semantic accuracy but the generated text lacks the natural flow of human-authored reports.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Using Large Language Models To Translate Machine Results To Human Results] --> B(核心问题/Problem: Structured AI outputs vs. narrative clinical reports)
        A --> C(主要方法/Method: YOLO detection + LLM (GPT-4) report generation)
        A --> D(关键结果/Results: High semantic similarity (0.88), good clarity but poor natural flow)
    ```

- **[arXiv260101] Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression**
  - **tags:** [cv], [video compression], [VQ-VAE, hierarchical latents, perceptual loss, vector quantization, 3D convolutions]
  - **authors:** Manikanta Kotthapalli, Banafsheh Rekabdar
  - **institution:** Portland State University
  - **link:** https://arxiv.org/pdf/2512.24547
  - **contributions:** 1. Proposes a Multi-Scale Vector Quantized VAE (MS-VQ-VAE) for low-resolution video compression, extending VQ-VAE-2 to a spatiotemporal domain with a two-level hierarchical latent structure. 2. Introduces a lightweight architecture (~18.5M parameters) using 3D residual convolutions, optimized for deployment on resource-constrained edge devices. 3. Incorporates a perceptual loss from a pre-trained VGG16 network to enhance the perceptual quality of the reconstructed video.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d388ce684f252e97356733df6d2d163284bb7c1b55eb7a05e5ff13f6b1a929b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the need for video compression methods that generate machine-learning-friendly latent representations for bandwidth-sensitive applications. It proposes a lightweight, hierarchical VQ-VAE model for low-resolution video that uses 3D convolutions and perceptual loss. The model shows improved performance over a baseline and is suitable for edge deployment in scenarios like streaming and mobile analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression] --> B
        A --> C
        A --> D
        B[核心问题/Problem: High bandwidth/storage demands for video; Traditional codecs lack ML-friendly latents]
        C[主要方法/Method: Multi-Scale VQ-VAE (MS-VQ-VAE) with hierarchical latents, 3D convolutions, perceptual loss]
        D[关键结果/Results: 25.96 dB PSNR, 0.8375 SSIM; Improves over baseline; Suitable for edge devices]
    ```

- **[arXiv260101] OCP-LS: An Efficient Algorithm for Visual Localization**
  - **tags:** [mlsys], [others], [second-order optimization, Hessian approximation, visual localization]
  - **authors:** Jindi Zhong, Hongxia Wang, Huanshui Zhang
  - **institution:** Shandong University of Science and Technology, Shandong University
  - **link:** https://arxiv.org/pdf/2512.24552
  - **contributions:** 1. Proposes a novel second-order optimization algorithm (OCP-LS) that incorporates the OCP method and approximates the diagonal elements of the Hessian matrix. 2. Applies the algorithm to large-scale optimization problems in deep learning for visual localization, demonstrating its independence from specific network architectures. 3. Achieves superior performance on standard benchmarks, including faster convergence, enhanced training stability, and improved robustness to noise compared to conventional optimizers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ce7efe81a3ae02d86a4b55e643a250e273037d6af6755d5dfc5185a4581838_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes OCP-LS, a novel second-order optimization algorithm for deep learning that incorporates the OCP method and approximates the Hessian's diagonal. It is applied to visual localization tasks using a MapNet architecture. Experiments show the method achieves competitive accuracy with faster convergence, better stability, and improved noise robustness compared to conventional optimizers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OCP-LS: An Efficient Algorithm for Visual Localization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大规模深度学习优化/Large-scale Deep Learning Optimization]
        C --> C1[二阶优化算法/Second-order Optimization Algorithm]
        C1 --> C2[结合OCP方法/Incorporates OCP Method]
        C1 --> C3[近似Hessian对角元素/Approximates Hessian Diagonal]
        D --> D1[更快的收敛/Faster Convergence]
        D --> D2[增强的训练稳定性/Enhanced Training Stability]
        D --> D3[改进的噪声鲁棒性/Improved Robustness to Noise]
    ```

- **[arXiv260101] PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation**
  - **tags:** [cv], [video generation], [direct preference optimization, physics-aware learning, vision-language model, Plackett-Luce model, LoRA]
  - **authors:** Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou
  - **institution:** Meta Superintelligence Labs, Johns Hopkins University, Meta BizAI, CUHK
  - **link:** https://arxiv.org/pdf/2512.24551
  - **code:** https://caiyuanhao1998.github.io/project/PhyGDPO
  - **contributions:** 1. Introduces PhyAugPipe, a pipeline using a vision-language model with chain-of-thought reasoning to construct a large-scale physics-augmented video dataset (PhyVidGen-135K). 2. Proposes PhyGDPO, a physics-aware groupwise direct preference optimization framework based on the Plackett-Luce model for holistic preference learning. 3. Designs a Physics-Guided Rewarding (PGR) scheme and a LoRA-Switch Reference (LoRA-SR) scheme to embed physics rewards and enable efficient training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65a1d8df7eb5b611a50baf19a4ea822c74b9bb43565ee7d4123aa4ab4a91efbc_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating physically consistent videos from text. It proposes a new method, PhyGDPO, which uses a physics-aware groupwise preference optimization framework and a novel data construction pipeline to improve physical plausibility. The method significantly outperforms state-of-the-art open-source models on physics-focused benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[文本生成视频的物理一致性差/Physically inconsistent T2V generation]
        B --> B2[缺乏物理交互训练数据/Lack of physics-rich training data]
        C --> C1[数据构建管道 PhyAugPipe/Data Pipeline PhyAugPipe]
        C --> C2[组偏好优化框架 PhyGDPO/Groupwise DPO Framework PhyGDPO]
        C2 --> C21[物理引导奖励 PGR/Physics-Guided Rewarding PGR]
        C2 --> C22[高效训练方案 LoRA-SR/LoRA-Switch Reference LoRA-SR]
        D --> D1[构建数据集 PhyVidGen-135K/Built dataset PhyVidGen-135K]
        D --> D2[在PhyGenBench和VideoPhy2上表现优异/Outperforms SOTA on PhyGenBench & VideoPhy2]
    ```

- **[arXiv260101] RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios**
  - **tags:** [cv], [visual grounding], [RGB-TIR, multi-modal fusion, benchmark, robust perception, thermal infrared]
  - **authors:** Tianyi Zhao, Jiawen Xi, Linhui Xiao, Junnan Li, Xue Yang, Maoxun Yuan, Xingxing Wei
  - **institution:** Beihang University, Pengcheng Laboratory, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.24561
  - **contributions:** 1. Introduces RGBT-Ground, the first large-scale visual grounding benchmark with aligned RGB and thermal infrared image pairs for complex real-world scenarios. 2. Proposes a unified visual grounding framework supporting uni-modal (RGB/TIR) and multi-modal (RGB-TIR) inputs. 3. Presents RGBT-VGNet, a simple yet effective baseline model that outperforms adapted methods, especially in challenging conditions like nighttime and long-distance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0755cf2abb7623aaedd6b399439e49a428afb715c921daca7861337c872a0cb9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RGBT-Ground, a new benchmark for visual grounding that uses aligned RGB and thermal infrared images to address robustness in complex real-world conditions like poor lighting and weather. The authors also propose RGBT-VGNet, a multi-modal fusion model, which shows superior performance, particularly in nighttime and long-distance scenarios, compared to adapted existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[RGBT-Ground Benchmark] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有基准缺乏现实复杂性/Existing benchmarks lack real-world complexity]
        Method[主要方法/Method] --> M1[提出RGB-TIR基准/RGBT-Ground Benchmark]
        Method --> M2[统一多模态框架/Unified Multi-modal Framework]
        Method --> M3[基线模型RGBT-VGNet/Baseline Model RGBT-VGNet]
        Results[关键结果/Results] --> R1[模型在夜间和远距离场景表现优异/Superior performance in nighttime & long-distance]
        Results --> R2[促进鲁棒视觉定位研究/Promotes robust visual grounding research]
    ```

- **[arXiv260101] Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning**
  - **tags:** [ai], [vision-language reasoning], [decision-ambiguous samples, reinforcement fine-tuning, group-relative policy optimization]
  - **authors:** Fuyu Dong, Ke Li, Di Wang, Nan Luo, Yiming Zhang, Kaiyu Li, Jianfei Yang, Quan Wang
  - **institution:** Xidian University, University of California, San Diego, Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.24591
  - **contributions:** 1. Formally defines Decision-Ambiguous Samples (DAS) as a key challenge in CDVQA, where model confidence is similar between the correct answer and strong distractors. 2. Proposes DARFT, a novel fine-tuning framework that first mines DAS using an SFT-trained policy and then applies group-relative policy optimization on this subset. 3. Demonstrates that DARFT effectively suppresses distractors and sharpens decision boundaries, leading to consistent performance gains over SFT baselines, especially in few-shot settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e5d66094df5593ae9dd42856f1adba2aea6d5f079e4b7c888e839a61516cc5_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of decision ambiguity in Change Detection Visual Question Answering (CDVQA), where models struggle to choose between the correct answer and strong distractors. It proposes DARFT, a reinforcement fine-tuning framework that identifies and optimizes these ambiguous samples using group-relative policy optimization. The method shows improved performance over standard supervised fine-tuning, particularly with limited data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improving Few-Shot CDVQA via DARFT] --> B[核心问题/Problem: Decision Ambiguity in CDVQA]
        A --> C[主要方法/Method: DARFT Framework]
        A --> D[关键结果/Results: Gains over SFT, Few-shot Robustness]
        B --> B1[模型对正确答案与强干扰项置信度相近/Model assigns similar confidence to correct answer and strong distractors]
        C --> C1[挖掘决策模糊样本/Mine Decision-Ambiguous Samples (DAS)]
        C --> C2[应用组内相对优势优化/Apply Group-Relative Policy Optimization]
        D --> D1[减少模糊性，锐化决策边界/Reduced ambiguity, sharper decision boundaries]
        D --> D2[在少样本设置下性能提升显著/Performance gains under few-shot settings]
    ```

- **[arXiv260101] SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks**
  - **tags:** [cv], [model evaluation and analysis], [error slice discovery, multi-instance vision tasks, fine-grained reasoning, vision-language models, benchmark]
  - **authors:** Wei Zhang, Chaoqun Wang, Zixuan Guan, Sam Kao, Pengfei Zhao, Peng Wu, Sifeng He
  - **institution:** Apple
  - **link:** https://arxiv.org/pdf/2512.24592
  - **contributions:** 1. Proposed SliceLens, a hypothesis-driven framework leveraging LLMs and VLMs for fine-grained and grounded error slice discovery in multi-instance vision tasks. 2. Introduced FeSD, the first benchmark for evaluating fine-grained error slice discovery across instance-level vision tasks with expert-annotated ground truth. 3. Demonstrated state-of-the-art performance on benchmarks and showed identified slices facilitate actionable model improvements via repair experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/728a923247ac54e5968776493b08bc65f1a41354b66a829473a0dd25289e7248_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of discovering systematic failure patterns (error slices) in multi-instance vision tasks like detection and segmentation, where existing methods are limited. It proposes SliceLens, a framework that uses LLMs and VLMs to generate and verify failure hypotheses through visual reasoning. The method achieves superior performance on a new benchmark and provides interpretable slices that help improve model robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SliceLens: Fine-Grained and Grounded Error Slice Discovery] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有方法限制/Existing methods limited to image classification]
        Problem --> P2[缺乏细粒度推理/Lack fine-grained reasoning for multi-instance tasks]
        Problem --> P3[基准不完善/Benchmarks biased or artificial]
        Method[主要方法/Method] --> M1[假设驱动框架/Hypothesis-driven framework]
        M1 --> M1_1[利用LLM和VLM/Leverages LLMs & VLMs]
        M1 --> M1_2[生成验证假设/Generates & verifies failure hypotheses]
        M1 --> M1_3[基于视觉的推理/Grounded visual reasoning]
        Results[关键结果/Results] --> R1[提出新基准FeSD/Proposes new benchmark FeSD]
        Results --> R2[性能SOTA/State-of-the-art performance]
        Results --> R3[可解释切片助力模型修复/Interpretable slices enable model repair]
    ```

- **[arXiv260101] 3D Semantic Segmentation for Post-Disaster Assessment**
  - **tags:** [cv], [3D semantic segmentation], [3D point clouds, Structure-from-Motion (SfM), Multi-View Stereo (MVS), Fast Point Transformer (FPT), Point Transformer v3 (PTv3)]
  - **authors:** Nhut Le, Maryam Rahnemoonfar
  - **institution:** Lehigh University
  - **link:** https://arxiv.org/pdf/2512.24593
  - **contributions:** 1. Constructed a specialized 3D dataset for post-disaster assessment using UAV footage and 3D reconstruction techniques (SfM/MVS). 2. Evaluated state-of-the-art 3D semantic segmentation models (FPT, PTv3, OA-CNNs) on this new dataset. 3. Identified significant limitations of existing models in disaster-stricken environments, highlighting the need for new techniques and benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7f40d3c2e21cef6917feeb765394fae515f763b5c2f2f5b374f921e3a8ac1ba_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of specialized datasets for 3D semantic segmentation in post-disaster scenarios by constructing a new 3D point cloud dataset from UAV footage of Hurricane Ian. The authors evaluated several state-of-the-art models on this dataset and found their performance to be significantly limited, demonstrating an urgent need for improved methods and benchmarks tailored to disaster environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[3D Semantic Segmentation for Post-Disaster Assessment] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏灾后3D数据集 / Lack of post-disaster 3D datasets]
        C --> C1[使用无人机与SfM/MVS构建3D数据集 / Construct 3D dataset using UAV & SfM/MVS]
        C --> C2[评估SOTA 3D分割模型 / Evaluate SOTA 3D segmentation models]
        D --> D1[现有模型存在显著局限 / Existing models have significant limitations]
        D --> D2[需要新技术与基准 / Need for new techniques & benchmarks]
    ```

- **[arXiv260101] Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers**
  - **tags:** [cv], [parameter-efficient fine-tuning], [low-rank adaptation, vision transformers, diversity enhancement, parameter-efficient fine-tuning, collaborative learning]
  - **authors:** Zheng Liu, Jinchao Zhu, Gao Huang
  - **institution:** University of Science and Technology Beijing, Nankai University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.24603
  - **contributions:** 1. Proposes a novel Collaborative Low-Rank Adaptation (CLoRA) method that balances learning performance and parameter efficiency for fine-tuning Vision Transformers. 2. Introduces a base-space sharing mechanism that allows all low-rank modules to share projection spaces, expanding learning capacity while maintaining parameter efficiency. 3. Designs a Sample-Agnostic Diversity Enhancement (SADE) component to regularize similarities among low-rank matrices and encourage diverse representations during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7b118cdde5f39c672a7a43fbf0a798c2d097cc700ad5018dfd61a3a755467de_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the trade-off between performance and parameter efficiency in fine-tuning pre-trained Vision Transformers. It proposes CLoRA, a method featuring base-space sharing and a diversity enhancement component to collaboratively construct low-rank modules. Experiments show CLoRA achieves a better balance between learning performance and parameter efficiency compared to state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers] --> B(核心问题/Problem: 现有方法在性能与参数效率间难以平衡/Existing methods fail to balance performance and parameter efficiency)
        A --> C(主要方法/Method: 提出CLoRA，包含基础空间共享与SADE/Propose CLoRA with base-space sharing and SADE)
        A --> D(关键结果/Results: 在性能与效率间取得更好平衡，点云分析所需GFLOPs最少/Achieves better balance and requires fewest GFLOPs for point cloud analysis)
    ```

- **[arXiv260101] MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding**
  - **tags:** [cv], [3D visual grounding], [multi-modal dataset, roadside infrastructure, point cloud, natural language understanding, 3D object localization]
  - **authors:** Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang
  - **institution:** Xiamen University, Pengcheng Laboratory
  - **link:** https://arxiv.org/pdf/2512.24605
  - **contributions:** 1. Introduces a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios from a roadside infrastructure perspective. 2. Constructs MoniRefer, the first real-world, large-scale multi-modal dataset for this task, featuring over 136k objects and 411k language expressions from complex traffic intersections. 3. Proposes a new end-to-end method, Moni3DVG, which leverages multi-modal features from images and point clouds for 3D object localization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c61ac917fbc36f6e9ad317d668ea3e64b3a82f0dbd27c014c4abf4fd960de2e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of data for 3D visual grounding in roadside monitoring scenarios by introducing a new task and constructing the large-scale MoniRefer dataset. It also proposes the Moni3DVG method, which fuses image and point cloud features for 3D object localization. Experiments show the proposed method is effective and superior on the new benchmark.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有3D视觉定位数据集和方法主要关注室内和驾驶场景，缺乏路边监控场景的配对数据/Existing 3D visual grounding focuses on indoor/driving scenes, lacking data for roadside monitoring.]
        C --> C1[提出新任务：用于户外监控场景的3D视觉定位/Propose new task: 3D Visual Grounding for Outdoor Monitoring Scenarios.]
        C --> C2[构建首个大规模真实世界多模态数据集MoniRefer/Build first large-scale real-world multi-modal dataset MoniRefer.]
        C --> C3[提出端到端方法Moni3DVG，融合图像和点云特征/Propose end-to-end method Moni3DVG, fusing image and point cloud features.]
        D --> D1[数据集包含约13.6万个物体和41.1万条语言描述/Dataset contains ~136k objects and 411k expressions.]
        D --> D2[实验证明了所提方法的优越性和有效性/Experiments demonstrate the method's superiority and effectiveness.]
    ```

- **[arXiv260101] LLHA-Net: A Hierarchical Attention Network for Two-View Correspondence Learning**
  - **tags:** [cv], [feature matching], [hierarchical attention, outlier removal, camera pose estimation, two-view correspondence, stage fusion]
  - **authors:** Shuyuan Lin, Yu Guo, Xiao Chen, Yanjie Liang, Guobao Xiao, Feiran Huang
  - **institution:** Jinan University, Guangzhou City University of Technology, Peng Cheng Laboratory, Tongji University
  - **link:** https://arxiv.org/pdf/2512.24620
  - **code:** http://www.linshuyuan.com
  - **contributions:** 1. A layer-by-layer channel fusion module that preserves semantic information from each network stage to enhance feature representation. 2. A hierarchical attention module that adaptively captures and fuses global and structural semantic information. 3. Two network architectures designed to extract and integrate features, improving the model's adaptability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/815e060fd0161c2bcb2bed3957a4053a624617e006ffadacd94f49bdc4d97dfe_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LLHA-Net, a hierarchical attention network for two-view correspondence learning. It addresses the challenge of outliers in feature matching by using stage fusion and hierarchical attention to better capture semantic information. Experiments on YFCC100M and SUN3D datasets show it outperforms state-of-the-art methods in outlier removal and camera pose estimation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LLHA-Net: A Hierarchical Attention Network for Two-View Correspondence Learning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[大量异常值影响匹配精度/Outliers degrade matching accuracy]
        C --> C1[分层注意力网络/Hierarchical Attention Network]
        C1 --> C2[逐层通道融合模块/Layer-by-Layer Channel Fusion]
        C1 --> C3[分层注意力模块/Hierarchical Attention Module]
        D --> D1[在YFCC100M和SUN3D上超越SOTA/Outperforms SOTA on YFCC100M & SUN3D]
        D1 --> D2[异常值去除/Outlier Removal]
        D1 --> D3[相机姿态估计/Camera Pose Estimation]
    ```

- **[arXiv260101] FireRescue: A UAV-Based Dataset and Enhanced YOLO Model for Object Detection in Fire Rescue Scenes**
  - **tags:** [cv], [object detection], [UAV, attention module, dynamic feature sampler, YOLO, dataset]
  - **authors:** Qingyu Xu, Runtong Zhang, Zihuan Qiu, Fanman Meng
  - **institution:** University of Electronic Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.24622
  - **contributions:** 1. Constructed the "FireRescue" dataset for UAV-based fire rescue scenes, covering urban, mountainous, forest, and water areas with 8 key categories. 2. Proposed the FRS-YOLO model, introducing a plug-and-play multidimensional collaborative enhancement attention module to reduce inter-class confusion. 3. Integrated a dynamic feature sampler into the model to strengthen foreground features and mitigate issues like smoke occlusion and small target miss-detection.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf6e55c31951249ccd5b18b3a05c08e7f08682d074fbcab089ea2b819535a76d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of object detection in complex fire rescue scenes by first creating a comprehensive UAV-based dataset named FireRescue. It then proposes an enhanced YOLO model, FRS-YOLO, which incorporates a novel attention module and a dynamic feature sampler to improve detection accuracy, especially for confused categories and small targets. Experiments show the method effectively boosts the performance of YOLO models in this challenging domain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("FireRescue: UAV-Based Dataset and Enhanced YOLO Model") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有研究不足/Existing Limitations")
        P1 --> P1_1("场景偏差: 缺乏复杂城市救援场景/Scenario Bias: Lack of complex urban scenes")
        P1 --> P1_2("类别不匹配: 缺少关键指挥目标/Category Mismatch: Missing key command targets")
        Problem --> P2("检测挑战: 类间混淆、小目标漏检/Detection Challenges: Inter-class confusion, small target miss")
        Method --> M1("构建新数据集 'FireRescue'/Construct new dataset 'FireRescue'")
        M1 --> M1_1("多场景: 城市、山地、森林、水域/Multi-scenario: Urban, mountainous, forest, water")
        M1 --> M1_2("8个关键类别，15,980张图像/8 key categories, 15,980 images")
        Method --> M2("提出改进模型 FRS-YOLO/Propose improved model FRS-YOLO")
        M2 --> M2_1("引入多维协同增强注意力模块/Introduce multidimensional collaborative enhancement attention module")
        M2 --> M2_2("集成动态特征采样器/Integrate dynamic feature sampler")
        Results --> R1("实验证明火灾救援场景检测极具挑战性/Experiments show fire rescue detection is highly challenging")
        Results --> R2("所提方法有效提升YOLO系列模型性能/The proposed method effectively improves YOLO model performance")
    ```

- **[arXiv260101] From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation**
  - **tags:** [mlsys], [multi-modal inference], [autoregressive visual generation, radial parallel prediction, nested attention mechanism]
  - **authors:** Siyang Wang, Hanting Li, Wei Li, Jie Hu, Xinghao Chen, Feng Zhao
  - **institution:** University of Science and Technology of China, Huawei Noah's Ark Lab
  - **link:** https://arxiv.org/pdf/2512.24639
  - **contributions:** 1. Proposed a radial parallel prediction framework (RadAR) that reorders the autoregressive generation process from sequential to spatial, grouping tokens into concentric rings for parallel prediction. 2. Introduced a nested attention mechanism to dynamically refine inconsistent predictions during the forward pass, mitigating error accumulation from parallel generation. 3. Designed a method that preserves the structural locality and spatial coherence of visual scenes while significantly improving inference efficiency and parallelizability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bffd5ae0463dd26d6d9beddadbbf4c0de1249205c2c3865172fb5172c4b22d2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the low inference efficiency of traditional autoregressive models in visual generation due to their sequential token-by-token decoding. It proposes RadAR, a framework that organizes generation around a radial topology, enabling parallel prediction of tokens within the same spatial ring and using a nested attention mechanism to correct errors. The method significantly accelerates generation while maintaining representational capacity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统自回归模型顺序解码导致推理效率低/Traditional autoregressive sequential decoding leads to low inference efficiency]
        C --> C1[提出RadAR框架：径向并行预测/Propose RadAR framework: Radial Parallel Prediction]
        C1 --> C2[将token按空间距离分组为同心环/Group tokens into concentric rings by spatial distance]
        C1 --> C3[引入嵌套注意力机制进行动态修正/Introduce nested attention for dynamic correction]
        D --> D1[保持视觉结构局部性和空间连贯性/Preserves structural locality and spatial coherence]
        D --> D2[显著提高并行化和生成效率/Significantly improves parallelization and generation efficiency]
    ```

- **[arXiv260101] Renormalization Group Guided Tensor Network Structure Search**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [tensor network structure search, renormalization group, multi-scale optimization, edge gates, node tension]
  - **authors:** Maolin Wang, Bowen Yu, Sheng Zhang, Linjie Mi, Wanyu Wang, Yiqi Wang, Pengyue Jia, Xuetao Wei, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao
  - **institution:** City University of Hong Kong, National University of Defense Technology, Southern University of Science and Technology, Fudan University, Intuit AI
  - **link:** https://arxiv.org/pdf/2512.24663
  - **code:** https://github.com/Applied-Machine-Learning-Lab/RGTN
  - **contributions:** 1. Proposes RGTN, a physics-inspired framework that uses multi-scale renormalization group flows for continuous tensor network structure evolution, overcoming the limitations of fixed-scale, discrete search methods. 2. Introduces learnable edge gates for dynamic topology modification and intelligent proposals based on physical quantities like node tension and edge information flow to guide the search. 3. Demonstrates state-of-the-art performance, achieving superior compression ratios and running 4-600 times faster than existing methods on tasks like light field data and video completion.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9fdccc30b2aad8da00c1fdf132004ba3347f8992425dfb71406c3fa11d073c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing Tensor Network Structure Search (TN-SS) methods, which struggle with computational tractability and structure adaptivity. The authors propose RGTN, a novel framework guided by renormalization group theory, which enables multi-scale, continuous structure optimization. Experiments show RGTN achieves better compression and is significantly faster than prior methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Renormalization Group Guided Tensor Network Structure Search] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有TN-SS方法局限/Limitations of existing TN-SS]
        P1 --> P1_1[单尺度优化/Single-scale optimization]
        P1 --> P1_2[离散搜索空间/Discrete search space]
        P1 --> P1_3[分离的结构-参数优化/Separated structure-parameter optimization]
        Method[主要方法/Method] --> M1[RGTN框架/RGTN Framework]
        M1 --> M1_1[多尺度重整化群流/Multi-scale RG flows]
        M1 --> M1_2[可学习边门/Learnable edge gates]
        M1 --> M1_3[智能提案(节点张力, 边信息流)/Intelligent proposals (node tension, edge info flow)]
        Results[关键结果/Results] --> R1[SOTA压缩比/State-of-the-art compression ratio]
        Results --> R2[4-600倍加速/4-600x speedup]
    ```

- **[arXiv260101] Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting**
  - **tags:** [cv], [reasoning segmentation], [evolutionary prompting, zero-shot learning, visual arena, semantic mutation, heterogeneous arena]
  - **authors:** Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao
  - **institution:** Xiamen University, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24702
  - **code:** https://github.com/AHideoKuzeA/Evol-SAM3
  - **contributions:** 1. Proposes EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. 2. Introduces a "Generate-Evaluate-Evolve" loop with a Visual Arena for reference-free fitness assessment and a Semantic Mutation operator for diversity and error correction. 3. Designs a Heterogeneous Arena module that integrates geometric priors with semantic reasoning for robust final selection.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b60e24e2f5e5668a6d7d977acfb32177365388575df74e72ccb5a8b3d4e7f7e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of static, training-free methods for reasoning segmentation by proposing EVOL-SAM3, a zero-shot framework that uses an evolutionary prompting strategy to iteratively refine prompt hypotheses at inference time. The method outperforms both static baselines and fully supervised state-of-the-art methods on the ReasonSeg benchmark without any training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EVOL-SAM3: 零样本推理分割的进化提示 / EVOL-SAM3: Zero-Shot Reasoning Segmentation via Evolutionary Prompting] --> B
        A --> C
        A --> D
        B[核心问题 / Problem] --> B1[静态推理范式 / Static Inference Paradigm]
        B1 --> B2[推理深度不足 / Insufficient Reasoning Depth]
        B1 --> B3[无法自我纠正 / Lack of Self-Correction]
        C[主要方法 / Method] --> C1[进化搜索 / Evolutionary Search]
        C1 --> C2[生成-评估-进化循环 / Generate-Evaluate-Evolve Loop]
        C2 --> C3[视觉竞技场 / Visual Arena]
        C2 --> C4[语义突变 / Semantic Mutation]
        C2 --> C5[异构竞技场 / Heterogeneous Arena]
        D[关键结果 / Results] --> D1[超越静态基线 / Outperforms Static Baselines]
        D --> D2[超越全监督SOTA / Surpasses Fully Supervised SOTA]
        D --> D3[零样本设置 / Zero-Shot Setting]
    ```

- **[arXiv260101] FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation**
  - **tags:** [mlsys], [diffusion models], [video generation, diffusion sampling, model capacity, velocity divergence, inference acceleration]
  - **authors:** Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh
  - **institution:** Yonsei University
  - **link:** https://arxiv.org/pdf/2512.24724
  - **code:** https://jibin86.github.io/flowblending_project_page
  - **contributions:** 1. The observation that model capacity impact varies across denoising timesteps, being crucial in early/late stages but negligible in intermediate stages., 2. The proposal of FlowBlending, a stage-aware multi-model sampling strategy that dynamically allocates large and small models to different stages., 3. The introduction of simple criteria and a velocity-divergence analysis to identify capacity-sensitive regions and choose stage boundaries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ac4128c7bf14ee06860b55c2869f6a243b40a94d93faf3a63e6549b8c5d06f8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of video diffusion models by proposing FlowBlending, a stage-aware sampling strategy that uses a large model for critical early/late denoising stages and a small model for the intermediate stage. This method achieves up to 1.65x faster inference with significantly fewer FLOPs while preserving the output quality of the large model, and is compatible with other acceleration techniques.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation") --> Problem("核心问题/Problem: High computational cost of video diffusion models")
        Root --> Method("主要方法/Method: Stage-aware multi-model sampling (large model for early/late stages, small model for intermediate stage)")
        Root --> Results("关键结果/Results: Faster inference, fewer FLOPs, maintained visual fidelity")
    ```

- **[arXiv260101] EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation**
  - **tags:** [mlsys], [multi-modal inference], [video-to-audio generation, event-centric control, hierarchical semantic control, symbolic representation, agentic generation]
  - **authors:** Bingxuan Li, Yiming Cui, Yicheng He, Yiwei Wang, Shu Zhang, Longyin Wen, Yulei Niu
  - **institution:** ByteDance, University of Illinois Urbana-Champaign, University of California, Merced, University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.24731
  - **code:** https://echofoley.github.io/
  - **contributions:** 1. Introduces EchoFoley, a new task for video-grounded sound generation with fine-grained event-level and hierarchical semantic control. 2. Constructs EchoFoley-6k, a large-scale, expert-curated benchmark with detailed sounding event annotations. 3. Proposes EchoVidia, a sounding-event-centric agentic generation framework with a slow-fast thinking strategy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e5e566b45c8f830a2ee00d47e98f66961d9c829c11fbf1ba96469f6be93ddc_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses limitations in video-text-to-audio generation by proposing a new task, EchoFoley, for fine-grained, controllable sound effect synthesis. It introduces a symbolic representation for sounding events and a corresponding benchmark, and develops the EchoVidia framework, which demonstrates superior controllability and perceptual quality compared to existing models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("视觉主导/Visual Dominance")
        Problem --> P2("缺乏细粒度控制定义/Lack of Fine-grained Control Definition")
        Problem --> P3("指令理解弱/Weak Instruction Following")
        Method --> M1("新任务: EchoFoley/New Task: EchoFoley")
        Method --> M2("符号化声音事件表示/Symbolic Sounding Event Representation")
        Method --> M3("基准数据集: EchoFoley-6k/Benchmark: EchoFoley-6k")
        Method --> M4("生成框架: EchoVidia/Generation Framework: EchoVidia")
        Results --> R1("可控性提升 40.7%/Controllability +40.7%")
        Results --> R2("感知质量提升 12.5%/Perceptual Quality +12.5%")
    ```

- **[arXiv260101] Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [3D Gaussian Splatting, Benchmark, Compression, Rendering Speed, Geometric Accuracy]
  - **authors:** Xiang Liu, Yimin Zhou, Jinxiang Wang, Yujun Huang, Shuzhao Xie, Shiyu Qin, Mingyao Hong, Jiawei Li, Yaowei Wang, Zhi Wang, Shu-Tao Xia, Bin Chen
  - **institution:** Tsinghua University, Harbin Institute of Technology, Shenzhen, Pengcheng Laboratory, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.24742
  - **code:** https://github.com/ (Inferred from "Code is available at Github" in the First Page Content)
  - **contributions:** 1. Introduces Splatwizard, a unified benchmark toolkit specifically designed for evaluating 3D Gaussian Splatting (3DGS) compression models. 2. Provides an easy-to-use framework for implementing new compression models and utilizing state-of-the-art techniques. 3. Includes an integrated pipeline that automates the calculation of key performance indicators, including image quality, geometric accuracy (chamfer distance), rendering speed, and resource consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7da61f07627db84d20834d2e3342abb34432aaec56ccf5e79f5b1b3e3b00775_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Splatwizard, a benchmark toolkit designed to address the lack of standardized evaluation tools for 3D Gaussian Splatting compression methods. It provides a unified framework to implement and automatically evaluate compression models on key metrics like rendering speed, quality, and geometric accuracy. The main conclusion is that Splatwizard fills a critical gap for consistent and comprehensive assessment in this rapidly growing field.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>缺乏标准化评估工具<br>Lack of Standardized Evaluation Tools]
        C[主要方法/Method<br>统一基准测试工具包<br>Unified Benchmark Toolkit]
        D[关键结果/Results<br>自动化评估流程<br>Automated Evaluation Pipeline]
    ```

- **[arXiv260101] Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow**
  - **tags:** [ai], [robotic manipulation], [3D object flow, video generation, zero-shot manipulation, trajectory optimization, reinforcement learning]
  - **authors:** Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.24766
  - **contributions:** 1. Proposes Dream2Flow, a framework that bridges video generation and robotic control using 3D object flow as an intermediate representation. 2. Demonstrates the ability to reconstruct 3D object motions from generated videos and formulate manipulation as object trajectory tracking, overcoming the embodiment gap. 3. Shows that the method enables zero-shot guidance from pre-trained video models to manipulate diverse object categories (rigid, articulated, deformable, granular) without task-specific demonstrations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Dream2Flow, a framework that uses 3D object flow extracted from videos generated by off-the-shelf models as an interface for robotic manipulation. It translates these generated motions into executable robot actions via trajectory optimization or reinforcement learning, enabling zero-shot manipulation of diverse objects in open-world settings. The results demonstrate 3D object flow as a general and scalable bridge between video generation models and robotic control.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow] --> B(核心问题/Problem: Translating human-like motions from video models into low-level robot actions)
        A --> C(主要方法/Method: Use 3D object flow as intermediate representation, reconstruct motions from videos, track trajectories)
        A --> D(关键结果/Results: Enables zero-shot manipulation of diverse objects, bridges video generation to robot control)
    ```

- **[arXiv260101] UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning**
  - **tags:** [cv], [3D instance segmentation], [3D Gaussian Splatting, contrastive learning, feature embedding, embedding-to-label, hard-mining]
  - **authors:** Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan
  - **institution:** Indian Institute of Science, Bangalore; Samsung R&D Institute India - Bangalore
  - **link:** https://arxiv.org/pdf/2512.24763
  - **code:** https://github.com/val-iisc/UniC-Lift
  - **contributions:** 1. A unified framework that merges feature learning and label decoding for 3D instance segmentation, reducing training time compared to two-stage approaches. 2. A novel "Embedding-to-Label" process to efficiently decode learnable feature embeddings into instance labels. 3. A stabilized hard-mining strategy using a linear layer on rasterized embeddings to improve performance at object boundaries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29fe9b6395a1fd545bcd589e312ef8c6e3ad1d451a08ccc87467409b090443b2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of inconsistent 2D instance labels across views for 3D instance segmentation. It proposes UniC-Lift, a unified framework that uses contrastive learning on learnable 3D Gaussian feature embeddings and a novel decoding process, enhanced by a stabilized hard-mining technique for object boundaries. The method outperforms baselines on multiple datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Inconsistent 2D instance labels across views] --> P1[导致/Leads to poor 3D predictions]
        Method[主要方法/Method: Unified framework] --> M1[引入/Introduces learnable feature embedding]
        M1 --> M2[解码/Decodes via Embedding-to-Label]
        Method --> M3[优化/Optimization: Stabilized hard-mining at boundaries]
        Results[关键结果/Results] --> R1[性能提升/Outperforms baselines]
        Results --> R2[数据集/Datasets: ScanNet, Replica3D, Messy-Rooms]
    ```

- **[arXiv260101] Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation**
  - **tags:** [cv], [monocular depth estimation], [adversarial attack, physics-in-the-loop optimization, sep-CMA-ES]
  - **authors:** Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono
  - **institution:** Kagoshima University
  - **link:** https://arxiv.org/pdf/2512.24792
  - **contributions:** 1. Proposes a projection-based adversarial attack method for monocular depth estimation models, using projected light as the perturbation. 2. Employs physics-in-the-loop (PITL) optimization to design perturbations in real-world environments, accounting for device specifications and disturbances. 3. Utilizes a distributed covariance matrix adaptation evolution strategy (sep-CMA-ES) for effective black-box optimization to generate adversarial examples.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a physical adversarial attack method for monocular depth estimation models. The method projects perturbation light onto a target object and uses physics-in-the-loop optimization with a distributed evolution strategy to create adversarial examples. Experiments confirmed the attack's success, causing depth misestimations that made parts of objects disappear from the scene.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: DNN-based monocular depth estimation models are vulnerable to adversarial attacks, threatening reliability in applications like autonomous systems.]
        Method[主要方法/Method: Proposes a projection-based attack using physics-in-the-loop optimization and sep-CMA-ES to generate adversarial light perturbations.]
        Results[关键结果/Results: Successfully created adversarial examples causing depth misestimation, making parts of objects disappear from the scene.]
    ```

- **[arXiv260101] Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training**
  - **tags:** [cv], [image denoising], [Noise2Noise, Monte Carlo denoising, high dynamic range, tone mapping, Jensen gap]
  - **authors:** Andrew Tinits, Stephen Mann
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.24794
  - **contributions:** 1. Identified that certain nonlinear functions can be applied to noisy targets in Noise2Noise training without introducing significant bias. 2. Developed a theoretical framework to analyze the effects of nonlinearities and described a class of functions with minimal bias. 3. Demonstrated the method's effectiveness for training Monte Carlo denoisers on HDR images using only noisy data, achieving results comparable to models trained with clean references.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7ef23248abb9edf960ef0ea8dac0c52ee12d9db03ab8dd602dfd8c83c62645_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of Noise2Noise training where applying nonlinear functions to noisy targets introduces bias. The authors propose a theoretical framework to identify low-bias nonlinearities and apply this to denoise high dynamic range Monte Carlo renderings using tone mapping. Their method, trained only on noisy data, achieves performance close to models trained with clean reference images.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Noise2Noise训练中非线性函数导致偏差/Bias from nonlinearities in Noise2Noise")
        Problem --> P2("HDR图像训练被异常值干扰/HDR training overwhelmed by outliers")
        Method --> M1("理论分析非线性影响/Theoretical analysis of nonlinear effects")
        Method --> M2("识别低偏差非线性函数类/Identify low-bias nonlinear function class")
        Method --> M3("特定损失与色调映射组合/Specific loss & tone mapping combination")
        Results --> R1("仅用噪声数据训练/Train with only noisy data")
        Results --> R2("性能接近干净数据训练模型/Performance approaches clean-data-trained model")
    ```

- **[arXiv260101] Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control**
  - **tags:** [mlsys], [multi-modal inference], [derivative-free optimisation, regret minimisation, multivariate mutual information, in-scene camera control, vision-language models]
  - **authors:** Jason Armitage, Rico Sennnrich
  - **institution:** University of Zurich
  - **link:** https://arxiv.org/pdf/2512.24826
  - **contributions:** 1. A new method that improves multivariate mutual information estimates using regret minimisation with derivative-free optimisation. 2. An algorithm enabling off-the-shelf 2D-trained cross-modal systems to adapt online to object occlusions and differentiate features in 3D scenes. 3. A pipeline that controls an in-scene camera to learn directly from noisy VLM outputs, improving performance on 3D multi-object scenes without pretraining or finetuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66c8b5c775488157feb95c4650ebd70cf6bfc3b2373d13cfde2db9f88f119e8d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the dimensional shift when 2D-trained vision-language models process 3D scenes. It proposes a method using derivative-free optimisation and regret minimisation to improve mutual information estimates and control an in-scene camera, allowing the system to adapt online and improve performance on cross-modal tasks for 3D multi-object scenes without additional training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("2D系统处理3D场景的维度偏移/Dimensional shift for 2D systems on 3D scenes")
        Problem --> P2("需要学习相机控制模块/Need to learn an in-scene camera control module")
        Method --> M1("基于遗憾最小化的多元互信息估计/Multivariate mutual information estimates via regret minimisation")
        Method --> M2("使用无导数优化/Using derivative-free optimisation")
        Results --> R1("使现成的2D系统能在线适应3D场景/Enables off-the-shelf 2D systems to adapt online to 3D scenes")
        Results --> R2("无需预训练或微调即可提升性能/Improves performance without pretraining or finetuning")
    ```

- **[arXiv260101] CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture**
  - **tags:** [cv], [multiple-object tracking], [appearance-motion association, re-identification, exponential moving average]
  - **authors:** Md Ahmed Al Muzaddid, Jordan A. James, William J. Beksi
  - **institution:** The University of Texas at Arlington
  - **link:** https://arxiv.org/pdf/2512.24838
  - **contributions:** 1. A reranking-enhanced appearance association module to improve feature matching. 2. A one-to-many association strategy with an appearance-based conflict resolution mechanism. 3. An exponential moving average prototype feature bank for robust appearance modeling.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408f95352ab182fdf244f790c589fbdfab17f28b86d4a84534385d1d25eb2fd_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CropTrack, a novel multiple-object tracking framework for agricultural environments that combines appearance and motion information to address challenges like occlusions and similar object appearances. It introduces techniques like reranking-enhanced association and a prototype feature bank to improve identity preservation. The method outperforms traditional motion-based trackers and achieves state-of-the-art association accuracy on agricultural MOT datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[农业MOT挑战: 遮挡, 外观相似/MOT Challenges: Occlusions, Similar Appearance]
        C --> C1[结合外观与运动信息/Combine Appearance & Motion]
        C --> C2[重排序增强关联/Reranking-enhanced Association]
        C --> C3[指数移动平均特征库/Exponential Moving Average Feature Bank]
        D --> D1[更优的身份保持/Better Identity Preservation]
        D --> D2[更高的关联准确率/Higher Association Accuracy]
    ```

- **[arXiv260101] VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents**
  - **tags:** [ai], [embodied ai], [Vision-and-Language Navigation, Multimodal Large Language Models, evaluation framework, spatial reasoning, sequential decision-making]
  - **authors:** Xunyi Zhao, Gengze Zhou, Qi Wu
  - **institution:** Adelaide University, Australian Institute of Machine Learning
  - **link:** https://arxiv.org/pdf/2512.24851
  - **contributions:** 1. Introduced VLN-MME, a unified and extensible evaluation framework for probing MLLMs as zero-shot agents in Vision-and-Language Navigation. 2. Provided a highly modular and accessible design that enables structured comparisons and component-level ablations across diverse MLLM architectures and agent designs. 3. Discovered that enhancing agents with Chain-of-Thought reasoning and self-reflection leads to performance degradation, revealing MLLMs' poor context awareness and low 3D spatial reasoning fidelity in embodied navigation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15acf4c91a5dc52a1564da96c7602f92c4733b8ba50a6e941b2f51f2a00f6501_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the performance of Multimodal Large Language Models (MLLMs) as embodied agents for Vision-and-Language Navigation (VLN). It proposes a unified evaluation framework called VLN-MME to benchmark MLLMs in a zero-shot setting. The key finding is that, contrary to expectations, advanced reasoning techniques like Chain-of-Thought degrade navigation performance, indicating MLLMs have significant limitations in 3D spatial reasoning and sequential decision-making for embodied tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[评估MLLMs作为具身智能体的性能/Evaluating MLLMs as Embodied Agents]
        C --> C1[提出统一评估框架VLN-MME/Propose Unified Evaluation Framework VLN-MME]
        D --> D1[CoT与反思导致性能下降/CoT & Self-Reflection Degrade Performance]
        D --> D2[MLLMs空间推理能力弱/MLLMs Have Poor Spatial Reasoning]
    ```

- **[arXiv260101] OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [Segment Anything Model 2 (SAM2), few-shot learning, prompt-free segmentation, online learning, adaptive fusion]
  - **authors:** Meng Lan, Lefei Zhang, Xiaomeng Li
  - **institution:** The Hong Kong University of Science and Technology, Wuhan University
  - **link:** https://arxiv.org/pdf/2512.24861
  - **code:** https://github.com/xmed-lab/OFL-SAM2
  - **contributions:** 1. Proposes a prompt-free SAM2 framework (OFL-SAM2) that eliminates the need for manual prompts in medical image segmentation. 2. Introduces an online few-shot learner that trains a lightweight mapping network to generate target features from limited data and supports online parameter updates during inference. 3. Designs an adaptive fusion module that dynamically integrates the learned target features with the frozen SAM2's memory-attention features for robust segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2aff9624bd2c875a87e76830cf04f7468fc6ad3ed0290f0a8189f5a76f92143d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting the Segment Anything Model 2 (SAM2) to medical image segmentation without requiring extensive annotated data or manual prompts. It proposes OFL-SAM2, a framework that uses an online few-shot learner to train a mapping network for generating target features and an adaptive module to fuse them with SAM2's features. Experiments on three datasets show that OFL-SAM2 achieves state-of-the-art performance with limited training data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OFL-SAM2: Prompt SAM2 with Online Few-shot Learner] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Adapting SAM2 to medical images needs大量标注和手动提示/Requires extensive annotation and manual prompts]
        C --> C1[在线小样本学习器训练轻量映射网络/Online few-shot learner trains lightweight mapping network]
        C --> C2[自适应融合模块整合特征/Adaptive fusion module integrates features]
        D --> D1[在三个数据集上实现SOTA/State-of-the-art on three datasets]
        D --> D2[仅需有限训练数据/Requires only limited training data]
    ```

- **[arXiv260101] FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation**
  - **tags:** [ai], [multimodal reasoning], [multimodal large language models, financial numerical reasoning, retrieval-augmented generation, benchmark, scenario awareness]
  - **authors:** Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao
  - **institution:** Beijing University of Posts and Telecommunications, Hithink RoyalFlush Information Network Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.24903
  - **code:** https://bupt-reasoning-lab.github.io/FinMMDocR
  - **contributions:** 1. Introduces scenario awareness with 12 types of implicit financial scenarios integrated into 57.9% of problems. 2. Provides extensive document understanding with 837 bilingual documents averaging 50.8 pages across 9 types. 3. Requires complex multi-step computation averaging 11 reasoning steps, with 65% of problems needing cross-page evidence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a504876f4f69142bef32716f3ea6e7e385402763fd546c94f689c9b33389ebe8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FinMMDocR, a bilingual multimodal benchmark designed to evaluate MLLMs on complex, real-world financial numerical reasoning. It challenges models with scenario-aware problems, long documents, and multi-step computations. The best-performing model achieved only 58.0% accuracy, highlighting the difficulty of the benchmark and its potential to drive improvements in MLLMs and reasoning methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FinMMDocR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[评估MLLMs在真实世界金融数值推理上的表现/Evaluating MLLMs on Real-World Financial Numerical Reasoning]
        C --> C1[构建包含场景感知、文档理解、多步计算的基准/Building a Benchmark with Scenario Awareness, Document Understanding, Multi-Step Computation]
        D --> D1[最佳模型准确率仅为58.0%/Best Model Accuracy is Only 58.0%]
        D --> D2[RAG方法表现差异显著/Significant Performance Variations in RAG Methods]
    ```

- **[arXiv260101] Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection**
  - **tags:** [cv], [3D object detection], [domain adaptation, LiDAR, neuron activation patterns, continual learning, annotation budget]
  - **authors:** Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak
  - **institution:** Warsaw University of Technology, IDEAS NCBR
  - **link:** https://arxiv.org/pdf/2512.24922
  - **contributions:** 1. A novel LiDAR domain adaptation method based on neuron activation patterns for selecting a small, diverse subset of target domain samples for annotation., 2. A combined approach that uses a minimal annotation budget and incorporates post-training techniques inspired by continual learning to prevent model weight drift., 3. Empirical demonstration that the proposed method outperforms linear probing and state-of-the-art domain adaptation techniques.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9018c4d77628a42d8b5e9e448c8a20879fa60110552085be08d3517a806dbe44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the poor cross-domain generalization of 3D object detectors for autonomous vehicles by proposing a novel LiDAR domain adaptation method. The method selects a small, diverse, and representative subset of target domain samples for annotation based on neuron activation patterns and combines this with continual learning-inspired techniques to prevent weight drift. The approach achieves state-of-the-art performance with a very low annotation cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[3D目标检测器跨域泛化能力差/3D object detectors generalize poorly across domains]
        C --> C1[基于神经元激活模式选择样本/Select samples via neuron activation patterns]
        C --> C2[标注少量多样化的目标域样本/Annotate small, diverse target subset]
        C --> C3[结合持续学习防止权重漂移/Combine with continual learning to prevent weight drift]
        D --> D1[以极低标注成本实现SOTA性能/Achieves SOTA with very low annotation cost]
        D --> D2[优于线性探测和现有域适应方法/Outperforms linear probing & SOTA DA methods]
    ```

- **[arXiv260101] CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement**
  - **tags:** [ai], [vision-language models], [LLM-as-a-Judge, training-free, few-shot, explainable AI, agricultural VQA]
  - **authors:** Wentao Zhang, Tao Fang, Lina Lu, Lifei Wang, Weihe Zhong
  - **institution:** Shandong University of Technology, Macau Millennium College
  - **link:** https://arxiv.org/pdf/2512.24947
  - **code:** https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis
  - **contributions:** 1. Proposes a novel Caption-Prompt-Judge (CPJ) framework for training-free, few-shot agricultural pest diagnosis. 2. Introduces an LLM-as-Judge module for iterative refinement of structured, multi-angle image captions to enhance interpretability. 3. Demonstrates significant performance improvements on a benchmark dataset using lightweight models, advancing robust and explainable diagnosis without fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/677e58cca4c831f09b054974c399f7092a40a64b068915b259e624f2728ef082_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CPJ, a training-free framework that uses large vision-language models to generate and refine image captions, which are then used to improve agricultural visual question answering. The method significantly boosts disease classification and QA performance on the CDDMBench dataset, providing transparent, evidence-based reasoning without requiring model fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CPJ: Explainable Agricultural Pest Diagnosis] --> B[核心问题/Problem: Existing methods rely on costly fine-tuning and lack explainability under domain shifts.]
        A --> C[主要方法/Method: CPJ framework uses LVLMs to generate captions, refines them via LLM-as-Judge, and performs caption-informed VQA.]
        A --> D[关键结果/Results: Significant performance gains (+22.7 pp in classification, +19.5 in QA score) on CDDMBench.]
    ```

- **[arXiv260101] ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT**
  - **tags:** [cv], [medical image analysis], [diffusion models, motion correction, synthetic data, coronary artery calcium, non-gated CT]
  - **authors:** Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou
  - **institution:** Northwestern University, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.24948
  - **contributions:** 1. A CAC motion simulation data engine that synthesizes realistic non-gated CT acquisitions from gated CTs for supervised training without paired data. 2. A property-aware learning strategy that incorporates calcium-specific priors via a differentiable consistency loss to preserve lesion integrity. 3. A progressive correction scheme that gradually reduces motion artifacts across diffusion steps to enhance stability and calcium fidelity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd53987de700ad7024fad5ea5d53da57ecb375707cda1bff594c7264ec4ca118_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes ProDM, a diffusion model framework to correct motion artifacts in coronary calcium lesions from non-gated chest CT scans. The method uses a synthetic data engine for training, incorporates calcium-specific priors, and applies progressive correction. Experiments show it improves scoring accuracy, lesion fidelity, and risk stratification compared to baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[非门控CT中冠脉钙化运动伪影影响评分/Non-gated CT CAC scoring affected by motion artifacts]
        C --> C1[合成数据引擎/Synthetic Data Engine]
        C --> C2[属性感知学习/Property-aware Learning]
        C --> C3[渐进式校正/Progressive Correction]
        D --> D1[提高评分准确性和病灶保真度/Improved scoring accuracy & lesion fidelity]
        D --> D2[提升风险分层性能/Enhanced risk stratification]
        D --> D3[抑制伪影，提升临床可用性/Suppresses artifacts & improves clinical usability]
    ```

- **[arXiv260101] VIPER: Process-aware Evaluation for Generative Video Reasoning**
  - **tags:** [cv], [video generation and reasoning], [Chain-of-Frames reasoning, Process-outcome Consistency, VLM-as-Judge]
  - **authors:** Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu
  - **institution:** Renmin University of China, ByteDance
  - **link:** https://arxiv.org/pdf/2512.24952
  - **contributions:** 1. Proposes a process-aware evaluation paradigm for Generative Video Reasoning (GVR) to address outcome-hacking. 2. Introduces the VIPER benchmark spanning 16 tasks across six reasoning domains. 3. Proposes the Process-outcome Consistency (POC@r) metric, which uses a VLM-as-Judge with a hierarchical rubric to evaluate both intermediate steps and final results.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5284fdefc82fba6a147f2ca20a0d1efbfeec49032fc77145fce8a9fc02faf645_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a flaw in evaluating generative video reasoning, where models can get the right answer via a wrong process (outcome-hacking). To address this, the authors propose VIPER, a new benchmark with a process-aware evaluation metric called POC@r. Their experiments show state-of-the-art video models perform poorly (≈20% POC@1.0), revealing a significant gap between current video generation and true visual reasoning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VIPER: Process-aware Evaluation for Generative Video Reasoning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有评估导致结果作弊/Existing evaluation leads to outcome-hacking]
        C --> C1[提出过程感知评估范式/Propose process-aware evaluation paradigm]
        C --> C2[引入VIPER基准/Introduce VIPER benchmark]
        C --> C3[提出POC@r指标/Propose POC@r metric]
        D --> D1[SOTA模型POC@1.0仅约20%/SOTA models achieve only ~20% POC@1.0]
        D --> D2[存在显著结果作弊/Exhibit significant outcome-hacking]
    ```

- **[arXiv260101] HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films**
  - **tags:** [cv], [video restoration], [diffusion model, film restoration, high-resolution video, patch-wise training, global-local frequency module]
  - **authors:** Rongji Xun, Junjie Yuan, Zhongjie Wang
  - **institution:** Tongji University, Shanghai Film Restoration Laboratory
  - **link:** https://arxiv.org/pdf/2512.24946
  - **code:** https://anonymous.4open.science/r/HaineiFRDM
  - **contributions:** 1. Proposed HaineiFRDM, a film restoration framework leveraging diffusion models for content understanding to restore indistinguishable film defects. 2. Introduced a patch-wise training/testing strategy with a position-aware Global Prompt and Frame Fusion Module and a global-local frequency module to enable high-resolution restoration on a single 24GB GPU and ensure texture consistency. 3. Constructed a new film restoration dataset containing restored real-degraded films and realistic synthetic data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49ddd01a0523d32c1f20822a4a1d270659f5454212011a90e04a39fe796d1ab_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes HaineiFRDM, a diffusion model-based framework for restoring high-resolution, real-world films. It addresses limitations of existing open-source methods by using a patch-wise strategy and novel modules to handle high-resolution videos efficiently and introduces a new dataset. Experiments show the model outperforms existing open-source methods in defect restoration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HaineiFRDM: Explore Diffusion to Restore High-resolution Real-World Films] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[开源方法性能有限/Open-source methods have limited performance]
        B --> B2[高分辨率电影未探索/High-resolution films unexplored]
        C --> C1[基于扩散模型的修复框架/Diffusion-based restoration framework]
        C --> C2[分块训练与测试策略/Patch-wise training & testing]
        C --> C3[全局-局部频率模块/Global-local frequency module]
        C --> C4[构建新数据集/Construct new dataset]
        D --> D1[缺陷修复能力优越/Superior defect restoration ability]
        D --> D2[代码与数据集将开源/Code & dataset to be released]
    ```

- **[arXiv260101] ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands**
  - **tags:** [ai], [human-computer interaction], [flow-based generative model, GUI automation, continuous trajectory prediction, unified discrete-continuous actions, ScreenDrag benchmark]
  - **authors:** Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou
  - **institution:** Show Lab, National University of Singapore
  - **link:** https://arxiv.org/pdf/2512.24965
  - **code:** https://github.com/showlab/showui-pi
  - **contributions:** 1. Proposed ShowUI-π, the first flow-based generative model for GUI dexterous manipulation, unifying discrete clicks and continuous drags in a shared model. 2. Introduced a flow-based action generation method for drag modeling, predicting incremental cursor adjustments from continuous visual observations. 3. Created ScreenDrag, a benchmark with 20K drag trajectories across five domains and comprehensive evaluation protocols to assess GUI agents' drag capabilities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6391f609bd9b67bc717f5e3756501bf8f4dedd5a207352ff5b4f02bc902207_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing GUI agents that only perform discrete clicks, lacking the ability for continuous, closed-loop drag interactions. The authors propose ShowUI-π, a flow-based generative model that unifies discrete and continuous actions and generates smooth drag trajectories from visual observations. Experiments show ShowUI-π outperforms proprietary GUI agents on the new ScreenDrag benchmark, demonstrating effective dexterous control for GUI automation.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands] --> B[核心问题/Problem: Existing GUI agents only support discrete clicks, lacking continuous drag capability for closed-loop trajectories]
    A --> C[主要方法/Method: Flow-based generative model with unified discrete-continuous actions and incremental trajectory prediction]
    A --> D[关键结果/Results: Outperforms proprietary agents on ScreenDrag benchmark (score 26.98), demonstrating effective dexterous control]
    ```

- **[arXiv260101] Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantization, pruning, weight clustering, robustness, multiobjective assessment]
  - **authors:** Itallo Patrick Castro Alves Da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza, Baldoino Fonseca dos Santos Neto, Marcio de Medeiros Ribeiro
  - **institution:** Federal University of Alagoas, Federal University of Rio Grande do Norte
  - **link:** https://arxiv.org/pdf/2512.24971
  - **contributions:** 1. Conducted a comprehensive evaluation of individual and combined compression techniques (quantization, pruning, weight clustering) on CNNs for robustness under natural corruptions. 2. Demonstrated that certain compression strategies can preserve or even improve model robustness, especially on complex architectures. 3. Utilized multiobjective assessment to identify optimal compression configurations that balance robustness, accuracy, and compression ratio for real-world deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8640f8930863d5573a7df0bb7287b8800d92c67a60523b6b0dab2eb044c58bb_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how compression techniques like quantization, pruning, and weight clustering affect the robustness of CNNs (ResNet-50, VGG-19, MobileNetV2) against natural image corruptions on CIFAR-10-C/100-C. It finds that specific compression strategies, particularly when combined, can maintain or enhance robustness, with multiobjective analysis revealing the best trade-offs for efficient and robust model deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Evaluating the Impact of Compression Techniques on CNNs under Natural Corruptions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[压缩模型在自然损坏下的鲁棒性/Robustness of compressed models under natural corruptions]
        C --> C1[评估量化、剪枝、权重聚类技术/Evaluate quantization, pruning, weight clustering]
        C --> C2[使用CIFAR-10/100-C数据集/Use CIFAR-10/100-C datasets]
        C --> C3[多目标评估/Multiobjective assessment]
        D --> D1[特定策略保持或提升鲁棒性/Certain strategies preserve or improve robustness]
        D --> D2[定制组合产生有益结果/Customized combinations yield beneficial results]
    ```

- **[arXiv260101] DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments**
  - **tags:** [cv], [embodied vision-language reasoning], [low-light vision, embodied question answering, vision-language models, image enhancement, benchmark]
  - **authors:** Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh
  - **institution:** Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)
  - **link:** https://arxiv.org/pdf/2512.24985
  - **contributions:** 1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --> B[核心问题/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]
        A --> C[主要方法/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]
        A --> D[关键结果/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]
    ```

- **[arXiv260101] Bi-C2R: Bidirectional Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification**
  - **tags:** [cv], [person re-identification], [lifelong learning, compatible representation, re-indexing free, continual learning, feature distillation]
  - **authors:** Zhenyu Cui, Jiahuan Zhou, Yuxin Peng
  - **institution:** Peking University
  - **link:** https://arxiv.org/pdf/2512.25000
  - **code:** https://github.com/PKU-ICST-MIPL/Bi-C2R-TPAMI2026
  - **contributions:** 1. Introduces a new, more challenging task called Re-indexing Free Lifelong Person Re-identification (RFL-ReID) that eliminates the need for costly and privacy-sensitive re-indexing of historical gallery images. 2. Proposes the Bi-C2R framework, which includes a bidirectional compatible transfer network to update old gallery features to the new feature space and distillation modules to balance compatibility and prevent forgetting. 3. Designs a feature-level exponential moving average strategy to adaptively handle knowledge gaps across different data domains.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b768cca9f9f0fa320a67a2646aa080f3367ff2175466f98a339c2aee634a4bd_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of lifelong person re-identification without re-indexing historical gallery images, a task termed RFL-ReID. To solve it, the authors propose the Bi-C2R framework, which updates old gallery features for compatibility with new models and uses distillation techniques to balance new and old knowledge. Experiments show the method achieves leading performance on both the new RFL-ReID task and the traditional lifelong Re-ID task.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Bi-C2R: Bidirectional Continual Compatible Representation] --> B[核心问题/Problem: 终身Re-ID中重索引成本高、有隐私问题 / Re-indexing in L-ReID is costly and has privacy issues]
        A --> C[主要方法/Method: 提出Bi-C2R框架，双向兼容特征更新与蒸馏 / Propose Bi-C2R framework for bidirectional compatible feature update and distillation]
        A --> D[关键结果/Results: 在新RFL-ReID和传统L-ReID任务上取得领先性能 / Achieves leading performance on both new RFL-ReID and traditional L-ReID tasks]
    ```

- **[arXiv260101] PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes**
  - **tags:** [cv], [3D scene understanding and animation], [3D Gaussian Splatting, physics simulation, large language model, real-time animation, open-vocabulary]
  - **authors:** Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam
  - **institution:** Sapienza University of Rome, Technical University of Munich, Munich Center for Machine Learning (MCML)
  - **link:** https://arxiv.org/pdf/2512.24986
  - **contributions:** 1. Introduces the first framework to directly couple 3D Gaussian Splatting with a physics simulator, bypassing slow mesh extraction. 2. Enables open-vocabulary, real-time, and interactive 4D animation through an LLM that generates executable code to modify scene parameters. 3. Presents a train-free and computationally lightweight pipeline, shifting animation workflows from offline rendering to interactive dialogue.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1113cf9009e210565f13d799d298867f98470a28066f31b961bce3b6577044fe_w640_q70.webp
  - **Simple LLM Summary:** The paper presents PhysTalk, a system that generates real-time, physics-based 4D animations from text prompts and 3D Gaussian Splatting scenes. It uses a large language model to produce code that manipulates scene proxies for physics simulation, enabling interactive and realistic motion without requiring model training. This approach makes physically realistic animation more accessible and interactive.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法缺乏物理真实性和实时交互性/Current methods lack physical realism and real-time interaction]
        C --> C1[使用LLM将文本提示转化为可执行代码/Uses LLM to translate prompts into executable code]
        C --> C2[通过轻量级代理直接修改3DGS参数/Modifies 3DGS parameters via lightweight proxies]
        C --> C3[集成物理模拟器进行碰撞感知动画/Integrates physics simulator for collision-aware animation]
        D --> D1[实现实时、开集词汇的4D动画/Achieves real-time, open-vocabulary 4D animation]
        D --> D2[无需训练，计算轻量/Train-free and computationally lightweight]
        D --> D3[将工作流转向交互式对话/Shifts workflow to interactive dialogue]
    ```

- **[arXiv260101] FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM**
  - **tags:** [cv], [visual SLAM], [monocular dense SLAM, depth foundation models, bundle adjustment, optical flow, geometric consistency]
  - **authors:** Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai
  - **institution:** Beihang University, University of Bologna
  - **link:** https://arxiv.org/pdf/2512.25008
  - **contributions:** 1. A Hybrid Flow Network that integrates geometric guidance from depth foundation models to produce geometry-aware correspondences. 2. A Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints for global consistency. 3. A Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f726314fa4bdd1cd04071665eebafc2932edc767a54b92772f39f3f9d16942af_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FoundationSLAM, a learning-based monocular dense SLAM system that addresses the lack of geometric consistency in flow-based approaches by leveraging depth foundation models to guide correspondence estimation and multi-view optimization. The method introduces a hybrid flow network, a bi-consistent bundle adjustment layer, and a reliability-aware refinement mechanism. Experiments show it achieves superior trajectory accuracy and reconstruction quality in real-time, demonstrating strong generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FoundationSLAM] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏几何一致性<br>Lack of Geometric Consistency]
        C --> C1[混合光流网络<br>Hybrid Flow Network]
        C --> C2[双向一致BA层<br>Bi-Consistent Bundle Adjustment]
        C --> C3[可靠性感知细化<br>Reliability-Aware Refinement]
        D --> D1[高精度轨迹与重建<br>High Trajectory & Reconstruction Accuracy]
        D --> D2[实时性能(18 FPS)<br>Real-time Performance (18 FPS)]
        D --> D3[强泛化能力<br>Strong Generalization]
    ```

- **[arXiv260101] Generative Classifiers Avoid Shortcut Solutions**
  - **tags:** [ai], [generative models], [generative classifiers, spurious correlations, distribution shift, diffusion models, autoregressive models]
  - **authors:** Alexander C. Li, Ananya Kumar, Deepak Pathak
  - **institution:** Carnegie Mellon University, Stanford University
  - **link:** https://arxiv.org/pdf/2512.25034
  - **code:** https://github.com/alexlioralexli/generative-classifiers
  - **contributions:** 1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]
        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]
        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]
        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]
        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]
    ```

- **[arXiv260101] FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion**
  - **tags:** TBD
  - **authors:** Dian Shao, Mingfei Shi, Like Liu
  - **institution:** 
  - **link:** https://arxiv.org/pdf/2512.25067
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5283ff5d47b7344e785800154886fe627c149576029db00dc6b0fd29ebf4b9fb_w640_q70.webp

- **[arXiv260101] From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing**
  - **tags:** [cv], [audio-visual synthesis], [visual dubbing, diffusion transformer, self-bootstrapping, video-to-video editing, lip synchronization]
  - **authors:** Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu
  - **institution:** Tsinghua University, Kuaishou Technology (Kling Team), Beihang University, Hong Kong University of Science and Technology, The Chinese University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.25066
  - **code:** https://hjrphoebus.github.io/X-Dub/
  - **contributions:** 1. A novel self-bootstrapping framework that reframes visual dubbing from a mask-inpainting task into a well-conditioned video-to-video editing problem. 2. A timestep-adaptive multi-phase learning strategy to disentangle conflicting editing objectives across diffusion timesteps for stable training. 3. The introduction of ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d6232f4b29adb66dc490f9b2d29652266a1c5fe1f49e7b7502c3833d4ad4fbb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of audio-driven visual dubbing, where ideal paired training data is lacking. It proposes a self-bootstrapping framework that first uses a Diffusion Transformer to generate ideal, lip-altered companion videos, and then trains an audio-driven editor on these aligned pairs. This approach enables precise lip synchronization, faithful identity preservation, and robustness in complex, real-world scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing"]
        Root --> Problem["核心问题/Problem: Lack of ideal training data (aligned video pairs) for visual dubbing"]
        Root --> Method["主要方法/Method: Self-bootstrapping framework using DiT as data generator and audio-driven editor"]
        Root --> Results["关键结果/Results: Accurate lip sync, identity preservation, and robustness in challenging scenarios"]
    ```

- **[arXiv260101] GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction**
  - **tags:** [cv], [3D reconstruction], [sparse-view reconstruction, multi-view outpainting, 3D Gaussian Splatting, diffusion models, geometry-aware denoising]
  - **authors:** Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu
  - **institution:** National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.25073
  - **contributions:** 1. Proposes a novel sparse-view 3D reconstruction framework (GaMO) that reformulates the problem as multi-view outpainting to expand the field of view from existing poses, preserving geometric consistency. 2. Introduces a zero-shot, geometry-aware denoising strategy within a multi-view diffusion model, eliminating the need for training. 3. Achieves state-of-the-art reconstruction quality and a significant speedup (25x faster) compared to prior diffusion-based methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb876042c4f6f417db4c360c60e273183cfc5b6039f35f552b59209074fc463e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of low-quality 3D reconstruction from sparse input views. It proposes GaMO, a method that uses multi-view diffusion outpainting to widen the field of view of existing images, which are then used to refine a 3D Gaussian Splatting model. This approach improves geometric consistency and visual quality while being significantly faster than previous state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction") --> Problem("核心问题/Problem: Sparse-view 3D reconstruction struggles with limited coverage, geometric inconsistency, and high computation")
        Root --> Method("主要方法/Method: Multi-view outpainting to expand FOV from known poses, using zero-shot geometry-aware diffusion")
        Root --> Results("关键结果/Results: SOTA quality on benchmarks, 25x speedup, improved geometric consistency")
    ```

- **[arXiv260101] Edit3r: Instant 3D Scene Editing from Sparse Unposed Images**
  - **tags:** [cv], [3D scene editing], [feed-forward 3D editing, SAM2-based recoloring, asymmetric input strategy, DL3DV-Edit-Bench]
  - **authors:** Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang
  - **institution:** University of Massachusetts Amherst, University of California, Merced, NVIDIA, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.25071
  - **code:** https://edit3r.github.io/edit3r/
  - **contributions:** 1. Proposes Edit3r, a feed-forward framework for instant 3D scene editing from sparse, unposed, and view-inconsistent 2D edited images, eliminating the need for per-scene optimization or pose estimation. 2. Introduces a novel training strategy using a SAM2-based recoloring method to generate cross-view-consistent supervision and an asymmetric input strategy to fuse disparate observations, addressing the lack of multi-view consistent training data. 3. Establishes DL3DV-Edit-Bench, a new benchmark for large-scale quantitative evaluation of 3D scene editing methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e25767650653d1a03ee1a7fcb676ec0786928604bf424428075decbe81db98f_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Edit3r, a fast, feed-forward model that directly reconstructs and edits 3D scenes from sparse, unposed, and view-inconsistent 2D edited images. To overcome the lack of multi-view consistent training data, it uses a SAM2-based recoloring strategy for supervision and an asymmetric input strategy. The method achieves superior semantic alignment and 3D consistency compared to baselines at high inference speed (~0.5s), enabling real-time applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Edit3r: Instant 3D Scene Editing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[从无位姿、视图不一致的2D编辑图像进行3D编辑/3D editing from unposed, view-inconsistent 2D edits]
        B --> B2[缺乏多视图一致的训练数据/Lack of multi-view consistent training data]
        C --> C1[前馈框架/Feed-forward framework]
        C --> C2[SAM2重着色监督/SAM2-based recoloring supervision]
        C --> C3[非对称输入策略/Asymmetric input strategy]
        D --> D1[语义对齐与3D一致性更好/Better semantic alignment & 3D consistency]
        D --> D2[推理速度快(~0.5秒)/Fast inference (~0.5s)]
        D --> D3[新评测基准: DL3DV-Edit-Bench/New benchmark: DL3DV-Edit-Bench]
    ```

- **[arXiv260101] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time**
  - **tags:** [cv], [video generation], [video diffusion model, space-time disentanglement, temporal-warping training, camera-conditioning, generative rendering]
  - **authors:** Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang
  - **institution:** University of Cambridge, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.25075
  - **code:** https://github.com/zheninghuang/Space-Time-Pilot
  - **contributions:** 1. Introduced an animation time-embedding mechanism for explicit motion sequence control in video diffusion. 2. Proposed a temporal-warping training scheme to repurpose multi-view datasets for learning temporal variations. 3. Created the CamxTime synthetic dataset and an improved camera-conditioning mechanism for precise dual space-time control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SpaceTimePilot, a video diffusion model that independently controls camera viewpoint and motion sequence to re-render dynamic scenes from a monocular video. The method uses a novel time-embedding mechanism and a temporal-warping training strategy to achieve robust space-time disentanglement. Experiments show the model enables continuous exploration across space and time, outperforming prior work.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time] --> B[核心问题/Problem: 如何从单目视频中解耦空间和时间以进行可控生成渲染/How to disentangle space and time from a monocular video for controllable generative rendering]
        A --> C[主要方法/Method: 引入动画时间嵌入机制和时域扭曲训练方案/Introduce animation time-embedding and temporal-warping training scheme]
        A --> D[关键结果/Results: 实现鲁棒的时空解耦与连续可控渲染/Achieve robust space-time disentanglement and continuous controllable rendering]
    ```

- **[arXiv260101] q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI using Physics-Informed Diffusion Models**
  - **tags:** [cv], [medical imaging], [diffusion models, quantitative MRI, data consistency, physics-informed, multi-parametric mapping]
  - **authors:** Shishuai Wang, Florian Wiesinger, Noemi Sgambelluri, Carolin Pirkl, Stefan Klein, Juan A. Hernandez-Tamames, Dirk H.J. Poot
  - **institution:** Erasmus MC (Erasmus University Medical Center)
  - **link:** https://arxiv.org/pdf/2512.23726
  - **contributions:** 1. Proposes a diffusion model-based method (q3-MuPa) for quantitative MRI mapping that combines a deep generative model with a physics-based data consistency constraint., 2. Enables high-quality mapping from a fourfold-accelerated, nearly silent MRI scan (MuPa-ZTE), reducing acquisition time to ~1 minute., 3. Demonstrates successful training on synthetic data from digital phantoms alone, with strong generalization to real patient and phantom scans.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e081389f251cbbacaf7c704cd1a34c3a032b2173e63192833db976abd6541d5e_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes q3-MuPa, a method that uses a physics-informed diffusion model to generate high-quality quantitative MRI maps (T1, T2, proton density) from accelerated, silent scans. The method integrates a denoising diffusion model with the MRI signal physics as a constraint during inference. It achieves accurate mapping from 1-minute scans and generalizes well to real data despite being trained only on synthetic phantoms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI] --> B(核心问题/Problem: Need for fast, quiet, and accurate quantitative MRI mapping)
        A --> C(主要方法/Method: Physics-informed diffusion model with data consistency)
        A --> D(关键结果/Results: High-accuracy maps from 1-min scans, trained on synthetic data)
    ```

- **[arXiv260101] Leveraging Machine Learning for Early Detection of Lung Diseases**
  - **tags:** [cv], [medical image analysis], [deep learning, convolutional neural networks, chest x-ray, disease classification, VGG16]
  - **authors:** Bahareh Rahmani, Harsha Reddy Bindela, Rama Kanth Reddy Gosula, Krishna Yedubati, Mohammad Amir Salari, Leslie Hinyard, Payam Norouzzadeh, Eli Snir, Martin Schoen
  - **institution:** Saint Louis University, Washington University at Saint Louis
  - **link:** https://arxiv.org/pdf/2512.23757
  - **contributions:** 1. Proposes a diagnostic framework combining traditional image processing with advanced neural networks for lung disease detection. 2. Trains and validates multiple deep learning models (CNNs, VGG16, InceptionV3, EfficientNetB0) on chest x-rays for COVID-19, lung cancer, and pneumonia. 3. Demonstrates high accuracy, precision, recall, and F1 scores, highlighting the potential for real-world, non-invasive diagnostic applications in resource-limited settings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5eaabf315a0348dff119282fff830baf854bc7edaf57b6601b94745f8aa312_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using deep learning models, including CNNs, VGG16, InceptionV3, and EfficientNetB0, to diagnose lung diseases like COVID-19, lung cancer, and pneumonia from chest x-rays. The method combines traditional image processing with neural networks to create a rapid, non-invasive diagnostic tool. The study concludes that these models achieve high performance metrics, showing reliability and potential for real-world healthcare applications, especially where radiologists are scarce.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Leveraging Machine Learning for Early Detection of Lung Diseases<br>利用机器学习进行肺部疾病早期检测] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Limited access to radiologists & resources<br>放射科医生和资源有限]
        B --> B2[Need for rapid, non-invasive diagnosis<br>需要快速、无创诊断]
        C --> C1[Combine image processing & neural networks<br>结合图像处理和神经网络]
        C --> C2[Train models (CNN, VGG16, etc.) on chest X-rays<br>在胸部X光片上训练模型]
        D --> D1[High accuracy, precision, recall, F1 scores<br>高准确率、精确率、召回率、F1分数]
        D --> D2[Potential for real-world diagnostic applications<br>具有实际诊断应用潜力]
    ```

- **[arXiv260101] A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe**
  - **tags:** [cv], [remote sensing image analysis], [InSAR, Transformer, ground deformation forecasting, cross-site generalization, multimodal learning]
  - **authors:** Wendong Yao, Binhua Huang, Soumyabrata Dev
  - **institution:** ADAPT SFI Research Centre, University College Dublin
  - **link:** https://arxiv.org/pdf/2512.23906
  - **contributions:** 1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[论文标题: A multimodal Transformer for InSAR-based ground deformation forecasting] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: 如何利用历史InSAR数据预测未来的地表形变?] --> P1[挑战/Challenges: 长期趋势、季节周期、突变事件的叠加]
        Method[主要方法/Method: 多模态Transformer] --> M1[输入/Inputs: 近期形变图、静态运动学指标、时间编码]
        Method --> M2[任务/Task: 单步、固定间隔的下一时期临近预报]
        Results[关键结果/Results] --> R1[性能/Performance: RMSE=0.90mm, R²=0.97 (爱尔兰测试集)]
        Results --> R2[泛化/Generalization: 跨欧洲5个未见区域，R²≥0.93]
    ```

- **[arXiv260101] One-Shot Structured Pruning of Quantum Neural Networks via $q$-Group Engineering and Quantum Geometric Metrics**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [quantum neural networks, structured pruning, q-deformed groups, quantum geometry, noise calibration]
  - **authors:** Haijian Shao, Wei Liu, Xing Deng, Yingtao Jiang
  - **institution:** Jiangsu University of Science and Technology, University of Nevada, Las Vegas
  - **link:** https://arxiv.org/pdf/2512.24019
  - **contributions:** 1. A one-shot structured pruning framework (q-iPrune) for QNNs based on q-deformed group algebra and task-conditioned quantum geometry., 2. Rigorous theoretical guarantees including completeness of pruning, bounded functional equivalence, and computational feasibility., 3. Introduction of a noise-calibrated deformation parameter to adapt pruning decisions to hardware imperfections.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cacc21f5549082cf6a65797fca25540bc063dd1ba9c51a7d5be4a807cd4d1b35_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes q-iPrune, a one-shot structured pruning method for Quantum Neural Networks (QNNs) that uses q-deformed group theory and quantum geometric metrics to identify and remove redundant gates while provably bounding task performance degradation. The method provides theoretical guarantees and adapts to hardware noise. Experiments show it achieves significant gate reduction with controlled performance loss.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("One-Shot Structured Pruning of QNNs via q-Group Engineering and Quantum Geometric Metrics") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("QNNs suffer from gate-level redundancy / QNN存在门级冗余")
        Problem --> P2("Hinders deployment on NISQ devices / 阻碍在NISQ设备上的部署")
        Method --> M1("Proposes q-iPrune framework / 提出q-iPrune框架")
        Method --> M2("Uses q-deformed groups & task-conditioned quantum geometry / 使用q变形群和任务条件量子几何")
        Method --> M3("Defines q-overlap distance for gate comparison / 定义q重叠距离进行门比较")
        Results --> R1("Theoretical guarantees: completeness, bounded error, feasibility / 理论保证: 完备性、有界误差、可行性")
        Results --> R2("Substantial gate reduction with bounded performance degradation / 显著减少门数量且性能下降有界")
        Results --> R3("Noise adaptation via parameter λ / 通过参数λ适应噪声")
    ```

- **[arXiv260101] Targeted Semantic Segmentation of Himalayan Glacial Lakes Using Time-Series SAR: Towards Automated GLOF Early Warning**
  - **tags:** [cv], [semantic segmentation], [U-Net, EfficientNet-B3, temporal-first training, Sentinel-1 SAR, Dockerized pipeline]
  - **authors:** Pawan Adhikari, Satish Raj Regmi, Hari Ram Shrestha
  - **institution:** Space Research Center, Nepal Academy of Science and Technology (NAST)
  - **link:** https://arxiv.org/pdf/2512.24117
  - **contributions:** 1. A "temporal-first" training strategy for targeted semantic segmentation of glacial lakes using time-series SAR data. 2. An end-to-end automated deep learning pipeline achieving high IoU (0.9130) on a curated dataset of four high-risk Himalayan lakes. 3. A proposed operational engineering architecture featuring a Dockerized pipeline with automated data ingestion and a RESTful API, enabling dynamic early warning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6518e1c9ef6798da83807e183229f6799ac7cd2d2484a373f3a792675390241f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of monitoring high-risk Himalayan glacial lakes for GLOF early warning by proposing an automated deep learning pipeline using time-series Sentinel-1 SAR imagery. The method employs a U-Net with an EfficientNet-B3 backbone trained with a novel "temporal-first" strategy on a curated dataset of four lakes. The model achieves high segmentation accuracy (IoU 0.9130), and the proposed Dockerized system architecture provides a scalable foundation for transitioning from static mapping to dynamic, automated early warning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Targeted Semantic Segmentation of Himalayan Glacial Lakes Using Time-Series SAR] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>GLOF灾害监测/GLOF Hazard Monitoring<br>云层遮挡/Cloud Occlusion<br>通用模型局限/General Model Limitations]
        C[主要方法/Method<br>时间优先训练/Temporal-First Training<br>U-Net + EfficientNet-B3<br>哨兵1号SAR/ Sentinel-1 SAR<br>Docker化管道/Dockerized Pipeline]
        D[关键结果/Results<br>高IoU得分/High IoU (0.9130)<br>自动化预警系统/Automated Early Warning System<br>可扩展架构/Scalable Architecture]
    ```

- **[arXiv260101] Automated Classification of First-Trimester Fetal Heart Views Using Ultrasound-Specific Self-Supervised Learning**
  - **tags:** [cv], [medical image classification], [self-supervised learning, masked autoencoder, vision transformer, ultrasound foundation model, fetal echocardiography]
  - **authors:** Youssef Megahed, Aylin Erman, Robin Ducharme, Mark C. Walker, Steven Hawken, Adrian D. C. Chan
  - **institution:** Carleton University, University of Ottawa, Ottawa Hospital Research Institute
  - **link:** https://arxiv.org/pdf/2512.24492
  - **contributions:** 1. Evaluated a self-supervised ultrasound foundation model (USF-MAE) for the challenging task of first-trimester fetal heart view classification. 2. Demonstrated that ultrasound-specific pretraining on a large, unlabeled dataset yields more transferable representations than models pretrained on natural images (ImageNet) or standard supervised CNNs. 3. Showed robust performance without the need for aggressive image preprocessing or region-of-interest cropping, and improved discrimination of non-diagnostic frames.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f73d9d90b381ff2a6f244e15b64b3e24d9b28e8c2fbe78fa50c79ef8f58d1c66_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of automated first-trimester fetal heart view classification in ultrasound by fine-tuning a self-supervised ultrasound foundation model (USF-MAE). The model, pretrained on over 370,000 unlabeled ultrasound images, outperformed supervised CNN baselines and a natural image-pretrained Vision Transformer, achieving over 90% accuracy. The results indicate that ultrasound-specific self-supervised pretraining enables more generalizable representations for early fetal cardiac imaging.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated Classification of First-Trimester Fetal Heart Views<br>早孕期胎儿心脏视图自动分类] --> B
        A --> C
        A --> D
        B[Problem: Early detection of congenital heart disease is challenging<br>核心问题: 先天性心脏病早期检测困难]
        C[Method: Fine-tune USF-MAE, an ultrasound-specific self-supervised model<br>主要方法: 微调超声专用自监督模型USF-MAE]
        D[Results: Achieved SOTA 90.57% accuracy, outperforming baselines<br>关键结果: 达到90.57%准确率，超越基线模型]
    ```

- **[arXiv260101] Towards autonomous time-calibration of large quantum-dot devices: Detection, real-time feedback, and noise spectroscopy**
  - **tags:** [other], [quantum computing], [quantum-dot, charge stability diagrams, noise spectroscopy, autonomous calibration, electrostatic drift]
  - **authors:** Anantha S. Rao, Barnaby van Straaten, Valentin John, Cécile X. Yu, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, M. D. Stewart Jr., Menno Veldhorst, Francesco Borsoi, Justyna P. Zwolak
  - **institution:** University of Maryland, Delft University of Technology, National Institute of Standards and Technology
  - **link:** https://arxiv.org/pdf/2512.24894
  - **contributions:** 1. Introduces a method using repeated charge stability diagrams as a multidimensional probe to autonomously detect and compensate for electrostatic drift in quantum-dot devices. 2. Demonstrates robust stabilization and real-time diagnostic access to dot-specific noise processes on a 10-quantum-dot device. 3. Enables time-domain noise spectroscopy to extract noise power spectral densities, identify two-level fluctuators, and analyze spatial noise correlations across the array.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd18e88d286e5eeb4d0c678ec6f483639d082a566d746717bc187c721ae3b41a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of electrostatic drift in large quantum-dot arrays by proposing an autonomous calibration method that tracks charge transitions in stability diagrams to provide real-time feedback and compensation. The approach also enables detailed noise spectroscopy, revealing the noise characteristics and spatial correlations within the device. This forms a scalable foundation for maintaining stable, high-fidelity operations in quantum processors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards autonomous time-calibration of large quantum-dot devices<br>大型量子点器件自主时间校准] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Electrostatic drift & noise limit scalability<br>静电漂移和噪声限制可扩展性]
        C --> C1[Track transitions in CSDs for feedback<br>通过CSD中的跃迁线进行反馈跟踪]
        D --> D1[Stable operation on 10-QD device<br>在10-Q点器件上实现稳定操作]
        D --> D2[Noise spectroscopy & correlation analysis<br>噪声谱与相关性分析]
    ```
