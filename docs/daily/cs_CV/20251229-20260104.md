---
slug: /daily/cscv/20251229-20260104
---
# 20251229-20260104 (cs.CV)

## 2025-12-29

- **[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation**
  - **tags:** [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]
  - **authors:** Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani; GenimeLabs
  - **link:** https://arxiv.org/pdf/2512.21402
  - **contributions:** 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传统指标无法评估真实观众参与度/Traditional metrics fail to assess real viewer engagement]
        Method[主要方法/Method] --> M1[使用VLM提取无监督视听特征/Use VLM to extract unsupervised audiovisual features]
        Method --> M2[聚类特征并训练回归评估器/Cluster features and train regression evaluator]
        Results[关键结果/Results] --> R1[预测与真实参与度强相关/Strong correlation between predicted and actual engagement]
        Results --> R2[提供可解释且可扩展的评估/Provides interpretable and scalable assessment]
    ```

- **[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding**
  - **tags:** [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]
  - **authors:** Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli
  - **institution:** California Institute of Technology, Stanford University
  - **link:** https://arxiv.org/pdf/2512.21414
  - **code:** https://github.com/christinaliu2020/tool-bottleneck-framework
  - **contributions:** 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]
        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]
        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]
    ```

- **[arXiv251229] Scalable Deep Subspace Clustering Network**
  - **tags:** [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]
  - **authors:** Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami
  - **institution:** University of Quebec at Montreal
  - **link:** https://arxiv.org/pdf/2512.21434
  - **contributions:** 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Deep Subspace Clustering Network"] --> Problem["核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity"]
        Root --> Method["主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization"]
        Root --> Results["关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance"]
    ```

- **[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset**
  - **tags:** [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]
  - **authors:** Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh
  - **institution:** Simon Fraser University, AIP Labs
  - **link:** https://arxiv.org/pdf/2512.21472
  - **code:** /githubsfu-mial/IMAplusplus
  - **contributions:** 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"] --> Problem["核心问题/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets"]
        Root --> Method["主要方法/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata"]
        Root --> Results["关键结果/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research"]
    ```

- **[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism**
  - **tags:** [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]
  - **authors:** Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21452
  - **contributions:** 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
    
        Problem --> P1("Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效")
        Problem --> P2("Data scarcity <br/> 数据稀缺")
    
        Method --> M1("DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强")
        Method --> M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络")
        Method --> M3("MS COCO Transfer Learning <br/> MS COCO迁移学习")
    
        Results --> R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能")
        Results --> R2("Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒")
        Results --> R3("New paradigm for automated detection <br/> 自动化检测新范式")
    ```

- **[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection**
  - **tags:** [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]
  - **authors:** Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu
  - **institution:** Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC
  - **link:** https://arxiv.org/pdf/2512.21459
  - **code:** https://github.com/chloeqxq/CCAD
  - **contributions:** 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]
        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]
        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]
    ```

- **[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification**
  - **tags:** [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]
  - **authors:** Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian
  - **institution:** Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital
  - **link:** https://arxiv.org/pdf/2512.21476
  - **code:** https://github.com/JeremyXSC/GPF-Net
  - **contributions:** 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]
        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]
        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]
    ```

- **[arXiv251229] Generative Multi-Focus Image Fusion**
  - **tags:** [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]
  - **authors:** Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li
  - **institution:** Zhejiang University, Donghai Laboratory
  - **link:** https://arxiv.org/pdf/2512.21495
  - **code:** https://github.com/Xinzhe99/StackMFF-Series
  - **contributions:** 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Multi-Focus Image Fusion] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法假设总有对焦图像/Existing methods assume always an in-focus image]
        B --> B2[边缘伪影/Edge artifacts]
        C --> C1[阶段一: 确定性融合/Stage 1: Deterministic Fusion (StackMFF V4)]
        C --> C2[阶段二: 生成式恢复/Stage 2: Generative Restoration (IFControlNet)]
        D --> D1[SOTA性能/State-of-the-art performance]
        D --> D2[处理复杂多焦内容/Handles complex multi-focal content]
    ```

- **[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering**
  - **tags:** [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]
  - **authors:** Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu
  - **institution:** University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University
  - **link:** https://arxiv.org/pdf/2512.21510
  - **contributions:** 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --> Problem["核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]
        Root --> Method["主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]
        Root --> Results["关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness."]
    ```

- **[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art**
  - **tags:** [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]
  - **authors:** Md Ashik Khan, Arafat Alam Jion
  - **institution:** Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology
  - **link:** https://arxiv.org/pdf/2512.21512
  - **contributions:** 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection."]
        Method["主要方法/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion."]
        Results["关键结果/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance."]
    ```

- **[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning**
  - **tags:** [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]
  - **authors:** Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang
  - **institution:** Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21507
  - **code:** https://github.com/Gloria2tt/SVBench-Evaluation
  - **contributions:** 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SVBench: 视频生成模型的社会推理评估<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有视频生成模型缺乏社会推理能力<br>Current models lack social reasoning]
        C --> C1[构建基于社会心理学范式的基准<br>Build benchmark based on social psychology paradigms]
        C --> C2[使用基于智能体的训练免费流程进行评估<br>Use training-free agent-based pipeline for evaluation]
        D --> D1[模型在表面合理性上表现良好<br>Models perform well on surface-level plausibility]
        D --> D2[模型在社会推理维度上系统性失败<br>Models fail systematically on social reasoning dimensions]
    ```

- **[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification**
  - **tags:** [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]
  - **authors:** Md Ashik Khan, Md Nahid Siddique
  - **institution:** Indian Institute of Technology Kharagpur, Florida International University
  - **link:** https://arxiv.org/pdf/2512.21508
  - **contributions:** 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 多模态胸部X光分析的计算成本高，泛化性差 / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]
        C[主要方法/Method: 使用冻结编码器和参数高效训练策略 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]
        D[关键结果/Results: PET方法性能优于全微调，但校准性差 / PET methods outperform full fine-tuning but have degraded calibration]
    ```

- **[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO**
  - **tags:** [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]
  - **authors:** Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji
  - **institution:** Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21514
  - **contributions:** 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]
        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]
        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]
        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]
        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]
        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]
        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]
    ```

- **[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions**
  - **tags:** [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]
  - **authors:** Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu
  - **institution:** Fuzhou University, Research Institute of Highway, Ministry of Transport
  - **link:** https://arxiv.org/pdf/2512.21513
  - **code:** https://github.com/WangPuyun/MuS-Polar3D
  - **contributions:** 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(现有数据集缺乏多样性的散射和观测条件/Existing datasets lack diversity in scattering and observation conditions)
        C --> C1(构建包含42个物体、7种散射条件、5个视角的偏振图像基准数据集/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)
        C --> C2(提出解耦的两阶段成像流程：去散射后3D重建/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)
        D --> D1(实现最佳平均角度误差15.49度/Achieve best mean angular error of 15.49 degrees)
        D --> D2(首个公开的定量浑浊水下偏振3D成像基准数据集/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)
    ```

- **[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data**
  - **tags:** [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]
  - **authors:** Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu
  - **institution:** Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University
  - **link:** https://arxiv.org/pdf/2512.21516
  - **contributions:** 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]
        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]
        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]
        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]
        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]
        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]
        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]
    ```

- **[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models**
  - **tags:** [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]
  - **authors:** Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu
  - **institution:** University of Washington, Intel
  - **link:** https://arxiv.org/pdf/2512.21529
  - **contributions:** 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification")
        Problem --> P2("标准方法预测不一致/Standard methods produce inconsistent predictions")
        Method --> M1("提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework")
        Method --> M2("结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses")
        Method --> M3("集成轻量级LoRA适配/Integrate lightweight LoRA adaptation")
        Results --> R1("提升全路径精度/Improves Full-Path Accuracy")
        Results --> R2("降低不一致性错误/Reduces Tree-based Inconsistency Error")
        Results --> R3("参数开销最小/Minimal parameter overhead")
    ```

- **[arXiv251229] Vision Transformers are Circulant Attention Learners**
  - **tags:** [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]
  - **authors:** Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21542
  - **code:** https://github.com/LeapLabTHU/Circulant-Attention
  - **contributions:** 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Vision Transformers are Circulant Attention Learners] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Self-attention quadratic complexity O(N²) is computationally heavy for high-resolution vision tasks.]
        C[主要方法/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]
        D[关键结果/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]
    ```

- **[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration**
  - **tags:** [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]
  - **authors:** Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani
  - **link:** https://arxiv.org/pdf/2512.21560
  - **contributions:** 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Toward Intelligent Scene Augmentation<br>智能场景增强] --> B(Problem: Existing image editing lacks contextual appropriateness<br>核心问题: 现有图像编辑缺乏上下文合理性)
        A --> C(Method: Introduce two new tasks & build datasets<br>主要方法: 提出两个新任务并构建数据集)
        A --> D(Results: Identifies research gap for joint reasoning & generation<br>关键结果: 指出了联合推理与生成的研究空白)
        B --> E(Task 1: Context-aware object insertion<br>任务1: 上下文感知物体插入)
        B --> F(Task 2: Sponsor-product logo augmentation<br>任务2: 赞助商产品商标增强)
        C --> G(Utilize VLMs and diffusion models<br>利用视觉语言模型和扩散模型)
        C --> H(Build annotated datasets<br>构建带标注的数据集)
    ```

- **[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal**
  - **tags:** [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]
  - **authors:** Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim
  - **institution:** Seoul National University, OGQ
  - **link:** https://arxiv.org/pdf/2512.21545
  - **contributions:** 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]
        C[主要方法/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]
        D[关键结果/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]
    ```

- **[arXiv251229] Exploration of Reproducible Generated Image Detection**
  - **tags:** [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]
  - **authors:** Yihang Duan
  - **institution:** Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)
  - **link:** https://arxiv.org/pdf/2512.21562
  - **contributions:** 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploration of Reproducible Generated Image Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]
        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]
        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]
    ```

- **[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation**
  - **tags:** [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]
  - **authors:** Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li
  - **institution:** Beihang University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2512.21576
  - **contributions:** 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]
        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]
        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]
    ```

- **[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings**
  - **tags:** [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]
  - **authors:** Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura
  - **institution:** Keio University
  - **link:** https://arxiv.org/pdf/2512.21582
  - **contributions:** 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("LLM-Free Image Captioning Evaluation in Reference-Flexible Settings") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LLM-based metrics lack neutrality/LLM指标缺乏中立性")
        Problem --> P2("LLM-free metrics lack performance/无LLM指标性能不足")
        Method --> M1("Propose Pearl metric/提出Pearl指标")
        Method --> M2("Learn image-caption & caption-caption similarity/学习图像-描述与描述-描述相似性")
        Method --> M3("Construct large human-annotated dataset/构建大规模人工标注数据集")
        Results --> R1("Outperforms LLM-free metrics on benchmarks/在基准测试中超越其他无LLM指标")
        Results --> R2("Works in reference-based & reference-free settings/适用于有参考和无参考设置")
        Results --> R3("Fast & aligned with human judgment/快速且与人类判断一致")
    ```

- **[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation**
  - **tags:** [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]
  - **authors:** Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu
  - **institution:** Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.
  - **link:** https://arxiv.org/pdf/2512.21584
  - **code:** this https URL
  - **contributions:** 1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]
        C[主要方法/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]
        D[关键结果/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]
    ```

- **[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement**
  - **tags:** [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]
  - **authors:** Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou
  - **institution:** University of Electronic Science and Technology of China, Southwestern University of Finance and Economics
  - **link:** https://arxiv.org/pdf/2512.21598
  - **contributions:** 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]
        C[主要方法/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]
        D[关键结果/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]
    ```

- **[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians**
  - **tags:** [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]
  - **authors:** Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han
  - **institution:** Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)
  - **link:** https://arxiv.org/pdf/2512.21599
  - **contributions:** 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --> B(核心问题/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)
        A --> C(主要方法/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)
        A --> D(关键结果/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)
    ```

- **[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care**
  - **tags:** [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]
  - **authors:** Yusuf Brima, Marcellin Atemkeng
  - **institution:** Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)
  - **link:** https://arxiv.org/pdf/2512.21602
  - **contributions:** 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]
        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]
        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]
    ```

- **[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]
  - **authors:** Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou
  - **institution:** University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21616
  - **contributions:** 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TAMEing Long Contexts in Personalization<br>论文标题] --> B[Problem: Existing MLLM personalization lacks long-context support<br>核心问题: 现有MLLM个性化方法缺乏长上下文支持]
        A --> C[Method: Proposes TAME framework with double memory & RA2G<br>主要方法: 提出TAME框架, 包含双重记忆和RA2G范式]
        A --> D[Results: TAME achieves best performance on LCMP benchmark<br>关键结果: TAME在LCMP基准测试中取得最佳性能]
    ```

- **[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration**
  - **tags:** [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]
  - **authors:** Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang
  - **institution:** Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University
  - **link:** https://arxiv.org/pdf/2512.21618
  - **contributions:** 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp
  - **Simple LLM Summary:** SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --> B[核心问题/Problem: 现有方法难以同时实现高保真渲染和交互式交通编辑 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]
        A --> C[主要方法/Method: 提出对称自回归在线修复范式与免训练协调机制 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]
        A --> D[关键结果/Results: 在新视角增强和3D车辆插入中实现最先进性能 / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]
    ```

- **[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective**
  - **tags:** [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]
  - **authors:** Zhiwen Yang, Jinglin Xu, Yuxin Pen
  - **institution:** Peking University, University of Science and Technology Beijing
  - **link:** https://arxiv.org/pdf/2512.21617
  - **code:** https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM
  - **contributions:** 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]
        Root --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[Support set as confounder/支持集作为混淆变量]
        Problem --> P2[Biased data distribution/有偏数据分布]
        Problem --> P3[Spurious correlations/虚假关联]
        Method --> M1[Causal Intervention/因果干预]
        Method --> M2[IMSE: Sample-level/IMSE: 样本层面]
        Method --> M3[IMFR: Feature-level/IMFR: 特征层面]
        Results --> R1[SOTA Performance/最优性能]
        Results --> R2[Datasets: CUB, Dogs, Cars/数据集: CUB, Dogs, Cars]
    ```

- **[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints**
  - **tags:** [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]
  - **authors:** Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira
  - **institution:** Universitas Brawijaya
  - **link:** https://arxiv.org/pdf/2512.21637
  - **contributions:** 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --> B(核心问题/Problem: Attribute Entanglement in Text-Driven Image Editing)
        A --> C(主要方法/Method: Sparse Latent Constraints via L1 Regularization)
        A --> D(关键结果/Results: More Focused Edits, Reduced Unintended Changes)
    ```

- **[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding**
  - **tags:** [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]
  - **authors:** Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai
  - **institution:** Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA
  - **link:** https://arxiv.org/pdf/2512.21643
  - **code:** https://github.com/Zhouzone/OmniWeather
  - **contributions:** 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Omni-Weather: 统一多模态天气基础模型 / Unified Multimodal Foundation Model for Weather] --> B
        A --> C
        A --> D
        B[核心问题 / Problem: 天气建模中生成与理解任务分离 / Separation of generation and understanding in weather modeling]
        C[主要方法 / Method: 统一架构，共享自注意力，思维链数据集 / Unified architecture, shared self-attention, Chain-of-Thought dataset]
        D[关键结果 / Results: SOTA性能，任务互增强 / State-of-the-art performance, mutual enhancement of tasks]
    ```

- **[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References**
  - **tags:** [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]
  - **authors:** Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
  - **institution:** Zhejiang University, Fudan University, Huawei Technologies Ltd.
  - **link:** https://arxiv.org/pdf/2512.21641
  - **contributions:** 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]
        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]
        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]
        C1 --> C3[构建UniScene表示/Build UniScene Representation]
        C1 --> C4[利用运动历史推理/Reason with Motion History]
        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]
        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]
        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]
    ```

- **[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds**
  - **tags:** [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]
  - **authors:** Subramanyam Sahoo, Jared Junkin
  - **institution:** University of California, Berkeley, Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.21670
  - **code:** https://github.com/SubramanyamSahoo/The-Deepfake-Detective
  - **contributions:** 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the "black box" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]
        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]
        C --> C2[法证流形分析/Forensic Manifold Analysis]
        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]
        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]
    ```

- **[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles**
  - **tags:** [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]
  - **authors:** Jalal Khan
  - **institution:** United Arab Emirates University
  - **link:** https://arxiv.org/pdf/2512.21673
  - **contributions:** 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp
  - **Simple LLM Summary:** This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]
    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]
    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]
    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]
    ```

- **[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture**
  - **tags:** [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]
  - **authors:** Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu
  - **institution:** Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI
  - **link:** https://arxiv.org/pdf/2512.21675
  - **code:** https://github.com/thunderbolt215/UniPercept
  - **contributions:** 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["UniPercept: 统一感知级图像理解 / Unified Perceptual-Level Image Understanding"] --> Problem["MLLMs感知能力有限 / MLLMs' Perceptual Ability is Limited"]
        Root --> Method["提出统一基准与模型 / Proposes Unified Benchmark & Model"]
        Root --> Results["性能超越现有模型 / Outperforms Existing Models"]
        Problem --> P1["感知级特征理解不足 / Limited Perceptual-Level Feature Understanding"]
        Method --> M1["UniPercept-Bench基准 / UniPercept-Bench Benchmark"]
        Method --> M2["DAPT与Task-Aligned RL训练 / DAPT & Task-Aligned RL Training"]
        Results --> R1["在VR与VQA任务上泛化 / Generalizes on VR & VQA Tasks"]
        Results --> R2["可作为图像生成奖励模型 / Serves as Reward Model for Generation"]
    ```

- **[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration**
  - **tags:** [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]
  - **authors:** Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang
  - **institution:** Rensselaer Polytechnic Institute
  - **link:** https://arxiv.org/pdf/2512.21684
  - **contributions:** 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --> B[核心问题/Problem: VLM语义输出难以验证、复现和审计，存在不一致性 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]
        A --> C[主要方法/Method: 基于区块链的溯源框架，从幻灯片中提取概念和关系三元组，哈希上链 / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]
        A --> D[关键结果/Results: 揭示模型间显著差异，实现完美篡改检测和确定性复现，提供可扩展的完整性 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]
    ```

- **[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation**
  - **tags:** [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]
  - **authors:** Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao
  - **institution:** Nanjing University of Science and Technology, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21683
  - **code:** https://github.com/primebo1/C-Graph
  - **contributions:** 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法过滤域特定信息，损害性能/Existing methods filter domain-specific info, harming performance]
        C --> C1[利用结构一致性作为先验/Use structural consistency as prior]
        C --> C2[图建模: SPG层, SMD解码, CNC损失/Graph Modeling: SPG layer, SMD decoding, CNC loss]
        D --> D1[跨域性能SOTA/Cross-domain SOTA performance]
        D --> D2[保持源域精度/Preserves source-domain accuracy]
    ```

- **[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective**
  - **tags:** [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]
  - **authors:** Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu
  - **institution:** Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21691
  - **contributions:** 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[VGGT中的注意力崩溃现象/Attention Collapse in VGGT]
        C --> C1[将注意力迭代建模为退化扩散过程/Model Attention Iteration as Degenerate Diffusion]
        D --> D1[推导出预测崩溃的均值场PDE/Derive Mean-Field PDE Predicting Collapse]
        D --> D2[理论解释令牌合并的缓解作用/Theory Explains Token-Merging Remedy]
    ```

- **[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields**
  - **tags:** [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]
  - **authors:** Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch
  - **institution:** Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC
  - **link:** https://arxiv.org/pdf/2512.21692
  - **contributions:** 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法难以建模各向异性高光/Existing methods struggle with anisotropic specular reflections]
        C --> C1[提出ShinyNeRF框架/Propose ShinyNeRF framework]
        C1 --> C2[使用各向同性vMF混合近似出射辐射度/Use isotropic vMF mixture to approximate outgoing radiance]
        C2 --> C3[联合估计法线、切线、高光参数/Jointly estimate normals, tangents, specular parameters]
        D --> D1[实现SOTA性能/Achieves SOTA performance]
        D --> D2[提供物理解释和材质编辑/Provides physical interpretation and material editing]
    ```

- **[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating**
  - **tags:** [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]
  - **authors:** Li Yang, Yuting Liu
  - **institution:** Wannan Medical College
  - **link:** https://arxiv.org/pdf/2512.21693
  - **contributions:** 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Prior-AttUNet: 视网膜OCT液体分割 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[模糊边界与设备差异 / Ambiguous Boundaries & Device Heterogeneity]
        C --> C1[双路径架构 / Dual-Path Architecture]
        C --> C2[变分自编码器先验 / VAE Priors]
        C --> C3[三重注意力机制 / Triple-Attention Mechanism]
        D --> D1[高Dice分数 / High Dice Scores]
        D --> D2[低计算成本 / Low Computational Cost]
    ```

- **[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks**
  - **tags:** [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]
  - **authors:** Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu
  - **institution:** Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.
  - **link:** https://arxiv.org/pdf/2512.21694
  - **contributions:** 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]
        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]
        C --> C2[Use self-collected dataset<br/>使用自收集数据集]
        C --> C3[Pre-process images<br/>预处理图像]
        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]
        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]
    ```

- **[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection**
  - **tags:** [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]
  - **authors:** Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam
  - **institution:** Not explicitly stated in provided content. Affiliation/email domain not present.
  - **link:** https://arxiv.org/pdf/2512.21695
  - **contributions:** 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"] --> Problem["核心问题/Problem: Reliable detection of AI-generated images"]
        Root --> Method["主要方法/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training"]
        Root --> Results["关键结果/Results: SOTA on Chameleon, strong generalization across datasets"]
    ```

- **[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction**
  - **tags:** [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]
  - **authors:** Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang
  - **institution:** Nanjing University of Science and Technology, Nanjing Forestry University
  - **link:** https://arxiv.org/pdf/2512.21707
  - **code:** https://github.com/alanyz106/ST-MoE
  - **contributions:** 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("不灵活的时空表示/Inflexible spatiotemporal representation")
        Problem --> P2("高计算成本/High computational cost")
        Method --> M1("提出ST-MoE模型/Propose ST-MoE model")
        M1 --> M2("四种时空专家/Four spatiotemporal experts")
        M2 --> M3("双向时空Mamba/Bidirectional spatiotemporal Mamba")
        Results --> R1("精度超越SOTA/Outperforms SOTA in accuracy")
        Results --> R2("参数减少41.38%/Reduces parameters by 41.38%")
        Results --> R3("训练加速3.6倍/3.6x training speedup")
    ```

- **[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention**
  - **tags:** [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]
  - **authors:** Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang
  - **institution:** Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21710
  - **code:** https://github.com/Thelegendzz/RAPTOR
  - **contributions:** 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RAPTOR: 实时高分辨率无人机视频预测<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>视频预测的"三难困境": 速度、分辨率、质量<br>Video Prediction Trilemma: Speed, Resolution, Quality]
        C[主要方法/Method<br>高效视频注意力 (EVA) 模块<br>Efficient Video Attention (EVA) Module]
        D[关键结果/Results<br>首个在Jetson上512^2视频>30 FPS<br>First >30 FPS for 512^2 video on Jetson]
    ```

- **[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency**
  - **tags:** [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]
  - **authors:** Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang
  - **institution:** Amap Alibaba, Peking University (PKU), Tsinghua University (THU)
  - **link:** https://arxiv.org/pdf/2512.21714
  - **code:** https://astra-amap.github.io/AstraNav-World.github.io/
  - **contributions:** 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AstraNav-World] --> B[核心问题/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]
        A --> C[主要方法/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]
        A --> D[关键结果/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]
    ```

- **[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation**
  - **tags:** [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]
  - **authors:** Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21734
  - **code:** https://humanaigc.github.io/knot_forcing_demo_page/
  - **contributions:** 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate to maintain long-term coherence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: 驯服自回归视频扩散模型"] --> B["核心问题/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>实时肖像动画需要低延迟和一致性，但自回归模型存在误差累积和运动不连续问题。"]
        A --> C["主要方法/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and 'Running Ahead' mechanism.<br>分块生成与KV缓存，用于平滑过渡的时序结模块，以及'超前运行'机制。"]
        A --> D["关键结果/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>在消费级GPU上实现高保真、无限、交互式的实时肖像动画。"]
    ```

- **[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild**
  - **tags:** [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]
  - **authors:** Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.21736
  - **code:** https://humanaigc.github.io/sync_anyone_demo_page/
  - **contributions:** 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SyncAnyone] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["现有方法破坏时空上下文 / Existing methods disrupt spatiotemporal context"]
        C --> C1["阶段1: 基于掩码的扩散变换器训练 / Stage 1: Mask-based DiT training"]
        C --> C2["阶段2: 无掩码调优与自校正 / Stage 2: Mask-free tuning & self-correction"]
        D --> D1["SOTA视觉质量与时间一致性 / SOTA visual quality & temporal coherence"]
        D --> D2["更好的身份与背景保持 / Better identity & background preservation"]
    ```

- **[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning**
  - **tags:** [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]
  - **authors:** Hengyi Wu, Zhenyi Wang, Heng Huang
  - **institution:** University of Maryland, College Park, University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.21743
  - **contributions:** 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]
        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]
        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]
        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]
        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]
        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]
    ```

- **[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG**
  - **tags:** [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]
  - **authors:** Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy
  - **institution:** Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad
  - **link:** https://arxiv.org/pdf/2512.21747
  - **contributions:** 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --> B(核心问题/Problem: Driver drowsiness detection for road safety)
        A --> C(主要方法/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)
        A --> D(关键结果/Results: Improved stability on SEED-VIG, SOTA on STEW)
    ```

- **[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets**
  - **tags:** [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]
  - **authors:** Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R
  - **institution:** Anna University, National Institute of Technology Tiruchirappalli
  - **link:** https://arxiv.org/pdf/2512.21760
  - **contributions:** 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]
        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]
        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]
    ```

- **[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization**
  - **tags:** [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]
  - **authors:** Evgeny Alves Limarenko, Anastasiia Studenikina
  - **institution:** Moscow Institute of Physics and Technology
  - **link:** https://arxiv.org/pdf/2512.21769
  - **contributions:** 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("BertsWin: 3D MAE优化") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE")
        Problem --> P2("破坏空间关系/Destroys Spatial Context")
        Method --> M1("BertsWin混合架构/BertsWin Hybrid Architecture")
        Method --> M2("完整3D令牌网格/Full 3D Token Grid")
        Method --> M3("Swin窗口 & 结构损失/Swin Windows & Structural Loss")
        Results --> R1("5.8x语义收敛加速/5.8x Faster Convergence")
        Results --> R2("15倍训练轮次减少/15x Fewer Epochs")
        Results --> R3("FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction")
    ```

- **[arXiv251229] Inference-based GAN Video Generation**
  - **tags:** [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]
  - **authors:** Jingbo Yang, Adrian G. Bors
  - **institution:** University of York
  - **link:** https://arxiv.org/pdf/2512.21776
  - **contributions:** 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Inference-based GAN Video Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]
        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]
        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]
        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]
        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]
        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]
        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]
    ```

- **[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models**
  - **tags:** [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]
  - **authors:** Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky
  - **institution:** Ben-Gurion University, Amazon Prime Video, Tel-Aviv University
  - **link:** https://arxiv.org/pdf/2512.21778
  - **contributions:** 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Scene-VLM: Multimodal Video Scene Segmentation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("现有方法限制/Limitations of existing methods")
        P1 --> P1_1("视觉中心偏见/Visual-centric bias")
        P1 --> P1_2("孤立分类/Isolated shot classification")
        P1 --> P1_3("缺乏可解释性/Lack explainability")
        Method --> M1("微调VLM框架/Fine-tuned VLM framework")
        M1 --> M1_1("多模态推理/Multimodal reasoning")
        M1 --> M1_2("序列预测/Sequential prediction")
        M1 --> M1_3("上下文聚焦窗口/Context-focus window")
        Method --> M2("置信度提取/Confidence score extraction")
        Method --> M3("生成解释/Generate rationales")
        Results --> R1("SOTA性能/State-of-the-art performance")
        Results --> R2("显著提升/Significant improvement on MovieNet")
    ```

- **[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning**
  - **tags:** [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]
  - **authors:** Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles
  - **institution:** The Pennsylvania State University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21789
  - **contributions:** 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]
        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]
        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]
        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]
        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]
        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]
        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]
    ```

- **[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation**
  - **tags:** [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]
  - **authors:** Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan
  - **institution:** ByteDance Inc., Rutgers University
  - **link:** https://arxiv.org/pdf/2512.21788
  - **code:** https://github.com/yanq095/InstructMoLE
  - **contributions:** 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]
        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]
        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]
    ```

- **[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge**
  - **tags:** [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]
  - **authors:** Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane
  - **institution:** University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds
  - **link:** https://arxiv.org/pdf/2512.21792
  - **contributions:** 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp
  - **Simple LLM Summary:** This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI用于组织病理学图像中的足菌肿诊断：MICCAI 2024挑战赛") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("足菌肿诊断困难，尤其在资源匮乏地区<br>Mycetoma diagnosis is challenging, especially in low-resource settings")
        Method --> M1("组织mAIcetoma挑战赛，开发AI模型<br>Organized mAIcetoma challenge to develop AI models")
        Method --> M2("提供MyData数据集，用于分割和分类<br>Provided MyData dataset for segmentation and classification")
        Results --> R1("模型实现高精度分割<br>Models achieved high segmentation accuracy")
        Results --> R2("顶级模型分类性能显著<br>Top models showed significant classification performance")
    ```

- **[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise**
  - **tags:** [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]
  - **authors:** Abu Hanif Muhammad Syarubany
  - **institution:** Korea Advanced Institute of Science & Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2512.21797
  - **contributions:** 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Diffusion Posterior Sampling for Super-Resolution<br>扩散后验采样用于超分辨率"] --> Problem["核心问题/Problem<br>Single-image super-resolution under Gaussian noise<br>高斯噪声下的单图像超分辨率"]
        Root --> Method["主要方法/Method<br>Diffusion Posterior Sampling (DPS)<br>扩散后验采样<br>Unconditional prior + likelihood-guided conditioning<br>无条件先验 + 似然引导的条件化"]
        Root --> Results["关键结果/Results<br>Optimal PS scale 0.95, noise σ=0.01<br>最佳PS尺度0.95, 噪声σ=0.01<br>Balancing prior and data yields sharp details<br>平衡先验与数据得到清晰细节"]
    ```

- **[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection**
  - **tags:** [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]
  - **authors:** Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu
  - **institution:** University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21803
  - **contributions:** 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]
        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]
        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]
    ```

- **[arXiv251229] S&P 500 Stock's Movement Prediction using CNN**
  - **tags:** [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]
  - **authors:** Rahul Gupta
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.21804
  - **contributions:** 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势"] --> P1["传统方法依赖特征工程<br>Traditional methods rely on engineered features"]
        Problem --> P2["现有研究多使用单维数据<br>Existing research often uses single-dimension data"]
        Method["主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN"] --> M1["将历史数据矩阵视为图像<br>Treat historical data matrices as images"]
        Method --> M2["包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)"]
        Results["关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果"] --> R1["支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction"]
    ```

- **[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models**
  - **tags:** [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]
  - **authors:** Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang
  - **institution:** Australian National University, The University of Queensland, GE Research
  - **link:** https://arxiv.org/pdf/2512.21815
  - **contributions:** 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp
  - **Simple LLM Summary:** This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]
        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]
        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]
        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]
        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]
        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]
        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]
    ```

- **[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration**
  - **tags:** [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]
  - **authors:** Zhenwei Yang, Yibo Ai, Weidong Zhang
  - **institution:** University of Science and Technology Beijing, National Center for Materials Service Safety
  - **link:** https://arxiv.org/pdf/2512.21831
  - **contributions:** 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[感知挑战:遮挡,视角限制,通信延迟/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]
    C --> C1[提出XET-V2X框架/Proposes XET-V2X Framework]
    C1 --> C2[双层级空间交叉注意力模块/Dual-layer Spatial Cross-Attention Module]
    C2 --> C3[多视图图像特征聚合/Multi-view Image Feature Aggregation]
    C2 --> C4[点云融合/Point Cloud Fusion]
    D --> D1[在V2X-Seq-SPD等数据集上性能提升/Performance Improvements on V2X-Seq-SPD, etc.]
    D1 --> D2[检测与跟踪性能增强/Enhanced Detection & Tracking Performance]
    D2 --> D3[鲁棒且时序稳定的感知/Robust & Temporally Stable Perception]
    ```

- **[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse**
  - **tags:** [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]
  - **authors:** Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen
  - **institution:** Not explicitly stated in the provided content. Affiliation information is not included.
  - **link:** https://arxiv.org/pdf/2512.21845
  - **code:** this https URLETF2
  - **contributions:** 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题 / Problem"] --> P1["过拟合新数据 / Overfitting to new data"]
        Problem --> P2["灾难性遗忘旧数据 / Catastrophic forgetting of old data"]
        Problem --> P3["特征差异与类别错位 / Feature difference & Class misalignment"]
    
        Method["主要方法 / Method"] --> M1["SCL-PNC方法 / SCL-PNC Method"]
        M1 --> M1_1["自适应层扩展主干 / Adapt-layer for backbone expansion"]
        M1 --> M1_2["动态参数化ETF分类器 / Dynamic Parametric ETF Classifier"]
        M1 --> M1_3["并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation"]
    
        Results["关键结果 / Results"] --> R1["高效处理类别增长 / Efficiently handles increasing categories"]
        Results --> R2["解决类别错位 / Addresses class misalignment"]
        Results --> R3["确保特征一致性 / Ensures feature consistency"]
    ```

- **[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection**
  - **tags:** [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]
  - **authors:** Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li
  - **institution:** Dalian Minzu University, Shandong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21856
  - **code:** https://github.com/HTUTU2/TPS-SCL
  - **contributions:** 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Breaking Alignment Barriers: TPS-SCL<br>突破对齐壁垒: TPS-SCL] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法依赖对齐数据集<br>Existing methods rely on aligned datasets]
        B --> B2[真实场景图像未对齐<br>Real-world images are unaligned]
        C --> C1[双流MobileViT编码器<br>Dual-stream MobileViT encoder]
        C --> C2[TPS对齐模块<br>TPS Alignment Module]
        C --> C3[语义关联约束模块<br>Semantic Correlation Constraint Module]
        C --> C4[跨模态关联模块<br>Cross-Modal Correlation Module]
        D --> D1[轻量级SOTA性能<br>Lightweight SOTA performance]
        D --> D2[超越主流RGB-T方法<br>Outperforms mainstream RGB-T methods]
    ```

- **[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models**
  - **tags:** [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]
  - **authors:** Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue
  - **institution:** CyberAgent
  - **link:** https://arxiv.org/pdf/2512.21860
  - **code:** https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings
  - **contributions:** 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 全局图像嵌入无法聚焦于特定文本条件]
        C[主要方法/Method: DIOR - 利用LVLM生成单字描述并提取最后token的隐藏状态]
        D[关键结果/Results: 在条件图像相似性任务上超越现有方法]
    ```

- **[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees**
  - **tags:** [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]
  - **authors:** Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou
  - **institution:** Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University
  - **link:** https://arxiv.org/pdf/2512.21857
  - **code:** https://github.com/Haodong-Lei-Ray/ADT-Tree
  - **contributions:** 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Fast Inference of Visual AR Model with ADT-Tree<br>视觉自回归模型快速推理与ADT-Tree"]
        Root --> Problem["核心问题/Problem<br>Visual AR models have slow sequential inference.<br>视觉AR模型推理慢"]
        Root --> Method["主要方法/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>提出邻接自适应动态草稿树"]
        Root --> Results["关键结果/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>在基准测试上实现3.13x/3.05x加速"]
        Problem --> P1["Spatially varying token prediction difficulty.<br>空间变化的token预测难度"]
        Method --> M1["Dynamically adjusts tree depth & width.<br>动态调整树深度与宽度"]
        Method --> M2["Leverages adjacency & prior acceptance rates.<br>利用邻接关系和先验接受率"]
        Results --> R1["Integrates with relaxed sampling.<br>可与松弛采样方法结合"]
    ```

- **[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening**
  - **tags:** [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]
  - **authors:** Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan
  - **institution:** North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh
  - **link:** https://arxiv.org/pdf/2512.21861
  - **contributions:** 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]
        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]
        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]
    ```

- **[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition**
  - **tags:** [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]
  - **authors:** Yihan Hu, Xuelin Chen, Xiaodong Cun
  - **institution:** Great Bay University, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21865
  - **code:** https://github.com/GVCLab/EasyOmnimatte
  - **contributions:** 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EasyOmnimatte] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法慢且次优/Existing methods are slow and suboptimal]
        C --> C1[双专家微调/Dual-Expert Finetuning]
        C1 --> C2[效果专家/Effect Expert]
        C1 --> C3[质量专家/Quality Expert]
        D --> D1[高质量分解/High-quality decomposition]
        D --> D2[高效快速/Efficient and fast]
    ```

- **[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation**
  - **tags:** [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]
  - **authors:** Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball
  - **institution:** University of California, San Diego, Dolby Laboratories
  - **link:** https://arxiv.org/pdf/2512.21867
  - **contributions:** 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]
        Method[主要方法/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]
        Results[关键结果/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]
    ```

- **[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer**
  - **tags:** [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]
  - **authors:** Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang
  - **institution:** Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University
  - **link:** https://arxiv.org/pdf/2512.21883
  - **code:** https://github.com/dtc111111/Reloc-VGGT
  - **contributions:** 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --> B[核心问题/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]
        A --> C[主要方法/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]
        A --> D[关键结果/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]
    ```

- **[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis**
  - **tags:** [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]
  - **authors:** Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen
  - **institution:** Southern University of Science and Technology, University of Warwick, Fudan University
  - **link:** https://arxiv.org/pdf/2512.21881
  - **contributions:** 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法瓶颈/Bottlenecks of Existing Methods]
        B1 --> B1_1[图谱方法: 丢失细节, 需大数据/Atlas-based: lose details, need big data]
        B1 --> B1_2[无图谱方法: 内存计算成本高/Atlas-free: high memory & compute cost]
        C --> C1[两阶段自适应设计/Two-stage Adaptive Design]
        C1 --> C1_1[轻量时序提取器: 全局上下文与显著性排序/Lightweight Temporal Extractor: global context & saliency ranking]
        C1 --> C1_2[4D分层编码器: 从Top-k窗口学习/4D Hierarchical Encoder: learn from top-k windows]
        D --> D1[性能/Performance]
        D --> D2[效率/Efficiency]
        D1 --> D1_1[在七个基准上达到SOTA/Achieves SOTA on seven benchmarks]
        D2 --> D2_1[仅需4千次预训练会话/Only 4k pre-training sessions]
        D2 --> D2_2[GPU内存降至30%/GPU memory reduced to ~30%]
    ```

- **[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model**
  - **tags:** [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]
  - **authors:** Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen
  - **institution:** Hong Kong University of Science and Technology, University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.21890
  - **contributions:** 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp
  - **Simple LLM Summary:** This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CrownGen: Patient-customized Crown Generation<br>基于点扩散模型的个性化牙冠生成] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Digital crown design is labor-intensive<br>数字化牙冠设计费时费力]
        C --> C1[Uses a point diffusion model<br>使用点扩散模型]
        C --> C2[Has a boundary prediction module<br>包含边界预测模块]
        D --> D1[Surpasses SOTA in geometric fidelity<br>几何保真度超越现有方法]
        D --> D2[Reduces active design time<br>减少主动设计时间]
        D --> D3[Crowns are clinically non-inferior<br>临床质量不劣于人工设计]
    ```

- **[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer**
  - **tags:** [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]
  - **authors:** Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang
  - **institution:** Guangzhou Quwan Network Technology, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.21905
  - **contributions:** 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>基于扩散Transformer的高保真长时人体图像动画"] --> B["核心问题/Problem"]
        A --> C["主要方法/Method"]
        A --> D["关键结果/Results"]
        B --> B1["长视频生成挑战<br>Long-duration Video Generation"]
        B --> B2["面部与手部细节合成不足<br>Lack of Fine-grained Facial/Hand Details"]
        C --> C1["混合隐式引导信号<br>Hybrid Implicit Guidance"]
        C --> C2["位置偏移自适应模块<br>Position Shift Adaptive Module"]
        C --> C3["数据增强与骨架对齐<br>Data Augmentation & Skeleton Alignment"]
        D --> D1["超越现有SOTA方法<br>Outperforms SOTA"]
        D --> D2["实现超1分钟动画<br>Exceeds 1-minute Animation"]
    ```

- **[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition**
  - **tags:** [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]
  - **authors:** Zeyu Liang, Hailun Xia, Naichuan Zheng
  - **institution:** Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.21916
  - **contributions:** 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[RGB与骨架模态异构融合困难/RGB-Skeleton Heterogeneous Fusion Difficulty]
        C --> C1[以人为中心的图表示学习/Human-Centric Graph Representation Learning]
        C1 --> C2[基于注意力的后校准/Attention-Based Post Calibration]
        C1 --> C3[双变体: PAN-Ensemble与PAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]
        D --> D1[三个数据集上SOTA性能/SOTA Performance on Three Datasets]
    ```

- **[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning**
  - **tags:** [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]
  - **authors:** Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.21924
  - **contributions:** 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[泛化性差与异常残留/Generalizability & Residuals]
        C --> C1[解耦表示模块/Disentangled Representation Module]
        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]
        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]
    ```

- **[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization**
  - **tags:** [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]
  - **authors:** Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law
  - **institution:** JD.COM
  - **link:** https://arxiv.org/pdf/2512.21921
  - **code:** https://github.com/JD-GenX/AutoPP
  - **contributions:** 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]
        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]
        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]
        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]
        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]
        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]
        D --> D2[代码与数据集公开/Code & dataset released]
    ```

- **[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement**
  - **tags:** [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]
  - **authors:** Yiquan Gao, John See
  - **institution:** Heriot-Watt University
  - **link:** https://arxiv.org/pdf/2512.21944
  - **contributions:** 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")
        Method("主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")
        Results("关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")
    ```

- **[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials**
  - **tags:** [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]
  - **authors:** Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe
  - **institution:** University of Saskatchewan, New Mexico State University
  - **link:** https://arxiv.org/pdf/2512.21948
  - **contributions:** 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[自动化发现用于植被分类的紧凑光谱指数/Automated discovery of compact spectral indices for vegetation classification]
        C --> C1[生成归一化差异多项式候选特征/Generate candidate features via normalized difference polynomials]
        C --> C2[使用特征选择方法挑选指数/Use feature selection methods to pick indices]
        D --> D1[单个二阶指数达到96.26%准确率/Single degree-2 index achieves 96.26% accuracy]
        D --> D2[指数简单，可直接部署于GEE/Indices are simple and deployable on GEE]
    ```

- **[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models**
  - **tags:** [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]
  - **authors:** Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng
  - **institution:** The Chinese University of Hong Kong (CUHK)
  - **link:** https://arxiv.org/pdf/2512.21964
  - **contributions:** 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --> B(核心问题/Problem: MLLMs are sensitive to input noise, undermining clinical use)
        A --> C(主要方法/Method: Training-free IMC framework with PDC for vision and SMS for text)
        A --> D(关键结果/Results: SOTA performance on new multi-modal noise benchmark)
    ```

- **[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models**
  - **tags:** [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]
  - **authors:** Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner
  - **institution:** Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)
  - **link:** https://arxiv.org/pdf/2512.21985
  - **contributions:** 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]
        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]
        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]
        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]
        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]
        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]
    ```

- **[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation**
  - **tags:** [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]
  - **authors:** Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang
  - **institution:** The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.
  - **link:** https://arxiv.org/pdf/2512.21984
  - **code:** https://github.com/hhwmortal/PELD-Instance-segmentation
  - **contributions:** 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[狭窄视野, 伪影, 硬件限制/Narrow FOV, Artifacts, Hardware Constraints]
        C --> C1[轻量级多尺度注意力框架/Lightweight Multi-scale Attention Framework]
        C1 --> C2[主干: C2f-Pro (重参数化卷积+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]
        C1 --> C3[颈部: SSFF与TFE/Neck: SSFF and TFE]
        C1 --> C4[头部: 轻量共享头 (LMSH)/Head: Lightweight Shared Head (LMSH)]
        D --> D1[性能优越, 参数量少 (1.8M)/High Performance, Few Params (1.8M)]
        D --> D2[发布PELD数据集/Release PELD Dataset]
        D --> D3[良好泛化性/Good Generalization]
    ```

- **[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs**
  - **tags:** [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]
  - **authors:** Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He
  - **institution:** Chongqing University, Xinjiang University
  - **link:** https://arxiv.org/pdf/2512.21999
  - **code:** https://github.com/hujiayu1223/ALEAHallu
  - **contributions:** 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]
        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]
        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]
        C --> C1[激活数据集/Activation Dataset]
        C --> C2[定位关键参数/Locate Critical Parameters]
        C --> C3[对抗性编辑/Adversarial Editing]
    ```

- **[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception**
  - **tags:** [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]
  - **authors:** Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian
  - **institution:** Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad
  - **link:** https://arxiv.org/pdf/2512.22009
  - **contributions:** 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision "slow mode" and an efficient "fast mode" based on task demands. 3. Employs special perception tokens to dynamically guide the model's visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --> B[核心问题/Problem: Building efficient and precise GUI agents is challenging]
        A --> C[主要方法/Method: Slow-fast hybrid inference with adaptive perception tokens]
        A --> D[关键结果/Results: Matches SOTA performance with compact 2.5B size]
    ```

- **[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration**
  - **tags:** [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]
  - **authors:** Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji
  - **institution:** Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University
  - **link:** https://arxiv.org/pdf/2512.22010
  - **contributions:** 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]
        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]
        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]
    ```

- **[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching**
  - **tags:** [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]
  - **authors:** Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu
  - **institution:** Beijing Technology and Business University, The University of Sydney
  - **link:** https://arxiv.org/pdf/2512.22016
  - **contributions:** 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --> B[核心问题/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]
        A --> C[主要方法/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]
        A --> D[关键结果/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]
    ```

- **[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection**
  - **tags:** [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]
  - **authors:** Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma
  - **institution:** Shanghai Jiao Tong University, Chinese Academy of Military Science
  - **link:** https://arxiv.org/pdf/2512.22027
  - **code:** https://gendf.github.io/
  - **contributions:** 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Patch-Discontinuity Mining for Generalized Deepfake Detection"] --> B["核心问题/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns."]
        A --> C["主要方法/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model."]
        A --> D["关键结果/Results: Achieves SOTA generalization with only 0.28M parameters."]
    ```

- **[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models**
  - **tags:** [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]
  - **authors:** Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang
  - **institution:** Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University
  - **link:** https://arxiv.org/pdf/2512.22046
  - **contributions:** 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]
        C[主要方法/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]
        D[关键结果/Results: High ASR, preserves clean performance, defenses ineffective]
    ```

- **[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars**
  - **tags:** [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]
  - **authors:** Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu
  - **institution:** Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University
  - **link:** https://arxiv.org/pdf/2512.22065
  - **code:** https://streamavatar.github.io
  - **contributions:** 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]
        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]
        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]
    ```

- **[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents**
  - **tags:** [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]
  - **authors:** Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi
  - **institution:** Tongyi Lab, Alibaba Group
  - **link:** https://arxiv.org/pdf/2512.22047
  - **code:** https://github.com/Tongyi-MAI/MAI-UI
  - **contributions:** 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp
  - **Simple LLM Summary:** The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --> B1(核心问题/Problem)
        A --> B2(主要方法/Method)
        A --> B3(关键结果/Results)
        B1 --> C1(缺乏原生人机交互/Lack of Native Agent-User Interaction)
        B1 --> C2(仅UI操作的限制/Limits of UI-Only Operation)
        B1 --> C3(缺乏实用部署架构/Absence of Practical Deployment Architecture)
        B1 --> C4(动态环境脆弱性/Brittleness in Dynamic Environments)
        B2 --> D1(自演进数据管道/Self-Evolving Data Pipeline)
        B2 --> D2(原生设备-云协作系统/Native Device-Cloud Collaboration System)
        B2 --> D3(在线RL框架/Online RL Framework)
        D1 --> E1(包含用户交互/Includes User Interaction)
        D1 --> E2(包含MCP工具调用/Includes MCP Tool Calls)
        D3 --> E3(扩展并行环境/Scales Parallel Environments)
        D3 --> E4(扩展上下文长度/Scales Context Length)
        B3 --> F1(GUI Grounding SOTA/GUI Grounding SOTA)
        B3 --> F2(移动导航SOTA/Mobile Navigation SOTA)
        B3 --> F3(系统性能提升/System Performance Gains)
        F1 --> G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)
        F1 --> G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)
        F2 --> G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)
        F2 --> G4(MobileWorld: 41.7%/MobileWorld: 41.7%)
        F3 --> G5(设备性能提升33%/On-Device Perf. +33%)
        F3 --> G6(云调用减少40%/Cloud Calls -40%)
    ```

- **[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model**
  - **tags:** [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]
  - **authors:** Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang
  - **institution:** Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22096
  - **code:** https://github.com/stdstu12/YUME
  - **contributions:** 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1[大模型参数与慢推理/Large Model & Slow Inference]
        Problem --> P2[缺乏文本控制/Lack of Text Control]
        Method --> M1[长视频生成框架/Long-Video Gen Framework]
        Method --> M2[实时流加速策略/Real-time Streaming]
        Method --> M3[文本控制事件生成/Text-Controlled Events]
        M1 --> M1_Sub[统一上下文压缩与线性注意力/Unified Context Compression & Linear Attention]
        M2 --> M2_Sub[双向注意力蒸馏与文本嵌入/Bidirectional Attention Distillation & Text Embedding]
        Results --> R1[生成交互式世界/Generates Interactive Worlds]
        Results --> R2[支持键盘探索/Supports Keyboard Exploration]
        Results --> R3[公开代码库/Public Codebase]
    ```

- **[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking**
  - **tags:** [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]
  - **authors:** Momir Adžemović
  - **institution:** University of Belgrade
  - **link:** https://arxiv.org/pdf/2512.22105
  - **code:** https://github.com/Robotmurlock/TDLP
  - **contributions:** 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]
        C[主要方法/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]
        D[关键结果/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]
    ```

- **[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning**
  - **tags:** [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]
  - **authors:** Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang
  - **institution:** Microsoft Research, Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.22120
  - **code:** https://github.com/zss02/BiPS
  - **contributions:** 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional "where-to-look" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]
        C[主要方法/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]
        D[关键结果/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]
    ```

- **[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right**
  - **tags:** [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]
  - **authors:** Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng
  - **institution:** Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2512.22118
  - **code:** https://isee-laboratory.github.io/ProEdit/
  - **contributions:** 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ProEdit: Inversion-based Editing From Prompts Done Right] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法过度注入源图像信息/Existing methods overly inject source info]
        B1 --> B2[阻碍属性编辑如姿态、数量、颜色/Hinders editing attributes like pose, number, color]
        C --> C1[注意力层面: KV-mix/Attention Aspect: KV-mix]
        C1 --> C11[混合源与目标KV特征/Mix source & target KV features]
        C --> C2[潜在层面: Latents-Shift/Latent Aspect: Latents-Shift]
        C2 --> C21[扰动编辑区域的源潜在表示/Perturb source latent in edited region]
        D --> D1[SOTA性能/SOTA performance]
        D --> D2[即插即用设计/Plug-and-play design]
    ```

- **[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI**
  - **tags:** [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]
  - **authors:** Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud
  - **institution:** Daffodil International University
  - **link:** https://arxiv.org/pdf/2512.21372
  - **contributions:** 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --> B(核心问题 / Problem: 胃肠道疾病图像分类挑战 / GI Disease Image Classification Challenge)
        A --> C(主要方法 / Method: 基于知识蒸馏的双流Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)
        A --> D(关键结果 / Results: 高准确率与可解释性 / High Accuracy & Explainability)
        B --> B1[数据量大, 类间差异小 / Large Data Volume, Subtle Inter-class Variation]
        C --> C1[教师模型: Swin + ViT / Teacher: Swin + ViT]
        C --> C2[学生模型: Tiny-ViT / Student: Tiny-ViT]
        C --> C3[可解释性分析: Grad-CAM等 / XAI: Grad-CAM etc.]
        D --> D1[准确率 > 0.99 / Accuracy > 0.99]
        D --> D2[AUC = 1.0000]
        D --> D3[适用于临床环境 / Suitable for Clinical Settings]
    ```

- **[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models**
  - **tags:** [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]
  - **authors:** Takuro Kutsuna
  - **institution:** Toyota Central R&D Labs., Inc.
  - **link:** https://arxiv.org/pdf/2512.21593
  - **contributions:** 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Residual Prior Diffusion (RPD) / 残差先验扩散模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details"]
        Method --> M1["两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model"]
        Method --> M2["概率模型与可处理ELBO / Probabilistic model with tractable ELBO"]
        Results --> R1["在合成数据上准确捕捉细节 / Accurately captures details on synthetic data"]
        Results --> R2["自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines"]
        Results --> R3["少步推理保持性能 / Maintains performance with few inference steps"]
    ```

- **[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring**
  - **tags:** [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]
  - **authors:** Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li
  - **institution:** Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21975
  - **code:** https://github.com/ReaganWu/RT-Focuser
  - **contributions:** 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[运动模糊降低图像质量 / Motion blur degrades image quality]
        B --> B2[实时应用挑战 / Challenges for real-time applications]
        C --> C1[轻量级U型网络 / Lightweight U-shaped network]
        C --> C2[三个关键组件 / Three key components: LD, MLIA, X-Fuse]
        D --> D1[30.67 dB PSNR / 30.67 dB PSNR]
        D --> D2[5.85M参数, 15.76 GMACs / 5.85M params, 15.76 GMACs]
        D --> D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]
    ```

- **[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology**
  - **tags:** [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]
  - **authors:** Sungwoo Kang
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.21988
  - **contributions:** 1. Identifies and defines the "color-clinical decoupling" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker's disproportionate sensitivity to noise in the b* color channel.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed "color-clinical decoupling," primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Color-Clinical Decoupling<br>颜色-临床解耦] --> B[核心问题/Problem<br>Does color calibration ensure clinical reliability?<br>色彩校准能否确保临床可靠性？]
        A --> C[主要方法/Method<br>Analyze 43,425 images across devices with CCM<br>使用CCM分析43,425张跨设备图像]
        A --> D[关键结果/Results<br>Color-clinical decoupling: ∆E↓ but ICC(ITA) poor<br>颜色-临床解耦：∆E下降但ITA的ICC差]
        B --> D
        C --> D
    ```
