---
slug: /daily/cscv/20260105-20260111
---
# 20260105-20260111 (cs.CV)

## 2026-01-05

- **[arXiv260105] From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers**
  - **tags:** [ai], [diffusion models], [generative AI, diffusion models, architectural intelligence, computational reasoning, vernacular architecture]
  - **authors:** Abolhassan Pishahang, Maryam Badiei
  - **institution:** Florida Atlantic University, North Carolina State University
  - **link:** https://arxiv.org/pdf/2601.00029
  - **contributions:** 1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI's interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of "computational vernacular reasoning" as an analytical framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root["From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"] --> Problem["核心问题/Problem: How does generative AI interpret the architectural intelligence embedded in vernacular forms?"]
    Root --> Method["主要方法/Method: Test three diffusion models across three prompt stages (referential, adaptive, speculative) using a five-criteria evaluation framework."]
    Root --> Results["关键结果/Results: AI reproduces geometry but misreads material/climatic reasoning; reference aids realism but limits creativity."]
    ```

- **[arXiv260105] It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models**
  - **tags:** [cv], [diffusion models], [mode collapse, noise optimization, frequency characteristics, text-to-image generation, inference-time scaling]
  - **authors:** Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros
  - **institution:** UC Berkeley, University of Tübingen (Tübingen AI Center), Technical University of Munich (MCML)
  - **link:** https://arxiv.org/pdf/2601.00090
  - **contributions:** 1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models") --> Problem("核心问题/Problem: Mode collapse in text-to-image models")
    Root --> Method("主要方法/Method: Noise optimization with frequency analysis")
    Root --> Results("关键结果/Results: Improved generation diversity and quality")
    ```

- **[arXiv260105] TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model**
  - **tags:** [cv], [video generation], [4D world model, autoregressive diffusion, Macro-from-Micro Planning (MMPL), Distribution Matching Distillation (DMD), generation-reconstruction-guidance]
  - **authors:** Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li
  - **institution:** TeleWorld Team (Institution inferred from corresponding author's email domain: ieee.org, but specific institution not explicitly stated in provided content. Likely an academic or research lab.)
  - **link:** https://arxiv.org/pdf/2601.00051
  - **contributions:** 1. Proposes TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term memory in a closed-loop system. 2. Introduces a novel generation-reconstruction-guidance paradigm where a reconstructed 4D spatio-temporal representation guides subsequent generation for consistency. 3. Employs an autoregressive diffusion video model enhanced with Macro-from-Micro Planning (MMPL) and Distribution Matching Distillation (DMD) for efficient, long-horizon, real-time synthesis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp
  - **Simple LLM Summary:** This paper presents TeleWorld, a framework for building practical world models by integrating video generation and dynamic scene reconstruction into a closed-loop 4D system. It uses a novel generation-reconstruction-guidance paradigm and efficient planning/distillation techniques to achieve long-term consistency and real-time performance. The results demonstrate strong performance in world understanding and generation, advancing towards interactive, memory-enabled AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TeleWorld: 4D World Model<br/>TeleWorld: 4D世界模型] --> B[核心问题/Problem<br/>Video models lack real-time interaction,<br/>long-horizon consistency, and memory<br/>视频模型缺乏实时交互、<br/>长时一致性与记忆]
        A --> C[主要方法/Method<br/>Generation-Reconstruction-Guidance paradigm<br/>with MMPL & DMD for real-time 4D synthesis<br/>生成-重建-引导范式<br/>使用MMPL与DMD实现实时4D合成]
        A --> D[关键结果/Results<br/>Strong performance in static/dynamic<br/>understanding, consistency, and efficiency<br/>在静态/动态理解、<br/>一致性与效率上表现优异]
    ```

- **[arXiv260105] Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark**
  - **tags:** [cv], [multimodal reasoning], [spatial intelligence, multimodal large language models, benchmark, spatiotemporal reasoning, evaluation]
  - **authors:** Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu
  - **institution:** Huawei Technologies, CUHK-Shenzhen, HKUST-GZ, Zhejiang University, Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00092
  - **code:** https://spatial4d-bench.github.io/spatial4d/
  - **contributions:** 1. Introduces Spatial4D-Bench, a large-scale benchmark with ~40,000 QA pairs for evaluating 4D spatial intelligence in MLLMs. 2. Systematically organizes 18 tasks into six cognitive categories for structured and comprehensive assessment. 3. Benchmarks state-of-the-art MLLMs, revealing their substantial limitations in 4D spatial reasoning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Spatial4D-Bench, a large-scale benchmark designed to comprehensively evaluate the 4D spatial reasoning abilities of Multimodal Large Language Models (MLLMs). The benchmark covers 18 tasks across six cognitive categories. The evaluation reveals that current MLLMs have significant limitations in achieving human-level 4D spatial intelligence.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SPATIAL4D-BENCH: A VERSATILE 4D SPATIAL INTELLIGENCE BENCHMARK] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[评估MLLMs的4D空间智能水平/Assess MLLMs' 4D spatial intelligence]
        C --> C1[构建大规模多任务基准/Build large-scale multi-task benchmark]
        C --> C2[覆盖18个任务，6个认知类别/Cover 18 tasks, 6 cognitive categories]
        D --> D1[MLLMs在4D空间推理上存在显著局限/MLLMs show substantial limitations in 4D spatial reasoning]
    ```

- **[arXiv260105] A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data**
  - **tags:** [cv], [multimodal image segmentation], [Synthetic Aperture Radar (SAR), Multispectral Imaging (MSI), Spatially Masked Adaptive Gated Network (SMAGNet), feature fusion, missing data robustness]
  - **authors:** Hyunho Lee, Wenwen Li
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2601.00123
  - **contributions:** 1. Proposes SMAGNet, a novel multimodal deep learning model for post-flood water extent mapping that uses SAR as the primary input and adaptively integrates MSI data. 2. Introduces a method for handling incomplete or partially available MSI data, enhancing model applicability in real-world scenarios. 3. Demonstrates superior performance and robustness compared to other multimodal models, maintaining performance even when MSI data is completely missing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of integrating incomplete multispectral data with SAR imagery for post-flood water mapping. It proposes SMAGNet, a multimodal deep learning model that adaptively fuses features from both data sources. The model shows improved accuracy and robustness to missing data, making it more practical for real-world disaster response.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data"] --> Problem["核心问题/Problem: How to adaptively integrate partially available MSI data with SAR for water mapping?"]
        Root --> Method["主要方法/Method: Propose SMAGNet, a multimodal model using SAR as primary input with adaptive feature fusion."]
        Root --> Results["关键结果/Results: Outperforms other models; robust to missing MSI data; enhances real-world applicability."]
    ```

- **[arXiv260105] Explicit Abstention Knobs for Predictable Reliability in Video Question Answering**
  - **tags:** [ai], [selective prediction], [selective prediction, confidence-based abstention, risk-coverage tradeoff, distribution shift, video question answering]
  - **authors:** Jorge Ortiz
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2601.00138
  - **contributions:** 1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model's confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Explicit Abstention Knobs for Predictable Reliability in Video Question Answering] --> B[核心问题/Problem: Can confidence-based abstention provide reliable error rate control in video QA, especially under distribution shift?]
        A --> C[主要方法/Method: Use confidence thresholding on a VLM (Gemini 2.0 Flash) evaluated on the NExT-QA dataset, testing under evidence degradation.]
        A --> D[关键结果/Results: In-distribution control works, but confidence fails to decrease under shift, motivating warrant-based prediction.]
    ```

- **[arXiv260105] Compressed Map Priors for 3D Perception**
  - **tags:** [cv], [3D object detection], [spatial priors, binarized hashmap, map compression, nuScenes dataset, end-to-end training]
  - **authors:** Brady Zhou, Philipp Krähenbühl
  - **institution:** UT Austin
  - **link:** https://arxiv.org/pdf/2601.00139
  - **contributions:** 1. Proposes Compressed Map Priors (CMP), a framework to learn spatial priors from historic traversals for 3D perception. 2. Introduces a highly efficient binarized hashmap representation requiring only 32KB/km², achieving a 20x memory reduction. 3. Demonstrates seamless integration into existing 3D perception architectures with minimal computational overhead, leading to consistent performance improvements on nuScenes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that autonomous vehicle perception systems ignore prior knowledge from repeated traversals of the same area. It proposes Compressed Map Priors (CMP), a framework that learns and stores spatial priors in a highly compressed binarized hashmap. The method integrates easily into existing systems with little computational cost and significantly improves 3D object detection performance on the nuScenes dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Compressed Map Priors for 3D Perception] --> B[核心问题/Problem: 自动驾驶系统忽略历史遍历的先验知识/Autonomous systems ignore prior knowledge from past traversals]
        A --> C[主要方法/Method: 提出压缩地图先验(CMP)框架，使用二值化哈希图存储空间先验/Propose CMP framework using binarized hashmap for spatial priors]
        A --> D[关键结果/Results: 内存减少20倍，计算开销小，3D检测性能显著提升/20x memory reduction, low overhead, significant 3D detection improvement]
    ```

- **[arXiv260105] Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection**
  - **tags:** [cv], [image forensics / ai-generated content detection], [GLASS, stratified sampling, global-local attention, high-resolution image detection, vision transformer]
  - **authors:** Lawrence Han
  - **institution:** Independent researcher (inferred from personal email domain)
  - **link:** https://arxiv.org/pdf/2601.00141
  - **code:** GitHub Project Code
  - **contributions:** 1. Proposes GLASS, a novel architecture that combines a globally resized view with multiple original-resolution local crops for AI-generated image detection. 2. Introduces a spatially stratified sampling method to efficiently select diverse, non-overlapping local regions from high-resolution images. 3. Demonstrates the integration and effectiveness of GLASS with various backbone models (ViT, ResNet, ConvNeXt), showing superior performance over standard transfer learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of losing fine-grained details when detecting AI-generated images due to standard downsampling. It proposes the GLASS architecture, which processes both a global resized view and multiple original-resolution local crops, aggregated via attention. Experiments show GLASS improves detection performance across different model backbones within practical computational limits.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 高分辨率AI生成图像检测中，下采样导致细粒度细节丢失/Fine-grained detail loss in high-resolution AI-generated image detection due to downsampling]
        C[主要方法/Method: GLASS架构，结合全局视图与分层采样的原始分辨率局部裁剪，使用注意力聚合/GLASS architecture combining global view & stratified-sampled original-resolution local crops with attention aggregation]
        D[关键结果/Results: 在多种骨干网络上超越标准迁移学习性能/Outperforms standard transfer learning across various backbone models]
    ```

- **[arXiv260105] Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions**
  - **tags:** [cv], [facial attribute analysis], [multi-attribute description, facial action units, vision-language model, fine-tuning, region-focal analysis]
  - **authors:** Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M.Jose, Han Hu, Xuri Ge
  - **institution:** University of Glasgow, Shandong University, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2601.00156
  - **contributions:** 1. Introduces the novel problem (FaceFocalDesc) of generating multi-attribute natural language descriptions for arbitrarily selected face regions. 2. Constructs a new dataset with region-level annotations and natural language descriptions for this task. 3. Proposes the Focal-RegionFace model, a fine-tuned vision-language model that incrementally refines focus on localized features for interpretable multi-attribute analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new task of generating fine-grained, multi-attribute descriptions for specific face regions. To address it, the authors create a new dataset and propose Focal-RegionFace, a model fine-tuned from Qwen2.5-VL that progressively focuses on local features. Experiments show the model achieves state-of-the-art performance on the new benchmark, demonstrating its effectiveness for region-focal facial analysis.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>生成任意选定面部区域的细粒度多属性描述<br>Generating multi-attribute descriptions for arbitrarily selected face regions]
        C[主要方法/Method<br>基于Qwen2.5-VL的多阶段渐进式微调模型<br>Multi-stage progressive fine-tuning of Qwen2.5-VL]
        D[关键结果/Results<br>在新基准上取得最佳性能<br>Achieves best performance on the new benchmark]
    ```

- **[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications**
  - **tags:** [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]
  - **authors:** Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen
  - **institution:** Qifu Technology, Fudan University
  - **link:** https://arxiv.org/pdf/2601.00150
  - **contributions:** 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏金融信贷领域专用多模态基准/Lack of domain-specific multimodal benchmark for financial credit]
        C --> C1[构建隐私合规的合成-采集管道/Build privacy-compliant synthesis-capture pipeline]
        C --> C2[设计三维评估框架/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]
        D --> D1[评估23个VLM/Evaluate 23 VLMs]
        D --> D2[Qfin-VL-Instruct性能最佳/Qfin-VL-Instruct achieves top score]
        D --> D3[鲁棒性挑战/Robustness remains a challenge]
    ```

- **[arXiv260105] Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework**
  - **tags:** [mlsys], [on-device ai], [hybrid feature engineering, wavelet decomposition, graph-theoretic descriptors, linear separability, model compression]
  - **authors:** Moirangthem Tiken Singh, Manibhushan Yaikhom
  - **institution:** Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences
  - **link:** https://arxiv.org/pdf/2601.00192
  - **contributions:** 1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 µs latency) for real-time arrhythmia detection on edge devices.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection<br>优化的混合特征工程用于资源高效的心律失常检测"] --> B
        A --> C
        A --> D
        B["Problem: Deep learning models are too heavy for edge devices.<br>核心问题: 深度学习模型在边缘设备上计算开销过大"]
        C["Method: Hybrid feature engineering (wavelet + graph) for linear separability.<br>主要方法: 混合特征工程（小波+图论）实现线性可分性"]
        D["Results: 98.44% accuracy, 8.54 KB model, 0.46 µs latency.<br>关键结果: 98.44% 准确率, 8.54 KB 模型, 0.46 µs 延迟"]
    ```

- **[arXiv260105] DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery**
  - **tags:** [cv], [underwater image restoration], [conditional generative adversarial network, hyperspectral imagery, underwater image formation equation, satellite imagery, PRISMA]
  - **authors:** Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao
  - **institution:** Deakin University, The University of Adelaide
  - **link:** https://arxiv.org/pdf/2601.00194
  - **contributions:** 1. Proposes DichroGAN, a novel cGAN architecture for restoring in-air seafloor colours from satellite imagery. 2. Introduces a two-step simultaneous training process with four generators to model atmospheric radiance and underwater light transmission based on the image formation equation. 3. Demonstrates competitive performance on satellite and underwater datasets using a compact dataset derived from PRISMA imagery.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of restoring the true colours of the seafloor from satellite images, which are degraded by light absorption and scattering in water. The authors propose DichroGAN, a conditional GAN that uses a multi-generator architecture to estimate and remove underwater effects based on hyperspectral data. Experiments show the method achieves competitive results compared to state-of-the-art underwater restoration techniques.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Recovering in-air seafloor colours from degraded satellite imagery due to light attenuation in water.]
        Method[主要方法/Method: A cGAN (DichroGAN) with a two-step, four-generator architecture to estimate atmospheric radiance and underwater transmission.]
        Results[关键结果/Results: Achieves competitive performance on satellite and underwater datasets.]
    ```

- **[arXiv260105] CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting**
  - **tags:** [cv], [3D instance segmentation], [Neural Radiance Field (NeRF), 3D instance segmentation, crop counting, mask consistency, view synthesis]
  - **authors:** Md Ahmed Al Muzaddid, William J. Beksi
  - **institution:** The University of Texas at Arlington
  - **link:** https://arxiv.org/pdf/2601.00207
  - **contributions:** 1. A novel framework for exact crop enumeration via 3D instance segmentation using multi-view images and NeRF. 2. Introduction of crop visibility and mask consistency scores to effectively segment instances in 3D. 3. Demonstration of consistent performance across diverse crops (cotton, apples, pears) without crop-specific parameter tuning and release of a new cotton plant dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces CropNeRF, a framework for accurate crop counting in agriculture. It uses multi-view 2D images and instance masks to train a Neural Radiance Field (NeRF), incorporating novel visibility and consistency scores to perform 3D instance segmentation and count crops. The method shows superior counting performance across different crop types and releases a new dataset to advance research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[户外遮挡与聚类导致计数困难/Outdoor occlusions and clustering make counting hard]
        C --> C1[使用多视角图像与NeRF进行3D实例分割/Use multi-view images & NeRF for 3D instance segmentation]
        C --> C2[引入可见性与掩码一致性分数/Introduce visibility & mask consistency scores]
        D --> D1[在多种作物上实现准确计数/Achieve accurate counting on multiple crops]
        D --> D2[性能优于现有方法/Outperforms state-of-the-art]
        D --> D3[贡献棉花数据集/Contribute a cotton dataset]
    ```

- **[arXiv260105] IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation**
  - **tags:** [cv], [domain adaptation], [style synthesis, contrastive learning, unpaired image translation, disentanglement]
  - **authors:** Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz
  - **institution:** Vanderbilt University, Siemens Healthineers, Johnson & Johnson Innovative Medicine
  - **link:** https://arxiv.org/pdf/2601.00212
  - **code:** https://github.com/han-liu/IntraStyler
  - **contributions:** 1. Proposes IntraStyler, an exemplar-based style synthesis method for cross-modality domain adaptation that can capture diverse intra-domain styles without requiring prior knowledge of the variations. 2. Introduces a style encoder based on contrastive learning to discriminatively extract style-only features for guiding the synthesis. 3. Demonstrates the method's efficacy in controllable style synthesis and the benefits of diverse synthetic data for improving downstream segmentation performance on the CrossMoDA 2023 dataset.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the under-explored issue of intra-domain variability in unsupervised domain adaptation by proposing IntraStyler, an exemplar-based method for synthesizing diverse target domain styles without prior knowledge. It uses a contrastive learning-based style encoder to extract style features and guide synthesis to match an exemplar's style. The method shows effective controllable style synthesis and improves downstream segmentation on a cross-modality medical imaging dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法忽略域内多样性 / Prior methods under-explore intra-domain variability]
        C --> C1[提出基于范例的风格合成 / Propose exemplar-based style synthesis]
        C --> C2[使用对比学习风格编码器 / Use contrastive learning style encoder]
        D --> D1[可控风格合成有效 / Controllable style synthesis is effective]
        D --> D2[多样数据提升分割性能 / Diverse data improves segmentation]
    ```

- **[arXiv260105] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing**
  - **tags:** [cv], [3D generation and morphing], [Structured Latent (SLAT), Morphing Cross-Attention (MCA), Temporal-Fused Self-Attention (TFSA), training-free, 3D morphing]
  - **authors:** Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang
  - **institution:** Nanjing University, Peking University
  - **link:** https://arxiv.org/pdf/2601.00204
  - **contributions:** 1. A training-free framework (MorphAny3D) that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. 2. The introduction of Morphing Cross-Attention (MCA) for structural coherence and Temporal-Fused Self-Attention (TFSA) for temporal consistency. 3. An orientation correction strategy to mitigate pose ambiguity, enabling state-of-the-art morphing even for challenging cross-category cases.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of generating semantically consistent and temporally smooth 3D morphing sequences, especially across categories. It proposes MorphAny3D, a training-free framework that intelligently blends Structured Latent (SLAT) features within a 3D generator's attention mechanisms using novel MCA and TFSA modules. The method achieves state-of-the-art results and supports advanced applications like decoupled morphing and 3D style transfer.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing] --> B(核心问题/Problem: 3D Morphing Challenges)
        A --> C(主要方法/Method: MorphAny3D Framework)
        A --> D(关键结果/Results: State-of-the-Art Morphing)
        B --> B1("挑战: 语义一致性与时序平滑性 / Challenge: Semantic Consistency & Temporal Smoothness")
        B --> B2("尤其跨类别 / Especially Cross-Category")
        C --> C1("核心: 结构化潜在表示 / Core: Structured Latent (SLAT)")
        C --> C2("训练无关 / Training-Free")
        C --> C3("关键模块: MCA & TFSA / Key Modules: MCA & TFSA")
        D --> D1("高质量变形序列 / High-Quality Morphing Sequences")
        D --> D2("支持高级应用 / Supports Advanced Applications")
        D --> D3("泛化至其他模型 / Generalizable to Other Models")
    ```

- **[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]
  - **authors:** Omar Sharif, Eftekhar Hossain, Patrick Ng
  - **institution:** Dartmouth College, University of Central Florida
  - **link:** https://arxiv.org/pdf/2601.00215
  - **contributions:** 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"] --> Problem["核心问题/Problem: MLLMs lack visual grounding in reasoning"]
        Root --> Method["主要方法/Method: RL with reward functions & GRPO"]
        Root --> Results["关键结果/Results: 5.56% improvement on Qwen-2.5-VL-7B"]
    ```

- **[arXiv260105] LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization**
  - **tags:** [ai], [model compression (quantization/pruning)], [vector quantization, compositional codebook, extrapolation-by-interpolation, codebook collapse, low-dimensional codevectors]
  - **authors:** Jie Li, Kwan-Yee K. Wong, Kai Han
  - **institution:** The University of Hong Kong
  - **link:** https://arxiv.org/pdf/2601.00222
  - **contributions:** 1. Introduces a parameter-efficient, low-dimensional compositional codebook that treats codevectors as compositional units, expanding the solution space and enabling a more compact representation. 2. Incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance feature smoothing and detail preservation during quantization. 3. Functions as a plug-and-play module for existing VQ-based methods, achieving state-of-the-art performance with full codebook usage and avoidance of collapse.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes LooC, a new vector quantization method that uses a low-dimensional, compositional codebook to achieve high capacity with a compact size. It introduces a novel way to combine codevectors and a feature smoothing mechanism, leading to better performance and full codebook utilization. Extensive evaluations show LooC outperforms existing VQ methods with a significantly smaller codebook.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization") --> Problem("核心问题/Problem: Need for high-capacity yet compact VQ methods")
        Root --> Method("主要方法/Method: Low-dimensional compositional codebook & extrapolation-by-interpolation")
        Root --> Results("关键结果/Results: SOTA performance with smaller codebook, plug-and-play")
    ```

- **[arXiv260105] Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions**
  - **tags:** [cv], [image quality assessment], [synthetic data distribution, generalization error, distribution reshaping, upsampling, downsampling]
  - **authors:** Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong
  - **institution:** Xidian University
  - **link:** https://arxiv.org/pdf/2601.00225
  - **code:** https://github.com/Li-aobo/SynDR-IQA
  - **contributions:** 1. Identified a key issue where models trained on synthetic IQA data learn discrete, clustered feature representations that hinder regression performance. 2. Proposed a novel framework, SynDR-IQA, which reshapes synthetic data distribution based on theoretical analysis of sample diversity and redundancy's impact on generalization. 3. Introduced two core strategies: distribution-aware diverse content upsampling to enhance visual diversity, and density-aware redundant cluster downsampling to balance sample density.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited generalization of Blind Image Quality Assessment (BIQA) models trained on synthetic data by identifying that the problem stems from the clustered distribution of synthetic data features. The authors propose the SynDR-IQA framework, which reshapes the data distribution through strategic upsampling and downsampling to improve sample diversity and balance. Extensive experiments across multiple cross-dataset settings demonstrate the effectiveness of their method in enhancing model generalization.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Syn-to-Real IQA<br>重塑合成数据分布] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Limited generalization of BIQA models trained on synthetic data due to clustered feature distributions]
        C[主要方法/Method<br>SynDR-IQA framework reshapes data distribution via diverse content upsampling and redundant cluster downsampling]
        D[关键结果/Results<br>Method improves generalization across synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic settings]
    ```

- **[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection**
  - **tags:** [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]
  - **authors:** Chao Yang, Haoyuan Zheng, Yue Ma
  - **institution:** Xi’an Jiaotong Liverpool University
  - **link:** https://arxiv.org/pdf/2601.00237
  - **contributions:** 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"] --> Problem["核心问题/Problem: 红外数据稀缺 / IR Data Scarcity"]
        Root --> Method["主要方法/Method: CycleGAN跨模态生成 + YOLOv8检测 / CycleGAN Cross-modal Generation + YOLOv8 Detection"]
        Root --> Results["关键结果/Results: 性能显著提升，接近全监督基准 / Performance Significantly Improved, Approaches Fully Supervised Benchmark"]
    ```

- **[arXiv260105] Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture**
  - **tags:** [cv], [few-shot object detection], [lightweight CNN, prototypical meta-learning, decision support system, precision agriculture, pest recognition]
  - **authors:** Anirudha Ghosh, Ritam Sarkar, Debaditya Barman
  - **institution:** Visva-Bharati, Uttar Banga Krishi Viswavidyalaya
  - **link:** https://arxiv.org/pdf/2601.00243
  - **contributions:** 1. A lightweight framework for pest detection and pesticide recommendation designed for low-resource devices like smartphones and drones. 2. A Pest Detection Module using a compact CNN with prototypical meta-learning for accurate identification with few training samples. 3. A Pesticide Recommendation Module that integrates environmental factors (crop type, growth stage) to suggest safe and eco-friendly pesticides.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a lightweight framework for precision agriculture that combines a few-shot pest detection model with a context-aware pesticide recommendation system. The method uses a compact CNN with prototypical learning for detection and a rule-based system incorporating environmental factors for recommendations. The framework achieves high accuracy with low computational cost, demonstrating potential for real-time use on resource-constrained devices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统害虫管理成本高、效率低、不环保/Traditional pest management is costly, inefficient, and harmful]
        C --> C1[轻量级框架/Lightweight Framework]
        C1 --> C1_1[害虫检测模块: 轻量CNN+原型网络/Pest Detection: Lightweight CNN + Prototypical Network]
        C1 --> C1_2[农药推荐模块: 环境感知决策系统/Pesticide Recommendation: Context-Aware Decision System]
        D --> D1[高精度，低计算成本/High accuracy, low computational cost]
        D --> D2[适用于智能手机和无人机/Suitable for smartphones and drones]
    ```

- **[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective**
  - **tags:** [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]
  - **authors:** Aly Sabri Abdalla, Vuk Marojevic
  - **institution:** Mississippi State University
  - **link:** https://arxiv.org/pdf/2601.00257
  - **contributions:** 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --> P1["子问题/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]
        Method["主要方法/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --> M1["组件/Component: Semantic-aware rApp (terrain interpreter)"]
        Method --> M2["组件/Component: RL-enabled xApp (trajectory planner)"]
        Results["关键结果/Results: Framework enables closed-loop, AI-optimized LAE operations"] --> R1["评估/Evaluation: Feasibility and performance analysis presented"]
        Results --> R2["展望/Outlook: Research challenges and standardization needs surveyed"]
    ```

- **[arXiv260105] TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models**
  - **tags:** [cv], [medical image analysis], [3D-CT, vision-language model, organ separation, contrastive learning, zero-shot classification]
  - **authors:** Kohei Yamamoto, Tomohiro Kikuchi
  - **institution:** Jichi Medical University
  - **link:** https://arxiv.org/pdf/2601.00260
  - **contributions:** 1. Proposes TotalFM, a 3D-CT vision foundation model based on an organ-separated learning framework to balance computational efficiency and representation capability. 2. Introduces an automated pipeline for creating organ volume and finding-sentence pairs using segmentation and LLM-based report processing. 3. Demonstrates superior zero-shot performance in organ-wise and finding-wise lesion classification compared to baselines like CT-CLIP and Merlin.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TotalFM, a 3D-CT vision foundation model that uses an organ-separated framework and combines self-supervised pre-training with contrastive learning to efficiently align volumetric images with text. It shows strong zero-shot classification performance on clinical tasks, outperforming existing models, and demonstrates the framework's effectiveness for practical 3D-CT model implementation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models] --> B(核心问题/Problem: 3D-CT基础模型训练的计算成本高/High computational cost for training 3D-CT foundation models)
        A --> C(主要方法/Method: 器官分离框架，结合自监督预训练和对比学习/Organ-separated framework, combining self-supervised pre-training and contrastive learning)
        A --> D(关键结果/Results: 在零样本器官和病灶分类任务中性能优越/Superior performance in zero-shot organ-wise and finding-wise lesion classification tasks)
    ```

- **[arXiv260105] S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding**
  - **tags:** [cv], [multimodal dataset], [multimodal learning, image-text alignment, semantic enhancement, Qwen-VL, CLIP score]
  - **authors:** He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2601.00264
  - **code:** https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign
  - **contributions:** 1. Introduces S1-MMAlign, a large-scale, multi-disciplinary dataset with over 15.5 million scientific image-text pairs from 2.5 million papers. 2. Proposes an AI-ready semantic enhancement pipeline using the Qwen-VL model to recaption images by synthesizing context from abstracts and citations, improving alignment. 3. Demonstrates significant data quality improvement via technical validation, showing reduced semantic ambiguity and an 18.21% increase in CLIP scores for image-text alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces S1-MMAlign, a large-scale multimodal dataset for scientific figure-text understanding, addressing the semantic gap between complex scientific imagery and sparse textual descriptions. It proposes a semantic enhancement pipeline using the Qwen-VL model to recaption images by integrating context from paper abstracts and citations, which significantly improves image-text alignment as validated by CLIP scores. The dataset serves as a foundational resource for advancing scientific reasoning and cross-modal understanding in AI for Science.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[S1-MMAlign: 大规模跨学科数据集<br>S1-MMAlign: Large-Scale, Multi-Disciplinary Dataset] --> B(核心问题/Problem: 科学图像与文本语义鸿沟<br>Scientific Figure-Text Semantic Gap)
        A --> C(主要方法/Method: 基于Qwen-VL的语义增强管道<br>Qwen-VL-based Semantic Enhancement Pipeline)
        A --> D(关键结果/Results: 图像-文本对齐提升18.21%<br>18.21% Improvement in Image-Text Alignment)
    ```

- **[arXiv260105] ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching**
  - **tags:** [mlsys], [diffusion models], [concept erasure, activation patching, training-free, text-to-image, adversarial robustness]
  - **authors:** Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang
  - **institution:** Harbin Institute of Technology, Shenzhen; Tsinghua Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory
  - **link:** https://arxiv.org/pdf/2601.00267
  - **contributions:** 1. Proposes a novel training-free paradigm for concept erasure in diffusion models, eliminating the need for data-intensive fine-tuning. 2. Introduces a method that identifies and patches activation differences via prompt-pair analysis to precisely remove target concepts. 3. Demonstrates state-of-the-art performance across multiple erasure tasks while preserving model capability and showing robustness against adversarial attacks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ActErase, a training-free method for erasing sensitive concepts from text-to-image diffusion models. It works by identifying and patching activation differences during inference, avoiding costly fine-tuning. The method achieves strong erasure performance, maintains general generation quality, and is robust to attacks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[扩散模型生成不当内容 / Diffusion models generate inappropriate content]
        B --> B2[现有擦除方法依赖昂贵微调 / Existing erasure relies on costly fine-tuning]
        C --> C1[识别激活差异区域 / Identify activation difference regions]
        C --> C2[提取并动态替换目标激活 / Extract and dynamically replace target activations]
        D --> D1[SOTA擦除性能 / SOTA erasure performance]
        D --> D2[保持生成能力 / Preserves generative capability]
        D --> D3[对抗攻击鲁棒性 / Robust against adversarial attacks]
    ```

- **[arXiv260105] FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering**
  - **tags:** [mlsys], [multi-modal inference], [hallucination detection, vision-language models, uncertainty estimation, model-driven learning, LLM-as-a-Judge]
  - **authors:** Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu
  - **institution:** Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute
  - **link:** https://arxiv.org/pdf/2601.00269
  - **contributions:** 1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FaithSCAN: Faithful VQA Hallucination Detection] --> B[核心问题/Problem: VQA模型产生流畅但视觉无根据的答案/Faithfulness Hallucinations in VQA]
        A --> C[主要方法/Method: 利用VLM内部信号与不确定性感知融合/Exploit VLM Internal Signals & Uncertainty-Aware Fusion]
        A --> D[关键结果/Results: 高效且优于现有方法/More Effective & Efficient than Prior Methods]
    ```

- **[arXiv260105] Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification**
  - **tags:** [cv], [long-tailed classification], [uncertainty estimation, evidential deep learning, remote sensing, long-tailed distribution, adaptive label smoothing]
  - **authors:** Chi Ding, Junxiao Xue, Xinyi Yin, Shi Chen, Yunyun Shi, Yiduo Wang, Fengjian Xue, Xuecheng Wu
  - **institution:** Zhejiang Lab, Zhengzhou University, Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2601.00278
  - **contributions:** 1. Proposes a model-agnostic framework (DUAL) that disentangles prediction uncertainty into epistemic and aleatoric types to address long-tailed classification. 2. Introduces epistemic uncertainty to guide a reweighting strategy for hard-to-learn tail samples. 3. Leverages aleatoric uncertainty to quantify data ambiguity and employs an adaptive label smoothing mechanism to suppress noise.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of distinguishing hard-to-learn samples from noisy ones in long-tailed remote sensing image classification. The authors propose DUAL, an uncertainty-aware framework that uses epistemic uncertainty to reweight tail samples and aleatoric uncertainty to apply adaptive label smoothing for noise suppression. Experiments show the framework outperforms strong baselines and generalizes across different datasets and backbones.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification"]
        Root --> Problem["核心问题/Problem: Long-tailed distributions in remote sensing; need to disentangle hard tail samples from noisy ones."]
        Root --> Method["主要方法/Method: Propose DUAL framework using Evidential Deep Learning to separate epistemic (for reweighting) and aleatoric (for label smoothing) uncertainty."]
        Root --> Results["关键结果/Results: Outperforms baselines (TGN, SADE); effective and generalizable across datasets and backbones."]
    ```

- **[arXiv260105] SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting**
  - **tags:** [cv], [4D reconstruction], [Gaussian Splatting, Sparse View, Skeleton-Driven, Deformation Field, Motion Interpolation]
  - **authors:** Jun-Jee Chao, Volkan Isler
  - **institution:** University of Minnesota, The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2601.00285
  - **contributions:** 1. A framework (SV-GS) for 4D reconstruction from sparse observations in both time and viewpoint, using a skeleton-driven Gaussian Splatting approach. 2. A novel skeleton-driven deformation field with a time-dependent joint pose estimator and a separate fine-grained deformation module, enabling smooth motion interpolation. 3. Demonstrating that the method's initial static reconstruction input can be replaced by a diffusion-based generative prior, enhancing practical applicability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenging problem of 4D reconstruction from sparse observations in time and viewpoint. It proposes SV-GS, a method that uses a skeleton-driven deformation field with Gaussian Splatting to simultaneously estimate motion and geometry. The approach outperforms existing methods under sparse settings and shows practical potential by relaxing input requirements with generative priors.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting") --> Problem("核心问题/Problem: 4D reconstruction from sparse observations in time and viewpoint")
        Root --> Method("主要方法/Method: Skeleton-driven deformation field with Gaussian Splatting")
        Root --> Results("关键结果/Results: Outperforms baselines under sparse views; comparable to dense methods with fewer frames; works with diffusion priors")
    ```

- **[arXiv260105] Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies**
  - **tags:** [cv], [medical image classification], [Swin Transformer, BatchFormer, Focal Loss, ReduceLROnPlateau, ISIC2019]
  - **authors:** Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua
  - **institution:** University of Technology Sydney, University of Sydney, Shaoyang University
  - **link:** https://arxiv.org/pdf/2601.00286
  - **contributions:** 1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model's potential as a clinical support tool.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Towards Automated Differential Diagnosis of Skin Diseases<br>皮肤疾病自动鉴别诊断] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Limited dermatologist access & need for diagnostic tools<br>皮肤科医生资源有限，需要诊断工具]
        C[主要方法/Method<br>Deep learning (Swin Transformer) with imbalance-aware strategies<br>深度学习（Swin Transformer）与不平衡感知策略]
        D[关键结果/Results<br>87.71% accuracy on ISIC2019 dataset<br>在ISIC2019数据集上达到87.71%准确率]
    ```

- **[arXiv260105] TimeColor: Flexible Reference Colorization via Temporal Concatenation**
  - **tags:** [cv], [video colorization], [diffusion models, temporal concatenation, spatiotemporal correspondence-masked attention, modality-disjoint RoPE indexing, sketch-based colorization]
  - **authors:** Bryan Constantine Sadihin, Yihao Meng, Michael Hua Wang, Matteo Jiahao Chen, Hang Su
  - **institution:** Tsinghua University, HKUST
  - **link:** https://arxiv.org/pdf/2601.00296
  - **code:** https://bconstantine.github.io/TimeColor/
  - **contributions:** 1. Proposes a method to support heterogeneous, variable-count references for video colorization via temporal concatenation of reference latents, keeping model parameters fixed. 2. Introduces spatiotemporal correspondence-masked attention and modality-disjoint RoPE indexing to enforce subject-reference binding and prevent shortcutting and palette leakage. 3. Demonstrates improved color fidelity, identity consistency, and temporal stability over prior baselines on the SAKUGA-42M dataset under single- and multi-reference protocols.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of existing video colorization models that rely on a single reference frame, which restricts the use of diverse references like character sheets. It proposes TimeColor, a sketch-based video colorization model that uses temporal concatenation to incorporate multiple references and novel attention mechanisms to bind subjects to references, improving color consistency and stability. Experiments show TimeColor outperforms prior methods in color fidelity and temporal coherence.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TimeColor: Flexible Reference Colorization via Temporal Concatenation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法仅支持单参考帧/Existing methods condition on single reference]
        B --> B2[忽略其他参考源/Ignore other references (e.g., character sheets)]
        C --> C1[时间拼接参考隐变量/Temporally concatenate reference latents]
        C --> C2[时空对应掩码注意力/Spatiotemporal correspondence-masked attention]
        C --> C3[模态分离RoPE索引/Modality-disjoint RoPE indexing]
        D --> D1[提升颜色保真度/Improves color fidelity]
        D --> D2[提升身份一致性/Improves identity consistency]
        D --> D3[提升时间稳定性/Improves temporal stability]
    ```

- **[arXiv260105] VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning**
  - **tags:** [cv], [person re-identification], [feature fusion, alpha-divergence loss, dynamic multi-task learning, semantic clustering, computational efficiency]
  - **authors:** Anns Ijaz, Muhammad Azeem Javed
  - **institution:** University of Management and Technology
  - **link:** https://arxiv.org/pdf/2601.00307
  - **contributions:** 1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[VisNet: Efficient Person Re-Identification] --> B[核心问题/Problem: Accuracy vs. Computational Cost Trade-off]
        A --> C[主要方法/Method: Feature Fusion, Semantic Clustering, Dynamic Multi-Task Learning, α-Divergence Loss]
        A --> D[关键结果/Results: 87.05% Rank-1, 77.65% mAP, 4.601 GFLOPs]
    ```

- **[arXiv260105] ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition**
  - **tags:** [cv], [video behavior recognition], [data augmentation, representation-aware mixing, spatiotemporal coherence, plug-and-play, motion-aware masking]
  - **authors:** Feng-Qi Cui, Jinyang Huang, Sirui Zhao, Jinglong Guo, Qifan Cai, Xin Yan, Zhi Liu
  - **institution:** University of Science and Technology of China, Hefei University of Technology
  - **link:** https://arxiv.org/pdf/2601.00311
  - **contributions:** 1. Proposes a novel plug-and-play, training-free augmentation strategy (ReMA) that formulates video mixing as a controlled replacement process to expand representations while preserving class-conditional stability. 2. Introduces a Representation Alignment Mechanism (RAM) to perform structured intra-class mixing under distributional constraints, suppressing irrelevant intra-class drift. 3. Introduces a Dynamic Selection Mechanism (DSM) to generate motion-aware spatiotemporal masks, localizing perturbations away from discrimination-sensitive regions to promote temporal coherence.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that current perturbation-driven video data augmentation methods introduce uncontrolled variations that harm representation stability. To solve this, it proposes ReMA, a training-free plug-and-play method that uses representation alignment and dynamic spatiotemporal masking to control how and where mixing is applied. Experiments show ReMA consistently improves model generalization and robustness across various video behavior benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ReMA: 视频行为识别的即插即用混合增强] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有视频增强引入非判别性变化，破坏类内分布和时序一致性]
        C --> C1[表示对齐机制 / Representation Alignment Mechanism (RAM)]
        C --> C2[动态选择机制 / Dynamic Selection Mechanism (DSM)]
        D --> D1[提升泛化性和鲁棒性 / Improves generalization and robustness]
    ```

- **[arXiv260105] Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation**
  - **tags:** [cv], [image reflection separation], [Mamba, state-space model, depth-aware, memory expert, nighttime dataset]
  - **authors:** Siyan Fang, Long Peng, Yuntao Wang, Ruonan Wei, Yuehuan Wang
  - **institution:** Huazhong University of Science and Technology, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2601.00322
  - **code:** https://github.com/fashyon/DMDNet
  - **contributions:** 1. Proposed the Depth-Memory Decoupling Network (DMDNet) with Depth-Aware Scanning (DAScan) and a Depth-Synergized State-Space Model (DS-SSM) to guide Mamba for better layer disentanglement. 2. Introduced a Memory Expert Compensation Module (MECM) that leverages historical knowledge to provide layer-specific compensation. 3. Constructed a new Nighttime Image Reflection Separation (NightIRS) dataset to address the lack of data for nighttime scenes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenging problem of separating reflection and transmission layers from a single blended image, especially in low-contrast nighttime scenes. The authors propose DMDNet, a novel network that integrates depth-aware guidance into a Mamba-based state-space model and utilizes a memory expert module for compensation. The method, evaluated on a newly created nighttime dataset, is shown to outperform existing state-of-the-art approaches for both daytime and nighttime reflection separation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[单图像信息有限，夜间分离困难/Single image info limited, severe at night]
        C --> C1[深度记忆解耦网络 DMDNet/Depth-Memory Decoupling Network DMDNet]
        C1 --> C2[深度感知扫描 DAScan/Depth-Aware Scanning DAScan]
        C1 --> C3[深度协同状态空间模型 DS-SSM/Depth-Synergized State-Space Model DS-SSM]
        C1 --> C4[记忆专家补偿模块 MECM/Memory Expert Compensation Module MECM]
        C --> C5[构建夜间数据集 NightIRS/Construct NightIRS dataset]
        D --> D1[在白天和夜间均超越SOTA/Outperforms SOTA in daytime & nighttime]
    ```

- **[arXiv260105] HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection**
  - **tags:** [cv], [anomaly detection], [frequency-guided learning, structural attention, semantic consistency, dual-branch framework, CLIP]
  - **authors:** Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao
  - **institution:** Tianjin University of Science and Technology, The University of Sydney, National University of Singapore
  - **link:** https://arxiv.org/pdf/2601.00327
  - **contributions:** 1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HarmoniAD: 异常检测/HarmoniAD: Anomaly Detection] --> B[核心问题/Problem: 结构-语义权衡/Structure-Semantics Trade-off]
        A --> C[主要方法/Method: 频率引导双分支框架/Frequency-Guided Dual-Branch Framework]
        A --> D[关键结果/Results: SOTA性能/SOTA Performance]
        B --> B1[结构模型噪声敏感/Structure Models: Noise-Sensitive]
        B --> B2[语义模型忽略细节/Semantic Models: Miss Details]
        C --> C1[高频分支: FSAM模块/High-Freq Branch: FSAM]
        C --> C2[低频分支: GSCM模块/Low-Freq Branch: GSCM]
        D --> D1[数据集: MVTec-AD, VisA, BTAD/Datasets: MVTec-AD, VisA, BTAD]
        D --> D2[结果: 高敏感性与鲁棒性/Results: High Sensitivity & Robustness]
    ```

- **[arXiv260105] Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion**
  - **tags:** [cv], [3D human reconstruction], [bridge diffusion, latent representation, 3D Gaussian splatting, variational autoencoder, unified modeling]
  - **authors:** Yingzhi Tang, Qijian Zhang, Junhui Hou
  - **institution:** City University of Hong Kong, Tencent Games
  - **link:** https://arxiv.org/pdf/2601.00328
  - **code:** https://github.com/haiantyz/JGA-LBD
  - **contributions:** 1. Proposes a unified framework (JGA-LBD) that jointly models human geometry and appearance in a single latent space, addressing inconsistency issues in decoupled pipelines. 2. Introduces a method to unify heterogeneous input conditions (e.g., depth, SMPL) into 3D Gaussian representations and compresses them into a shared latent space via a sparse VAE. 3. Formulates the generation process using a specialized bridge diffusion model that infers missing components from a partially observed latent code, enabling high-fidelity reconstruction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenging problem of reconstructing consistent 3D digital humans from a single RGB image. It proposes JGA-LBD, a novel framework that unifies geometry and appearance modeling into a joint latent representation and uses bridge diffusion for generation. Experiments show the method outperforms state-of-the-art approaches in both geometry and appearance quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Joint Geometry–Appearance Human Reconstruction] --> B(核心问题/Problem: 单视图RGB图像重建高保真3D数字人/Single-view RGB to high-fidelity 3D human)
        A --> C(主要方法/Method: JGA-LBD框架/JGA-LBD Framework)
        C --> C1(统一潜在空间/Unified Latent Space)
        C1 --> C1a(3D高斯表示/3D Gaussian Representation)
        C1 --> C1b(稀疏变分自编码器/Sparse VAE)
        C --> C2(桥接扩散/Bridge Diffusion)
        C2 --> C2a(部分观测/Partial Observation)
        C2 --> C2b(推断缺失组件/Infer Missing Components)
        A --> D(关键结果/Results: 超越SOTA的几何与外观质量/Superior geometry & appearance quality vs. SOTA)
    ```

- **[arXiv260105] Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation**
  - **tags:** [cv], [object detection], [YOLOv8, CNN, Transformer, License Plate Recognition, Speed Estimation]
  - **authors:** Bruce Mugizi, Sudi Murindanyi, Olivia Nakacwa, Andrew Katumba
  - **institution:** Makerere University
  - **link:** https://arxiv.org/pdf/2601.00344
  - **contributions:** 1. Developed a real-time intelligent traffic surveillance system specifically tailored for resource-constrained environments like Uganda, integrating vehicle detection, license plate recognition, and speed estimation. 2. Achieved high performance with a 97.9% mAP for license plate detection using YOLOv8 and a low 1.79% character error rate for recognition using a Transformer model, significantly improving over a CNN baseline. 3. Implemented a practical enforcement pipeline by creating a database to correlate vehicle data with user information and enabling automated ticket issuance via SMS using the Africa's Talking API.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a real-time computer vision system to tackle speeding in developing countries by detecting vehicles, recognizing license plates, and estimating speed. The method uses YOLOv8 for detection and a Transformer for character recognition, achieving high accuracy and integrating with an SMS-based ticketing system. The work demonstrates a practical, automated solution for traffic enforcement in resource-limited settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("超速导致事故/Speeding causes accidents")
        Problem --> P2("发展中国家缺乏基础设施/Developing countries lack infrastructure")
        Method --> M1("车辆检测/Vehicle Detection")
        Method --> M2("车牌识别/License Plate Recognition")
        Method --> M3("速度估计/Speed Estimation")
        M1 --> M1_1("使用YOLOv8/Using YOLOv8")
        M2 --> M2_1("使用CNN和Transformer/Using CNN & Transformer")
        M3 --> M3_1("使用ROI/Using ROI")
        Results --> R1("高检测精度/High detection mAP: 97.9%")
        Results --> R2("低字符错误率/Low CER: 1.79%")
        Results --> R3("速度估计误差小/Speed error: ±10 km/h")
        Results --> R4("自动罚单系统/Automated ticketing via SMS")
    ```

- **[arXiv260105] OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning**
  - **tags:** [cv], [multimodal learning], [visual-tactile learning, domain generalization, fractional Fourier transform, multimodal fusion, hierarchical tree structure]
  - **authors:** Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha
  - **institution:** Fuzhou University, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2601.00352
  - **contributions:** 1. Formulates a new task, Single Domain Generalization for Multimodal Visual-Tactile Learning (SDG-VTL), to address modality and domain gaps in VTL. 2. Proposes the OmniVaT framework, which introduces a Multimodal Fractional Fourier Adapter (MFFA) to map visual and tactile embeddings into a unified embedding-frequency space, mitigating the modality gap. 3. Incorporates a Discrete Tree Generation (DTG) module to obtain diverse and reliable multimodal fractional representations via a hierarchical tree structure, enhancing adaptability to unseen domain shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of modality discrepancies and domain gaps in visual-tactile learning by proposing a new task (SDG-VTL) and a framework called OmniVaT. OmniVaT uses a Multimodal Fractional Fourier Adapter to unify visual and tactile features and a Discrete Tree Generation module to enhance generalization to unseen domains. Experiments show that OmniVaT achieves superior cross-domain generalization performance on the SDG-VTL task.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[OmniVaT: 单域泛化多模态视觉触觉学习<br>OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[模态差异与域差距<br>Modality & Domain Gaps]
        B1 --> B2[新任务: SDG-VTL<br>New Task: SDG-VTL]
        C --> C1[多模态分数傅里叶适配器 MFFA<br>Multimodal Fractional Fourier Adapter]
        C --> C2[离散树生成模块 DTG<br>Discrete Tree Generation Module]
        D --> D1[卓越的跨域泛化性能<br>Superior Cross-Domain Generalization]
    ```

- **[arXiv260105] Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers**
  - **tags:** [cv], [semantic segmentation], [dense visual embeddings, knowledge distillation, RGB-D transformer, real-time inference, Alpha-CLIP]
  - **authors:** Söhnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross
  - **institution:** Technische Universität Ilmenau
  - **link:** https://arxiv.org/pdf/2601.00359
  - **contributions:** 1. Proposes DVEFormer, an efficient RGB-D Transformer-based model for predicting dense, text-aligned visual embeddings via knowledge distillation from Alpha-CLIP. 2. Enables flexible, open-vocabulary scene understanding (e.g., text-based querying) beyond fixed-class semantic segmentation while maintaining the ability to perform classical segmentation. 3. Demonstrates real-time performance on embedded hardware (NVIDIA Jetson AGX Orin), making it suitable for mobile robotics applications like 3D mapping.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the need for robots to have a detailed, open-vocabulary understanding of indoor environments. It proposes DVEFormer, an efficient model that uses an RGB-D Transformer and knowledge distillation from Alpha-CLIP to predict dense visual embeddings, enabling both classical segmentation and flexible text-based querying. The method achieves competitive performance and real-time inference speeds, making it a practical drop-in replacement for traditional segmentation in mobile robotics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[机器人需要全面、开放词汇的场景理解/Robots need comprehensive, open-vocabulary scene understanding]
        C --> C1[使用RGB-D Transformer和知识蒸馏/Use RGB-D Transformer and Knowledge Distillation]
        C1 --> C2[从Alpha-CLIP教师模型学习密集视觉嵌入/Learn Dense Visual Embeddings from Alpha-CLIP teacher]
        D --> D1[实现实时性能与有竞争力的结果/Achieves real-time performance & competitive results]
        D --> D2[支持文本查询和3D映射/Enables text-based querying & 3D mapping]
    ```

- **[arXiv260105] Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting**
  - **tags:** [cv], [3D reconstruction and inpainting], [voxel diffusion, mask-conditioned inpainting, joint geometry-color completion, 3D U-Net, cultural heritage]
  - **authors:** Aarya Sumuk
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2601.00368
  - **contributions:** 1. A lightweight two-stage framework for joint geometry and color inpainting of 3D objects, separating damage localization from reconstruction. 2. A mask-conditioned volumetric diffusion model (3D U-Net) that directly inpaints voxel grids to reconstruct occupancy and color while preserving intact regions. 3. A composite training objective combining occupancy reconstruction, masked color reconstruction, and perceptual regularization for joint prediction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a two-stage method for digitally restoring damaged 3D cultural heritage artifacts. It first predicts a 3D damage mask from 2D RGB slices, then uses a mask-conditioned voxel diffusion model to jointly inpaint the missing geometry and color. The results show this approach produces more complete and coherent reconstructions than symmetry-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Mask-Conditioned Voxel Diffusion<br>掩码条件体素扩散"] --> Problem["Joint Geometry & Color Inpainting of Damaged 3D Objects<br>受损3D物体的几何与颜色联合修复"]
        Root --> Method["Two-Stage Framework: Mask Prediction + Mask-Conditioned Voxel Diffusion<br>两阶段框架：掩码预测 + 掩码条件体素扩散"]
        Root --> Results["More Complete & Coherent Reconstructions vs. Baselines<br>相比基线方法更完整、更连贯的重建结果"]
    ```

- **[arXiv260105] BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition**
  - **tags:** [cv], [skeleton-based action recognition], [probabilistic fusion, multi-modal integration, reliability-aware learning, noisy-or, cross-modal ensemble]
  - **authors:** Seungyeon Cho, Tae-kyun Kim
  - **institution:** Imperial College London (Inferred from author Tae-Kyun Kim's affiliation)
  - **link:** https://arxiv.org/pdf/2601.00369
  - **contributions:** 1. A calibration-free preprocessing pipeline that learns directly from native skeleton coordinates, removing canonical-space transformations. 2. A probabilistic Noisy-OR fusion mechanism for reliability-aware dual-stream learning without explicit confidence supervision. 3. An intra- to cross-modal ensemble that couples four skeleton modalities with RGB representations in a unified framework.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of body-centric skeleton action recognition by proposing BHaRNet, a probabilistic dual-stream framework that integrates body and hand modalities for fine-grained recognition. The method introduces reliability-aware learning and a unified cross-modal ensemble of skeleton and RGB data. It demonstrates improved performance and robustness across multiple benchmarks, including a new hand-centric dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[BHaRNet: 细粒度骨架动作识别 / Fine-grained Skeleton Action Recognition] --> B[核心问题/Problem: 现有方法忽略手部细微动作 / Existing methods neglect subtle hand articulations]
        A --> C[主要方法/Method: 概率双流框架 / Probabilistic Dual-Stream Framework]
        C --> C1[校准无关预处理 / Calibration-Free Preprocessing]
        C --> C2[噪声或概率融合 / Probabilistic Noisy-OR Fusion]
        C --> C3[跨模态集成 / Intra- to Cross-Modal Ensemble]
        A --> D[关键结果/Results: 多基准测试性能提升 / Performance gains across multiple benchmarks & 噪声下鲁棒性 / Robustness under noise]
    ```

- **[arXiv260105] RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection**
  - **tags:** [cv], [object detection], [small-object detection, dashcam dataset, roadside litter, long-tail distribution, transformer detectors]
  - **authors:** Tao Wu, Qing Xu, Xiangjian He, Oakleigh Weekes, James Brown, Wenting Duan
  - **institution:** University of Nottingham Ningbo China, University of Lincoln
  - **link:** https://arxiv.org/pdf/2601.00398
  - **code:** https://github.com/xq141839/RoLID-11K
  - **contributions:** 1. Introduces RoLID-11K, the first large-scale dataset for roadside litter detection from dashcam footage, featuring over 11k annotated images with extreme small-object and long-tail characteristics. 2. Provides a comprehensive benchmark of modern object detectors, including transformer and YOLO models, on this challenging task. 3. Establishes a new benchmark for extreme small-object detection in dynamic driving scenes to support scalable, low-cost litter monitoring systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RoLID-11K, a new dataset for detecting small roadside litter from dashcam videos, and benchmarks various modern object detectors on it. The results show that transformer-based models like CO-DETR achieve the best localization accuracy, while real-time models are limited by coarse feature hierarchies. The dataset aims to facilitate the development of scalable systems for automated roadside litter monitoring.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection] --> B1
        A --> B2
        A --> B3
        B1[核心问题/Problem: 路边垃圾监测困难，现有数据集不适用于行车记录仪场景/Roadside litter monitoring is challenging, existing datasets are unsuitable for dashcam footage]
        B2[主要方法/Method: 提出首个大规模行车记录仪路边垃圾检测数据集并提出基准测试/Propose the first large-scale dashcam roadside litter dataset and benchmark]
        B3[关键结果/Results: CO-DETR等Transformer模型定位最准，实时模型受限/CO-DETR and related transformers achieve best accuracy, real-time models are constrained]
    ```

- **[arXiv260105] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos**
  - **tags:** [cv], [4D reconstruction and generation], [4D Gaussian Splatting, monocular video, feed-forward reconstruction, novel view synthesis, world model]
  - **authors:** Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang
  - **institution:** NLPR & MAIS, CASIA; CreateAI
  - **link:** https://arxiv.org/pdf/2601.00393
  - **code:** https://neoverse-4d.github.io
  - **contributions:** 1. A scalable 4D world model (NeoVerse) built from diverse in-the-wild monocular videos, eliminating the need for expensive multi-view data or cumbersome pre-processing. 2. A pose-free, feed-forward 4D reconstruction method using 4D Gaussian Splatting (4DGS). 3. An online monocular degradation pattern simulation technique to enable high-quality, coherent novel-trajectory video generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes NeoVerse, a scalable 4D world model that performs feed-forward 4D reconstruction from monocular videos and generates novel-viewpoint videos. It addresses scalability limitations of prior methods by not requiring multi-view data or complex pre-processing. The model achieves state-of-the-art performance on standard benchmarks for reconstruction and generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[当前4D世界模型的可扩展性受限/Scalability limitation in current 4D world models]
        B1 --> B2[依赖昂贵多视角数据或复杂预处理/Reliance on expensive multi-view data or cumbersome pre-processing]
        C --> C1[基于单目视频的可扩展流程/Scalable pipeline using monocular videos]
        C1 --> C2[免姿态前馈4D重建/Pose-free feed-forward 4D reconstruction]
        C1 --> C3[在线单目退化模式模拟/Online monocular degradation pattern simulation]
        D --> D1[实现最先进的性能/Achieves state-of-the-art performance]
        D1 --> D2[在标准重建与生成基准上/On standard reconstruction & generation benchmarks]
    ```

- **[arXiv260105] ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis**
  - **tags:** [cv], [medical image analysis], [Kolmogorov-Arnold Network (KAN), Functional Connectivity (FC), Transformer, Autism Spectrum Disorder (ASD), ABIDE I]
  - **authors:** Tyler Ward, Abdullah Imran
  - **institution:** University of Kentucky
  - **link:** https://arxiv.org/pdf/2601.00416
  - **code:** https://github.com/tbwa233/ABFR-KAN
  - **contributions:** 1. Proposes ABFR-KAN, a novel transformer-based classification network that integrates advanced brain function representation components with Kolmogorov-Arnold Networks (KANs) to address limitations of atlas-based functional connectivity analysis. 2. Introduces a method designed to mitigate structural bias, improve anatomical conformity, and enhance the reliability of functional connectivity estimation for brain disorder diagnosis. 3. Demonstrates through extensive experiments, including cross-site evaluation and ablation studies on the ABIDE I dataset, that the proposed model consistently outperforms state-of-the-art baselines in autism spectrum disorder (ASD) classification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the issues of selection bias and lack of subject specificity in traditional atlas-based functional connectivity (FC) analysis for brain disorders. It proposes ABFR-KAN, a transformer-based classification network that incorporates novel brain function representation components and Kolmogorov-Arnold Networks (KANs) to improve FC estimation. Experiments on the ABIDE I dataset show that ABFR-KAN outperforms state-of-the-art methods in classifying autism spectrum disorder (ASD).
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis] --> B[核心问题/Problem: Atlas-based parcellation leads to selection bias and lacks subject specificity in functional connectivity analysis.]
        A --> C[主要方法/Method: Proposes a transformer-based network (ABFR-KAN) with advanced brain function representation and Kolmogorov-Arnold Networks (KANs).]
        A --> D[关键结果/Results: Outperforms state-of-the-art baselines in ASD classification on the ABIDE I dataset.]
    ```

- **[arXiv260105] Robust Assembly Progress Estimation via Deep Metric Learning**
  - **tags:** [cv], [metric learning], [assembly progress estimation, deep metric learning, quadruplet loss, anomaly detection, small-scale dataset]
  - **authors:** Kazuma Miura, Sarthak Pathak, Kazunori Umeda
  - **institution:** Chuo University
  - **link:** https://arxiv.org/pdf/2601.00422
  - **contributions:** 1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robust Assembly Progress Estimation via Deep Metric Learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 装配任务中视觉变化细微导致误分类/Misclassification due to subtle visual changes in assembly tasks]
        C[主要方法/Method: 基于四元组损失和自定义数据加载器的异常检测网络/Anomaly Quadruplet-Net with Quadruplet Loss & custom data loader]
        D[关键结果/Results: 在PC装配数据集上准确率提升1.3%，相邻任务误分类减少1.9%/1.3% accuracy improvement & 1.9% reduction in adjacent task misclassification on PC dataset]
    ```

- **[arXiv260105] Deep Delta Learning**
  - **tags:** [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]
  - **authors:** Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu
  - **institution:** Princeton University, University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2601.00417
  - **code:** https://github.com/yifanzhang-pro/deep-delta-learning
  - **contributions:** 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Delta Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[残差网络限制/ResNet Limitation]
        B1 --> B2["刚性相加偏置/Rigid Additive Bias"]
        B2 --> B3["限制复杂状态转换/Limits Complex State Transitions"]
        C --> C1[Delta 算子/Delta Operator]
        C1 --> C2["秩-1扰动/ Rank-1 Perturbation"]
        C2 --> C3["可学习几何变换/Learnable Geometric Transform"]
        C3 --> C4["动态门控/Dynamic Gating (β)"]
        D --> D1["谱分析/Spectral Analysis"]
        D1 --> D2["插值身份/投影/反射/Interpolates Identity/Projection/Reflection"]
        D --> D3["同步秩-1注入/Synchronous Rank-1 Injection"]
        D3 --> D4["控制转换谱/Controls Transition Spectrum"]
        D4 --> D5["保持稳定训练/Preserves Stable Training"]
    ```

- **[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models**
  - **tags:** [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]
  - **authors:** Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2601.00423
  - **code:** https://github.com/shengjun-zhang/VisualGRPO
  - **contributions:** 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法在多个去噪步上优化，奖励信号稀疏模糊/Existing methods suffer from sparse & ambiguous rewards over multiple steps]
        C --> C1[提出E-GRPO: 熵感知分组相对策略优化/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]
        C1 --> C2[合并低熵步为高熵SDE采样步，其他步用ODE采样/Merge low-entropy steps for SDE, use ODE elsewhere]
        C1 --> C3[引入多步分组归一化优势计算/Introduce multi-step group normalized advantage]
        D --> D1[在不同奖励设置下验证了方法的有效性/Method effectiveness demonstrated across different reward settings]
    ```

- **[arXiv260105] CPPO: Contrastive Perception for Vision Language Policy Optimization**
  - **tags:** [ai], [reinforcement learning], [contrastive perception loss, entropy shift, vision-language models, policy optimization, multimodal reasoning]
  - **authors:** Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari
  - **institution:** Huawei Technologies Canada Co. Ltd., Huawei Cloud
  - **link:** https://arxiv.org/pdf/2601.00501
  - **contributions:** 1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp
  - **Simple LLM Summary:** CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CPPO: Contrastive Perception for Vision Language Policy Optimization] --> B[核心问题/Problem: Disentangling perception and reasoning tokens in VLMs is difficult with prior methods requiring extra LLMs or indiscriminate rewards]
        A --> C[主要方法/Method: Detect perception tokens via entropy shifts under perturbed images; apply Contrastive Perception Loss (CPL) for consistency/sensitivity]
        A --> D[关键结果/Results: CPPO surpasses previous perception-rewarding methods, avoids extra models, and improves training efficiency/scalability]
    ```

- **[arXiv260105] MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation**
  - **tags:** [cv], [physics-based simulation], [motion distillation, differentiable simulation, multimodal large language model, material parameter estimation, video diffusion models]
  - **authors:** Miaowei Wang, Jakub Zadrożny, Oisin Mac Aodha, Amir Vaxman
  - **institution:** The University of Edinburgh
  - **link:** https://arxiv.org/pdf/2601.00504
  - **code:** https://wangmiaowei.github.io/MotionPhysics.github.io/
  - **contributions:** 1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[物理参数调优耗时且需专业知识/Physical parameter tuning is time-consuming and requires expertise]
        C --> C1[使用多模态大语言模型估计参数/Use multimodal LLM to estimate parameters]
        C --> C2[提出可学习运动蒸馏损失/Propose learnable motion distillation loss]
        C2 --> C2a[从视频扩散模型提取运动先验/Extract motion priors from video diffusion models]
        D --> D1[在30+场景中评估/Evaluated across 30+ scenarios]
        D --> D2[超越现有方法/Surpasses state-of-the-art]
        D --> D3[自动确定合理参数/Automatically determines plausible parameters]
    ```

- **[arXiv260105] All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations**
  - **tags:** [cv], [video restoration], [all-in-one restoration, smoothly evolving degradation, prompt learning, coarse intensity estimation, flow prompt generation]
  - **authors:** Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan
  - **institution:** Harbin Institute of Technology, University of Macau, Peking University
  - **link:** https://arxiv.org/pdf/2601.00533
  - **code:** https://github.com/Friskknight/ORCANet-SEUD
  - **contributions:** 1. Introduces the Smoothly Evolving Unknown Degradations (SEUD) scenario for video restoration, where degradation types and intensities change continuously over time. 2. Proposes ORCANet, featuring a Coarse Intensity Estimation Dehazing (CIED) module for initialization and a Flow Prompt Generation (FPG) module that generates static and dynamic prompts to capture degradation types and intensity variations. 3. Designs a flexible synthesis pipeline to generate temporally coherent videos with single, compound, and evolving degradations for training and evaluation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of all-in-one video restoration under temporally continuous and evolving weather degradations. It proposes ORCANet, a network that uses coarse intensity estimation and a novel prompting mechanism to adapt to degradation changes over time. Experiments show the method achieves superior restoration quality and temporal consistency compared to existing baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Extending all-in-one restoration to videos with temporally continuous, evolving degradations]
        C[主要方法/Method: ORCANet with CIED for initialization and FPG for static/dynamic prompt generation]
        D[关键结果/Results: Superior restoration quality, temporal consistency, and robustness]
    ```

- **[arXiv260105] FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection**
  - **tags:** [cv], [text-to-image generation], [diffusion transformers, attention localization, spectral glyph injection, training-free, text rendering]
  - **authors:** Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2601.00535
  - **contributions:** 1. Proposes a training-free framework (FreeText) that decomposes text rendering into "where to write" and "what to write" problems. 2. Introduces a method to localize writing regions using endogenous image-to-text attention with sink-like tokens and topology-aware refinement. 3. Presents Spectral-Modulated Glyph Injection (SGMI) to inject a noise-aligned glyph prior with frequency-domain modulation to enhance structure and suppress semantic leakage.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FreeText, a training-free framework to improve text rendering in Diffusion Transformer models. It localizes writing regions via attention mechanisms and injects glyph structure using spectral modulation. Experiments show it improves text readability while preserving image quality with minimal overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FreeText: Training-Free Text Rendering in Diffusion Transformers] --> B[核心问题/Problem: Diffusion models struggle with precise text rendering, especially for multi-line layouts and long-tailed scripts]
        A --> C[主要方法/Method: Decomposes into "where to write" (attention localization) and "what to write" (spectral-modulated glyph injection)]
        A --> D[关键结果/Results: Consistent gains in text readability, preserves semantic alignment and aesthetics, modest inference overhead]
    ```

- **[arXiv260105] DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction**
  - **tags:** [cv], [image editing], [drag-style editing, motion prediction, predict-and-move framework, motion supervision]
  - **authors:** Jiacheng Sui, Yujie Zhou, Li Niu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2601.00542
  - **contributions:** 1. Proposes a novel "predict-and-move" framework for drag-style image editing, which is the first of its kind. 2. Introduces an iterative method combining Motion Prediction and Motion Supervision to proactively guide handle points. 3. Proposes a dynamic adjustment mechanism for valid handle points to improve editing performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies issues like miss tracking and poor editability in existing drag-style image editing methods. It proposes DynaDrag, a new method under a "predict-and-move" framework that iteratively uses motion prediction and motion supervision to guide handle points. Experiments on face and human datasets show it outperforms previous works.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction] --> B[核心问题/Problem: 现有拖拽式图像编辑方法存在跟踪丢失、模糊跟踪、编辑性差等问题]
        A --> C[主要方法/Method: 提出首个"预测-移动"框架，迭代执行运动预测和运动监督，动态调整有效控制点]
        A --> D[关键结果/Results: 在人脸和人体数据集上的实验展示了优于先前方法的性能]
    ```

- **[arXiv260105] Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios**
  - **tags:** [cv], [image segmentation], [Segment Anything Model, zero-shot segmentation, low-level features, fine-tuning, visually non-salient]
  - **authors:** Guangqian Guo, Pengfei Chen, Yong Guo, Huafeng Chen, Boqiang Zhang, Shan Gao
  - **institution:** Northwestern Polytechnical University, University of Chinese Academy of Sciences, Max Planck Institute for Informatics, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2601.00537
  - **code:** https://guangqian-guo.github.io/VNS-SAM/
  - **contributions:** 1. Proposed VNS-SAM, a method to enhance SAM's performance in visually non-salient (low-contrast) scenarios while preserving its zero-shot generalizability. 2. Introduced two key designs: a Mask-Edge Token Interactive decoder and a Non-Salient Feature Mining module to effectively exploit SAM's low-level features. 3. Created VNS-SEG, a large-scale unified dataset with over 35K images for training and benchmarking models on various visually non-salient segmentation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem where the Segment Anything Model (SAM) struggles with visually non-salient scenarios where foreground and background have low contrast. The authors propose VNS-SAM, which enhances SAM's perception using a novel decoder and feature mining module, and introduce a new dataset called VNS-SEG. Experiments show VNS-SAM achieves superior performance, especially in zero-shot settings, demonstrating its practical potential.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[SAM在视觉非显著场景表现不佳/SAM underperforms in visually non-salient scenarios]
        Method[主要方法/Method] --> M1[提出VNS-SAM模型/Propose VNS-SAM model]
        M1 --> M2[设计Mask-Edge Token Interactive解码器/Design Mask-Edge Token Interactive decoder]
        M1 --> M3[设计Non-Salient Feature Mining模块/Design Non-Salient Feature Mining module]
        M1 --> M4[构建VNS-SEG数据集/Build VNS-SEG dataset]
        Results[关键结果/Results] --> R1[性能显著提升/Superior performance]
        Results --> R2[保持零样本泛化能力/Preserves zero-shot generalizability]
        Results --> R3[参数和计算成本低/Low parameter & computational cost]
    ```

- **[arXiv260105] A Comprehensive Dataset for Human vs. AI Generated Image Detection**
  - **tags:** [cv], [image forensics], [AI-Generated Images, Detection Techniques, Synthetic Media, Generative AI, Multimodal AI]
  - **authors:** Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das
  - **institution:** Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San José State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI
  - **link:** https://arxiv.org/pdf/2601.00553
  - **contributions:** 1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Comprehensive Dataset for Human vs. AI Generated Image Detection"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["AI生成图像难以区分/AI-Generated Images Hard to Distinguish"]
        Problem --> P2["误导性内容传播/Spread of Misleading Content"]
        Method --> M1["构建MS COCOAI数据集/Build MS COCOAI Dataset"]
        Method --> M2["使用五种生成器/Use Five Generators"]
        Results --> R1["提供96000个数据点/Provide 96k Datapoints"]
        Results --> R2["定义两项检测任务/Define Two Detection Tasks"]
    ```

- **[arXiv260105] SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array**
  - **tags:** [cv], [medical imaging reconstruction], [photoacoustic imaging, point cloud, iterative reconstruction, irregular array, hierarchical optimization]
  - **authors:** Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li
  - **institution:** Peking University, Nanjing University, Pohang University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00551
  - **code:** https://github.com/ShuangLiPKU/SlingBAG-Pro
  - **contributions:** 1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SingBAG Pro: 3D光声成像/SlingBAG Pro: 3D Photoacoustic Imaging] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[不规则阵列重建慢/Irregular Array Reconstruction is Slow]
        C --> C1[点云迭代与分层优化/Point Cloud Iteration & Hierarchical Optimization]
        D --> D1[速度提升2.2倍/Speedup of 2.2x]
        D --> D2[仿真与活体验证/Simulation & In Vivo Validation]
    ```

- **[arXiv260105] A Cascaded Information Interaction Network for Precise Image Segmentation**
  - **tags:** [cv], [image segmentation], [cascaded convolutional neural network, Global Information Guidance Module, multi-scale feature fusion]
  - **authors:** Hewen Xiao, Jie Mei, Guangfu Ma, Weiren Wu
  - **institution:** Harbin Institute of Technology, Shenzhen
  - **link:** https://arxiv.org/pdf/2601.00562
  - **contributions:** 1. Proposes a cascaded convolutional neural network architecture for robust image segmentation. 2. Introduces a novel Global Information Guidance Module to fuse low-level texture and high-level semantic features. 3. Demonstrates superior performance on benchmark datasets, particularly in cluttered or blurred environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of robust image segmentation in complex scenarios by proposing a cascaded CNN with a novel Global Information Guidance Module. This module effectively fuses multi-scale features to overcome the limitations of single-scale extraction. Experimental results show the method outperforms state-of-the-art approaches, highlighting its potential for practical robotic applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Cascaded Information Interaction Network for Precise Image Segmentation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[复杂场景下鲁棒分割是挑战/Robust segmentation in complex scenarios is a challenge]
        C --> C1[级联卷积神经网络/Cascaded CNN]
        C --> C2[全局信息引导模块/Global Information Guidance Module]
        C2 --> C3[融合多尺度特征/Fuse multi-scale features]
        D --> D1[在基准测试中表现优异/Superior performance on benchmarks]
        D --> D2[超越现有方法/Outperforms SOTA methods]
    ```

- **[arXiv260105] AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models**
  - **tags:** [cv], [multimodal evaluation], [unified multimodal models, world knowledge, deterministic evaluation, benchmark, reasoning]
  - **authors:** Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu
  - **institution:** University of Hong Kong, The Hong Kong Polytechnic University, The Chinese University of Hong Kong, Huawei Research
  - **link:** https://arxiv.org/pdf/2601.00561
  - **contributions:** 1. Proposes AEGIS, a comprehensive multi-task benchmark for evaluating world knowledge capabilities of Unified Multimodal Models across visual understanding, generation, editing, and interleaved generation. 2. Introduces Deterministic Checklist-based Evaluation (DCE), a protocol using atomic "Y/N" judgments to replace ambiguous prompt-based scoring for more reliable evaluation. 3. Conducts extensive experiments revealing severe world knowledge deficits in current UMMs, performance degradation with complex reasoning, and the partial mitigation offered by plug-in reasoning modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a critical gap in evaluating Unified Multimodal Models' (UMMs) ability to apply world knowledge. To address this, it proposes the AEGIS benchmark and a Deterministic Checklist-based Evaluation (DCE) protocol. The experiments show that current UMMs have significant world knowledge deficiencies, especially in complex reasoning tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Multimodal Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有基准不足/Existing benchmarks are insufficient]
        C --> C1[提出AEGIS基准/Propose AEGIS benchmark]
        C --> C2[提出DCE评估协议/Propose DCE evaluation protocol]
        D --> D1[模型存在知识缺陷/Models have knowledge deficits]
        D --> D2[复杂推理性能下降/Performance degrades with complex reasoning]
    ```

- **[arXiv260105] GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval**
  - **tags:** [cv], [video moment retrieval], [zero-shot, granularity mismatch, query rewriting, caption generation, vision-language models]
  - **authors:** Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim
  - **institution:** Chung-Ang University, Korea Advanced Institute of Science and Technology (KAIST), Electronics and Telecommunications Research Institute (ETRI)
  - **link:** https://arxiv.org/pdf/2601.00584
  - **contributions:** 1. Proposes a training-free framework (GranAlign) to address semantic granularity mismatch in zero-shot video moment retrieval. 2. Introduces granularity-based query rewriting to generate queries at varied semantic levels. 3. Introduces query-aware caption generation to embed query intent into video content descriptions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the granularity mismatch problem in zero-shot video moment retrieval by proposing GranAlign, a training-free framework that uses query rewriting and query-aware caption generation to align multi-level semantic representations. The method achieves state-of-the-art performance on three major benchmarks, including a 3.23% mAP@avg improvement on QVHighlights.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[GranAlign] --> Problem(核心问题: 粒度不匹配 / Problem: Granularity Mismatch)
        Root --> Method(主要方法: 粒度感知对齐 / Method: Granularity-Aware Alignment)
        Root --> Results(关键结果: SOTA性能 / Results: SOTA Performance)
        Problem --> P1[查询与视频内容语义粒度不一致 / Query-Video Semantic Granularity Mismatch]
        Method --> M1[粒度查询重写 / Granularity-based Query Rewriting]
        Method --> M2[查询感知描述生成 / Query-aware Caption Generation]
        Results --> R1[在三大基准测试中达到SOTA / SOTA on Three Major Benchmarks]
        Results --> R2[在QVHighlights上显著提升 / Notable Improvement on QVHighlights]
    ```

- **[arXiv260105] Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception**
  - **tags:** [cv], [object detection], [RGB-Infrared fusion, modality imbalance, cross-modal learning, optimization bias, multimodal perception]
  - **authors:** Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu
  - **institution:** Tongji University, Soochow University, The University of Arizona
  - **link:** https://arxiv.org/pdf/2601.00598
  - **contributions:** 1. Proposes the Modality Dominance Index (MDI) to quantify optimization bias caused by asymmetric modality characteristics in RGB-IR fusion. 2. Develops the Modality Dominance-Aware Cross-modal Learning (MDACL) framework to regulate cross-modal optimization. 3. Introduces Hierarchical Cross-modal Guidance (HCG) and Adversarial Equilibrium Regularization (AER) within MDACL to enhance feature alignment and balance optimization dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the optimization bias problem in RGB-Infrared multimodal object detection, where disparities in information density cause training to over-rely on a dominant modality. The authors propose a Modality Dominance Index (MDI) to measure this bias and a Modality Dominance-Aware Cross-modal Learning (MDACL) framework to mitigate it. Experiments show that MDACL effectively reduces optimization bias and achieves state-of-the-art performance on RGB-IR benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modality Dominance-Aware Optimization for Embodied RGB–Infrared Perception] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[模态优化偏差/Modality Optimization Bias]
        C --> C1[模态主导指数/MDI]
        C --> C2[模态主导感知学习框架/MDACL]
        C2 --> C2_1[分层跨模态指导/HCG]
        C2 --> C2_2[对抗均衡正则化/AER]
        D --> D1[缓解优化偏差/Mitigates Bias]
        D --> D2[实现SOTA性能/Achieves SOTA]
    ```

- **[arXiv260105] SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation**
  - **tags:** [cv], [text-to-motion generation], [machine unlearning, diffusion models, motion safety, continuous kinematics, safe dataset]
  - **authors:** Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang
  - **institution:** The Australian National University, Peking University
  - **link:** https://arxiv.org/pdf/2601.00590
  - **code:** https://github.com/YilingWang98/SafeMo
  - **contributions:** 1. Proposed SafeMo, a trustworthy motion generation framework integrating a two-stage Minimal Motion Unlearning (MMU) strategy for safe generation in continuous space. 2. Introduced the first safe text-to-motion dataset, SafeMoVAE-29K, with rewritten safe prompts and refined motions. 3. Demonstrated superior safety-utility trade-offs, achieving significantly higher forget-set FID scores than prior state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses safety concerns in text-to-motion generation by proposing SafeMo, a framework that uses a two-stage machine unlearning strategy to remove unsafe behaviors while preserving motion quality in continuous space. It also introduces a new safe dataset for training. Experiments show SafeMo effectively forgets unsafe prompts while maintaining good performance on safe ones, outperforming previous methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有方法缺陷/Existing Method Flaws]
        B1 --> B1a[离散代码本替换导致性能下降/Discrete codebook replacement degrades benign performance]
        B1 --> B1b[量化导致伪影和不连贯/Quantization causes artifacts & jerky transitions]
        B --> B2[数据集包含不安全内容/Datasets contain unsafe intents & motions]
        C --> C1[提出SafeMo框架/Propose SafeMo Framework]
        C1 --> C1a[最小化运动遗忘策略/Minimal Motion Unlearning (MMU)]
        C1 --> C1b[连续空间生成/Generation in Continuous Space]
        C --> C2[构建安全数据集/Build Safe Dataset SafeMoVAE-29K]
        D --> D1[有效遗忘不安全提示/Effective Forgetting on Unsafe Prompts]
        D1 --> D1a[遗忘集FID显著提升/Forget-set FID greatly improved]
        D --> D2[保持良性性能/Benign Performance Preserved]
    ```

- **[arXiv260105] Noise-Robust Tiny Object Localization with Flows**
  - **tags:** [cv], [object detection], [Tiny Object Detection, Noise Robustness, Normalizing Flows, Uncertainty-Guided Optimization, Flow-Based Error Modeling]
  - **authors:** Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao
  - **institution:** Beihang University, Communication University of China, National University of Singapore
  - **link:** https://arxiv.org/pdf/2601.00617
  - **contributions:** 1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Noise-Robust Tiny Object Localization with Flows] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[微小物体检测对标注噪声敏感/Tiny object detection is sensitive to annotation noise]
        C --> C1[基于归一化流的误差建模/Flow-based error modeling]
        C --> C2[不确定性引导的优化/Uncertainty-guided optimization]
        D --> D1[在多个数据集上验证有效/Validated on multiple datasets]
        D --> D2[提升DINO基线1.2% AP/Boosts DINO baseline by 1.2% AP]
    ```

- **[arXiv260105] HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis**
  - **tags:** [cv], [medical image analysis], [Hypergraph Neural Network, Learning Using Privileged Information, Knowledge Distillation, Severed Graph Strategy, dual-stream distillation]
  - **authors:** Shuren Gabriel Yu, Sikang Ren, Yongji Tian
  - **institution:** Tsinghua University, Beijing Tiantan Hospital
  - **link:** https://arxiv.org/pdf/2601.00626
  - **contributions:** 1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HyperPriv-EPN] --> B[核心问题/Problem: Pre-op prognosis lacks semantic insights from post-op reports]
        A --> C[主要方法/Method: Hypergraph LUPI with Severed Graph Strategy & dual-stream distillation]
        A --> D[关键结果/Results: SOTA accuracy & survival stratification on 311 patients]
    ```

- **[arXiv260105] RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation**
  - **tags:** [cv], [human pose estimation], [real-time 3D pose estimation, biomechanical analysis, SmoothNet, multi-camera tracking, Unity visualization]
  - **authors:** Junxiao Xue, Pavel Smirnov, Ziao Li, Yunyun Shi, Shi Chen, Xinyi Yin, Xiaohan Yue, Lei Wang, Yiduo Wang, Feng Lin, Yijia Chen, Xiao Ma, Xiaoran Yan, Qing Zhang, Fengjian Xue, Xuecheng Wu
  - **institution:** Zhejiang Lab, Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2601.00625
  - **contributions:** 1. A unified, end-to-end pipeline for real-time 3D human pose estimation and motion analysis from multi-camera RGB video for rehabilitation. 2. A fast tracking method for multi-person scenarios in medical rehabilitation, achieving tracking in under 1ms per frame. 3. A modification of SmoothNet for real-time posture estimation to reduce errors and produce smoother, more accurate motion states.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes RePose, a real-time framework for 3D human pose estimation and biomechanical analysis to assist rehabilitation training. The method uses a multi-camera pipeline, a fast multi-person tracker, and a modified SmoothNet for smooth pose estimation, integrated with Unity for real-time feedback and muscle stress visualization. The system aims to provide immediate guidance to help patients perform exercises correctly.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RePose: 康复实时3D姿态估计与生物力学分析框架<br/>RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Need for automatic, real-time monitoring and assessment in rehabilitation training] --> B1[远程康复监控需求<br/>Remote rehabilitation monitoring need]
        C[主要方法/Method<br/>Real-time 3D pose estimation and analysis pipeline] --> C1[多摄像头RGB视频输入<br/>Multi-camera RGB video input]
        C --> C2[快速多人跟踪 (<1ms/帧)<br/>Fast multi-person tracking (<1ms/frame)]
        C --> C3[改进SmoothNet用于实时姿态估计<br/>Modified SmoothNet for real-time pose estimation]
        C --> C4[Unity平台实时监控与肌肉应力显示<br/>Unity platform for real-time monitoring & muscle stress display]
        D[关键结果/Results<br/>A unified framework for real-time motion correction and guidance] --> D1[实时反馈辅助正确康复训练<br/>Real-time feedback to assist correct rehabilitation training]
    ```

- **[arXiv260105] Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach**
  - **tags:** [cv], [image classification], [transfer learning, vision transformer, DenseNet, sprout detection, shelf-life prediction]
  - **authors:** Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande
  - **institution:** TCS Research
  - **link:** https://arxiv.org/pdf/2601.00645
  - **contributions:** 1. Designed a high-precision binary classifier for potato sprout detection using transfer learning, achieving 98.03% accuracy with DenseNet. 2. Developed an advanced multi-class predictor for estimating weight loss and forecasting remaining shelf-life, demonstrating best performance with coarse class divisions. 3. Demonstrated the feasibility of integrating image-based deep learning models into automated sorting systems for improved inventory management and reduced food waste.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using transfer learning with CNN and Vision Transformer models for non-invasive quality detection of stored potatoes. The method involves a binary classifier for sprout detection and a multi-class predictor for weight loss and shelf-life estimation. The study concludes that this approach is effective for automated sorting, with coarse class divisions yielding robust performance for practical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Quality Detection of Stored Potatoes via Transfer Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[监测存储马铃薯质量/Monitoring stored potato quality]
        B --> B2[检测发芽、估计重量损失/Predicting sprouting, weight loss & shelf-life]
        C --> C1[使用预训练模型/Using pre-trained models (ResNet, VGG, DenseNet, ViT)]
        C --> C2[设计二分类与多分类器/Designing binary & multi-class classifiers]
        D --> D1[高精度发芽检测/High-accuracy sprout detection (98.03%)]
        D --> D2[粗粒度保质期预测更优/Coarse class shelf-life prediction works best]
        D --> D3[支持自动化分拣系统/Supports automated sorting systems]
    ```

- **[arXiv260105] CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [hallucination mitigation, contrastive decoding, vision-language models, training-free]
  - **authors:** Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman
  - **institution:** Indian Institute of Technology (ISM) Dhanbad, Transmute AI, National University of Singapore
  - **link:** https://arxiv.org/pdf/2601.00659
  - **code:** https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models
  - **contributions:** 1. Proposes a novel hallucinated model that captures hallucination effects by selectively removing key text tokens, addressing the limitation of visual information propagation. 2. Introduces Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. 3. Presents CRoPS, a training-free framework that significantly improves hallucination metrics (e.g., 20% CHAIR score gain) across multiple benchmarks and LVLM families.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of hallucinations in Large Vision-Language Models (LVLMs). It proposes CRoPS, a training-free framework that introduces a novel hallucinated model based on text token removal and a Generalized Contrastive Decoding method to mitigate diverse hallucination sources. The method achieves significant improvements in hallucination metrics across several benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models] --> B[核心问题/Problem: LVLMs tend to generate hallucinated content, undermining reliability.]
        A --> C[主要方法/Method: Novel hallucinated model via text token removal + Generalized Contrastive Decoding.]
        A --> D[关键结果/Results: Improves CHAIR scores by 20%, gains across 6 benchmarks & 3 LVLM families.]
    ```

- **[arXiv260105] Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network**
  - **tags:** [cv], [3D reconstruction], [SAR tomography, point cloud, deep learning, building height estimation, dual-topology network]
  - **authors:** Zhaiyu Chen, Yuanyuan Wang, Yilei Shi, Xiao Xiang Zhu
  - **institution:** Technical University of Munich (TUM)
  - **link:** https://arxiv.org/pdf/2601.00658
  - **code:** https://github.com/zhu-xlab/tomosar2height
  - **contributions:** 1. Proposes a novel learning-based framework for generating high-resolution building height maps directly from raw, noisy TomoSAR point clouds. 2. Introduces a dual-topology network architecture that alternates between a point branch and a grid branch to jointly model irregular scatterer features and enforce spatial consistency. 3. Demonstrates the first proof of concept for large-scale urban height mapping from TomoSAR and shows the framework's extensibility to incorporate optical imagery for enhanced quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of accurate building height reconstruction from noisy and incomplete spaceborne TomoSAR point clouds. It proposes a dual-topology deep learning network that processes both point-based and grid-based representations to denoise and inpaint the data, producing continuous height maps. Experiments on urban datasets validate the method's effectiveness, and the framework is shown to be extensible by fusing with optical imagery.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[TomoSAR点云噪声多、分布不均、有数据空洞/TomoSAR point clouds are noisy, anisotropic, and have voids]
        C --> C1[提出双拓扑网络/Propose a dual-topology network]
        C1 --> C1_1[点分支处理不规则散射体/Point branch models irregular scatterers]
        C1 --> C1_2[网格分支保证空间一致性/Grid branch enforces spatial consistency]
        D --> D1[有效生成高分辨率高度图/Effectively produces high-resolution height maps]
        D --> D2[首个大规模城市高度测绘概念验证/First proof of concept for large-scale urban height mapping]
        D --> D3[可扩展融合光学影像/Extensible to incorporate optical imagery]
    ```

- **[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation**
  - **tags:** [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]
  - **authors:** Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang
  - **institution:** KAIST, NTU Singapore, DeepAuto.ai
  - **link:** https://arxiv.org/pdf/2601.00664
  - **code:** https://taekyungki.github.io/AvatarForcing
  - **contributions:** 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[缺乏真正互动/Lacks truly interactive communication]
        Problem --> P2[单向反应缺乏情感/One-way responses lack emotional engagement]
        Method[主要方法/Method] --> M1[扩散驱动框架/Diffusion forcing framework]
        Method --> M2[无标签直接偏好优化/Label-free direct preference optimization]
        Results[关键结果/Results] --> R1[低延迟实时交互/Low-latency real-time interaction (~500ms)]
        Results --> R2[6.8倍加速/6.8x speedup]
        Results --> R3[80%用户偏好/Over 80% user preference]
    ```

- **[arXiv260105] Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians**
  - **tags:** [cv], [video generation], [3D Gaussians, camera-controlled generation, temporal consistency, single-image-conditioned]
  - **authors:** Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson
  - **institution:** University of Glasgow
  - **link:** https://arxiv.org/pdf/2601.00678
  - **code:** https://melonienimasha.github.io/Pixel-to-4D-Website/
  - **contributions:** 1. Proposes a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion from a single image in a single forward pass., 2. Enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into rendered frames., 3. Achieves state-of-the-art video quality and inference efficiency on multiple datasets (KITTI, Waymo, RealEstate10K, DL3DV-10K).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of generating temporally consistent and geometrically sound videos from a single image with user-controlled camera paths. It proposes Pixel-to-4D, a method that builds a dynamic 3D Gaussian representation of the scene in one pass to enable fast, camera-guided video synthesis. The approach demonstrates superior video quality and efficiency compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pixel-to-4D: Camera-Controlled Image-to-Video Generation<br>Pixel-to-4D: 相机控制的图像到视频生成] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>缺乏用户可控性，现有方法在相机运动建模、时间一致性和几何完整性方面存在挑战<br>Lack of user controllability, challenges in camera motion modeling, temporal consistency, and geometric integrity]
        C[主要方法/Method<br>构建动态3D高斯场景表示，单次前向传播采样物体运动<br>Construct dynamic 3D Gaussian scene representation, sample object motion in a single forward pass]
        D[关键结果/Results<br>在多个数据集上实现SOTA视频质量和推理效率<br>Achieves SOTA video quality and inference efficiency on multiple datasets]
    ```

- **[arXiv260105] DefVINS: Visual-Inertial Odometry for Deformable Scenes**
  - **tags:** [cv], [visual-inertial odometry], [deformable scenes, observability analysis, embedded deformation graph, IMU anchoring]
  - **authors:** Samuel Cerezo, Javier Civera
  - **institution:** Universidad de Zaragoza
  - **link:** https://arxiv.org/pdf/2601.00702
  - **contributions:** 1. A VIO framework (DefVINS) that explicitly separates rigid motion (IMU-anchored) from non-rigid deformation (modeled by an embedded deformation graph). 2. An observability analysis characterizing how inertial measurements constrain rigid motion and identify modes in deformable scenes. 3. A conditioning-based activation strategy that progressively enables non-rigid degrees of freedom to prevent ill-posed updates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DefVINS, a visual-inertial odometry framework designed for deformable scenes. It separates rigid and non-rigid motion, uses an observability analysis to guide a progressive activation strategy for deformation, and shows improved robustness in non-rigid environments through ablation studies.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DefVINS: Visual-Inertial Odometry for Deformable Scenes] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[刚性假设失效 / Rigidity Assumption Violated]
        B --> B2[VIO在非刚性场景中漂移 / VIO Drift in Non-Rigid Scenes]
        C --> C1[分离刚性状态与非刚性形变 / Separate Rigid & Non-Rigid State]
        C --> C2[可观测性分析与IMU锚定 / Observability Analysis & IMU Anchoring]
        C --> C3[基于条件的渐进激活 / Conditioning-Based Progressive Activation]
        D --> D1[提升非刚性环境鲁棒性 / Improved Robustness in Non-Rigid Environments]
        D --> D2[消融实验验证有效性 / Ablation Studies Validate Benefits]
    ```

- **[arXiv260105] Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks**
  - **tags:** [cv], [image demosaicing], [isotropic networks, spatial downsampling, joint-demosaicing-and-denoising (JDD), DeepMAD, JD3Net]
  - **authors:** Cory Fan, Wenchao Zhang
  - **institution:** Cornell University, OmniVision Technologies
  - **link:** https://arxiv.org/pdf/2601.00703
  - **contributions:** 1. Proposes that spatial downsampling can improve the efficiency and performance of isotropic networks for demosaicing, contrary to common design practices. 2. Designs and validates simple fully convolutional networks with downsampling using a mathematical architecture design technique adapted from DeepMAD. 3. Introduces JD3Net, a downsampled variant, which demonstrates strong empirical performance on various image demosaicing and JDD tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational inefficiency of isotropic networks for image demosaicing on mobile platforms. It proposes using spatial downsampling within these networks to improve efficiency and performance, validating the claim through network design and empirical testing of a model called JD3Net. The results show that the downsampled networks achieve strong performance on demosaicing and joint-demosaicing-and-denoising tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("移动设备上深度学习去马赛克的计算成本高/High computational cost of deep learning demosaicing on mobile")
        Method --> M1("在各项同性网络中使用空间下采样/Using spatial downsampling in isotropic networks")
        Method --> M2("基于DeepMAD设计全卷积网络/Designing fully convolutional networks based on DeepMAD")
        Method --> M3("提出JD3Net模型/Proposing the JD3Net model")
        Results --> R1("下采样提高效率和性能/Downsampling improves efficiency and performance")
        Results --> R2("JD3Net在多种任务上表现强劲/JD3Net shows strong performance on various tasks")
    ```

- **[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization**
  - **tags:** [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]
  - **authors:** Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang
  - **institution:** National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2601.00705
  - **contributions:** 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GS-SLAM的残差驱动致密化效率低/Inefficient residual-driven densification in GS-SLAM]
        C --> C1[一次性密集初始化/One-shot dense initialization]
        C1 --> C2[使用DINOv3特征与置信内点分类器/Using DINOv3 features & confidence-aware inlier classifier]
        C2 --> C3[多视角三角化生成高斯先验/Multi-view triangulation for Gaussian prior]
        D --> D1[收敛加速~20%/~20% faster convergence]
        D --> D2[更高渲染保真度/Higher rendering fidelity]
        D --> D3[实时性能达925 FPS/Real-time performance up to 925 FPS]
    ```

- **[arXiv260105] Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection**
  - **tags:** [cv], [continual learning], [multi-level feature fusion, catastrophic forgetting, visual quality inspection]
  - **authors:** Johannes C. Bauer, Paul Geng, Stephan Trattnig, Petr Dokládal, Rüdiger Daub
  - **institution:** Technical University of Munich, MINES Paris, Fraunhofer IGCV
  - **link:** https://arxiv.org/pdf/2601.00725
  - **contributions:** 1. Proposes a multi-level feature fusion (MLFF) approach for continual learning that utilizes representations from different depths of a pretrained network. 2. Demonstrates that MLFF matches end-to-end training performance for quality inspection tasks while using significantly fewer trainable parameters. 3. Shows that the approach reduces catastrophic forgetting and improves generalization robustness to new product types or defects.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of continual learning for visual quality inspection in volatile manufacturing scenarios like remanufacturing. It proposes a Multi-Level Feature Fusion (MLFF) method that fuses features from different network depths to enable efficient model adaptation. The results show that MLFF achieves performance comparable to full retraining with fewer parameters, while also mitigating catastrophic forgetting and improving robustness to new defects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection] --> B[核心问题/Problem: 模型在动态场景（如再制造）中需要持续适应，面临灾难性遗忘和计算效率挑战]
        A --> C[主要方法/Method: 提出多层级特征融合方法，利用预训练网络不同深度的表征]
        A --> D[关键结果/Results: 性能匹配端到端训练，参数更少，减少遗忘，提升泛化鲁棒性]
    ```

- **[arXiv260105] Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model**
  - **tags:** [ai], [domain adaptation], [data shift detection, performance degradation monitoring, vision-language model, confidence-based indicator, digital pathology]
  - **authors:** Hao Guan, Li Zhou
  - **institution:** Brigham and Women's Hospital, Harvard Medical School
  - **link:** https://arxiv.org/pdf/2601.00716
  - **contributions:** 1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[VLM性能在数据偏移后下降/VLM performance degrades after data shift]
        C --> C1[开发输入数据偏移检测工具/DomainSAT toolbox for input shift]
        C --> C2[提出基于置信度的输出监测指标/Confidence-based output indicator]
        D --> D1[输入偏移检测有效但不总对应性能下降/Input shift detection effective but not always correlates with degradation]
        D --> D2[置信度指标与性能下降密切相关/Confidence indicator closely related to degradation]
        D --> D3[结合两者实现更可靠的监测/Combining both enables more reliable monitoring]
    ```

- **[arXiv260105] Grading Handwritten Engineering Exams with Multimodal Large Language Models**
  - **tags:** [mlsys], [multi-modal inference], [multimodal LLM, handwritten exam grading, reference grounding, ensemble grading, deterministic validation]
  - **authors:** Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec
  - **institution:** University of Ljubljana, Faculty of Electrical Engineering
  - **link:** https://arxiv.org/pdf/2601.00730
  - **contributions:** 1. An end-to-end workflow for grading scanned handwritten engineering exams using multimodal LLMs that preserves standard paper-based exam processes, requiring only a handwritten reference solution and grading rules from the lecturer. 2. A multi-stage reliability design featuring format/presence checks, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. 3. Empirical evaluation demonstrating the pipeline's effectiveness with state-of-the-art backends (GPT-5.2, Gemini-3 Pro), achieving ≈8-point mean absolute difference to lecturer grades and low bias, while ablations confirm the necessity of structured prompting and reference grounding.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an automated workflow for grading handwritten STEM exams using multimodal large language models. The method uses a lecturer's handwritten reference solution and grading rules, processed through a multi-stage pipeline with checks and ensemble grading for reliability. Evaluation shows the system achieves close agreement with human grades, confirming that structured prompting and reference grounding are essential for accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Grading Handwritten Engineering Exams with Multimodal LLMs] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动评分耗时且难以扩展/Manual grading is slow and hard to scale]
        C --> C1[多模态LLM端到端工作流/Multimodal LLM end-to-end workflow]
        C --> C2[参考解决方案与分级规则/Reference solution & grading rules]
        C --> C3[多阶段可靠设计/Multi-stage reliability design]
        D --> D1[≈8分平均绝对差/≈8-point mean absolute difference]
        D --> D2[低偏差与触发率/Low bias & trigger rate]
        D --> D3[结构化提示与参考至关重要/Structured prompting & reference essential]
    ```

- **[arXiv260105] Unified Primitive Proxies for Structured Shape Completion**
  - **tags:** [cv], [3D shape completion], [primitive proxies, structured shape completion, primitive assembly, online target updates, Chamfer distance]
  - **authors:** Zhaiyu Chen, Yuqing Wang, Xiao Xiang Zhu
  - **institution:** Technical University of Munich, Munich Center for Machine Learning
  - **link:** https://arxiv.org/pdf/2601.00759
  - **code:** https://unico-completion.github.io
  - **contributions:** 1. Proposes UniCo, a model that predicts a complete set of primitives (geometry, semantics, inlier membership) in a single feed-forward pass for structured shape completion. 2. Introduces primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. 3. Presents a training strategy that couples primitives and points with online target updates for consistent optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of structured 3D shape completion from partial scans. It proposes UniCo, a model that uses primitive proxies to jointly predict a complete set of parametric primitives in a single pass, outperforming baselines by significantly lowering Chamfer distance and improving normal consistency.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Unified Primitive Proxies for Structured Shape Completion] --> B(核心问题/Problem: Incomplete 3D scans lack structural regularities for downstream tasks)
    A --> C(主要方法/Method: UniCo model with primitive proxies and a dedicated decoding pathway)
    A --> D(关键结果/Results: Lowers Chamfer distance by up to 50%, improves normal consistency by up to 7%)
    ```

- **[arXiv260105] Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection**
  - **tags:** [ai], [audio deepfake detection], [Multimodal Large Language Models, audio deepfake detection, zero-shot, fine-tuning, multi-prompt]
  - **authors:** Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall
  - **institution:** Indian Institute of Technology Ropar, Birla Institute of Technology and Science Pilani Hyderabad Campus, Monash University
  - **link:** https://arxiv.org/pdf/2601.00777
  - **contributions:** 1. Pioneering exploration of Multimodal Large Language Models for audio deepfake detection, a largely unexplored area. 2. Introduction of a text-aware, context-rich, question-answer based multi-prompt approach to facilitate multimodal understanding for the task. 3. Comprehensive evaluation of models (Qwen2-Audio-7B-Instruct, SALMONN) in zero-shot and fine-tuned modes, demonstrating their potential on in-domain data with minimal supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the use of Multimodal Large Language Models for detecting audio deepfakes by combining audio inputs with text prompts. The method employs a multi-prompt, question-answer approach and evaluates models in zero-shot and fine-tuned settings. The results show that while models struggle without training and on out-of-domain data, they achieve promising performance on in-domain data with minimal supervision, indicating a viable path forward.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Investigating MLLMs for Audio Deepfake Detection<br/>探究MLLMs用于音频深度伪造检测"] --> Problem
        Root --> Method
        Root --> Results
        Problem["MLLMs for audio deepfakes unexplored<br/>MLLMs用于音频深度伪造检测未被探索"]
        Method["Audio + Multi-prompt QA approach<br/>音频+多提示问答方法"]
        Results["Poor zero-shot, good fine-tuned on in-domain data<br/>零样本效果差，域内微调效果好"]
    ```

- **[arXiv260105] Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection**
  - **tags:** [cv], [deepfake detection], [self-supervised learning, auxiliary task, feature fusion, cross-dataset generalization, local directional pattern]
  - **authors:** Shukesh Reddy, Srijan Das, Abhijit Das
  - **institution:** Birla Institute of Technology and Science, Pilani; University of North Carolina at Charlotte
  - **link:** https://arxiv.org/pdf/2601.00789
  - **contributions:** 1. Proposes a novel framework that uses self-supervised learning as an auxiliary task to optimize the primary task of generalized deepfake detection. 2. Introduces a feature fusion strategy to combine representations from the self-supervised and primary tasks, creating a more powerful and unique feature set. 3. Demonstrates superior cross-dataset generalization performance on multiple deepfake datasets compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of poor cross-dataset generalization in deepfake detection. The proposed method, Fusion-SSAT, leverages self-supervised learning as an auxiliary task and fuses its features with those from the primary detection task to create a more robust representation. The results show that this approach achieves better generalizability across multiple datasets than current state-of-the-art detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection] --> B(核心问题/Problem: Poor cross-dataset generalization of deepfake detectors)
        A --> C(主要方法/Method: Fuse features from self-supervised auxiliary task with primary detection task)
        A --> D(关键结果/Results: Better generalizability on cross-dataset evaluation)
    ```

- **[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing**
  - **tags:** [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]
  - **authors:** Sunny Gupta, Amit Sethi
  - **institution:** Indian Institute of Technology Bombay
  - **link:** https://arxiv.org/pdf/2601.00785
  - **code:** github.com/sunnyinAI/FedHypeVAE
  - **contributions:** 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FedHypeVAE] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[非IID数据与隐私挑战/non-IID & Privacy]
        C --> C1[超网络生成条件VAE/Hypernetwork-Generated Conditional VAE]
        C --> C2[差分隐私训练/Differentially Private Training]
        C --> C3[MMD对齐与正则化/MMD Alignment & Regularization]
        D --> D1[个性化与隐私统一/Unified Personalization & Privacy]
        D --> D2[可控多域合成/Controllable Multi-Domain Synthesis]
    ```

- **[arXiv260105] Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI**
  - **tags:** [cv], [medical image segmentation], [U-Net, layer normalization, instance-batch normalization, left ventricle segmentation, cardiac MRI]
  - **authors:** Wenhui Chu, Nikolaos V. Tsekos
  - **institution:** University of Houston
  - **link:** https://arxiv.org/pdf/2601.00794
  - **contributions:** 1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies—layer normalization and a combined instance-batch normalization—to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[左心室分割对临床诊断至关重要/Left ventricle segmentation is critical for clinical diagnosis]
        C --> C1[提出LNU-Net和IBU-Net/Propose LNU-Net and IBU-Net]
        C1 --> C2[基于U-Net，采用不同归一化策略/Based on U-Net with different normalization strategies]
        D --> D1[在805张MRI图像上评估/Evaluated on 805 MRI images]
        D1 --> D2[性能优于现有方法/Outperforms state-of-the-art approaches]
    ```

- **[arXiv260105] Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures**
  - **tags:** [other], [quantum computing], [quantum dot, charge stability diagram, automated characterization, machine learning, image processing]
  - **authors:** Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak
  - **institution:** National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)
  - **link:** https://arxiv.org/pdf/2601.00067
  - **contributions:** 1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method's effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Automated electrostatic characterization of quantum dot devices<br>量子点器件自动静电表征] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Manual interpretation of CSDs is slow and error-prone<br>CSD手动解释慢且易错]
        C --> C1[Integrates ML, image processing, object detection<br>集成ML、图像处理、目标检测]
        D --> D1[Enables rapid extraction of device parameters (lever arms, couplings)<br>实现器件参数快速提取]
    ```

- **[arXiv260105] Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging**
  - **tags:** [cv], [medical image classification], [Convolutional Neural Network, Transfer Learning, Chest X-ray, Pediatric Pneumonia, RegNet]
  - **authors:** Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani
  - **institution:** Zahedan University of Medical Sciences, Guilan University of Medical Sciences
  - **link:** https://arxiv.org/pdf/2601.00041
  - **contributions:** 1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep Learning Approach for Pediatric Pneumonia Diagnosis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[儿童肺炎诊断困难 / Pediatric Pneumonia Diagnosis is Challenging]
        C --> C1[使用预训练CNN进行迁移学习 / Use Pretrained CNNs with Transfer Learning]
        C --> C2[评估ResNetRS, RegNet, EfficientNetV2 / Evaluate ResNetRS, RegNet, EfficientNetV2]
        D --> D1[RegNet性能最佳 / RegNet Achieved Best Performance]
        D --> D2[准确率92.4%, 敏感度90.1% / Accuracy 92.4%, Sensitivity 90.1%]
    ```

- **[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes**
  - **tags:** [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]
  - **authors:** Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani
  - **institution:** Ben Gurion University of the Negev, Tel Aviv University
  - **link:** https://arxiv.org/pdf/2601.00012
  - **code:** https://github.com/Shaharak88/neural-brain-fields
  - **contributions:** 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>EEG data challenges: low SNR, variability, limited datasets"] --> P1["具体挑战/Challenges<br>Varying length, low SNR, participant differences"]
        Method["主要方法/Method<br>NeRF-inspired neural network for EEG"] --> M1["核心类比/Core Analogy<br>Viewpoints (NeRF) ↔ Electrodes (EEG)"]
        Method --> M2["技术实现/Technique<br>Train on single sample to get fixed weight vector"]
        Results["关键结果/Results<br>Enables continuous visualization & reconstruction"] --> R1["功能一/Capability 1<br>Render signal at unseen times/positions"]
        Results --> R2["功能二/Capability 2<br>Simulate non-existent electrodes"]
        Results --> R3["实证结果/Empirical Result<br>Improves standard EEG network performance"]
    ```

- **[arXiv260105] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction**
  - **tags:** [cv], [dynamic scene reconstruction], [Adaptive Gabor Representation, Cubic Hermite Splines, Temporal Curvature Regularization]
  - **authors:** Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu
  - **institution:** National Yang Ming Chiao Tung University, University of Zaragoza
  - **link:** https://arxiv.org/pdf/2601.00796
  - **contributions:** 1. Proposed Adaptive Gabor Representation, which extends Gaussian primitives with learnable frequency weights and adaptive energy compensation to capture high-frequency details while maintaining stability. 2. Introduced a temporal continuity modeling approach using Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution and reduce interpolation artifacts. 3. Designed an Adaptive Initialization mechanism that leverages depth estimation, point tracking, and foreground masks to establish a stable initial point cloud distribution for efficient training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AdaGaR, a novel framework for dynamic 3D scene reconstruction from monocular videos. It introduces an Adaptive Gabor Representation to capture high-frequency details and uses Cubic Hermite Splines with regularization to ensure temporal smoothness. Experiments show state-of-the-art performance on the DAVIS dataset, achieving superior rendering quality and strong generalization in tasks like frame interpolation and video editing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法问题: Existing Method Issues]
        B1 --> B1_1[高斯原语低通滤波: Gaussian Primitives Low-Pass Filtering]
        B1 --> B1_2[标准Gabor能量不稳定: Standard Gabor Energy Instability]
        B1 --> B1_3[缺乏时间连续性导致伪影: Lack of Temporal Continuity Causes Artifacts]
        C --> C1[自适应Gabor表示: Adaptive Gabor Representation]
        C1 --> C1_1[可学习频率权重: Learnable Frequency Weights]
        C1 --> C1_2[自适应能量补偿: Adaptive Energy Compensation]
        C --> C2[时间连续性建模: Temporal Continuity Modeling]
        C2 --> C2_1[三次Hermite样条: Cubic Hermite Splines]
        C2 --> C2_2[时间曲率正则化: Temporal Curvature Regularization]
        C --> C3[自适应初始化: Adaptive Initialization]
        C3 --> C3_1[深度估计: Depth Estimation]
        C3 --> C3_2[点跟踪: Point Tracking]
        C3 --> C3_3[前景掩码: Foreground Masks]
        D --> D1[最先进性能: State-of-the-Art Performance]
        D1 --> D1_1[PSNR: 35.49]
        D1 --> D1_2[SSIM: 0.9433]
        D1 --> D1_3[LPIPS: 0.0723]
        D --> D2[强大泛化能力: Strong Generalization]
        D2 --> D2_1[帧插值: Frame Interpolation]
        D2 --> D2_2[深度一致性: Depth Consistency]
        D2 --> D2_3[视频编辑: Video Editing]
        D2 --> D2_4[立体视图合成: Stereo View Synthesis]
    ```

- **[arXiv260105] The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification**
  - **tags:** [cv], [medical image analysis], [lesion attention, explainable AI, Grad-CAM, sensitivity analysis, transfer learning]
  - **authors:** Tanay Donde
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2601.00355
  - **contributions:** 1. Analyzed the relationship between model attention on lesion areas and diagnostic performance metrics (precision, recall, F1-score) in melanoma classification. 2. Employed a multi-faceted methodology involving masked images, bounding box detection, and transfer learning to investigate lesion focus. 3. Demonstrated that models with higher focus on lesion areas achieve better diagnostic performance, highlighting the value of interpretable AI for building trustworthy medical models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how the focus (attention) of AI models on lesion areas affects their performance in classifying melanoma from skin images. The authors used methods like masked images and explainability techniques (e.g., Grad-CAM) to analyze this relationship. They found that models which pay more attention to the actual lesion regions perform better, suggesting that improving model interpretability can lead to more accurate and reliable diagnostic tools.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Impact of Lesion Focus on AI-Based Melanoma Classification<br>病灶焦点对AI黑色素瘤分类性能的影响] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>CNN models lack reliability due to inconsistent focus on lesion areas.<br>CNN模型因对病灶区域关注不一致而缺乏可靠性。]
        C[主要方法/Method<br>Analyze lesion attention using masked images, bounding box detection, transfer learning, and explainability methods.<br>使用掩码图像、边界框检测、迁移学习和可解释性方法分析病灶注意力。]
        D[关键结果/Results<br>Higher model focus on lesion areas correlates with better diagnostic performance (precision, recall, F1-score).<br>模型对病灶区域的更高关注与更好的诊断性能（精确率、召回率、F1分数）相关。]
    ```
