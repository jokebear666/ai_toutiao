# 20251215-20251221 (cs.CY)

## 2025-12-18

- **[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI**
  - **tags:** [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]
  - **authors:** Ryan Cartularo
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.14712
  - **Simple LLM Summary:** This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.

- **[arXiv251218] AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally**
  - **tags:** [mlsys], [others], [multi-agent reasoning, chain-of-responsibility, modular architecture, governance mechanisms, multilingual interactions, real-time tools]
  - **authors:** Nadine Angela Cantonjos, Arpita Biswas
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2512.14910
  - **Simple LLM Summary:** This paper presents AgroAskAI, a multi-agent AI framework designed to support smallholder farmers with climate adaptation queries. It uses a modular, role-specialized architecture coordinated via a chain-of-responsibility approach, integrating real-time data and multilingual support. The experimental results show that this system delivers more actionable and grounded outputs for agricultural decision support.

- **[arXiv251218] Epistemic diversity across language models mitigates knowledge collapse**
  - **tags:** [mlsys], [llm training], [model collapse, epistemic diversity, AI ecosystem, self-training, distributed training]
  - **authors:** Damian Hodel, Jevin D. West
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.15011
  - **Simple LLM Summary:** The paper investigates whether diversity across language models (an "AI ecosystem") can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.

- **[arXiv251218] Governing rapid technological change: Policy Delphi on the future of European AI governance**
  - **tags:** [ai], [policy analysis], [Policy Delphi, anticipatory governance, future-proof regulation, AI Act]
  - **authors:** Atte Ojanen, Johannes Anttila, Thilo H. K. Thelitz, Anna Bjork
  - **institution:** Demos Helsinki, University of Turku
  - **link:** https://arxiv.org/pdf/2512.15196
  - **Simple LLM Summary:** This paper uses a two-round Policy Delphi method with European experts to study the future of AI governance. It finds a consensus that effective regulation depends more on practical implementation and enforcement than on technical specifics, and identifies a gap between desirable policy directions (like citizen participation) and their perceived feasibility.

- **[arXiv251218] ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I**
  - **tags:** [mlsys], [multi-modal inference], [multimodal reasoning, perception-cognition gap, calculation-conceptualization discrepancy, process hallucination, OCR, AI-resistant questions]
  - **authors:** Seok-Hyun Ga, Chun-Yen Chang
  - **institution:** Institute for Research Excellence in Learning Sciences, National Taiwan Normal University, Seoul National University, Universitas Negeri Malang
  - **link:** https://arxiv.org/pdf/2512.15298
  - **Simple LLM Summary:** This study evaluates the multimodal scientific reasoning of LLMs like GPT-4o and Gemini on the Korean CSAT Earth Science I exam under different input conditions. It finds that models suffer from fundamental cognitive flaws, such as a perception-cognition gap and calculation-conceptualization discrepancy, even with optimized inputs. The paper concludes by suggesting these vulnerabilities can be exploited to design AI-resistant assessment questions to ensure academic integrity.

- **[arXiv251218] Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality**
  - **tags:** [mlsys], [others], [crowdsourcing, user study, extended reality, conversational agents, privacy, technology acceptance model]
  - **authors:** Efe Bozkir, Enkelejda Kasneci
  - **institution:** Technical University of Munich
  - **link:** https://arxiv.org/pdf/2512.15343
  - **Simple LLM Summary:** This paper conducted a large-scale crowdsourcing study with 1036 participants to explore user acceptance and concerns regarding LLM-powered conversational agents in Extended Reality (XR). The study found that while users generally accept these technologies, they express significant concerns about security, privacy, social implications, and trust, with location data being the most sensitive. The results highlight the importance of practitioner transparency and that familiarity with generative AI increases acceptance, while prior XR device ownership is linked to lower acceptance.

## 2025-12-19

- **[arXiv251219] Value Lens: Using Large Language Models to Understand Human Values**
  - **tags:** [mlsys], [llm inference], [large language models, value detection, generative AI, dual-LLM approach, expert verification]
  - **authors:** Eduardo de la Cruz Fernández, Marcelo Karanik, Sascha Ossowski
  - **institution:** Universidad Politécnica de Madrid, Universidad Rey Juan Carlos
  - **link:** https://arxiv.org/pdf/2512.15722
  - **Simple LLM Summary:** The paper proposes Value Lens, a two-stage model that uses Large Language Models (LLMs) to detect human values in text. The first stage uses an LLM to conceptualize a value theory verified by experts, and the second stage employs a dual-LLM approach for detection and critical review. The results show that Value Lens performs comparably to or better than other models in similar tasks.

- **[arXiv251219] D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models**
  - **tags:** [mlsys], [multi-modal inference], [CLIP, Stable Diffusion XL, zero-shot classification, demographic bias mitigation, data generation]
  - **authors:** Javon Hickmon
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.15747
  - **Simple LLM Summary:** This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.

- **[arXiv251219] Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance**
  - **tags:** [ai], [AI ethics and governance], [cultural rights, right to development, algorithmic design, AI governance, human rights law]
  - **authors:** Alexander Kriebitz, Caitlin Corrigan, Aive Pevkur, Alberto Santos Ferro, Amanda Horzyk, Dirk Brand, Dohee Kim, Dodzi Koku Hattoh, Flavia Massucci, Gilles Fayad, Kamil Strzepek, Laud Ammah, Lavina Ramkissoon, Mariette Awad, Natalia Amasiadi, Nathan C. Walker, Nicole Manger, Sophia Devlin
  - **institution:** Ludwig Maximilian University of Munich, Technical University of Munich, Tallinn University of Technology, University of Edinburgh, Stellenbosch University, Changwon National University, University of Ghana, Cardinal Stefan Wyszyński University in Warsaw, African Union, American University of Beirut, University of Patras, Rutgers University, Ulster University
  - **link:** https://arxiv.org/pdf/2512.15786
  - **Simple LLM Summary:** The paper conceptually analyzes the impact of AI on cultural rights and the right to development, examining the epistemic and normative limitations in algorithmic design. It concludes that AI governance frameworks have significant gaps in protecting these rights and risks exacerbating global inequities. The study calls for integrating cultural and developmental considerations into future AI policy and research.

- **[arXiv251219] Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces**
  - **tags:** [mlsys], [others], [generative AI, multi-agent systems, edge computing, sustainability, ambient intelligence, AI agents, sustainable ecosystems]
  - **authors:** Przemek Pospieszny, Dominika P. Brodowicz
  - **institution:** EPAM Systems, Warsaw School of Economics
  - **link:** https://arxiv.org/pdf/2512.15787
  - **Simple LLM Summary:** The paper proposes the concept of "agentic environments," a framework leveraging generative AI, multi-agent systems, and edge computing to create sustainable, human-centric spaces. It concludes that this approach can reduce the environmental impact of AI by optimizing resource use and enhancing data privacy through decentralized, edge-driven deployment models.

- **[arXiv251219] A Systematic Analysis of Biases in Large Language Models**
  - **tags:** [ai], [fairness and bias analysis], [news summarization, stance classification, UN voting patterns, multilingual story completion, World Values Survey]
  - **authors:** Xulang Zhang, Rui Mao, Erik Cambria
  - **institution:** Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.15792
  - **Simple LLM Summary:** This paper systematically analyzes biases in large language models (LLMs) across political, ideological, alliance, language, and gender dimensions using experiments like news summarization and stance classification. The main conclusion is that despite being aligned for neutrality, the studied LLMs still exhibit various types of biases and affinities.

- **[arXiv251219] Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud**
  - **tags:** [ai], [ai ethics evaluation], [Model Cards, ALTAI, FactSheets, Harms Modeling, literature survey, interviews]
  - **authors:** Jhessica Silva, Diego A. B. Moreira, Gabriel O. dos Santos, Alef Ferreira, Helena Maia, Sandra Avila, Helio Pedrini
  - **institution:** Universidade Estadual de Campinas (UNICAMP), Universidade Federal de Goiás (UFG)
  - **link:** https://arxiv.org/pdf/2512.15791
  - **Simple LLM Summary:** The paper presents a methodology to evaluate AI Ethics Tools (AIETs) for language models by selecting four tools (Model Cards, ALTAI, FactSheets, Harms Modeling) and applying them to Portuguese language models, with developer interviews. The results indicate that these tools help guide general ethical considerations but fail to address language-specific aspects like idiomatic expressions or identify negative impacts for Portuguese.

- **[arXiv251219] Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms**
  - **tags:** [ai], [ethical ai], [contrastive learning, social norms generation, moral reasoning, explainable ai, valence prediction]
  - **authors:** Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang
  - **institution:** Hong Kong Baptist University, Singapore Management University, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.15793
  - **Simple LLM Summary:** The paper introduces ClarityEthic, a method that enhances ethical assessment of human actions by generating conflicting social norms to explain and predict valence (support/oppose). It uses a contrastive learning strategy to strengthen the moral reasoning of language models. Experiments show the method outperforms baselines and human evaluations confirm the generated norms provide plausible explanations.

- **[arXiv251219] Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India**
  - **tags:** [ai], [cybersecurity], [computer forensics, explainable AI (XAI), data minimization, algorithmic bias, deepfakes, adversarial AI, Digital Personal Data Protection Act]
  - **authors:** Sahibpreet Singh, Shikha Dhiman
  - **institution:** Guru Nanak Dev University, Amritsar
  - **link:** https://arxiv.org/pdf/2512.15799
  - **Simple LLM Summary:** This paper employs a doctrinal legal methodology to analyze the integration of AI into cybercrime and forensics in India, focusing on the Digital Personal Data Protection Act, 2023. It concludes that there is a critical tension between privacy principles and forensic needs, and proposes a human-centric forensic model using explainable AI (XAI) to ensure evidence admissibility while advocating for legislative synchronization with international standards.

- **[arXiv251219] DSO: Direct Steering Optimization for Bias Mitigation**
  - **tags:** [ai], [fairness and bias mitigation], [activation steering, reinforcement learning, linear transformations, inference-time control]
  - **authors:** Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff
  - **institution:** Apple, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.15926
  - **Simple LLM Summary:** The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention.

- **[arXiv251219] Cross-Language Bias Examination in Large Language Models**
  - **tags:** [ai], [fairness and bias evaluation], [multilingual bias evaluation, BBQ benchmark, prompt-based Implicit Association Test, explicit bias, implicit bias]
  - **authors:** Yuxuan Liang, Marwa Mahmoud
  - **institution:** Georgia Institute of Technology, University of Glasgow
  - **link:** https://arxiv.org/pdf/2512.16029
  - **Simple LLM Summary:** This paper introduces a multilingual bias evaluation framework that combines explicit bias assessment using the BBQ benchmark with implicit bias measurement via a prompt-based Implicit Association Test, applied across five languages. The results show significant variation in bias across languages, with Arabic and Spanish exhibiting higher stereotype bias, and reveal contrasting patterns between explicit and implicit bias, such as age having low explicit but high implicit bias. The study highlights the importance of cross-lingual bias analysis for developing equitable multilingual LLMs.

- **[arXiv251219] Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams**
  - **tags:** [mlsys], [llm inference], [large language models, safety filters, conversation study, social engineering, automation]
  - **authors:** Gilad Gressel, Rahul Pankajakshan, Shir Rozenfeld, Ling Li, Ivan Franceschini, Krishnahsree Achuthan, Yisroel Mirsky
  - **institution:** Amrita Vishwa Vidyapeetham, Ca’ Foscari University of Venice, University of Melbourne, Ben Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2512.16280
  - **Simple LLM Summary:** The paper investigates the role of LLMs in romance-baiting scams through interviews with insiders and victims, a blinded long-term conversation study comparing LLM agents to human operators, and an evaluation of commercial safety filters. It finds that LLMs are already widely used in scams, can elicit greater trust and compliance than humans, and that current safety filters are ineffective at detecting such dialogues, suggesting scams are ripe for full LLM automation.

- **[arXiv251219] Comprehensive AI Literacy: The Case for Centering Human Agency**
  - **tags:** [ai], [education], [AI literacy, human agency, critical thinking, epistemology, educational frameworks]
  - **authors:** Sri Yash Tadimalla, Justin Cary, Gordon Hull, Jordan Register, Daniel Maxwell, David Pugalee, Tina Heafner
  - **institution:** UNC Charlotte
  - **link:** https://arxiv.org/pdf/2512.16656
  - **Simple LLM Summary:** This position paper proposes a shift towards comprehensive AI literacy frameworks that center human agency and critical thinking in education. It concludes that educators and students must be empowered to make intentional, critical choices about AI use, rather than focusing solely on operational skills.

- **[arXiv251219] From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, deductive reasoning, conflict-aware trust-score, reasoning-trace-augmented framework, supervised fine-tuning]
  - **authors:** Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.16795
  - **Simple LLM Summary:** This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems.

- **[arXiv251219] Impacts of Racial Bias in Historical Training Data for News AI**
  - **tags:** [ai], [algorithmic auditing], [multi-label classifier, explainable AI, word2vec, New York Times Annotated Corpus]
  - **authors:** Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza
  - **institution:** Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group
  - **link:** https://arxiv.org/pdf/2512.16901
  - **Simple LLM Summary:** The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic "blacks" label acts as a general "racism detector" but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.
