---
slug: /daily/cscy/20251229-20260104
---
# 20251229-20260104 (cs.CY)

## 2025-12-29

- **[arXiv251229] SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram**
  - **tags:** [sec], [cyber threat intelligence], [multi-modal fusion, large language models, graph neural networks, early detection, social media analysis]
  - **authors:** Mohammad Hammas Saeed, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21380
  - **contributions:** 1. Proposes SENTINEL, a multi-modal framework for early cyber threat detection by aligning social media discussions with real-world attacks. 2. Combines language modeling (using LLMs) and network coordination analysis (using GNNs) to fuse textual and relational signals from platforms like Telegram. 3. Demonstrates the framework's effectiveness on a dataset of 365k messages from 16 Telegram channels, achieving an F1 score of 0.89 for threat alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3e47a235de6afcb4955afd774a9e4b3883efcafc7e4aaa97649575ef2fb34d0_w640_q70.webp
  - **Simple LLM Summary:** The paper presents SENTINEL, a framework for the early detection of cyber threats by analyzing multi-modal signals from social media platforms like Telegram. It combines large language models for text understanding with graph neural networks to model user coordination, successfully aligning online discussions to real-world attacks. The evaluation on Telegram data shows the approach is effective, achieving a high F1 score of 0.89.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram"] --> Problem["核心问题/Problem: Post-hoc detection of cyber attacks is reactive; need for proactive, early warning systems."]
        Root --> Method["主要方法/Method: Multi-modal framework combining LLMs (language) and GNNs (coordination graphs) to analyze social media signals."]
        Root --> Results["关键结果/Results: Achieves F1 of 0.89 aligning Telegram discussions to real-world cyber threats."]
    ```

- **[arXiv251229] ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks**
  - **tags:** [sec], [social network security], [Graph Neural Networks, influence campaigns, temporal link prediction, troll detection, Reddit]
  - **authors:** Mohammad Hammas Saeed, Isaiah J. King, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21391
  - **contributions:** 1. Proposes ALETHEIA, a system that formalizes the detection of malicious accounts in influence campaigns as a node classification and link prediction problem using a graph-based representation. 2. Demonstrates that a detection pipeline combining topological (graph) and linguistic features outperforms standard interaction and user features, achieving a 3.7% F1-score improvement. 3. Introduces a novel temporal link prediction mechanism for influence campaigns by stacking a GNN over an RNN to forecast future troll interactions (TTE/TUE) with high accuracy (96.6% AUC).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/473a865358e998c62938f17660edc395a7658bf360ef15e62ea79317e8734aec_w640_q70.webp
  - **Simple LLM Summary:** This paper presents ALETHEIA, a system that uses Graph Neural Networks (GNNs) to detect malicious accounts and predict their future interactions in social media influence campaigns. By modeling campaigns as graphs and combining structural and linguistic features, it improves detection performance and forecasts troll behavior with high accuracy. The results underscore the importance of leveraging network structure to combat coordinated malicious activity online.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks] --> B[核心问题/Problem: Detecting and predicting malicious influence campaigns on social media]
        A --> C[主要方法/Method: Graph Neural Networks (GNNs) with topological & linguistic features, GNN+RNN for temporal link prediction]
        A --> D[关键结果/Results: 3.7% F1-score improvement in detection, 96.6% AUC for predicting future troll interactions]
    ```

- **[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments**
  - **tags:** [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]
  - **authors:** Hua Shen
  - **institution:** NYU Shanghai, New York University
  - **link:** https://arxiv.org/pdf/2512.21552
  - **contributions:** 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]
        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]
        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]
    ```

- **[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets**
  - **tags:** [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]
  - **authors:** Matyas Bohacek, Ignacio Vilanova Echavarri
  - **institution:** Stanford University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21775
  - **contributions:** 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation")
        Problem --> P2("数据来源与合法性信息丢失/Loss of data origin & legitimacy info")
        Method --> M1("提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework")
        Method --> M2("开发基于数据溯源技术的开源库/Develop open-source library using data provenance")
        Results --> R1("评估现有数据集的合规性/Evaluate compliance of existing datasets")
        Results --> R2("指导负责任的新数据集构建/Guide responsible construction of new datasets")
    ```

- **[arXiv251229] On The Conceptualization and Societal Impact of Cross-Cultural Bias**
  - **tags:** [nlp], [bias and fairness], [cultural bias, literature survey, societal impact, harm evaluation, bias mitigation]
  - **authors:** Vitthal Bhandari
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21809
  - **contributions:**  1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("On The Conceptualization and Societal Impact of Cross-Cultural Bias") --> Problem("核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.")
    Root --> Method("主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.")
    Root --> Results("关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.")
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management**
  - **tags:** [nlp], [AI Governance & Compliance], [lifecycle management, bias detection, differential privacy, federated learning, terminology drift]
  - **authors:** Sunil Arora, John Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.22060
  - **contributions:** 1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks."]
        Method["主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy)."]
        Results["关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study."]
    ```

- **[arXiv251229] Agent-based simulation of online social networks and disinformation**
  - **tags:** [ai], [agent-based simulation], [agent-based simulation, large language model, disinformation campaigns, synthetic social networks, behavioral automata]
  - **authors:** Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente
  - **institution:** University of Murcia
  - **link:** https://arxiv.org/pdf/2512.22082
  - **contributions:** 1. A simulation framework that models synthetic social networks using agents with demographic-based personality traits and finite-state behavioral automata for realistic and interpretable actions. 2. A generative module powered by an LLM to produce context-aware social media posts consistent with each agent's profile and memory. 3. A red module implementing DISARM-inspired workflows to orchestrate disinformation campaigns and a Mastodon-based visualization layer for real-time inspection and validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a3415bee27d71926e7770fd596253ec973faaa85d1fcba853a047ff6d08bfe3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an agent-based simulation framework to study online social networks and disinformation, addressing the limitations of platform opacity and data access. The framework uses LLM-powered agents with personality traits and behavioral automata to generate realistic content and simulate disinformation campaigns, with evaluation showing structural, behavioral, and linguistic realism. It provides a customizable and controllable environment for studying information dynamics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Agent-based simulation of online social networks and disinformation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[平台不透明与数据限制/Platform Opacity & Data Limits]
        Problem --> P2[现有模拟缺乏真实性与可解释性/Existing Simulations Lack Realism & Explainability]
        Method[主要方法/Method] --> M1[基于代理的合成社交网络/Agent-based Synthetic Social Networks]
        Method --> M2[LLM生成上下文感知内容/LLM Generates Context-aware Content]
        Method --> M3[红色模块模拟虚假信息活动/Red Module Simulates Disinformation Campaigns]
        Method --> M4[Mastodon可视化层/Mastodon Visualization Layer]
        Results[关键结果/Results] --> R1[展示结构、行为、语言真实性/Demonstrates Structural, Behavioral, Linguistic Realism]
        Results --> R2[为研究信息动态提供可定制环境/Provides Customizable Environment for Studying Information Dynamics]
    ```

## 2025-12-30

- **[arXiv251230] Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models**
  - **tags:** [ai], [peer review automation], [large language models, peer review, pre-review, citation prediction, review alignment]
  - **authors:** Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi
  - **institution:** AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI
  - **link:** https://arxiv.org/pdf/2512.22145
  - **contributions:** 1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]
        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]
        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]
        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]
    ```

- **[arXiv251230] AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History**
  - **tags:** [nlp], [semantic change detection], [diachronic embeddings, orthogonal Procrustes, lexical drift]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.22196
  - **contributions:** 1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("数字人文中语义变迁分析<br>Digital Humanities Semantic Shift Analysis")
        C --> C1("可复现的专家系统流程<br>Reproducible Expert-System Pipeline")
        C1 --> C2("分时段词嵌入与对齐<br>Temporal Embeddings & Alignment")
        C2 --> C3("几何位移与邻域变化度量<br>Geometric & Neighborhood Metrics")
        D --> D1("可视化法律概念演变<br>Visualizing Legal Concept Evolution")
        D1 --> D2("揭示与历史事件的关联<br>Revealing Links to Historical Events")
    ```

- **[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening**
  - **tags:** [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]
  - **authors:** Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf
  - **institution:** Radboud University Medical Center, Radboud University
  - **link:** https://arxiv.org/pdf/2512.22242
  - **contributions:** 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]
        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]
        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]
    ```

- **[arXiv251230] Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities**
  - **tags:** [other], [social network analysis], [deplatforming, toxicity detection, dynamic reputation modeling, network analysis, migration regimes]
  - **authors:** Aleksandar Tomašević, Ana Vranić, Aleksandra Alorić, Marija Mitrović Dankulov
  - **institution:** Institute of Physics Belgrade, University of Belgrade
  - **link:** https://arxiv.org/pdf/2512.22348
  - **contributions:**  1. Identifies and characterizes two distinct regimes ("Hostile Takeover" and "Toxic Equilibrium") of how deplatformed users transform receiving communities on alternative platforms. 2. Demonstrates that community transformation is driven by peripheral dynamics and volume, not by newcomers capturing central network positions. 3. Shows that the structure of the migrating community (loose vs. cohesive) determines whether they disperse into generalist spaces or form dedicated enclaves.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ecb07715a5eebfede95c0e808d5c2d61c5453c8fb7fa8b8c2b8260b568fa2f9_w640_q70.webp
  - **Simple LLM Summary:** This paper studies how Reddit deplatforming affects the communities on the alternative platform Voat. Using network analysis, toxicity detection, and dynamic reputation modeling, it finds that migration leads to increased toxicity through distinct phases and that platforms have a narrow window to intervene before toxic norms become entrenched.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities] --> B(核心问题/Problem);
        A --> C(主要方法/Method);
        A --> D(关键结果/Results);
        B --> B1[替代平台接收被禁用户的影响/Impact on alternative platforms receiving banned users];
        C --> C1[网络分析/Network Analysis];
        C --> C2[毒性检测/Toxicity Detection];
        C --> C3[动态声誉建模/Dynamic Reputation Modeling];
        D --> D1[敌对接管阶段/Hostile Takeover Phase (2015-2018)];
        D --> D2[毒性平衡阶段/Toxic Equilibrium Phase (2018-2020)];
        D --> D3[外围动态驱动转变/Peripheral Dynamics Drive Change];
    ```

- **[arXiv251230] Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy**
  - **tags:** [nlp], [human-ai interaction], [boundary objects, relational mediation, marginalized clients, therapeutic systems, dynamic framework]
  - **authors:** Jiatao Quan, Ziyue Li, Tian Qi Zhu, Yuxuan Li, Baoying Wang, Wanda Pratt, Nan Gao
  - **institution:** University of Washington, The Hong Kong Polytechnic University, Nankai University
  - **link:** https://arxiv.org/pdf/2512.22462
  - **contributions:** 1. Identifies enduring relational challenges in psychotherapy for marginalized clients, such as trust-building and self-disclosure burdens. 2. Proposes the Dynamic Boundary Mediation Framework, which re-conceptualizes LLMs as adaptive boundary objects. 3. Delineates three specific forms of mediation (Epistemic, Relational, Contextual) to address knowledge gaps, power asymmetries, and therapy-life discontinuities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8695c76e0392473389276989f78ab825dab06f2e38bdd785d2418d8ca9a1d80_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current framings of LLMs in mental health overlook their potential to mediate complex therapeutic relationships. Based on interviews with therapists and marginalized clients in China, the authors propose the Dynamic Boundary Mediation Framework, which positions LLM chatbots as adaptive boundary objects to bridge knowledge, power, and contextual gaps. This offers a pathway for designing AI systems that more effectively and accountably support therapeutic relationships for marginalized users.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["现有视角的局限/Current Framing Limitations"]
        Problem --> P2["边缘化客户的关系挑战/Relational Challenges for Marginalized Clients"]
        Method --> M1["动态边界调解框架/Dynamic Boundary Mediation Framework"]
        Method --> M2["作为边界对象的LLM/LLMs as Boundary Objects"]
        Results --> R1["三种调解形式/Three Forms of Mediation"]
        Results --> R2["关系问责的AI系统/Relationally Accountable AI Systems"]
    ```

- **[arXiv251230] Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability**
  - **tags:** [other], [social computing], [urban agriculture, hydroponics, qualitative study, social learning theory, sustainable food production]
  - **authors:** Ewa Duda, Adamina Korwin-Szymanowska
  - **institution:** Uniwersytet Łódzki, Uniwersytet Warszawski (University of Łódź, University of Warsaw)
  - **link:** https://arxiv.org/pdf/2512.22594
  - **contributions:** 1. Conducted a qualitative study on resident participation in an innovative urban hydroponic farming project. 2. Identified key motivations and experiences of urban residents engaging in community-based food self-production. 3. Provided insights for urban educators and policymakers on fostering sustainable food initiatives through social learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e23a23f4c52bbdc6c863186ff1aad1ec49304917fba3f15168891df1f7d6b610_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates urban residents' participation in a community hydroponic farming project in Poland. Using purposive sampling and in-depth interviews, the study explores the motivations, experiences, and educational pathways of participants. The findings highlight the role of social learning in empowering urban self-sustainability and offer guidance for stakeholders in urban education and development.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Urban food security & climate change/城市粮食安全与气候变化]
        C --> C1[Qualitative study: interviews/定性研究：访谈]
        C --> C2[Two communities in Poland/波兰的两个社区]
        C --> C3[Hydroponic cabinets in flats/公寓中的水培柜]
        D --> D1[Understand resident motivations/理解居民动机]
        D --> D2[Outline farming experiences/概述种植经验]
        D --> D3[Relevance for urban educators/对城市教育者的意义]
    ```

- **[arXiv251230] Mitigating Social Desirability Bias in Random Silicon Sampling**
  - **tags:** [nlp], [llm evaluation], [silicon sampling, social desirability bias, prompt engineering, jensen-shannon divergence, american national election study]
  - **authors:** Sashank Chapala, Maksym Mironov, Songgaojun Deng
  - **institution:** Eindhoven University of Technology
  - **link:** https://arxiv.org/pdf/2512.22725
  - **contributions:** 1. Replicates and confirms the presence of persistent Social Desirability Bias (SDB) in LLM-based silicon sampling. 2. Proposes and systematically evaluates four psychologically grounded prompt-based methods (reformulated, reverse-coded, priming, preamble) for mitigating SDB. 3. Demonstrates that reformulated prompts (neutral, third-person phrasing) are the most effective method for improving alignment between silicon and human survey response distributions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e514b34cb5fd24baebc22116c45f73b1898e7c713905a13d372408df0900782b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how to reduce Social Desirability Bias in LLM-generated survey responses (silicon sampling). It tests four prompt-based mitigation methods and finds that reformulating questions into neutral, third-person phrasing most effectively aligns the LLM outputs with real human data from the American National Election Study.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mitigating Social Desirability Bias in Random Silicon Sampling] --> B[核心问题/Problem: LLM硅采样存在社会期望偏差/Social Desirability Bias in Silicon Sampling]
        A --> C[主要方法/Method: 测试四种基于提示的缓解方法/Test Four Prompt-based Mitigation Methods]
        A --> D[关键结果/Results: 重构提示最有效，改善与人类数据对齐/Reformulated Prompts Most Effective, Improve Alignment]
        C --> C1[重构/Reformulated]
        C --> C2[反向编码/Reverse-coded]
        C --> C3[启动/Priming]
        C --> C4[序言/Preamble]
    ```

- **[arXiv251230] Ungraded Assignments in Introductory Computing: A Report**
  - **tags:** [other], [computing education], [ungraded assignments, formative feedback, student engagement, mixed-methods, introductory computing]
  - **authors:** Yehya Sleiman Tellawi, Abhishek K. Umrawal
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.23004
  - **contributions:** 1. Developed and administered new optional ungraded assignments for a large introductory computer engineering course (ECE 120). 2. Employed a mixed-methods approach (surveys, interviews, performance analysis) to assess the impact of ungraded assignments on learning. 3. Found a positive relationship between participation in ungraded assignments and overall course performance, suggesting they appeal to high-achievers or support better outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03fe0f743f635ea37a635da0020b8df696d9ae620a38ea5dd83dd57902fd7911_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the effects of optional ungraded assignments in an introductory computing course. The authors developed such assignments and used surveys, interviews, and performance data to evaluate their impact. The main finding is a positive correlation between completing ungraded work and higher course grades, indicating potential benefits for student engagement and learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Ungraded Assignments in Introductory Computing: A Report] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(评估压力与内在动机/Grade pressure vs. intrinsic motivation)
        C --> C1(开发并实施未评分作业/Develop & administer ungraded assignments)
        C --> C2(混合方法评估/Mixed-methods evaluation)
        D --> D1(参与度与成绩正相关/Participation correlates with performance)
    ```

- **[arXiv251230] Inteligencia Artificial y Empleo: perspectiva Territorial y de Género**
  - **tags:** [other], [labor economics, computational social science], [AI exposure index, sector-based analysis, territorial disaggregation, gender gap, CNAE incidence matrix]
  - **authors:** Antoni Mestre, Xavier Naya, Manoli Albert, Vicente Pelechano
  - **institution:** Universitat de València (inferred from author names and Spanish context)
  - **link:** https://arxiv.org/pdf/2512.23059
  - **contributions:**  1. Proposes a novel methodological framework for estimating AI exposure using sector-based data (CNAE classification) instead of occupation-based approaches, addressing limitations in the Spanish context. 2. Constructs an AI CNAE incidence matrix and applies it to provincial employment data (2021-2023) to provide a territorial and gender-disaggregated assessment of AI's potential impact. 3. Reveals stable structural patterns of AI exposure, identifying higher exposure in metropolitan/service regions and a consistent gender gap where female employment is more exposed across all territories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/562053897ffb2e59883167293942d4f1e524304623f8d2e91e0026105669c93d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a sector-based methodological framework to estimate the potential exposure of employment to AI in Spain, addressing the limitations of occupation-centered approaches. By applying an AI CNAE incidence matrix to provincial employment data from 2021-2023, it provides a territorial and gender-disaggregated assessment. The results show higher AI exposure in metropolitan and service-oriented regions and a consistent gender gap, with female employment being more exposed across all territories, offering a structural perspective for policy planning rather than predicting job displacement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Inteligencia Artificial y Empleo: perspectiva Territorial y de Género] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>AI对劳动力市场的不均衡影响<br>Uneven AI impact on labor markets]
        C[主要方法/Method<br>构建行业AI暴露矩阵<br>Construct sector-based AI exposure matrix]
        D[关键结果/Results<br>大都市区暴露更高, 存在性别差距<br>Higher exposure in metro areas, consistent gender gap]
    ```

- **[arXiv251230] Identifying Barriers Hindering the Acceptance of Generative AI as a Work Associate, measured with the new AGAWA scale**
  - **tags:** [ai], [human-ai interaction], [AGAWA scale, technology acceptance, generative AI, workplace, moral dilemmas]
  - **authors:** Łukasz Sikorski, Albert Łukasik, Jacek Matulewski, Arkadiusz Gut
  - **institution:** Nicolaus Copernicus University in Toruń
  - **link:** https://arxiv.org/pdf/2512.23373
  - **contributions:** 1. Proposed the AGAWA scale, a concise 4-item tool for measuring attitudes toward generative AI as a coworker, 2. Investigated key factors (concerns, human-like characteristics, sense of human uniqueness) influencing acceptance of generative AI in the workplace, 3. Confirmed the relationship between affective/moral dimensions of trust and attitudes toward generative AI at work.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae604f8d4d4b73f323c396a472bef59c496be69f1ebbd75e34c80cb0805f530_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the AGAWA scale, a brief measurement tool based on TAM and UTAUT models, to study barriers to accepting generative AI as a work associate. The study found that positive attitudes toward AI coworkers are negatively correlated with concerns about interaction, human-like AI traits, and a sense of human superiority. The results highlight the link between trust dimensions and workplace AI acceptance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Identifying Barriers to Generative AI Acceptance / 识别生成式AI接受的障碍"] --> Problem["Core Problem: Student attitudes affect future workplace AI adoption / 核心问题: 学生态度影响未来工作场所AI采用"]
        Root --> Method["Method: Propose AGAWA scale (4-item tool) / 方法: 提出AGAWA量表(4项工具)"]
        Root --> Results["Results: Positive attitudes linked to reduced concerns, human-likeness, and superiority beliefs / 结果: 积极态度与减少的担忧、拟人特性和优越感信念相关"]
    ```

- **[arXiv251230] The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective**
  - **tags:** [other], [scientometrics], [gender diversity, team roles, author contribution statements, threshold regression, citation impact]
  - **authors:** Yi Zhao, Yongjun Zhu, Donghun Kim, Yuzhuo Wang, Heng Zhang, Chao Lu, Chengzhi Zhang
  - **institution:** Anhui University, Yonsei University, Nanjing University, Central China Normal University, Hohai University, Nanjing University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23429
  - **contributions:** 1. Introduced a team roles perspective by classifying authors into leadership and support roles using contribution statements, moving beyond aggregate diversity measures. 2. Discovered a non-linear (inverted U-shape) relationship between gender diversity and team impact for both leadership and support groups. 3. Revealed the moderating effect of team size, showing that the impact of leadership-group gender diversity shifts from negative to positive as team size increases, while support-group diversity remains consistently positive.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8e0d6494840d098b4c65bf9f6eb6b4b27a82bd62024c95c0f60db9e5b8fa31f_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how gender diversity within specific team roles (leadership vs. support) affects scientific team impact, measured by citations. By analyzing over 130,000 PLOS papers and using contribution statements to define roles, the authors employed multivariable and threshold regression. They found the relationship is an inverted U-shape, identified high-impact team compositions, and showed that team size significantly moderates the effect of leadership diversity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Effect of Gender Diversity on Scientific Team Impact<br>性别多样性对科研团队影响力的影响] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Inconsistent findings on gender diversity's effect,<br>lack of role-differentiated analysis.<br>性别多样性影响结论不一，缺乏基于团队角色的分析]
        C[主要方法/Method<br>Analyzed 130k+ PLOS papers, used contribution<br>statements for role classification (leadership/support),<br>applied multivariable & threshold regression.<br>分析13万+PLOS论文，利用贡献声明进行角色分类，应用多元及阈值回归]
        D[关键结果/Results<br>1. Inverted U-shape relationship.<br>2. All-female leadership + all-male support yields high impact.<br>3. Team size moderates leadership diversity effect.<br>1. 倒U型关系。<br>2. 全女性领导+全男性支持的团队影响力更高。<br>3. 团队规模调节领导组多样性效应。]
    ```

- **[arXiv251230] Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education**
  - **tags:** [ai], [academic integrity detection], [Large Language Models, AI-generated text detection, deceptive prompts, computing education, self-detection]
  - **authors:** Christopher Burger, Karmece Talley, Christina Trotter
  - **institution:** The University of Mississippi, Rust College
  - **link:** https://arxiv.org/pdf/2512.23587
  - **contributions:** 1. Evaluates the self-detection performance of three prominent LLMs (GPT-4, Claude, Gemini) in computing-specific contexts. 2. Tests detection under both standard and deceptive prompt conditions where models are instructed to evade detection. 3. Reveals significant instability in detection, showing high error rates for human-written work and susceptibility to simple prompt alterations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e1ff7ce7b5924fdb6e21130ee87b352dc8c7613c8706c156c27ac0e31017f8b_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the ability of LLMs (GPT-4, Claude, Gemini) to detect AI-generated text in computing education. It tests them under standard and deceptive prompt conditions, finding that while default AI text is easily identified, models struggle with human-written work and are highly fooled by deceptive prompts, making them unreliable for high-stakes academic judgments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education] --> B[核心问题/Problem: LLMs challenge academic integrity in computing education]
        A --> C[主要方法/Method: Evaluate GPT-4, Claude, Gemini on AI-generated text detection with standard/deceptive prompts]
        A --> D[关键结果/Results: Models unstable; high error on human text; easily fooled by deceptive prompts; unreliable for misconduct judgments]
    ```

- **[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms**
  - **tags:** [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]
  - **authors:** LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão
  - **institution:** Google, Eedi
  - **link:** https://arxiv.org/pdf/2512.23633
  - **contributions:** 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]
        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]
        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]
        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]
        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]
        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]
        Results --> R2[学生表现相当或更好/Student performance equal or better]
        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]
    ```

## 2026-01-01

- **[arXiv260101] New Exam Security Questions in the AI Era: Comparing AI-Generated Item Similarity Between Naive and Detail-Guided Prompting Approaches**
  - **tags:** [nlp], [question generation], [large language models, multiple-choice questions, cosine similarity, PubMedBERT, BioBERT]
  - **authors:** Ting Wang, Caroline Prendergast, Susan Lottridge
  - **institution:** American Board of Family Medicine, American Board of Surgery, Cambium Assessment
  - **link:** https://arxiv.org/pdf/2512.23729
  - **contributions:** 1. Introduced a comparative framework to assess the security risk of LLM-generated exam items by measuring the similarity between items created with proprietary ("guided") and publicly available ("naive") prompting strategies. 2. Demonstrated that LLMs using only public information can generate items highly similar to those created with proprietary guidance in narrowly defined clinical domains, identifying a specific security vulnerability. 3. Proposed concrete mitigation strategies for high-stakes exam security, including human-first AI-assisted development, separation of item pools, and systematic similarity surveillance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4fdc5d47472b074849e044e7c47d734f240a24f0b2daf6de26e97d0f0315992_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the security risk that LLMs pose to high-stakes exams by comparing the similarity of AI-generated multiple-choice questions created with two prompting strategies: a naive approach using only public information and a guided approach using proprietary exam materials. Using PubMedBERT and BioBERT embeddings to calculate cosine similarity, the study found that while guided items are generally distinct, naive prompts can produce highly similar items in constrained domains like viral pneumonia. The conclusion is that this convergence heightens item exposure risks, necessitating new safeguards like human oversight and systematic similarity checks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[New Exam Security Questions in the AI Era<br>AI时代的新考试安全问题] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>LLMs生成考题对考试安全的威胁] --> B1[LLMs can generate exam items from public data<br>LLMs可利用公开数据生成考题]
        C[主要方法/Method<br>比较两种提示策略的相似性] --> C1[Naive Prompting: Public EPA descriptors<br>朴素提示: 公开的EPA描述]
        C --> C2[Guided Prompting: Proprietary blueprints & guidelines<br>引导提示: 专有蓝图和指南]
        C --> C3[Models: GPT-4o, Claude 4, Gemini 2.5<br>模型: GPT-4o, Claude 4, Gemini 2.5]
        C --> C4[Similarity Analysis: PubMedBERT/BioBERT embeddings & cosine similarity<br>相似性分析: PubMedBERT/BioBERT嵌入与余弦相似度]
        D[关键结果/Results<br>引导与朴素提示的相似性] --> D1[High internal consistency within each strategy<br>各策略内部一致性高]
        D --> D2[Lower cross-strategy similarity overall<br>策略间总体相似性较低]
        D --> D3[High similarity (>0.65) in narrow domains (e.g., viral pneumonia)<br>狭窄领域(如病毒性肺炎)相似性高(>0.65)]
        D --> D4[Conclusion: Risk of item exposure, need for safeguards<br>结论: 考题暴露风险，需制定防护措施]
    ```

- **[arXiv260101] A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context**
  - **tags:** [se], [software fairness], [systematic mapping, fairness, software engineering, technology readiness level, group fairness]
  - **authors:** Kessia Nepomuceno, Fabio Petrillo
  - **institution:** École de Technologie Supérieure
  - **link:** https://arxiv.org/pdf/2512.23782
  - **contributions:** 1. A systematic literature mapping of 95 studies to categorize advancements in software fairness solutions. 2. A novel classification framework for analyzing software fairness research from the perspectives of trends, focus, and industrial viability. 3. An analysis revealing the field's focus on post-processing methods and group fairness, with limited industry collaboration and low-to-medium TRL, highlighting gaps for future work.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba1e54194de977ce396b8c19de44fec12f293e42ef1d03cfcc83d187b2d3da83_w640_q70.webp
  - **Simple LLM Summary:** This paper conducts a systematic mapping study to analyze research on fairness in software systems. It develops a classification framework applied to 95 studies, finding that current work is heavily algorithmic, focused on post-processing and group fairness, and lacks industrial collaboration and high readiness levels. The conclusion calls for integrating fairness across the entire software development lifecycle and increasing academia-industry partnerships.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[系统性映射研究:软件公平性<br/>A Systematic Mapping on Software Fairness] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>缺乏对软件公平性解决方案的全面理解<br/>Lack of comprehensive understanding of fairness solutions] --> Problem_Sub[挑战/Challenge<br/>研究分散，工业应用不明确<br/>Fragmented research, unclear industrial viability]
        Method[主要方法/Method<br/>系统性文献映射<br/>Systematic Literature Mapping] --> Method_Sub[应用分类框架分析95项研究<br/>Apply classification framework to 95 studies]
        Results[关键结果/Results<br/>研究发现/Findings] --> Results_Sub1[研究趋势/Research Trends<br/>扩展中但集中于算法<br/>Expanding but focused on algorithms]
        Results --> Results_Sub2[研究焦点/Research Focus<br/>后处理和群体公平性<br/>Post-processing & group fairness]
        Results --> Results_Sub3[工业背景/Industrial Context<br/>学术主导，TRL低至中<br/>Academic-led, low-to-medium TRL]
    ```

- **[arXiv260101] Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education**
  - **tags:** [other], [AI in Education], [AI literacy, teacher perceptions, quantitative survey, ethics, infrastructure challenges]
  - **authors:** Bruno Florentino, Camila Sestito, Wellington Cruz, André de Carvalho, Robson Bonidia
  - **institution:** University of São Paulo, Federal University of Technology-Paraná (UTFPR), Instituto Significare
  - **link:** https://arxiv.org/pdf/2512.23834
  - **contributions:** 1. Provides empirical data on the AI literacy levels and application interests of Brazilian K-12 teachers, revealing a high interest despite low knowledge. 2. Identifies key structural barriers (lack of training, technical support, and infrastructure) to AI adoption in Brazilian public education. 3. Highlights the critical importance teachers place on discussing ethics, digital citizenship, and responsible AI use within the pedagogical context.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9fd8e32651f0740b552d9443daf7c424819558f55a942221ff741ca45c296c9_w640_q70.webp
  - **Simple LLM Summary:** This study quantitatively analyzes Brazilian K-12 teachers' perceptions of AI in education through a survey of 346 educators. The results show strong teacher interest in using AI for pedagogical tasks despite limited knowledge, while identifying significant structural challenges and emphasizing the need for ethical discussions. The study concludes that effective AI integration in Brazil requires integrated public policies, teacher training, and equitable access to technology.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(巴西教师对AI的认知与态度/Brazilian Teachers' Perceptions and Attitudes towards AI)
        Problem --> P2(AI在教育中的伦理与公平挑战/Ethical and Equity Challenges of AI in Education)
        Method --> M1(定量问卷调查/Quantitative Questionnaire Survey)
        M1 --> M1_1(346名巴西K-12教师/346 Brazilian K-12 Teachers)
        Results --> R1(高兴趣但知识有限/High Interest but Limited Knowledge)
        Results --> R2(关注伦理与结构挑战/Concerns on Ethics and Structural Challenges)
        Results --> R3(需要政策与培训支持/Need for Policy and Training Support)
    ```

- **[arXiv260101] Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis**
  - **tags:** [nlp], [conversational ai], [mental health crisis, stages of change model, human-AI interaction, testimonial survey, expert interviews]
  - **authors:** Leah Hope Ajmani, Arka Ghosh, Benjamin Kaveladze, Eugenia Kim, Keertana Namuduri, Theresa Nguyen, Ebele Okoli, Jessica Schleider, Denae Ford, Jina Suh
  - **institution:** University of Minnesota, Northwestern University, Dartmouth College, Microsoft, Microsoft Research, Mental Health America
  - **link:** https://arxiv.org/pdf/2512.23859
  - **contributions:** 1. Provides first-person experiential data on using conversational AI during mental health crises via a testimonial survey (n=53). 2. Contrasts user experiences with mental health expert perspectives (n=16) to highlight the essential role of human connection in crisis management. 3. Proposes a responsible design framework for AI crisis intervention, positioning AI as a bridge to human support using the stages of change model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/557b0c2624da79d40758f334c7d781c4558951ee96a27d54b04a81b3f20ec2ea_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how people use conversational AI (e.g., ChatGPT) during mental health crises through a survey and expert interviews. It finds users turn to AI due to gaps in human support, but experts emphasize human connection is crucial. The study concludes that responsible AI should act as a bridge to human help, increasing preparedness for positive action and de-escalating crises.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis] --> B(核心问题/Problem: Can conversational AI responsibly support mental health crises?)
        A --> C(主要方法/Method: Testimonial survey (n=53) & expert interviews (n=16))
        A --> D(关键结果/Results: AI fills gaps in human support; Human connection is essential; Design AI as a bridge to human help)
    ```

- **[arXiv260101] Improving Reliability of Human Trafficking Alerts in Airports**
  - **tags:** [sys], [delay tolerant networks], [Delay Tolerant Networks, Mobile Ad Hoc Networks, Opportunistic Network Environment, Spray and Wait, Epidemic]
  - **authors:** Nana Oye Akrofi Quarcoo, Milena Radenkovic
  - **institution:** The University of Nottingham
  - **link:** https://arxiv.org/pdf/2512.23865
  - **contributions:** 1. Investigates the application of DTN protocols (Spray and Wait, Epidemic) for emergency alerting in airport human trafficking scenarios. 2. Simulates and evaluates the performance of these protocols in terms of delivery ratio and latency using the ONE simulator. 3. Discusses the potential role and limitations of DTN networks in combating human trafficking, bridging a technical evaluation with a critical real-world application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e1cbec09d2308ac06a3bef24acf6366e80996c8ef8af583749661d0fe4f3bf_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates using Delay Tolerant Network (DTN) protocols to improve the reliability of emergency alerts for human trafficking victims in airports where conventional networks are unavailable. It simulates the scenario using the ONE simulator to evaluate the performance of Spray and Wait and Epidemic protocols on delivery ratio and latency. The study compares the protocols' advantages and limitations, concluding on their potential role in addressing this global issue.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improving Reliability of Human Trafficking Alerts in Airports] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[机场人口贩卖紧急警报的可靠性/Reliability of emergency alerts for human trafficking in airports]
        C --> C1[应用DTN协议: Spray and Wait, Epidemic/Apply DTN protocols: Spray and Wait, Epidemic]
        C --> C2[使用ONE模拟器进行仿真/Simulation using the ONE simulator]
        D --> D1[评估交付率和延迟/Evaluate delivery ratio and latency]
        D --> D2[讨论协议优缺点与潜在作用/Discuss protocol pros/cons and potential role]
    ```

- **[arXiv260101] How Large Language Models Systematically Misrepresent American Climate Opinions**
  - **tags:** [nlp], [large language model evaluation], [large language models, public opinion simulation, intersectionality, bias evaluation, climate policy]
  - **authors:** Sola Kim, Jieshu Wang, Marco A. Janssen, John M. Anderies
  - **institution:** Arizona State University, Stony Brook University
  - **link:** https://arxiv.org/pdf/2512.23889
  - **contributions:** 1. Conducted the first comparative study of LLM-generated public opinion against real human survey responses across intersecting demographic identities (race and gender). 2. Identified a systematic "compression" bias in LLMs, where they flatten the diversity of climate opinions by overestimating concern in less-concerned groups and underestimating it in more-concerned groups. 3. Revealed that this bias is intersectional, showing that LLMs apply uniform gender assumptions that fail for specific racial groups (e.g., misrepresenting gender patterns among Black Americans), a flaw potentially invisible to standard audits.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0e5dfdc0fba7038277e876cd0c81062b3c5735782d5df732873aec991a84b3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how six large language models (LLMs) represent U.S. climate opinions by prompting them with profiles from a real national survey and comparing their generated responses to actual human answers. The study finds that LLMs systematically compress opinion diversity and misrepresent intersectional patterns, particularly for Black Americans, which could undermine equitable policy-making.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["How Large Language Models Systematically Misrepresent American Climate Opinions<br>论文标题"] --> B["Problem: LLMs used for public opinion analysis may misrepresent diverse, intersectional views.<br>核心问题：用于公众意见分析的LLM可能歪曲多样化的交叉性观点。"]
        A --> C["Method: Prompt 6 LLMs with real survey respondent profiles and compare outputs to human answers.<br>主要方法：用真实调查受访者档案提示6个LLM，并将输出与人类答案比较。"]
        A --> D["Results: LLMs compress opinion diversity and misapply gender assumptions across racial groups.<br>关键结果：LLM压缩了意见多样性，并在不同种族群体中误用了性别假设。"]
    ```

- **[arXiv260101] In Memorium: The Academic Journal**
  - **tags:** [other], [academic publishing], [academic journal, peer review, scholarly communication, publishing model, history of science]
  - **authors:** Russell Beale
  - **institution:** University of Birmingham
  - **link:** https://arxiv.org/pdf/2512.23915
  - **contributions:** 1. Provides a historical narrative tracing the evolution of the academic journal from its 17th-century origins to its modern commercial form. 2. Critically analyzes the shift in the journal's role from a tool for scientific dissemination to a metric for career advancement and gatekeeping. 3. Highlights the commercial exploitation of the academic publishing model, where publishers profit from free academic labor and a captive institutional market.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d84666cdf04fcf7aca61e98202f63431df72a15f98090898c0d56de610147bb_w640_q70.webp
  - **Simple LLM Summary:** This reflective piece examines the life cycle and societal impact of the academic journal. It traces its history from a scholarly dissemination tool to a commercialized metric for academic prestige, concluding that while its original ideals will be mourned, its final form had strayed so far that its passing will be less lamented.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[In Memoriam: The Academic Journal] --> Problem[核心问题/Problem: How did the academic journal evolve and what is its legacy?]
        Root --> Method[主要方法/Method: Historical analysis and critical reflection on its role and influence.]
        Root --> Results[关键结果/Results: Mourned for original ideals, but less missed due to commercial deviation.]
    ```

- **[arXiv260101] Disentangling Learning from Judgment: Representation Learning for Open Response Analytics**
  - **tags:** [nlp], [educational data mining], [sentence embeddings, rater effects, residualization, teacher priors, interpretability]
  - **authors:** Conrad Borchers, Manit Patel, Seiyon M. Lee, Anthony F. Botelho
  - **institution:** Carnegie Mellon University, University of Florida
  - **link:** https://arxiv.org/pdf/2512.23941
  - **contributions:** 1. An analytics-first framework that disentangles student response content from teacher grading tendencies, making rater effects visible and auditable. 2. A modeling pipeline using dynamic teacher priors and residualized sentence embeddings to mitigate prompt and rater confounds, validated temporally. 3. A projection method to surface disagreements between content and rater signals for qualitative inspection, transforming embeddings into reflective learning analytics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/969e6dbddbe7eb87582932064e8e7d3a0853ca2850b5329fafc1020e7228f365_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of conflating student response content with teacher grading bias in automated scoring. The proposed method uses dynamic teacher priors and residualized sentence embeddings to separate these signals, with linear models quantifying their contributions. The main conclusion is that teacher priors heavily influence predictions, and adjusting for them sharpens the content representation, enabling better analysis of student understanding versus grading practices.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Disentangling Learning from Judgment<br>学习与评判的解耦] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Automated scoring conflates content with rater bias<br>自动评分混淆了内容与评分者偏差]
        C --> C1[Framework separates content signals & teacher priors<br>框架分离内容信号与教师先验]
        C --> C2[Uses centering & residualization on embeddings<br>对嵌入使用中心化与残差化]
        C --> C3[Temporal validation & projection for inspection<br>时间验证与投影以供检查]
        D --> D1[Teacher priors heavily influence grades<br>教师先验严重影响评分]
        D --> D2[Combined model achieves best AUC (~0.815)<br>组合模型取得最佳AUC]
        D --> D3[Residual content reveals student understanding<br>残差内容揭示学生理解]
    ```

- **[arXiv260101] Statistical Guarantees in the Search for Less Discriminatory Algorithms**
  - **tags:** [ai], [algorithmic fairness], [model multiplicity, optimal stopping, disparate impact, statistical guarantees, less discriminatory algorithms]
  - **authors:** Chris Hays, Ben Laufer, Solon Barocas, Manish Raghavan
  - **institution:** MIT, Cornell University, Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.23943
  - **contributions:** 1. Formalizes the search for less discriminatory algorithms (LDAs) as an optimal stopping problem, providing a statistical framework to define a "good-faith effort" in model development. 2. Proposes an adaptive stopping algorithm that yields a high-probability upper bound on the potential gains from continued search, allowing developers to certify the sufficiency of their exploration. 3. Provides a flexible framework where developers can incorporate stronger assumptions about the model distribution to obtain correspondingly stronger statistical bounds, validated on real-world datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/323b958070f176d29545b148d49cdc3b6141db385ed752500b67c6d9556a8c78_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of how firms can demonstrate a good-faith effort to find less discriminatory algorithms. It proposes an adaptive stopping algorithm based on optimal stopping theory, which provides statistical guarantees on the potential benefits of further search. The method allows developers to certify that their search for fairer models was sufficient, as validated on credit, employment, and housing datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Statistical Guarantees in the Search for Less Discriminatory Algorithms<br>寻找更少歧视性算法的统计保证"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>What constitutes a good-faith search for less discriminatory models?<br>什么是对更少歧视性模型的真诚搜索？"]
        Method["主要方法/Method<br>Formalize search as an optimal stopping problem; propose adaptive stopping algorithm.<br>将搜索形式化为最优停止问题；提出自适应停止算法。"]
        Results["关键结果/Results<br>High-probability bound on search gains; framework for certification.<br>搜索收益的高概率上界；用于认证的框架。"]
    ```

- **[arXiv260101] From artificial to circular intelligence to support the well-being of our habitat**
  - **tags:** [other], [sustainable ai], [Circular Intelligence, CIntel, socio-environmental impact, bottom-up approach, ethical design]
  - **authors:** Francesca Larosa, Daniel Depellegrin, Andrea Conte, Marco Molinari, Silvia Santato, Adam Wickberg, Fermin Mallor, Anna Sperotto
  - **institution:** Royal Institute of Technology (KTH)
  - **link:** https://arxiv.org/pdf/2512.24131
  - **contributions:** 1. Proposes a novel conceptual and procedural framework called Circular Intelligence (CIntel) to address the environmental impact of AI. 2. Introduces a bottom-up, community-driven approach for AI design that learns from nature's regenerative and adaptive abilities. 3. Operationalizes the framework through a set of economic incentives promoting a shared-cost-distributed benefits paradigm.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c613cbcdae547ac2c22485aba0219d44e2ac5ed102d38078c7427fe50cf1e99_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies the significant socio-environmental impact of data- and resource-intensive AI technologies. To address this, it proposes a new framework called Circular Intelligence (CIntel), which incorporates ethical principles and a nature-inspired, community-driven design approach to promote habitat stability and human well-being. The main conclusion is that CIntel offers a pathway to develop AI tools with minimal negative impact.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[From artificial to circular intelligence to support the well-being of our habitat] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[AI技术对地球有负面影响/AI technologies have negative impact on Earth]
    C --> C1[提出循环智能框架/Propose Circular Intelligence (CIntel) framework]
    C1 --> C2[自下而上、社区驱动的方法/Bottom-up, community-driven approach]
    C1 --> C3[学习自然的再生与适应能力/Learn from nature's regeneration & adaptation]
    C1 --> C4[设计中融入伦理原则/Incorporate ethical principles in design]
    D --> D1[通过经济激励实现操作化/Operationalized via economic incentives]
    D --> D2[促进共享成本-分布式收益范式/Promote shared-cost-distributed benefits paradigm]
    ```

- **[arXiv260101] Effects of Algorithmic Visibility on Conspiracy Communities: Reddit after Epstein's 'Suicide'**
  - **tags:** [other], [social computing], [algorithmic visibility, survival analysis, linguistic integration, toxicity scores, user retention]
  - **authors:** Asja Attanasio, Francesco Corso, Gianmarco De Francisci Morales, Francesco Pierri
  - **institution:** Politecnico di Milano, CENTAI
  - **link:** https://arxiv.org/pdf/2512.24351
  - **contributions:** 1. Demonstrates that algorithmic homepage visibility acts as a selection mechanism, not just an amplifier, for conspiracy community membership. 2. Shows that users arriving via homepage exposure integrate less linguistically and have shorter, less stable engagement than organic discoverers. 3. Provides evidence that incidental algorithmic exposure in this context does not lead to durable radicalization, challenging a standard narrative.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1539ae39cc88306cb031a18e836539c07d88fd3d29d66ab934909984f6c9fd8c_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how algorithmic visibility on Reddit's homepage shapes the r/conspiracy community after Jeffrey Epstein's death. Using a computational framework combining toxicity scores, survival analysis, and lexical/semantic measures, it finds that homepage exposure selects for users who integrate poorly and leave quickly, unlike organic joiners. The results suggest algorithmic visibility reshapes community composition and limits organic growth, without producing the durable radicalization often assumed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Effects of Algorithmic Visibility on Conspiracy Communities: Reddit after Epstein's 'Suicide'] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(算法可见性如何影响阴谋论社区的用户行为？<br/>How does algorithmic visibility affect user behavior in conspiracy communities?)
        C --> C1(计算框架：毒性分数、生存分析、词汇/语义测量<br/>Computational framework: toxicity scores, survival analysis, lexical/semantic measures)
        D --> D1(主页可见性是选择机制，非放大器<br/>Homepage visibility is a selection mechanism, not an amplifier)
        D --> D2(有机用户整合更快更稳定<br/>Organic users integrate faster and are more stable)
        D --> D3(算法推荐用户参与短暂，整合弱<br/>Algorithmically-recommended users participate briefly, integrate weakly)
    ```

- **[arXiv260101] Learning Context: A Unified Framework and Roadmap for Context-Aware AI in Education**
  - **tags:** [ai], [ai for education], [Learning Context (LC) framework, Model Context Protocol (MCP), warm-start personalization, privacy-preserving data enclaves]
  - **authors:** Naiming Liu, Brittany Bradford, Johaun Hatchett, Gabriel Diaz, Lorenzo Luzi, Zichao Wang, Debshila Basu Mallick, Richard Baraniuk
  - **institution:** Rice University, OpenStax, SafeInsights, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.24362
  - **contributions:** 1. Proposes a unified Learning Context (LC) framework to encode cognitive, affective, and sociocultural factors for holistic learner understanding. 2. Outlines a roadmap to operationalize the LC theory into an interoperable computational data structure, leveraging the Model Context Protocol (MCP) to enable AI tools with durable context for long-term personalization. 3. Details a concrete implementation strategy through the OpenStax platform and SafeInsights infrastructure, ensuring privacy-first, ethical deployment at a national scale to reduce equity gaps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b97e47cc70cdc86357a1cf43eaf9957e6f2b2e079cda07dbe3e99b2877e3bb0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a unified Learning Context framework to move AI in education from context-blind approaches to a holistic, principled understanding of learners. It proposes operationalizing this framework into a computational data structure using the Model Context Protocol and details an implementation plan via the OpenStax and SafeInsights ecosystems for privacy-preserving, large-scale deployment. The work aims to achieve continual, personalized learning while maintaining high ethical standards and reducing educational inequities.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Learning Context: A Unified Framework and Roadmap for Context-Aware AI in Education] --> B
        A --> C
        A --> D
        B[核心问题/Problem: AI教育缺乏情境感知 / AI in Education Lacks Context-Awareness]
        C[主要方法/Method: 提出统一学习情境框架与MCP协议 / Propose Unified LC Framework & MCP Protocol]
        D[关键结果/Results: 在OpenStax/SafeInsights中实现隐私优先的个性化 / Implement Privacy-First Personalization in OpenStax/SafeInsights]
    ```

- **[arXiv260101] From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting**
  - **tags:** [cv], [urban scene understanding], [generative inpainting, semantic segmentation, multimodal visual features, MLLM, street view imagery]
  - **authors:** Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao
  - **institution:** Hunan Normal University, Beijing Normal University, China University of Geosciences
  - **link:** https://arxiv.org/pdf/2512.24513
  - **contributions:** 1. Proposed a controlled framework using semantic segmentation and MLLM-guided generative inpainting to create paired street view images with/without dynamic elements (pedestrians, vehicles) for perceptual impact analysis. 2. Conducted a perception experiment revealing that removing dynamic elements causes a significant, consistent decrease in perceived vibrancy (30.97%), with heterogeneous effects on other dimensions and individual-level variations. 3. Extended the analysis to city-scale by training machine learning models to predict vibrancy changes, finding these perceptual alterations are widespread and spatially structured, indicating a potential underestimation of urban liveliness in static-image-based assessments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b92ccdf405994f35d8bd91d63d957a4a7b12d20cff08ed5e4958c236edcb4a3_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the bias in urban perception studies that treat scenes as static by proposing a framework to isolate the effect of dynamic elements. Using semantic segmentation and MLLM-guided generative inpainting on street view images from Dongguan, China, the study found that removing pedestrians and vehicles significantly reduces perceived vibrancy. The findings were validated with machine learning models and extended to a city-scale, showing that static imagery likely underestimates urban liveliness.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting] --> B
    A --> C
    A --> D
    B[核心问题/Problem: Most urban perception studies treat scenes as static, ignoring dynamic elements like pedestrians and vehicles, causing potential bias.]
    C[主要方法/Method: Use semantic segmentation and MLLM-guided generative inpainting to create paired images (with/without dynamic elements) and conduct perception experiments.]
    D[关键结果/Results: Removing dynamic elements causes a 30.97% decrease in vibrancy; changes are widespread and spatially structured at city scale.]
    ```

- **[arXiv260101] Big AI is accelerating the metacrisis: What can we do?**
  - **tags:** [nlp], [ethics & society], [metacrisis, language engineers, human flourishing, planetary boundaries, technofeudalism]
  - **authors:** Steven Bird
  - **institution:** Charles Darwin University
  - **link:** https://arxiv.org/pdf/2512.24863
  - **contributions:** 1. Identifies and critiques the role of "Big AI" and language engineers in accelerating converging global crises (ecological, meaning, language). 2. Highlights the ethical conflict between professional obligations (e.g., ACL Code of Ethics) and the harms caused by current NLP/AI development practices. 3. Proposes a paradigm shift for NLP, advocating for a future centered on human flourishing and amplifying social networks rather than scaling through large, polluting models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the current trajectory of "Big AI," particularly in NLP, is accelerating a global metacrisis. It critiques the field's focus on scalability and value-neutral technology development, which benefits powerful interests at the expense of the public good and the planet. The paper concludes by urgently calling for an alternative, life-affirming future for NLP centered on human flourishing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Big AI is accelerating the metacrisis: What can we do?] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Big AI加速生态、意义和语言危机/Big AI accelerates ecological, meaning, and language crises]
        B --> B2[语言工程师的伦理困境/Ethical dilemma of language engineers]
        C --> C1[批判当前可扩展性叙事/Critique current scalability narrative]
        C --> C2[呼吁探索替代方案/Call to explore alternatives]
        D --> D1[需要以人类繁荣为中心的未来/NLP future must center human flourishing]
        D --> D2[利用集体智慧设计生命肯定的NLP/Design life-affirming NLP with collective intelligence]
    ```

- **[arXiv260101] The Impact of LLMs on Online News Consumption and Production**
  - **tags:** [ai], [ai economics], [staggered difference-in-differences, synthetic difference-in-differences, robots.txt]
  - **authors:** Hangcheng Zhao, Ron Berman
  - **institution:** Rutgers Business School, The Wharton School of the University of Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.24968
  - **contributions:** 1. Quantified a moderate decline in news publisher website traffic following the rise of generative AI. 2. Demonstrated that blocking GenAI bots via robots.txt can paradoxically reduce total and real consumer traffic for large publishers. 3. Provided empirical evidence that, contrary to predictions, LLMs have not yet reduced editorial hiring and have shifted publisher content strategy towards rich media and advertising.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7836df9e705d8a023a351c30d3595b1ff6e9614cf1d805218347987098aa3882_w640_q70.webp
  - **Simple LLM Summary:** This paper empirically investigates the impact of Large Language Models (LLMs) on online news publishers using high-frequency data and causal inference methods like difference-in-differences. It finds that blocking LLM crawlers reduces publisher traffic, LLMs have not yet replaced editorial jobs, and publishers are shifting to rich content and advertising tech. The results reveal unforeseen consequences of LLM adoption on the news ecosystem.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Impact of LLMs on Online News Consumption and Production] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs如何影响新闻生产与消费/How LLMs affect news production and consumption]
        C --> C1[高频数据与因果推断/High-frequency data & Causal inference]
        D --> D1[流量下降/Traffic decline]
        D --> D2[屏蔽爬虫反效果/Blocking bots backfires]
        D --> D3[编辑岗位未减少/Editorial jobs not reduced]
        D --> D4[内容转向富媒体/Shift to rich content]
    ```
