---
slug: /daily/cscy/20251229-20260104
---
# 20251229-20260104 (cs.CY)

## 2025-12-29

- **[arXiv251229] SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram**
  - **tags:** [sec], [cyber threat intelligence], [multi-modal fusion, large language models, graph neural networks, early detection, social media analysis]
  - **authors:** Mohammad Hammas Saeed, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21380
  - **contributions:** 1. Proposes SENTINEL, a multi-modal framework for early cyber threat detection by aligning social media discussions with real-world attacks. 2. Combines language modeling (using LLMs) and network coordination analysis (using GNNs) to fuse textual and relational signals from platforms like Telegram. 3. Demonstrates the framework's effectiveness on a dataset of 365k messages from 16 Telegram channels, achieving an F1 score of 0.89 for threat alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3e47a235de6afcb4955afd774a9e4b3883efcafc7e4aaa97649575ef2fb34d0_w640_q70.webp
  - **Simple LLM Summary:** The paper presents SENTINEL, a framework for the early detection of cyber threats by analyzing multi-modal signals from social media platforms like Telegram. It combines large language models for text understanding with graph neural networks to model user coordination, successfully aligning online discussions to real-world attacks. The evaluation on Telegram data shows the approach is effective, achieving a high F1 score of 0.89.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram"] --> Problem["核心问题/Problem: Post-hoc detection of cyber attacks is reactive; need for proactive, early warning systems."]
        Root --> Method["主要方法/Method: Multi-modal framework combining LLMs (language) and GNNs (coordination graphs) to analyze social media signals."]
        Root --> Results["关键结果/Results: Achieves F1 of 0.89 aligning Telegram discussions to real-world cyber threats."]
    ```

- **[arXiv251229] ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks**
  - **tags:** [sec], [social network security], [Graph Neural Networks, influence campaigns, temporal link prediction, troll detection, Reddit]
  - **authors:** Mohammad Hammas Saeed, Isaiah J. King, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21391
  - **contributions:** 1. Proposes ALETHEIA, a system that formalizes the detection of malicious accounts in influence campaigns as a node classification and link prediction problem using a graph-based representation. 2. Demonstrates that a detection pipeline combining topological (graph) and linguistic features outperforms standard interaction and user features, achieving a 3.7% F1-score improvement. 3. Introduces a novel temporal link prediction mechanism for influence campaigns by stacking a GNN over an RNN to forecast future troll interactions (TTE/TUE) with high accuracy (96.6% AUC).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/473a865358e998c62938f17660edc395a7658bf360ef15e62ea79317e8734aec_w640_q70.webp
  - **Simple LLM Summary:** This paper presents ALETHEIA, a system that uses Graph Neural Networks (GNNs) to detect malicious accounts and predict their future interactions in social media influence campaigns. By modeling campaigns as graphs and combining structural and linguistic features, it improves detection performance and forecasts troll behavior with high accuracy. The results underscore the importance of leveraging network structure to combat coordinated malicious activity online.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks] --> B[核心问题/Problem: Detecting and predicting malicious influence campaigns on social media]
        A --> C[主要方法/Method: Graph Neural Networks (GNNs) with topological & linguistic features, GNN+RNN for temporal link prediction]
        A --> D[关键结果/Results: 3.7% F1-score improvement in detection, 96.6% AUC for predicting future troll interactions]
    ```

- **[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments**
  - **tags:** [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]
  - **authors:** Hua Shen
  - **institution:** NYU Shanghai, New York University
  - **link:** https://arxiv.org/pdf/2512.21552
  - **contributions:** 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]
        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]
        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]
    ```

- **[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets**
  - **tags:** [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]
  - **authors:** Matyas Bohacek, Ignacio Vilanova Echavarri
  - **institution:** Stanford University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21775
  - **contributions:** 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation")
        Problem --> P2("数据来源与合法性信息丢失/Loss of data origin & legitimacy info")
        Method --> M1("提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework")
        Method --> M2("开发基于数据溯源技术的开源库/Develop open-source library using data provenance")
        Results --> R1("评估现有数据集的合规性/Evaluate compliance of existing datasets")
        Results --> R2("指导负责任的新数据集构建/Guide responsible construction of new datasets")
    ```

- **[arXiv251229] On The Conceptualization and Societal Impact of Cross-Cultural Bias**
  - **tags:** [nlp], [bias and fairness], [cultural bias, literature survey, societal impact, harm evaluation, bias mitigation]
  - **authors:** Vitthal Bhandari
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21809
  - **contributions:**  1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("On The Conceptualization and Societal Impact of Cross-Cultural Bias") --> Problem("核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.")
    Root --> Method("主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.")
    Root --> Results("关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.")
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management**
  - **tags:** [nlp], [AI Governance & Compliance], [lifecycle management, bias detection, differential privacy, federated learning, terminology drift]
  - **authors:** Sunil Arora, John Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.22060
  - **contributions:** 1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks."]
        Method["主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy)."]
        Results["关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study."]
    ```

- **[arXiv251229] Agent-based simulation of online social networks and disinformation**
  - **tags:** [ai], [agent-based simulation], [agent-based simulation, large language model, disinformation campaigns, synthetic social networks, behavioral automata]
  - **authors:** Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente
  - **institution:** University of Murcia
  - **link:** https://arxiv.org/pdf/2512.22082
  - **contributions:** 1. A simulation framework that models synthetic social networks using agents with demographic-based personality traits and finite-state behavioral automata for realistic and interpretable actions. 2. A generative module powered by an LLM to produce context-aware social media posts consistent with each agent's profile and memory. 3. A red module implementing DISARM-inspired workflows to orchestrate disinformation campaigns and a Mastodon-based visualization layer for real-time inspection and validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a3415bee27d71926e7770fd596253ec973faaa85d1fcba853a047ff6d08bfe3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an agent-based simulation framework to study online social networks and disinformation, addressing the limitations of platform opacity and data access. The framework uses LLM-powered agents with personality traits and behavioral automata to generate realistic content and simulate disinformation campaigns, with evaluation showing structural, behavioral, and linguistic realism. It provides a customizable and controllable environment for studying information dynamics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Agent-based simulation of online social networks and disinformation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[平台不透明与数据限制/Platform Opacity & Data Limits]
        Problem --> P2[现有模拟缺乏真实性与可解释性/Existing Simulations Lack Realism & Explainability]
        Method[主要方法/Method] --> M1[基于代理的合成社交网络/Agent-based Synthetic Social Networks]
        Method --> M2[LLM生成上下文感知内容/LLM Generates Context-aware Content]
        Method --> M3[红色模块模拟虚假信息活动/Red Module Simulates Disinformation Campaigns]
        Method --> M4[Mastodon可视化层/Mastodon Visualization Layer]
        Results[关键结果/Results] --> R1[展示结构、行为、语言真实性/Demonstrates Structural, Behavioral, Linguistic Realism]
        Results --> R2[为研究信息动态提供可定制环境/Provides Customizable Environment for Studying Information Dynamics]
    ```
