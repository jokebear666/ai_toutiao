---
slug: /daily/cscy/20251229-20260104
---
# 20251229-20260104 (cs.CY)

## 2025-12-29

- **[arXiv251229] SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram**
  - **tags:** [sec], [cyber threat intelligence], [multi-modal fusion, large language models, graph neural networks, early detection, social media analysis]
  - **authors:** Mohammad Hammas Saeed, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21380
  - **contributions:** 1. Proposes SENTINEL, a multi-modal framework for early cyber threat detection by aligning social media discussions with real-world attacks. 2. Combines language modeling (using LLMs) and network coordination analysis (using GNNs) to fuse textual and relational signals from platforms like Telegram. 3. Demonstrates the framework's effectiveness on a dataset of 365k messages from 16 Telegram channels, achieving an F1 score of 0.89 for threat alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3e47a235de6afcb4955afd774a9e4b3883efcafc7e4aaa97649575ef2fb34d0_w640_q70.webp
  - **Simple LLM Summary:** The paper presents SENTINEL, a framework for the early detection of cyber threats by analyzing multi-modal signals from social media platforms like Telegram. It combines large language models for text understanding with graph neural networks to model user coordination, successfully aligning online discussions to real-world attacks. The evaluation on Telegram data shows the approach is effective, achieving a high F1 score of 0.89.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram"] --> Problem["核心问题/Problem: Post-hoc detection of cyber attacks is reactive; need for proactive, early warning systems."]
        Root --> Method["主要方法/Method: Multi-modal framework combining LLMs (language) and GNNs (coordination graphs) to analyze social media signals."]
        Root --> Results["关键结果/Results: Achieves F1 of 0.89 aligning Telegram discussions to real-world cyber threats."]
    ```

- **[arXiv251229] ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks**
  - **tags:** [sec], [social network security], [Graph Neural Networks, influence campaigns, temporal link prediction, troll detection, Reddit]
  - **authors:** Mohammad Hammas Saeed, Isaiah J. King, Howie Huang
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.21391
  - **contributions:** 1. Proposes ALETHEIA, a system that formalizes the detection of malicious accounts in influence campaigns as a node classification and link prediction problem using a graph-based representation. 2. Demonstrates that a detection pipeline combining topological (graph) and linguistic features outperforms standard interaction and user features, achieving a 3.7% F1-score improvement. 3. Introduces a novel temporal link prediction mechanism for influence campaigns by stacking a GNN over an RNN to forecast future troll interactions (TTE/TUE) with high accuracy (96.6% AUC).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/473a865358e998c62938f17660edc395a7658bf360ef15e62ea79317e8734aec_w640_q70.webp
  - **Simple LLM Summary:** This paper presents ALETHEIA, a system that uses Graph Neural Networks (GNNs) to detect malicious accounts and predict their future interactions in social media influence campaigns. By modeling campaigns as graphs and combining structural and linguistic features, it improves detection performance and forecasts troll behavior with high accuracy. The results underscore the importance of leveraging network structure to combat coordinated malicious activity online.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks] --> B[核心问题/Problem: Detecting and predicting malicious influence campaigns on social media]
        A --> C[主要方法/Method: Graph Neural Networks (GNNs) with topological & linguistic features, GNN+RNN for temporal link prediction]
        A --> D[关键结果/Results: 3.7% F1-score improvement in detection, 96.6% AUC for predicting future troll interactions]
    ```

- **[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments**
  - **tags:** [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]
  - **authors:** Hua Shen
  - **institution:** NYU Shanghai, New York University
  - **link:** https://arxiv.org/pdf/2512.21552
  - **contributions:** 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]
        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]
        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]
    ```

- **[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets**
  - **tags:** [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]
  - **authors:** Matyas Bohacek, Ignacio Vilanova Echavarri
  - **institution:** Stanford University, Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21775
  - **contributions:** 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation")
        Problem --> P2("数据来源与合法性信息丢失/Loss of data origin & legitimacy info")
        Method --> M1("提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework")
        Method --> M2("开发基于数据溯源技术的开源库/Develop open-source library using data provenance")
        Results --> R1("评估现有数据集的合规性/Evaluate compliance of existing datasets")
        Results --> R2("指导负责任的新数据集构建/Guide responsible construction of new datasets")
    ```

- **[arXiv251229] On The Conceptualization and Societal Impact of Cross-Cultural Bias**
  - **tags:** [nlp], [bias and fairness], [cultural bias, literature survey, societal impact, harm evaluation, bias mitigation]
  - **authors:** Vitthal Bhandari
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21809
  - **contributions:**  1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp
  - **Simple LLM Summary:** This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("On The Conceptualization and Societal Impact of Cross-Cultural Bias") --> Problem("核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.")
    Root --> Method("主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.")
    Root --> Results("关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.")
    ```

- **[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**
  - **tags:** [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]
  - **authors:** Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
  - **institution:** Zhejiang University, University of California, Los Angeles, Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.21871
  - **code:** https://github.com/bluedream02/CopyGuard
  - **contributions:** 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]
        Root --> Problem["核心问题/Problem: LVLMs may infringe copyright when processing visual inputs"]
        Root --> Method["主要方法/Method: Benchmark dataset & Tool-augmented defense framework"]
        Root --> Results["关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk"]
    ```

- **[arXiv251229] Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management**
  - **tags:** [nlp], [AI Governance & Compliance], [lifecycle management, bias detection, differential privacy, federated learning, terminology drift]
  - **authors:** Sunil Arora, John Hastings
  - **institution:** Dakota State University
  - **link:** https://arxiv.org/pdf/2512.22060
  - **contributions:** 1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks."]
        Method["主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy)."]
        Results["关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study."]
    ```

- **[arXiv251229] Agent-based simulation of online social networks and disinformation**
  - **tags:** [ai], [agent-based simulation], [agent-based simulation, large language model, disinformation campaigns, synthetic social networks, behavioral automata]
  - **authors:** Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente
  - **institution:** University of Murcia
  - **link:** https://arxiv.org/pdf/2512.22082
  - **contributions:** 1. A simulation framework that models synthetic social networks using agents with demographic-based personality traits and finite-state behavioral automata for realistic and interpretable actions. 2. A generative module powered by an LLM to produce context-aware social media posts consistent with each agent's profile and memory. 3. A red module implementing DISARM-inspired workflows to orchestrate disinformation campaigns and a Mastodon-based visualization layer for real-time inspection and validation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a3415bee27d71926e7770fd596253ec973faaa85d1fcba853a047ff6d08bfe3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an agent-based simulation framework to study online social networks and disinformation, addressing the limitations of platform opacity and data access. The framework uses LLM-powered agents with personality traits and behavioral automata to generate realistic content and simulate disinformation campaigns, with evaluation showing structural, behavioral, and linguistic realism. It provides a customizable and controllable environment for studying information dynamics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Agent-based simulation of online social networks and disinformation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[平台不透明与数据限制/Platform Opacity & Data Limits]
        Problem --> P2[现有模拟缺乏真实性与可解释性/Existing Simulations Lack Realism & Explainability]
        Method[主要方法/Method] --> M1[基于代理的合成社交网络/Agent-based Synthetic Social Networks]
        Method --> M2[LLM生成上下文感知内容/LLM Generates Context-aware Content]
        Method --> M3[红色模块模拟虚假信息活动/Red Module Simulates Disinformation Campaigns]
        Method --> M4[Mastodon可视化层/Mastodon Visualization Layer]
        Results[关键结果/Results] --> R1[展示结构、行为、语言真实性/Demonstrates Structural, Behavioral, Linguistic Realism]
        Results --> R2[为研究信息动态提供可定制环境/Provides Customizable Environment for Studying Information Dynamics]
    ```

## 2025-12-30

- **[arXiv251230] Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models**
  - **tags:** [ai], [peer review automation], [large language models, peer review, pre-review, citation prediction, review alignment]
  - **authors:** Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi
  - **institution:** AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI
  - **link:** https://arxiv.org/pdf/2512.22145
  - **contributions:** 1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]
        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]
        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]
        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]
    ```

- **[arXiv251230] AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History**
  - **tags:** [nlp], [semantic change detection], [diachronic embeddings, orthogonal Procrustes, lexical drift]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.22196
  - **contributions:** 1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("数字人文中语义变迁分析<br>Digital Humanities Semantic Shift Analysis")
        C --> C1("可复现的专家系统流程<br>Reproducible Expert-System Pipeline")
        C1 --> C2("分时段词嵌入与对齐<br>Temporal Embeddings & Alignment")
        C2 --> C3("几何位移与邻域变化度量<br>Geometric & Neighborhood Metrics")
        D --> D1("可视化法律概念演变<br>Visualizing Legal Concept Evolution")
        D1 --> D2("揭示与历史事件的关联<br>Revealing Links to Historical Events")
    ```

- **[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening**
  - **tags:** [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]
  - **authors:** Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf
  - **institution:** Radboud University Medical Center, Radboud University
  - **link:** https://arxiv.org/pdf/2512.22242
  - **contributions:** 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp
  - **Simple LLM Summary:** This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]
        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]
        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]
    ```

- **[arXiv251230] Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities**
  - **tags:** [other], [social network analysis], [deplatforming, toxicity detection, dynamic reputation modeling, network analysis, migration regimes]
  - **authors:** Aleksandar Tomašević, Ana Vranić, Aleksandra Alorić, Marija Mitrović Dankulov
  - **institution:** Institute of Physics Belgrade, University of Belgrade
  - **link:** https://arxiv.org/pdf/2512.22348
  - **contributions:**  1. Identifies and characterizes two distinct regimes ("Hostile Takeover" and "Toxic Equilibrium") of how deplatformed users transform receiving communities on alternative platforms. 2. Demonstrates that community transformation is driven by peripheral dynamics and volume, not by newcomers capturing central network positions. 3. Shows that the structure of the migrating community (loose vs. cohesive) determines whether they disperse into generalist spaces or form dedicated enclaves.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ecb07715a5eebfede95c0e808d5c2d61c5453c8fb7fa8b8c2b8260b568fa2f9_w640_q70.webp
  - **Simple LLM Summary:** This paper studies how Reddit deplatforming affects the communities on the alternative platform Voat. Using network analysis, toxicity detection, and dynamic reputation modeling, it finds that migration leads to increased toxicity through distinct phases and that platforms have a narrow window to intervene before toxic norms become entrenched.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities] --> B(核心问题/Problem);
        A --> C(主要方法/Method);
        A --> D(关键结果/Results);
        B --> B1[替代平台接收被禁用户的影响/Impact on alternative platforms receiving banned users];
        C --> C1[网络分析/Network Analysis];
        C --> C2[毒性检测/Toxicity Detection];
        C --> C3[动态声誉建模/Dynamic Reputation Modeling];
        D --> D1[敌对接管阶段/Hostile Takeover Phase (2015-2018)];
        D --> D2[毒性平衡阶段/Toxic Equilibrium Phase (2018-2020)];
        D --> D3[外围动态驱动转变/Peripheral Dynamics Drive Change];
    ```

- **[arXiv251230] Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy**
  - **tags:** [nlp], [human-ai interaction], [boundary objects, relational mediation, marginalized clients, therapeutic systems, dynamic framework]
  - **authors:** Jiatao Quan, Ziyue Li, Tian Qi Zhu, Yuxuan Li, Baoying Wang, Wanda Pratt, Nan Gao
  - **institution:** University of Washington, The Hong Kong Polytechnic University, Nankai University
  - **link:** https://arxiv.org/pdf/2512.22462
  - **contributions:** 1. Identifies enduring relational challenges in psychotherapy for marginalized clients, such as trust-building and self-disclosure burdens. 2. Proposes the Dynamic Boundary Mediation Framework, which re-conceptualizes LLMs as adaptive boundary objects. 3. Delineates three specific forms of mediation (Epistemic, Relational, Contextual) to address knowledge gaps, power asymmetries, and therapy-life discontinuities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8695c76e0392473389276989f78ab825dab06f2e38bdd785d2418d8ca9a1d80_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that current framings of LLMs in mental health overlook their potential to mediate complex therapeutic relationships. Based on interviews with therapists and marginalized clients in China, the authors propose the Dynamic Boundary Mediation Framework, which positions LLM chatbots as adaptive boundary objects to bridge knowledge, power, and contextual gaps. This offers a pathway for designing AI systems that more effectively and accountably support therapeutic relationships for marginalized users.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["现有视角的局限/Current Framing Limitations"]
        Problem --> P2["边缘化客户的关系挑战/Relational Challenges for Marginalized Clients"]
        Method --> M1["动态边界调解框架/Dynamic Boundary Mediation Framework"]
        Method --> M2["作为边界对象的LLM/LLMs as Boundary Objects"]
        Results --> R1["三种调解形式/Three Forms of Mediation"]
        Results --> R2["关系问责的AI系统/Relationally Accountable AI Systems"]
    ```

- **[arXiv251230] Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability**
  - **tags:** [other], [social computing], [urban agriculture, hydroponics, qualitative study, social learning theory, sustainable food production]
  - **authors:** Ewa Duda, Adamina Korwin-Szymanowska
  - **institution:** Uniwersytet Łódzki, Uniwersytet Warszawski (University of Łódź, University of Warsaw)
  - **link:** https://arxiv.org/pdf/2512.22594
  - **contributions:** 1. Conducted a qualitative study on resident participation in an innovative urban hydroponic farming project. 2. Identified key motivations and experiences of urban residents engaging in community-based food self-production. 3. Provided insights for urban educators and policymakers on fostering sustainable food initiatives through social learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e23a23f4c52bbdc6c863186ff1aad1ec49304917fba3f15168891df1f7d6b610_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates urban residents' participation in a community hydroponic farming project in Poland. Using purposive sampling and in-depth interviews, the study explores the motivations, experiences, and educational pathways of participants. The findings highlight the role of social learning in empowering urban self-sustainability and offer guidance for stakeholders in urban education and development.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Urban food security & climate change/城市粮食安全与气候变化]
        C --> C1[Qualitative study: interviews/定性研究：访谈]
        C --> C2[Two communities in Poland/波兰的两个社区]
        C --> C3[Hydroponic cabinets in flats/公寓中的水培柜]
        D --> D1[Understand resident motivations/理解居民动机]
        D --> D2[Outline farming experiences/概述种植经验]
        D --> D3[Relevance for urban educators/对城市教育者的意义]
    ```

- **[arXiv251230] Mitigating Social Desirability Bias in Random Silicon Sampling**
  - **tags:** [nlp], [llm evaluation], [silicon sampling, social desirability bias, prompt engineering, jensen-shannon divergence, american national election study]
  - **authors:** Sashank Chapala, Maksym Mironov, Songgaojun Deng
  - **institution:** Eindhoven University of Technology
  - **link:** https://arxiv.org/pdf/2512.22725
  - **contributions:** 1. Replicates and confirms the presence of persistent Social Desirability Bias (SDB) in LLM-based silicon sampling. 2. Proposes and systematically evaluates four psychologically grounded prompt-based methods (reformulated, reverse-coded, priming, preamble) for mitigating SDB. 3. Demonstrates that reformulated prompts (neutral, third-person phrasing) are the most effective method for improving alignment between silicon and human survey response distributions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e514b34cb5fd24baebc22116c45f73b1898e7c713905a13d372408df0900782b_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how to reduce Social Desirability Bias in LLM-generated survey responses (silicon sampling). It tests four prompt-based mitigation methods and finds that reformulating questions into neutral, third-person phrasing most effectively aligns the LLM outputs with real human data from the American National Election Study.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mitigating Social Desirability Bias in Random Silicon Sampling] --> B[核心问题/Problem: LLM硅采样存在社会期望偏差/Social Desirability Bias in Silicon Sampling]
        A --> C[主要方法/Method: 测试四种基于提示的缓解方法/Test Four Prompt-based Mitigation Methods]
        A --> D[关键结果/Results: 重构提示最有效，改善与人类数据对齐/Reformulated Prompts Most Effective, Improve Alignment]
        C --> C1[重构/Reformulated]
        C --> C2[反向编码/Reverse-coded]
        C --> C3[启动/Priming]
        C --> C4[序言/Preamble]
    ```

- **[arXiv251230] Ungraded Assignments in Introductory Computing: A Report**
  - **tags:** [other], [computing education], [ungraded assignments, formative feedback, student engagement, mixed-methods, introductory computing]
  - **authors:** Yehya Sleiman Tellawi, Abhishek K. Umrawal
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.23004
  - **contributions:** 1. Developed and administered new optional ungraded assignments for a large introductory computer engineering course (ECE 120). 2. Employed a mixed-methods approach (surveys, interviews, performance analysis) to assess the impact of ungraded assignments on learning. 3. Found a positive relationship between participation in ungraded assignments and overall course performance, suggesting they appeal to high-achievers or support better outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03fe0f743f635ea37a635da0020b8df696d9ae620a38ea5dd83dd57902fd7911_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the effects of optional ungraded assignments in an introductory computing course. The authors developed such assignments and used surveys, interviews, and performance data to evaluate their impact. The main finding is a positive correlation between completing ungraded work and higher course grades, indicating potential benefits for student engagement and learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Ungraded Assignments in Introductory Computing: A Report] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(评估压力与内在动机/Grade pressure vs. intrinsic motivation)
        C --> C1(开发并实施未评分作业/Develop & administer ungraded assignments)
        C --> C2(混合方法评估/Mixed-methods evaluation)
        D --> D1(参与度与成绩正相关/Participation correlates with performance)
    ```

- **[arXiv251230] Inteligencia Artificial y Empleo: perspectiva Territorial y de Género**
  - **tags:** [other], [labor economics, computational social science], [AI exposure index, sector-based analysis, territorial disaggregation, gender gap, CNAE incidence matrix]
  - **authors:** Antoni Mestre, Xavier Naya, Manoli Albert, Vicente Pelechano
  - **institution:** Universitat de València (inferred from author names and Spanish context)
  - **link:** https://arxiv.org/pdf/2512.23059
  - **contributions:**  1. Proposes a novel methodological framework for estimating AI exposure using sector-based data (CNAE classification) instead of occupation-based approaches, addressing limitations in the Spanish context. 2. Constructs an AI CNAE incidence matrix and applies it to provincial employment data (2021-2023) to provide a territorial and gender-disaggregated assessment of AI's potential impact. 3. Reveals stable structural patterns of AI exposure, identifying higher exposure in metropolitan/service regions and a consistent gender gap where female employment is more exposed across all territories.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/562053897ffb2e59883167293942d4f1e524304623f8d2e91e0026105669c93d_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a sector-based methodological framework to estimate the potential exposure of employment to AI in Spain, addressing the limitations of occupation-centered approaches. By applying an AI CNAE incidence matrix to provincial employment data from 2021-2023, it provides a territorial and gender-disaggregated assessment. The results show higher AI exposure in metropolitan and service-oriented regions and a consistent gender gap, with female employment being more exposed across all territories, offering a structural perspective for policy planning rather than predicting job displacement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Inteligencia Artificial y Empleo: perspectiva Territorial y de Género] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>AI对劳动力市场的不均衡影响<br>Uneven AI impact on labor markets]
        C[主要方法/Method<br>构建行业AI暴露矩阵<br>Construct sector-based AI exposure matrix]
        D[关键结果/Results<br>大都市区暴露更高, 存在性别差距<br>Higher exposure in metro areas, consistent gender gap]
    ```

- **[arXiv251230] Identifying Barriers Hindering the Acceptance of Generative AI as a Work Associate, measured with the new AGAWA scale**
  - **tags:** [ai], [human-ai interaction], [AGAWA scale, technology acceptance, generative AI, workplace, moral dilemmas]
  - **authors:** Łukasz Sikorski, Albert Łukasik, Jacek Matulewski, Arkadiusz Gut
  - **institution:** Nicolaus Copernicus University in Toruń
  - **link:** https://arxiv.org/pdf/2512.23373
  - **contributions:** 1. Proposed the AGAWA scale, a concise 4-item tool for measuring attitudes toward generative AI as a coworker, 2. Investigated key factors (concerns, human-like characteristics, sense of human uniqueness) influencing acceptance of generative AI in the workplace, 3. Confirmed the relationship between affective/moral dimensions of trust and attitudes toward generative AI at work.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae604f8d4d4b73f323c396a472bef59c496be69f1ebbd75e34c80cb0805f530_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the AGAWA scale, a brief measurement tool based on TAM and UTAUT models, to study barriers to accepting generative AI as a work associate. The study found that positive attitudes toward AI coworkers are negatively correlated with concerns about interaction, human-like AI traits, and a sense of human superiority. The results highlight the link between trust dimensions and workplace AI acceptance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Identifying Barriers to Generative AI Acceptance / 识别生成式AI接受的障碍"] --> Problem["Core Problem: Student attitudes affect future workplace AI adoption / 核心问题: 学生态度影响未来工作场所AI采用"]
        Root --> Method["Method: Propose AGAWA scale (4-item tool) / 方法: 提出AGAWA量表(4项工具)"]
        Root --> Results["Results: Positive attitudes linked to reduced concerns, human-likeness, and superiority beliefs / 结果: 积极态度与减少的担忧、拟人特性和优越感信念相关"]
    ```

- **[arXiv251230] The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective**
  - **tags:** [other], [scientometrics], [gender diversity, team roles, author contribution statements, threshold regression, citation impact]
  - **authors:** Yi Zhao, Yongjun Zhu, Donghun Kim, Yuzhuo Wang, Heng Zhang, Chao Lu, Chengzhi Zhang
  - **institution:** Anhui University, Yonsei University, Nanjing University, Central China Normal University, Hohai University, Nanjing University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.23429
  - **contributions:** 1. Introduced a team roles perspective by classifying authors into leadership and support roles using contribution statements, moving beyond aggregate diversity measures. 2. Discovered a non-linear (inverted U-shape) relationship between gender diversity and team impact for both leadership and support groups. 3. Revealed the moderating effect of team size, showing that the impact of leadership-group gender diversity shifts from negative to positive as team size increases, while support-group diversity remains consistently positive.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8e0d6494840d098b4c65bf9f6eb6b4b27a82bd62024c95c0f60db9e5b8fa31f_w640_q70.webp
  - **Simple LLM Summary:** This study investigates how gender diversity within specific team roles (leadership vs. support) affects scientific team impact, measured by citations. By analyzing over 130,000 PLOS papers and using contribution statements to define roles, the authors employed multivariable and threshold regression. They found the relationship is an inverted U-shape, identified high-impact team compositions, and showed that team size significantly moderates the effect of leadership diversity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Effect of Gender Diversity on Scientific Team Impact<br>性别多样性对科研团队影响力的影响] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Inconsistent findings on gender diversity's effect,<br>lack of role-differentiated analysis.<br>性别多样性影响结论不一，缺乏基于团队角色的分析]
        C[主要方法/Method<br>Analyzed 130k+ PLOS papers, used contribution<br>statements for role classification (leadership/support),<br>applied multivariable & threshold regression.<br>分析13万+PLOS论文，利用贡献声明进行角色分类，应用多元及阈值回归]
        D[关键结果/Results<br>1. Inverted U-shape relationship.<br>2. All-female leadership + all-male support yields high impact.<br>3. Team size moderates leadership diversity effect.<br>1. 倒U型关系。<br>2. 全女性领导+全男性支持的团队影响力更高。<br>3. 团队规模调节领导组多样性效应。]
    ```

- **[arXiv251230] Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education**
  - **tags:** [ai], [academic integrity detection], [Large Language Models, AI-generated text detection, deceptive prompts, computing education, self-detection]
  - **authors:** Christopher Burger, Karmece Talley, Christina Trotter
  - **institution:** The University of Mississippi, Rust College
  - **link:** https://arxiv.org/pdf/2512.23587
  - **contributions:** 1. Evaluates the self-detection performance of three prominent LLMs (GPT-4, Claude, Gemini) in computing-specific contexts. 2. Tests detection under both standard and deceptive prompt conditions where models are instructed to evade detection. 3. Reveals significant instability in detection, showing high error rates for human-written work and susceptibility to simple prompt alterations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e1ff7ce7b5924fdb6e21130ee87b352dc8c7613c8706c156c27ac0e31017f8b_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the ability of LLMs (GPT-4, Claude, Gemini) to detect AI-generated text in computing education. It tests them under standard and deceptive prompt conditions, finding that while default AI text is easily identified, models struggle with human-written work and are highly fooled by deceptive prompts, making them unreliable for high-stakes academic judgments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education] --> B[核心问题/Problem: LLMs challenge academic integrity in computing education]
        A --> C[主要方法/Method: Evaluate GPT-4, Claude, Gemini on AI-generated text detection with standard/deceptive prompts]
        A --> D[关键结果/Results: Models unstable; high error on human text; easily fooled by deceptive prompts; unreliable for misconduct judgments]
    ```

- **[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms**
  - **tags:** [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]
  - **authors:** LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão
  - **institution:** Google, Eedi
  - **link:** https://arxiv.org/pdf/2512.23633
  - **contributions:** 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]
        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]
        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]
        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]
        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]
        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]
        Results --> R2[学生表现相当或更好/Student performance equal or better]
        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]
    ```
