# 20251215-20251221 (cs.DC)

## 2025-12-18

- **[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation**
  - **tags:** [mlsys], [others], [Shapley-CMI, Private Set Intersection, Conditional Mutual Information, Vertical Federated Learning, data valuation]
  - **authors:** Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza
  - **institution:** Mondragon Unibertsitatea
  - **link:** https://arxiv.org/pdf/2512.14767
  - **Simple LLM Summary:** This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.

- **[arXiv251218] LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs**
  - **tags:** [mlsys], [llm training], [8-bit training, activation checkpointing, offloading, copy-engine collectives, dynamic tensor-level scaling, ZeRO-1]
  - **authors:** Erik Schultheis, Dan Alistarh
  - **institution:** IST Austria
  - **link:** https://arxiv.org/pdf/2512.15306
  - **Simple LLM Summary:** LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.

- **[arXiv251218] Dynamic Rebatching for Efficient Early-Exit Inference with DREX**
  - **tags:** [mlsys], [llm inference], [early-exit, dynamic rebatching, copy-free buffer, SLA-aware scheduler, KV cache, state-copying]
  - **authors:** Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu
  - **institution:** University of Pennsylvania, Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.15705
  - **Simple LLM Summary:** The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.

## 2025-12-19

- **[arXiv251219] SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference**
  - **tags:** [mlsys], [llm inference], [vector quantization, spline networks, memory optimization, cache optimization, hardware-aware compilation]
  - **authors:** Jeff Smith
  - **institution:** 2nd Set AI
  - **link:** https://arxiv.org/pdf/2512.15742
  - **Simple LLM Summary:** The paper introduces SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Coupled with a hardware-aware compiler called LUTHAM, it achieves an 88x reduction in runtime memory while matching baseline accuracy, effectively decoupling the workload from DRAM bandwidth constraints.

- **[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]
  - **authors:** Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai
  - **institution:** Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.
  - **link:** https://arxiv.org/pdf/2512.15766
  - **Simple LLM Summary:** The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.

- **[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls**
  - **tags:** [mlsys], [llm inference], [speculative tool calls, tool cache, vLLM, prefix-caching]
  - **authors:** Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon
  - **institution:** Lawrence Livermore National Laboratory, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.15834
  - **Simple LLM Summary:** This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new "tool cache" API to facilitate adoption of these optimizations.

- **[arXiv251219] Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference**
  - **tags:** [mlsys], [llm inference], [staggered batch scheduling, load-aware global allocation, DP+EP, time-to-first-token, throughput, data parallelism, expert parallelism]
  - **authors:** Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen
  - **institution:** Baidu Inc.
  - **link:** https://arxiv.org/pdf/2512.16134
  - **Simple LLM Summary:** This paper proposes Staggered Batch Scheduling (SBS), a method that buffers requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. It also introduces a Load-Aware Global Allocation strategy to balance computational load. The system reduces Time-to-First-Token by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.

- **[arXiv251219] Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference**
  - **tags:** [mlsys], [llm inference], [sparse attention, dynamic programming, top-k selection, anchor layers, reuse layers, FlashAttention-3]
  - **authors:** Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee
  - **institution:** Microsoft Research India
  - **link:** https://arxiv.org/pdf/2512.16391
  - **Simple LLM Summary:** The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.

- **[arXiv251219] AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research**
  - **tags:** [mlsys], [cluster infrastructure], [federated cloud platform, service catalogue, interactive development environments, GPU resources, annotation tools, experiment tracking, federated learning, model deployment, traceability, reproducibility]
  - **authors:** Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez
  - **institution:** Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)
  - **link:** https://arxiv.org/pdf/2512.16455
  - **Simple LLM Summary:** The paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform integrates distributed e-Infrastructures to provide consistent access to development environments, GPU training, annotation tools, and deployment options. Its main conclusion is that this integrated, customizable platform lowers adoption barriers and facilitates reproducible AI workflows for the scientific community.

- **[arXiv251219] Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint**
  - **tags:** [sys], [edge computing], [mixed integer linear programming, heuristic algorithm, task offloading, server deployment, budget constraint]
  - **authors:** Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham
  - **institution:** Sebelas Maret University, Curtin University, University of Wollongong
  - **link:** https://arxiv.org/pdf/2512.16792
  - **Simple LLM Summary:** The paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework, solved via an optimal Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). The main conclusion is that the heuristic solution performs close to optimal for small networks and significantly outperforms alternative strategies in large-scale networks, improving task satisfaction by up to 21.57% under budget and demand growth constraints.

- **[arXiv251219] Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, QMIX, reactive jamming, channel hopping, power control, Upper Confidence Bound]
  - **authors:** Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella
  - **institution:** Nexcepta
  - **link:** https://arxiv.org/pdf/2512.16813
  - **Simple LLM Summary:** This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.

- **[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies**
  - **tags:** [mlsys], [others], [federated learning, immunofluorescence microscopy, collagen VI-related dystrophies, rare disease diagnosis, decentralized training]
  - **authors:** Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria
  - **institution:** National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai
  - **link:** https://arxiv.org/pdf/2512.16876
  - **Simple LLM Summary:** The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.
