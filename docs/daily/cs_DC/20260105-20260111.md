---
slug: /daily/csdc/20260105-20260111
---
# 20260105-20260111 (cs.DC)

## 2026-01-05

- **[arXiv260105] Impact of Clustering on the Observability and Controllability of Complex Networks**
  - **tags:** [other], [network controllability], [clustering, scale-free networks, observability, controllability, structured systems theory]
  - **authors:** Mohammadreza Doostmohammadian, Hamid R. Rabiee
  - **institution:** Semnan University, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2601.00221
  - **contributions:** 1. Investigates and quantifies the relationship between network clustering and the requirements for observability and controllability in scale-free networks., 2. Demonstrates through simulations that densely clustered networks require fewer driver and observer nodes, offering a structural optimization principle., 3. Provides practical insights for reducing sensor/actuator placement in resource-constrained applications like social networks and intelligent transportation systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee38f16190dd3c9de93bdef0b47cb72f5c579d59a7937f107666cb7810142401_w640_q70.webp
  - **Simple LLM Summary:** This paper studies how clustering affects the observability and controllability of complex scale-free networks. Using structured systems theory and Monte-Carlo simulations, it shows that higher clustering reduces the number of required driver and observer nodes. The findings suggest that network design can be optimized for control and monitoring by increasing clustering, especially in resource-limited scenarios.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Impact of Clustering on Observability and Controllability) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(网络可观测性与可控性需求/Network Observability & Controllability Requirements)
        Method --> M1(结构化系统理论/Structured Systems Theory)
        Method --> M2(蒙特卡洛模拟与案例研究/Monte-Carlo Simulations & Case Studies)
        Results --> R1(密集聚类减少驱动与观测节点/Dense Clustering Reduces Driver & Observer Nodes)
        Results --> R2(优化资源受限网络设计/Optimizes Resource-Constrained Network Design)
    ```

- **[arXiv260105] From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm**
  - **tags:** [sys], [distributed consensus], [RAFT, replay attack, message forgery, authenticated verification, freshness check]
  - **authors:** Tamer Afifi, Abdelfatah Hegazy, Ehab Abousaif
  - **institution:** Arab Academy for Science, Technology and Maritime Transport (AASTMT)
  - **link:** https://arxiv.org/pdf/2601.00273
  - **contributions:** 1. A systematic security analysis of the RAFT protocol, identifying its susceptibility to message replay and forgery attacks. 2. Examination of the practical feasibility of these attacks through simulated scenarios. 3. Proposal of a novel cryptographic approach for enhancing RAFT's security, incorporating authenticated message verification and freshness checks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f61353e1cee5179dd015df5e513ba3552de676525e8ac5e6c2fb0a48786e62f3_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies security vulnerabilities in the RAFT distributed consensus algorithm, specifically to replay and forgery attacks, which can disrupt consensus and cause data inconsistency. To address this, the authors propose a novel security framework using cryptography, authenticated message verification, and freshness checks. The proposed solution aims to enhance the security of RAFT implementations and guide the development of more resilient distributed systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm"] --> Problem["核心问题/Problem: RAFT协议的安全漏洞未被充分认知/Security vulnerabilities in RAFT are not fully recognized"]
        Root --> Method["主要方法/Method: 系统安全分析与基于密码学的解决方案/Systematic security analysis & cryptographic solution"]
        Root --> Results["关键结果/Results: 提出增强安全性的框架/Proposed a framework for enhancing security"]
        Problem --> P1["攻击类型/Attack Types: 消息重放与伪造/Message replay & forgery"]
        Problem --> P2["后果/Consequence: 共识破坏与数据不一致/Consensus disruption & data inconsistency"]
        Method --> M1["分析/Analysis: 模拟攻击场景/Simulated attack scenarios"]
        Method --> M2["方案/Solution: 认证、验证与新鲜性检查/Authentication, verification & freshness check"]
        Results --> R1["成果/Outcome: 识别设计弱点/Identified design weaknesses"]
        Results --> R2["成果/Outcome: 提供安全增强框架/Provided security enhancement framework"]
    ```

- **[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems**
  - **tags:** [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]
  - **authors:** Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lovén
  - **institution:** University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Luleå University of Technology, Peking University, TU Wien
  - **link:** https://arxiv.org/pdf/2601.00339
  - **contributions:** 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Bio-inspired Agentic Self-healing Framework<br>生物启发的智能体自愈框架"] --> Problem["核心问题/Problem<br>DCCS中的复杂性与故障频发<br>Complexity & Frequent Faults in DCCS"]
        Root --> Method["主要方法/Method<br>ReCiSt框架: 仿生四层与LM智能体<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]
        Root --> Results["关键结果/Results<br>数十秒内自愈，低CPU开销<br>Self-healing in tens of seconds, low CPU overhead"]
    ```

- **[arXiv260105] Word Frequency Counting Based on Serverless MapReduce**
  - **tags:** [sys], [serverless computing], [Serverless Computing, MapReduce, Word Frequency Counting, Function as a Service (FaaS), Cloud Computing]
  - **authors:** Hanzhe Li, Bingchen Lin, Mengyuan Xu
  - **institution:** Xi'an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00380
  - **contributions:** 1. Proposes a novel combination of the serverless computing paradigm (FaaS) with the MapReduce programming model for data processing tasks. 2. Investigates and determines the optimal number of Map and Reduce functions for a given workload within a serverless MapReduce framework. 3. Demonstrates through experiments that increasing the number of functions reduces execution time and improves overall efficiency for the word frequency counting task.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of optimizing big data processing efficiency by integrating the serverless computing model (FaaS) with the MapReduce framework. It proposes a serverless MapReduce approach for word frequency counting and experimentally finds the optimal number of Map and Reduce functions to minimize execution time. The results show that this method improves processing efficiency, offering a cost-effective solution for cloud-based data analytics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Word Frequency Counting Based on Serverless MapReduce] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[需求: 高性能与高效率计算 / Demand: High-performance & High-efficiency Computing]
        C --> C1[结合: 无服务器计算(FaaS)与MapReduce / Combine: Serverless Computing (FaaS) & MapReduce]
        C --> C2[优化: Map与Reduce函数数量 / Optimize: Number of Map & Reduce Functions]
        D --> D1[结果: 执行时间减少 / Result: Execution Time Reduces]
        D --> D2[结果: 程序效率提升 / Result: Program Efficiency Improves]
    ```

- **[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving**
  - **tags:** [mlsys], [llm inference], [time-warp emulation, CUDA interception, virtual time coordination, performance modeling, discrete-event simulation]
  - **authors:** Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov
  - **institution:** Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2601.00397
  - **contributions:** 1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp
  - **Simple LLM Summary:** The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[GPU集群评估成本高且慢/Evaluating serving configs on GPU clusters is slow and expensive]
        B --> B2[模拟器需要重写控制逻辑/Simulators require re-implementing complex control logic]
        C --> C1[拦截CUDA API调用/Intercept CUDA API calls]
        C --> C2[虚拟时间跳跃/Virtual time jumps based on kernel predictions]
        C --> C3[分布式协调协议/Distributed coordination protocol]
        D --> D1[<5%预测误差/<5% prediction error]
        D --> D2[5-17倍加速/5-17x faster than real execution]
    ```

- **[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution**
  - **tags:** [sec], [Privacy-preserving data aggregation], [unanimous-release confidentiality, consensus locking, malicious deviation detection]
  - **authors:** Prajwal Panth, Sahaj Raj Malla
  - **institution:** KIIT University, Kathmandu University
  - **link:** https://arxiv.org/pdf/2601.00418
  - **contributions:** 1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (σ_S, σ_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework's properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Secure, Verifiable, and Scalable Multi-Client Data Sharing<br>安全、可验证、可扩展的多客户端数据共享] --> B(核心问题/Problem: Secure multi-client data aggregation with privacy and verifiability<br>安全、可验证的多客户端隐私数据聚合)
        A --> C(主要方法/Method: Consensus-Based Privacy-Preserving Data Distribution (CPPDD)<br>基于共识的隐私保护数据分发)
        C --> C1(Affine Masking & Consensus Locking<br>仿射掩码与共识锁定)
        C --> C2(Step/Data Checksums (σ_S, σ_D)<br>步骤/数据校验和)
        A --> D(关键结果/Results: Linear scalability, 100% deviation detection, lower FLOPs vs MPC/HE<br>线性可扩展性，100%异常检测，相比MPC/HE更低的计算量)
    ```

- **[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights**
  - **tags:** [mlsys], [federated learning], [federated learning, prefix-tuning, large model customization, efficient fine-tuning, retrieval-augmented generation]
  - **authors:** Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato
  - **institution:** Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2601.00526
  - **contributions:** 1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp
  - **Simple LLM Summary:** This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Federated Customization of Large Models: Approaches, Experiments, and Insights] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[在联邦学习框架下定制大模型的挑战/Challenges of customizing large models within FL]
        C --> C1[回顾大模型定制技术/Review LM customization techniques]
        C --> C2[讨论联邦学习实现/Discuss FL implementations]
        C --> C3[实验联邦前缀调优/Experiment with federated prefix-tuning]
        D --> D1[验证联邦前缀调优可行性/Validate feasibility of federated prefix-tuning]
        D --> D2[性能接近集中式方法/Performance close to centralized]
        D --> D3[展示竞争力与鲁棒性/Show competitive performance & robustness]
    ```

- **[arXiv260105] Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure**
  - **tags:** [sys], [cloud computing], [cloud benchmarking, point-of-sale systems, performance analysis, cost optimization, retail technology]
  - **authors:** Ravi Teja Pagidoju
  - **institution:** Campbellsville University
  - **link:** https://arxiv.org/pdf/2601.00530
  - **contributions:** 1. Presents a systematic, repeatable, and transparent methodology for evaluating POS workloads on cloud platforms using free-tier resources and open-source benchmarking code. 2. Provides the first comprehensive, code-driven comparison of POS-specific workloads across Google Cloud Platform and Microsoft Azure, analyzing performance metrics and cost efficiency. 3. Establishes an open benchmarking framework and offers practical insights for merchants considering cloud POS implementation, linking architectural components to observed performance-cost trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/084205dec68906e341b24595e452d764be0491b5d3d16bfccff9cf4f8d4f1eca_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a systematic methodology to compare the performance and cost of cloud-based Point-of-Sale systems on Google Cloud Platform and Microsoft Azure using free-tier resources and open-source code. The analysis finds that GCP offers faster response times, while Azure demonstrates higher cost efficiency for steady-state operations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏零售工作负载的实证研究 / Lack of empirical research on retail workloads]
        C --> C1[使用免费资源和开源代码进行系统化基准测试 / Systematic benchmarking using free-tier resources and open-source code]
        D --> D1[GCP响应时间快23.0% / GCP 23.0% faster response]
        D --> D2[Azure成本效率高71.9% / Azure 71.9% higher cost efficiency]
    ```

- **[arXiv260105] FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding**
  - **tags:** [mlsys], [llm inference], [speculative decoding, edge-cloud collaboration, communication-efficient inference, adaptive speculation, shared-backbone architecture]
  - **authors:** Yuchen Li, Rui Kong, Zhonghao Lyu, Qiyang Li, Xinran Chen, Hengyi Cai, Lingyong Yan, Shuaiqiang Wang, Jiashu Zhao, Guangxu Zhu, Linghe Kong, Guihai Chen, Haoyi Xiong, Dawei Yin
  - **institution:** Baidu Inc., Shanghai Jiao Tong University, KTH Royal Institute of Technology, Wilfrid Laurier University, Shenzhen Research Institute of Big Data
  - **link:** https://arxiv.org/pdf/2601.00644
  - **contributions:** 1. Proposed a shared-backbone architecture enabling a single static edge-side draft model to be compatible with a family of evolving cloud-side target models, decoupling edge deployment from cloud updates. 2. Developed a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel conditions and device energy budgets. 3. Designed the FlexSpec framework, which reduces communication and maintenance costs for edge-cloud collaborative LLM inference, improving scalability and efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7429a9e45e5a64278f26ba8b33031aac6b2f433fdae9180afd7344ec56b1fffd_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FlexSpec, a communication-efficient framework for edge-cloud collaborative LLM inference using speculative decoding. Its core innovation is a shared-backbone architecture that allows a frozen edge-side draft model to work with evolving cloud target models, paired with an adaptive speculation mechanism. Experiments show it achieves superior inference efficiency compared to conventional speculative decoding approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[FlexSpec: Frozen Drafts Meet Evolving Targets] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[LLM部署在边缘受限/LLM Deployment on Edge is Constrained]
        Problem --> P2[现有协同推理通信开销大/Existing Collaborative Inference has High Comm. Overhead]
        Problem --> P3[模型频繁更新限制可扩展性/Frequent Model Updates Limit Scalability]
        Method[主要方法/Method] --> M1[共享主干架构/Shared-Backbone Architecture]
        Method --> M2[解耦边缘与云端部署/Decouple Edge and Cloud Deployment]
        Method --> M3[信道感知自适应推测/Channel-Aware Adaptive Speculation]
        Results[关键结果/Results] --> R1[减少通信与维护成本/Reduced Communication & Maintenance Cost]
        Results --> R2[提升推理效率/Improved Inference Efficiency]
        Results --> R3[优于传统推测解码方法/Superior to Conventional SD Approaches]
    ```
