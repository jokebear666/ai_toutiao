# 20251215-20251221 (cs.IR)

## 2025-12-18

- **[arXiv251218] Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems**
  - **tags:** [mlsys], [others], [A/B testing, cost-aware optimization, scroll-depth analysis, unbiased data collection, dedicated exploration container]
  - **authors:** Qiang Chen, Venkatesh Ganapati Hegde
  - **institution:** Tubi
  - **link:** https://arxiv.org/pdf/2512.14733
  - **Simple LLM Summary:** The paper introduces a method for safe content exploration in recommender systems by strategically placing a "Something Completely Different" row of randomized content only in low-engagement, high-reach scroll-depth regions of the UI. This approach preserves key business metrics while collecting unbiased interaction data. The collected data, when used for downstream candidate generation, significantly improves user engagement.

- **[arXiv251218] Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media**
  - **tags:** [mlsys], [llm inference], [large language models, knowledge graphs, viewpoint classification, fine-tuning, wikidata, semantic enrichment]
  - **authors:** Massimiliano Fadda, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino
  - **institution:** University of Cagliari, The Open University
  - **link:** https://arxiv.org/pdf/2512.14887
  - **Simple LLM Summary:** This paper improves a pipeline for analyzing political viewpoints in news by fine-tuning Large Language Models for classification and enriching claim representations with semantic actor descriptions from Wikidata. The integrated approach, evaluated on UK immigration debate data, shows that combining fine-tuned LLMs with knowledge graph context yields the best performance, particularly with models capable of processing long inputs.

- **[arXiv251218] Topological Metric for Unsupervised Embedding Quality Evaluation**
  - **tags:** [ai], [representation learning evaluation], [persistent homology, unsupervised metric, topological data analysis, embedding quality]
  - **authors:** Aleksei Shestov, Anton Klenitskiy, Daria Denisova, Amurkhan Dzagkoev, Daniil Petrovich, Andrey Savchenko, Maksim Makarenko
  - **institution:** Sber AI Laboratory
  - **link:** https://arxiv.org/pdf/2512.15285
  - **Simple LLM Summary:** The paper proposes "Persistence", a fully unsupervised metric for evaluating embedding quality based on persistent homology from topological data analysis. It captures the multi-scale geometric and topological structure of embedding spaces. Empirical results show it achieves strong correlation with downstream task performance, outperforming existing unsupervised metrics.

- **[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models**
  - **tags:** [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]
  - **authors:** Mikel Williams-Lekuona, Georgina Cosma
  - **institution:** Loughborough University
  - **link:** https://arxiv.org/pdf/2512.15372
  - **Simple LLM Summary:** The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.

- **[arXiv251218] BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems**
  - **tags:** [mlsys], [others], [neural collaborative filtering, BERT, CNN, hybrid recommendation system, deep learning]
  - **authors:** Abdullah Al Munem, Sumona Yeasmin, Mohammad Rezwanul Huq
  - **institution:** East West University
  - **link:** https://arxiv.org/pdf/2512.15526
  - **Simple LLM Summary:** This paper proposes a hybrid neural collaborative filtering (NCF) model that integrates BERT and CNN to process categorical and image data for recommendations. The model was trained on a MovieLens dataset and outperformed baseline NCF and BERT-based NCF models. The results show that incorporating both categorical and image data can improve recommendation system performance.

## 2025-12-19

- **[arXiv251219] ModelTables: A Corpus of Tables about Models**
  - **tags:** [mlsys], [others], [table retrieval, model lakes, data lakes, unionable tables, joinable tables, dense retrieval, sparse retrieval, hybrid retrieval, benchmark construction]
  - **authors:** Zhengyuan Dong, Victor Zhong, Renée J. Miller
  - **institution:** University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.16106
  - **Simple LLM Summary:** This paper introduces ModelTables, a benchmark corpus of structured tables describing AI models, built from sources like Hugging Face model cards and GitHub READMEs. It evaluates table search methods, finding that table-based dense retrieval performs best but leaves significant room for improvement. The work provides a foundation for better semantic retrieval and organization of structured model knowledge.

- **[arXiv251219] Science Consultant Agent**
  - **tags:** [mlsys], [others], [Retrieval-Augmented Generation (RAG), fine-tuning, knowledge distillation, prompting, AutoML]
  - **authors:** Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves
  - **institution:** Duke University, Amazon
  - **link:** https://arxiv.org/pdf/2512.16171
  - **Simple LLM Summary:** The paper introduces the Science Consultant Agent, a web-based AI tool that uses structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting optimal AI modeling strategies. It aims to prevent resource misallocation by providing evidence-based guidance, moving beyond brute-force exploration or example-induced bias to accelerate development.

- **[arXiv251219] The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models**
  - **tags:** [ai], [information retrieval], [reranking, cross-encoders, T5, Graph Neural Networks, knowledge distillation, Large Language Models]
  - **authors:** Tejul Pandit, Sakshi Mahendru, Meet Raval, Dhvani Upadhyay
  - **institution:** Palo Alto Networks, University of Southern California, Dhirubhai Ambani University
  - **link:** https://arxiv.org/pdf/2512.16236
  - **Simple LLM Summary:** This paper provides a comprehensive survey of reranking models in information retrieval, tracing their evolution from heuristic methods to modern neural architectures and Large Language Models. It analyzes various techniques including cross-encoders, T5, and GNNs, and discusses efficiency methods like knowledge distillation. The survey concludes by synthesizing the principles, effectiveness, and trade-offs of different reranking strategies, highlighting their critical role in improving retrieval relevance, especially within RAG pipelines.

- **[arXiv251219] Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach**
  - **tags:** [mlsys], [llm inference], [neuro-symbolic approach, vector search, knowledge graphs, retrieval-augmented generation (RAG), large language models (LLMs)]
  - **authors:** Allard Oelen, Mohamad Yaser Jaradeh, Sören Auer
  - **institution:** TIB – Leibniz Information Centre for Science and Technology, L3S Research Center, Leibniz University of Hannover
  - **link:** https://arxiv.org/pdf/2512.16425
  - **Simple LLM Summary:** This paper introduces ORKG ASK, a scholarly search system that uses a neuro-symbolic approach combining vector search, knowledge graphs, and LLMs with a Retrieval-Augmented Generation (RAG) framework to answer research questions. The system was evaluated for usability and found to be user-friendly, with users generally satisfied with its performance.

- **[arXiv251219] From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment**
  - **tags:** [ai], [fairness and bias in ai], [memory-enhanced personalization, large language models, AI agents, bias simulation, recruitment]
  - **authors:** Himanshu Gharat, Himanshi Agrawal, Gourab K. Patro
  - **institution:** Phi Labs, Quantiphi Inc.
  - **link:** https://arxiv.org/pdf/2512.16532
  - **Simple LLM Summary:** The paper simulates the behavior of memory-enhanced personalized AI agents using safety-trained LLMs in a recruitment use case. It finds that bias is systematically introduced and reinforced through personalization, highlighting the need for additional protective measures in such agents.

- **[arXiv251219] Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling**
  - **tags:** [ai], [display advertising], [self-supervised learning, sequential modeling, distributional pretraining, hybrid objective]
  - **authors:** Sullivan Castro, Artem Betlei, Thomas Di Martino, Nadir El Manouzi
  - **institution:** Criteo AI Lab, École Nationale des Ponts et Chaussées
  - **link:** https://arxiv.org/pdf/2512.16581
  - **Simple LLM Summary:** The paper introduces Abacus, a self-supervised pretraining method for sequential user modeling that predicts the empirical frequency distribution of user events to align with useful counting statistics. It combines this with a hybrid objective that unites distributional prediction with sequential learning. Experiments show that this approach accelerates downstream task convergence and improves AUC by up to 6.1% compared to baselines.

- **[arXiv251219] Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance**
  - **tags:** [mlsys], [llm inference], [graph neural networks, graph attention mechanisms, retrieval-augmented generation, knowledge graphs, attention-based pruning, subgraph retrieval]
  - **authors:** Jacob Reiss, Shikshya Shiwakoti, Samuel Goldsmith, Ujjwal Pandit
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.16661
  - **Simple LLM Summary:** The paper proposes an Attention-Based Subgraph Retriever, a GNN-as-retriever model that uses attention pruning to extract a refined subgraph from a knowledge graph, which is then passed to a large language model for advanced reasoning. This approach aims to improve research paper recommendation by leveraging relational and structural information from graphs to enhance retrieval-augmented generation and mitigate LLM hallucinations.

- **[arXiv251219] From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, deductive reasoning, conflict-aware trust-score, reasoning-trace-augmented framework, supervised fine-tuning]
  - **authors:** Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar
  - **institution:** Birla Institute of Technology and Science, Pilani, Carnegie Mellon University
  - **link:** https://arxiv.org/pdf/2512.16795
  - **Simple LLM Summary:** This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems.

- **[arXiv251219] LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation**
  - **tags:** [mlsys], [multi-modal inference], [cross-layer knowledge fusion MoE, VLLM, world-knowledge representation, token extraction, layer-wise fusion]
  - **authors:** Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu
  - **institution:** Northeastern University, LinkedIn, University of Wisconsin–Madison
  - **link:** https://arxiv.org/pdf/2512.16891
  - **Simple LLM Summary:** The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.
