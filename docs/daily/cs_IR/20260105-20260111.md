---
slug: /daily/csir/20260105-20260111
---
# 20260105-20260111 (cs.IR)

## 2026-01-05

- **[arXiv260105] The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs**
  - **tags:** [ai], [causal reasoning], [fuzzy cognitive maps, large-language-model agent, causal feedback, equilibrium limit cycles, agentic leash]
  - **authors:** Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko
  - **institution:** University of Southern California, Florida International University
  - **link:** https://arxiv.org/pdf/2601.00097
  - **contributions:** 1. A novel LLM agent designed to autonomously extract and construct causal feedback Fuzzy Cognitive Maps (FCMs) from raw text. 2. A three-step instruction-guided process for systematically extracting key concepts and causal edges to build the FCM dynamical system. 3. Demonstration that the LLM-generated FCMs converge to the same equilibrium dynamics as human-generated ones and that mixed FCMs from different LLMs can create new equilibria.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes an LLM agent to autonomously extract causal feedback Fuzzy Cognitive Maps from text. The agent uses a three-step process to identify concepts and causal edges, forming a dynamical system. The generated FCMs matched human-generated equilibrium dynamics and mixing models from different LLMs produced new equilibria for better causal approximation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs] --> B[核心问题/Problem: How to autonomously extract causal structures from text?]
        A --> C[主要方法/Method: Design an LLM agent with a three-step instruction process to build FCMs from text.]
        A --> D[关键结果/Results: LLM-generated FCMs match human equilibrium dynamics; mixed FCMs create new equilibria.]
    ```

- **[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings**
  - **tags:** [ai], [semantic communication], [reinforcement learning, unequal error protection, adaptive repetition coding, semantic distortion metric, per-dimension protection]
  - **authors:** Moirangthem Tiken Singh, Adnan Arif
  - **institution:** Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University
  - **link:** https://arxiv.org/pdf/2601.00186
  - **contributions:** 1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[带宽受限下保持语义/Bandwidth-constrained Semantic Preservation]
        C --> C1[基于RL的自适应重复编码/RL-based Adaptive Repetition Coding]
        C --> C2[复合语义失真度量/Composite Semantic Distortion Metric]
        D --> D1[性能显著超越均匀保护/Significant Gains Over Uniform Protection]
        D --> D2[挑战传统信道编码范式/Challenges Traditional Channel Coding]
    ```

- **[arXiv260105] Noise-Aware Named Entity Recognition for Historical VET Documents**
  - **tags:** [nlp], [named entity recognition], [Noise-Aware Training (NAT), OCR Noise, Data Augmentation, Transfer Learning, Multi-stage Fine-tuning]
  - **authors:** Alexander M. Esser, Jens Dörpinghaus
  - **institution:** Federal Institute for Vocational Education and Training (BIBB), University of Koblenz
  - **link:** https://arxiv.org/pdf/2601.00488
  - **contributions:** 1. Proposes a robust NER approach for historical VET documents using Noise-Aware Training with synthetic OCR errors. 2. Systematically compares three complementary training strategies (noisy, clean, and artificial data). 3. Demonstrates that domain-specific and noise-aware fine-tuning significantly improves robustness and accuracy under noisy conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles Named Entity Recognition in noisy, historical Vocational Education and Training documents by proposing a method using Noise-Aware Training with synthetic OCR errors, transfer learning, and multi-stage fine-tuning. The approach, one of the first to recognize multiple entity types in this domain, shows that domain-specific and noise-aware fine-tuning substantially increases model robustness and accuracy. The method is applied to German but is designed to be transferable to other languages.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Noise-Aware NER for Historical VET Documents] --> Problem(核心问题/Problem: NER in noisy historical VET documents)
        Root --> Method(主要方法/Method: Noise-Aware Training with synthetic OCR errors, transfer learning, multi-stage fine-tuning)
        Root --> Results(关键结果/Results: Increased robustness and accuracy under noisy conditions)
    ```

- **[arXiv260105] A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies**
  - **tags:** [nlp], [information retrieval], [query categorization, chain-of-thought, e-commerce taxonomy, semantic scoring, large language models]
  - **authors:** Jetlir Duraj, Ishita Khan, Kilian Merkelbach, Mehran Elyasi
  - **institution:** eBay Inc.
  - **link:** https://arxiv.org/pdf/2601.00510
  - **contributions:** 1. Proposes a novel Chain-of-Thought (CoT) paradigm for semantic query categorization that combines tree-search with LLM semantic scoring. 2. Demonstrates that the CoT approach outperforms embedding-based benchmarks and can detect problems within hierarchical taxonomies. 3. Introduces scalable LLM-based approaches for query categorization that are suitable for millions of queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2b32fe45914991e8c8d0dc90af7415aa3ed8d8e8ce666ea46e11465b8fa68fd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of categorizing user search queries into leaf categories of an e-commerce taxonomy to improve search relevance. The authors propose a novel Chain-of-Thought approach that navigates the taxonomy tree using LLM semantic scoring. Their method outperforms embedding-based benchmarks and is shown to scale for real-world applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies<br>基于思维链的电商分类语义查询方法"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Search query categorization in e-commerce taxonomies<br>电商分类中的搜索查询分类"] --> P1["目标/Goal<br>Select relevant leaf categories for a user query<br>为用户查询选择相关叶子类目"]
        Method["主要方法/Method<br>Chain-of-Thought (CoT) paradigm<br>思维链范式"] --> M1["技术/Technique<br>Combines tree-search with LLM semantic scoring<br>结合树搜索与LLM语义评分"]
        Results["关键结果/Results<br>Evaluation Findings<br>评估结果"] --> R1["性能/Performance<br>Outperforms embedding-based benchmarks<br>优于基于嵌入的基准方法"]
        Results --> R2["可扩展性/Scalability<br>Proposes scalable LLM-based approaches<br>提出可扩展的基于LLM的方法"]
    ```

- **[arXiv260105] Improving Scientific Document Retrieval with Academic Concept Index**
  - **tags:** [nlp], [information retrieval], [academic concept index, concept coverage-based generation (CCQGen), concept-focused auxiliary contexts (CCExpand)]
  - **authors:** Jeyun Lee, Junhyoung Lee, Wonbin Kweon, Bowen Jin, Yu Zhang, Susik Yoon, Dongha Lee, Hwanjo Yu, Jiawei Han, Seongku Kang
  - **institution:** Korea University, University of Illinois Urbana-Champaign, Texas A&M University, Yonsei University, Pohang University of Science and Technology
  - **link:** https://arxiv.org/pdf/2601.00567
  - **contributions:** 1. Introduces an academic concept index that extracts and organizes key concepts from scientific papers using an academic taxonomy. 2. Proposes CCQGen, a concept coverage-based query generation method that adaptively conditions LLMs on uncovered concepts to produce complementary queries with broader coverage. 3. Develops CCExpand, a context augmentation technique that leverages document snippets as concise responses to concept-aware queries for improved relevance matching.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfdf054b52cb87f6cd4e63982623a18245a08006e97e04ad3614fc5ac270f85a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of adapting general-domain retrievers to scientific domains by introducing an academic concept index. The proposed method improves synthetic query generation (CCQGen) and context augmentation (CCExpand) using this structured index, leading to higher-quality queries and better retrieval performance. Experiments demonstrate improved conceptual alignment and retrieval effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Improving Scientific Document Retrieval with Academic Concept Index] --> B[核心问题/Problem: Adapting general-domain retrievers to scientific domains is challenging due to vocabulary mismatch and lack of domain-specific annotations.]
        A --> C[主要方法/Method: Introduces academic concept index to enhance synthetic query generation (CCQGen) and context augmentation (CCExpand).]
        A --> D[关键结果/Results: Leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.]
    ```

- **[arXiv260105] TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications**
  - **tags:** [mlsys], [rag (retrieval-augmented generation)], [ticket troubleshooting, retrieval-augmented generation, instruction-tuning, domain-specific ranking, large language models]
  - **authors:** Mohamed Trabelsi, Huseyin Uzunalioglu
  - **institution:** Nokia Bell Labs
  - **link:** https://arxiv.org/pdf/2601.00691
  - **contributions:** 1. Proposes TeleDoCTR, an end-to-end system for telecom ticket troubleshooting integrating classification, retrieval, and generation tasks. 2. Introduces a domain-specific and contextual approach combining ranking and generative models tailored for the telecom domain. 3. Demonstrates superior performance over state-of-the-art methods on a real-world telecom dataset, enhancing troubleshooting accuracy and efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes TeleDoCTR, a system that automates telecom ticket troubleshooting by integrating domain-specific models for ticket classification, retrieval of similar historical tickets, and generation of fault analysis reports. It is evaluated on a real-world telecom dataset and shows improved performance over existing methods, making the troubleshooting process more accurate and efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications] --> B[核心问题/Problem: Telecom ticket troubleshooting is complex, time-consuming, and human-intensive.]
        A --> C[主要方法/Method: Integrates domain-specific ranking and generative models for classification, retrieval, and generation tasks.]
        A --> D[关键结果/Results: Superior performance over SOTA methods on real-world data, enhancing accuracy and efficiency.]
    ```
