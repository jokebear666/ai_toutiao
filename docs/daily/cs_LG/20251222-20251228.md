---
slug: /daily/cslg/20251222-20251228
---
# 20251222-20251228 (cs.LG)

## 2025-12-22

- **[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon**
  - **tags:** [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP2]
  - **authors:** Kwangjun Ahn, Noah Amsel, John Langford
  - **institution:** Microsoft Research, AI Frontiers, NYU
  - **link:** https://arxiv.org/pdf/2512.16928
  - **Simple LLM Summary:** The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.

- **[arXiv251222] BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control**
  - **tags:** [mlsys], [others], [EEG, EMG, sliding window classification, threshold-based detection, ESP32 microcontroller, NeuroSky MindWave Mobile 2, MyoWare 2.0]
  - **authors:** Pranesh Sathish Kumar
  - **institution:** Alliance Academy for Innovation, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.16929
  - **Simple LLM Summary:** This paper presents a low-cost prosthetic arm controlled by a dual-signal system using EEG (for hand open/close via blink detection) and EMG (for elbow movement via threshold-based detection). The prototype, built with ESP32 microcontrollers and commercial sensors, demonstrates a feasible, intuitive control method for upper-limb prostheses in resource-limited settings. The main conclusion is that this integrated neuro-muscular approach offers a viable pathway to affordable and biologically intuitive prosthetic control.

- **[arXiv251222] SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization**
  - **tags:** [mlsys], [llm inference], [dense embedding retrieval, graph-based exploration, BM25, LLM-based reasoning, code localization]
  - **authors:** Shravan Chaudhari, Rahul Thomas Jacob, Mononito Goswami, Jiajun Cao, Shihab Rashid, Christian Bock
  - **institution:** Johns Hopkins University, AWS AI Labs
  - **link:** https://arxiv.org/pdf/2512.16956
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c743ebe3d40c5416b7ff367b0e7e93ca8ff7bf1bd771b2359d8a7333521abcbc_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SpIDER, a method that enhances dense retrieval for code localization by using graph-based exploration of a codebase to gather auxiliary context, which is then reasoned over by an LLM. This approach addresses the limitations of standard embedding methods that underutilize code structure. Empirical results show that SpIDER consistently improves retrieval performance across multiple programming languages.

- **[arXiv251222] Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach**
  - **tags:** [sys], [pattern matching algorithms], [Ukkonen's Algorithm, Suffix Trees, pattern recognition, text-search algorithms]
  - **authors:** Xinyu Guan, Shaohua Zhang
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2512.16927
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a novel pattern matching algorithm that combines Ukkonen's Algorithm for constructing Suffix Trees with a new search technique using Python's dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.

- **[arXiv251222] Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models**
  - **tags:** [mlsys], [llm inference], [transformer autoencoder, mixture-of-experts (MoE), reconstruction error, latent vectors, sequence compression, routing mechanism]
  - **authors:** Zhongpan Tang
  - **institution:** Independent Researcher (based on gmail address)
  - **link:** https://arxiv.org/pdf/2512.16963
  - **Simple LLM Summary:** This paper proposes a novel "Compression is Routing" architecture using a Transformer Autoencoder to compress long sequences into latent vectors. It demonstrates that reconstruction error serves as an intrinsic domain fingerprint, enabling expert module scheduling without explicit gating networks. The method offers a scalable approach for handling long contexts and modular language model design.

- **[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework**
  - **tags:** [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]
  - **authors:** Kamer Ali Yuksel
  - **institution:** aiXplain Inc
  - **link:** https://arxiv.org/pdf/2512.16970
  - **Simple LLM Summary:** This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.

- **[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]
  - **authors:** Saksham Sahai Srivastava, Haoyu He
  - **institution:** University of Georgia
  - **link:** https://arxiv.org/pdf/2512.16962
  - **Simple LLM Summary:** This paper introduces MemoryGraft, a novel attack that poisons an LLM agent's long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent's semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT's DataInterpreter.

- **[arXiv251222] Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes**
  - **tags:** [ai], [aviation meteorology], [XGBoost, physics-guided feature engineering, SHAP analysis, METAR data, gradient boosting]
  - **authors:** Marcelo Cerda Castillo
  - **institution:** Pulsetech.cl
  - **link:** https://arxiv.org/pdf/2512.16967
  - **Simple LLM Summary:** This paper presents a lightweight XGBoost model for aviation visibility nowcasting, trained on surface observation data (METAR) and enhanced with physics-informed feature engineering. The model outperforms operational TAF forecasts at 3-hour horizons, achieving 2.5x to 4x higher recall with fewer false alarms across multiple climates. The SHAP analysis shows the model implicitly reconstructs local physical drivers like advection and radiation, providing explainable predictions.

- **[arXiv251222] QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification**
  - **tags:** [ai], [quantum-inspired machine learning], [Pretty Good Measurement (PGM), kernelized PGM (KPGM), Quantum SMOTE (QSMOTE), quantum state discrimination, density matrix-based learning]
  - **authors:** Bikash K. Behera, Giuseppe Sergioli, Robert Giuntini
  - **institution:** Università degli Studi di Cagliari, Technische Universität München
  - **link:** https://arxiv.org/pdf/2512.16960
  - **Simple LLM Summary:** This paper introduces and compares quantum-inspired classifiers, specifically Pretty Good Measurement (PGM) and kernelized PGM (kPGM), for imbalanced datasets, using Quantum SMOTE (QSMOTE) for synthetic oversampling. The results show that both PGM and kPGM outperform a classical random forest baseline, with PGM achieving the highest accuracy and kPGM demonstrating greater robustness across different data sampling strategies.

- **[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs**
  - **tags:** [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), turn-level MDP, advantage estimation, multi-turn RL]
  - **authors:** Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li
  - **institution:** The University of Texas at Austin, Amazon
  - **link:** https://arxiv.org/pdf/2512.17008
  - **Simple LLM Summary:** This paper introduces Turn-PPO, a reinforcement learning method that applies Proximal Policy Optimization at the turn level instead of the token level for training LLM agents in multi-turn tasks. It demonstrates that this approach is more robust and effective than the commonly used GRPO method, particularly for long-horizon reasoning scenarios, as validated on the WebShop and Sokoban datasets.

- **[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows**
  - **tags:** [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]
  - **authors:** Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu
  - **institution:** Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.16969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.

- **[arXiv251222] A Women's Health Benchmark for Large Language Models**
  - **tags:** [ai], [healthcare AI evaluation], [women's health benchmark, large language models, error types, model stumps, query types]
  - **authors:** Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard
  - **institution:** Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine
  - **link:** https://arxiv.org/pdf/2512.17028
  - **Simple LLM Summary:** The paper introduces the Women's Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women's health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women's health advice.

- **[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning**
  - **tags:** [ai], [reinforcement learning], [gradient boosting, DQN, ensemble learning, Bellman residual, non-stationary environments]
  - **authors:** Chang-Hwan Lee, Chanseung Lee
  - **institution:** Florida Atlantic University, Morrow Company
  - **link:** https://arxiv.org/pdf/2512.17034
  - **Simple LLM Summary:** The paper proposes GB-DQN, a method that uses gradient boosting to create an ensemble of Q-networks, where each new network learns the residual error of the current ensemble to adapt to non-stationary environments. Experiments show that GB-DQN achieves faster recovery and greater robustness compared to standard DQN and other baselines in tasks with changing dynamics.

- **[arXiv251222] SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples**
  - **tags:** [ai], [generative models], [diffusion models, bridge models, entropic optimal transport, distribution restoration, SFBD-OMNI]
  - **authors:** Haoye Lu, Yaoliang Yu, Darren Ho
  - **institution:** University of Waterloo, Vector Institute
  - **link:** https://arxiv.org/pdf/2512.17051
  - **Simple LLM Summary:** The paper introduces SFBD-OMNI, a bridge model framework that generalizes Stochastic Forward-Backward Deconvolution to restore a clean data distribution from abundant noisy samples and a black-box corruption process, framed as a one-sided entropic optimal transport problem. It shows that with a small number of clean samples, the underlying distribution becomes largely recoverable even in cases of per-sample information loss. Experiments demonstrate significant improvements across diverse measurement settings beyond Gaussian corruption.

- **[arXiv251222] Dynamic Tool Dependency Retrieval for Efficient Function Calling**
  - **tags:** [mlsys], [llm inference], [dynamic tool dependency retrieval, function calling, tool retrieval, on-device agents, tool-augmented llms]
  - **authors:** Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermovol, Bence Major
  - **institution:** Qualcomm Research, University of Maryland, College Park
  - **link:** https://arxiv.org/pdf/2512.17052
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1d880d31eb189fff50465aa10379459bec7bfbbf666206f8ad6ea98793a534a_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Dynamic Tool Dependency Retrieval (DTDR), a lightweight method that retrieves relevant tools for LLM function calling by conditioning on both the initial user query and the evolving execution context. It models tool dependencies from demonstrations to adaptively retrieve tools as a plan unfolds. The results show that this dynamic retrieval improves function calling success rates by 23% to 104% compared to static retrieval methods.

- **[arXiv251222] Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III**
  - **tags:** [ai], [statistical learning theory], [k-nearest neighbor classifier, universal consistency, Nagata dimension, Lebesgue-Besicovitch differentiation property, metric spaces]
  - **authors:** Vladimir G. Pestov
  - **institution:** Universidade Federal de Santa Catarina, University of Ottawa
  - **link:** https://arxiv.org/pdf/2512.17058
  - **Simple LLM Summary:** This paper proves the final implication needed to establish the equivalence between the universal consistency of the k-nearest neighbor classifier, the strong Lebesgue-Besicovitch differentiation property, and a metric space being sigma-finite dimensional in the sense of Nagata. The core method extends a measure construction from Hilbert spaces to any complete separable metric space lacking the Nagata property. The main conclusion is that the k-NN classifier is universally consistent in a metric space if and only if the space is sigma-finite dimensional.

- **[arXiv251222] Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?**
  - **tags:** [ai], [mathematical reasoning], [chain-of-thought prompting, reinforcement learning, GRPO, fine-tuning, error recovery]
  - **authors:** Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah
  - **institution:** MIT
  - **link:** https://arxiv.org/pdf/2512.17079
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp
  - **Simple LLM Summary:** The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.

- **[arXiv251222] Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts, offloading, dynamic quantization, low-rank compensation, router-guided precision restoration]
  - **authors:** Zhenyu Liu, Yunzhen Liu, Zehao Fan, Garrett Gagnon, Yayue Hou, Nan Wu, Yangwook Kang, Liu Liu
  - **institution:** Rensselaer Polytechnic Institute, University of Massachusetts Amherst, George Washington University, Samsung Semiconductor
  - **link:** https://arxiv.org/pdf/2512.17073
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c6e9afd2a3ff75dfe81968ba3c73da71d577f341a443ab96f8bb14a681ed3f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a bandwidth-efficient method for Mixture-of-Experts (MoE) inference that uses router-guided, low-rank compensation to dynamically restore precision for the most important experts while keeping others in low-bit form. This approach reduces I/O traffic during offloading without significantly harming model accuracy. The method demonstrates a superior bandwidth-accuracy trade-off and improved throughput on GPU and GPU-NDP systems.

- **[arXiv251222] Perturb Your Data: Paraphrase-Guided Training Data Watermarking**
  - **tags:** [mlsys], [llm training], [SPECTRA, watermarking, training data detection, membership inference attack, paraphrase generation, scoring model, token probability comparison]
  - **authors:** Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso
  - **institution:** JPMorgan AI Research
  - **link:** https://arxiv.org/pdf/2512.17075
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SPECTRA, a watermarking method that subtly paraphrases text using an LLM to embed a detectable signature into training data without altering its statistical distribution. It verifies unauthorized use by comparing token probabilities between a suspect model and a scoring model. The approach reliably detects watermarked data even when it constitutes a minuscule fraction of the training corpus, providing a scalable pre-release watermark for data owners.

- **[arXiv251222] How to Square Tensor Networks and Circuits Without Squaring Them**
  - **tags:** [ai], [probabilistic modeling], [tensor networks, squared circuits, probabilistic circuits, marginalization, canonical forms, unitary matrices, distribution estimation]
  - **authors:** Lorenzo Loconte, Adrián Javaloy, Antonio Vergari
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.17090
  - **Simple LLM Summary:** This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.

- **[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making**
  - **tags:** [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]
  - **authors:** Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman
  - **institution:** Toyota Research Institute
  - **link:** https://arxiv.org/pdf/2512.17091
  - **Simple LLM Summary:** This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.

- **[arXiv251222] UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data**
  - **tags:** [ai], [interpretability], [counterfactual explanations, model-agnostic, time series, ECG, LIME, SHAP]
  - **authors:** Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun
  - **institution:** Boston University, Sandia National Laboratories, Boston Medical Center
  - **link:** https://arxiv.org/pdf/2512.17100
  - **Simple LLM Summary:** The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.

- **[arXiv251222] Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse**
  - **tags:** [mlsys], [multi-modal inference], [modular reuse, on-device execution, model decomposition, parallel execution, quantization]
  - **authors:** Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan
  - **institution:** University of Massachusetts Amherst, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.17108
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27–33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.

- **[arXiv251222] Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models**
  - **tags:** [mlsys], [fault-tolerance], [differentiable physical model, gradient-based fault parameters identification, Adahessian optimizer, I-V curve reconstruction]
  - **authors:** Zenan Yang, Yuanliang Li, Jingwei Zhang, Yongjie Liu, Kun Ding
  - **institution:** Hohai University, Concordia University, Aalborg University
  - **link:** https://arxiv.org/pdf/2512.17107
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5899b49765c1a033a8227b9785fb68b2ee7e7efe7e5a6fd98b6b7f264267e08c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a differentiable fast fault simulation model (DFFSM) and a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer to quantify faults in photovoltaic arrays. The method accurately models I-V characteristics under multiple faults and uses analytical gradients for efficient parameter identification. Experimental results show high quantification accuracy with I-V reconstruction errors below 3%, demonstrating the effectiveness of differentiable physical simulators for PV fault diagnosis.

- **[arXiv251222] Bridging Training and Merging Through Momentum-Aware Optimization**
  - **tags:** [mlsys], [llm training], [low-rank factorization, momentum-aware optimization, curvature-aware merging, model composition, memory-efficient training]
  - **authors:** Alireza Moayedikia, Alicia Troncoso
  - **institution:** Swinburne University of Technology, Universidad Pablo de Olavide
  - **link:** https://arxiv.org/pdf/2512.17109
  - **Simple LLM Summary:** This paper introduces a unified framework that maintains factorized momentum and curvature statistics during model training, then reuses this information for geometry-aware model merging. This approach eliminates the need to recompute curvature data, saving computation and enabling more principled model composition. The method demonstrates improved performance on language understanding benchmarks and offers better hyperparameter robustness compared to existing low-rank optimizers.

- **[arXiv251222] The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining**
  - **tags:** [mlsys], [multi-modal training], [contrastive fine-tuning, token attribution, t-SNE projection, attention-head ablation, image retrieval, negation handling]
  - **authors:** Jasmine Vu, Shivanand Sheshappanavar
  - **institution:** Santa Clara University, University of Wyoming
  - **link:** https://arxiv.org/pdf/2512.17121
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcadf51218711774df214ce0c80d2175e2b72d14b2fab69a5a27aebd9bc17bd2_w640_q70.webp
  - **Simple LLM Summary:** This paper fine-tunes the CLIP-based CheXagent model to improve its ability to understand negated phrases in medical image retrieval tasks. The method uses contrastive fine-tuning and analyzes internal model behavior through techniques like token attribution. The results show improved negation handling with a slight trade-off in accuracy for positive prompts.

- **[arXiv251222] Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts**
  - **tags:** [ai], [handwritten text recognition], [encoder-decoder architectures, data-centric techniques, line-level transcription, transfer learning, data augmentation, character error rate]
  - **authors:** Anjali Sarawgi, Esteban Garces Arias, Christof Zotter
  - **institution:** LMU Munich, Heidelberg Academy of Sciences and Humanities, Munich Center for Machine Learning (MCML)
  - **link:** https://arxiv.org/pdf/2512.17111
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bae7b70bc5a974847e0434967f87fb01f452c81f46d95e3cea515746090bd7f5_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an end-to-end Handwritten Text Recognition (HTR) pipeline for Old Nepali manuscripts, employing line-level transcription and exploring encoder-decoder architectures with data-centric methods. The best model achieves a low Character Error Rate (CER) of 4.9%, demonstrating effective digitization for this low-resource historical language. The authors release their training code and evaluation scripts to support further research.

- **[arXiv251222] DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations**
  - **tags:** [mlsys], [others], [differentiable simulation, graph neural network, SE(3)-equivariance, attention mechanism, 3D Zernike polynomials, shape-matching loss, implicit differentiation, bilevel optimization]
  - **authors:** Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz
  - **institution:** Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard
  - **link:** https://arxiv.org/pdf/2512.17129
  - **Simple LLM Summary:** The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues.

- **[arXiv251222] Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs**
  - **tags:** [mlsys], [llm training], [Generalized Primal Averaging (GPA), DiLoCo, Schedule-Free, AdamW, Nesterov's method, primal averaging, optimizer, iterate averaging]
  - **authors:** Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao
  - **institution:** Meta Superintelligence Labs
  - **link:** https://arxiv.org/pdf/2512.17131
  - **Simple LLM Summary:** The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov's method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.

- **[arXiv251222] Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors**
  - **tags:** [mlsys], [others], [soft prompt attacks, adversarial auditing, agentic framework, risk metrics, embedding-space robustness]
  - **authors:** Huixin Zhan
  - **institution:** New Mexico Institute of Mining and Technology
  - **link:** https://arxiv.org/pdf/2512.17146
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SAGE, an agentic framework that audits genomic foundation models by injecting soft prompt perturbations and evaluating performance degradation. It finds that models like ESM2 are vulnerable to such attacks, revealing hidden security risks in biomedical applications.

- **[arXiv251222] Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing**
  - **tags:** [ai], [wireless communication, distributed learning], [restless multi-armed bandit (RMAB), Gale-Shapley matching, interference graph, SMILE algorithm, distributed optimization]
  - **authors:** Liad Lea Didi, Kobi Cohen
  - **institution:** Ben-Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2512.17161
  - **Simple LLM Summary:** The paper proposes SMILE, a distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination for stable spectrum sharing in interference graphs. It proves that SMILE converges to the optimal stable allocation and achieves logarithmic regret. Simulations validate the algorithm's robustness, scalability, and efficiency.

- **[arXiv251222] BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions**
  - **tags:** [ai], [pde solving], [BumpNet, sparse neural network, basis function expansion, sigmoid activation, physics-informed neural networks, PINNs, DeepONet, EDNN, meshless method, h-adaptivity]
  - **authors:** Shao-Ting Chiu, Ioannis G. Kevrekidis, Ulisses Braga-Neto
  - **institution:** Texas A&M University, The Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.17198
  - **Simple LLM Summary:** This paper introduces BumpNet, a sparse neural network framework that constructs adaptive, trainable basis functions from sigmoid activations for solving PDEs. It combines BumpNet with existing architectures like PINNs, DeepONet, and EDNN to create efficient, interpretable models for PDE solution and operator learning. The proposed methods demonstrate improved accuracy and reduced computational cost compared to standard approaches.

- **[arXiv251222] Learning solution operator of dynamical systems with diffusion maps kernel ridge regression**
  - **tags:** [ai], [dynamical systems modeling], [kernel ridge regression, diffusion maps, operator learning, geometry-aware learning, data-driven kernels]
  - **authors:** Jiwoo Song, Daning Huang, John Harlim
  - **institution:** The Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.17203
  - **Simple LLM Summary:** This paper proposes a Diffusion Maps Kernel Ridge Regression (DM-KRR) method for learning solution operators of complex dynamical systems. It combines a simple kernel ridge regression framework with a data-driven kernel from diffusion maps to adapt to the system's intrinsic geometry without explicit manifold modeling. The method is shown to outperform state-of-the-art approaches in accuracy and data efficiency across various systems, highlighting the importance of geometric constraints for long-term prediction.

- **[arXiv251222] Do Foundational Audio Encoders Understand Music Structure?**
  - **tags:** [ai], [music information retrieval], [music structure analysis, foundational audio encoders, self-supervised learning, masked language modeling, boundary detection, function prediction]
  - **authors:** Keisuke Toyama, Zhi Zhong, Akira Takahashi, Shusuke Takahashi, Yuki Mitsufuji
  - **institution:** Sony Group Corporation, Sony AI
  - **link:** https://arxiv.org/pdf/2512.17209
  - **Simple LLM Summary:** This paper investigates the use of pretrained foundational audio encoders (FAEs) for music structure analysis (MSA). Through comprehensive experiments on 11 FAE types, it finds that models using self-supervised learning with masked language modeling on music data are particularly effective for MSA tasks like boundary detection and function prediction.

- **[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency**
  - **tags:** [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]
  - **authors:** Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang
  - **institution:** Xidian University, Brown University, Capital Medical University
  - **link:** https://arxiv.org/pdf/2512.17213
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.

- **[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs**
  - **tags:** [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]
  - **authors:** Madhava Gaikwad
  - **institution:** Microsoft
  - **link:** https://arxiv.org/pdf/2512.17251
  - **Simple LLM Summary:** The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.

- **[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning**
  - **tags:** [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]
  - **authors:** Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu
  - **institution:** Nankai University, University of Louisville, University of North Texas
  - **link:** https://arxiv.org/pdf/2512.17254
  - **Simple LLM Summary:** The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.

- **[arXiv251222] Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods**
  - **tags:** [mlsys], [others], [time series forecasting, LSTM, GRU, Transformer, ARIMA, XGBoost]
  - **authors:** Iason Kyriakopoulos, Yannis Theodoridis
  - **institution:** University of Piraeus
  - **link:** https://arxiv.org/pdf/2512.17257
  - **Simple LLM Summary:** This paper experimentally compares five time series forecasting models, including traditional statistical methods, machine learning, and deep learning (e.g., ARIMA, XGBoost, LSTM, GRU, Transformer), for predicting electric vehicle charging load. The evaluation across multiple real-world datasets, temporal horizons, and spatial aggregation levels shows that recurrent neural networks (GRU, LSTM) generally perform best for mid- and long-term forecasting, while Transformers excel in short-term forecasting at higher aggregation levels.

- **[arXiv251222] SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS**
  - **tags:** [mlsys], [others], [hyperbolic convolution, Poincaré ball, adaptive feature-sharing, gated feature fusion, EMA-based loss balancing, multi-task learning, graph convolution networks]
  - **authors:** Suraj Kumar, Arvind Kumar, Soumi Chattopadhyay
  - **institution:** Indian Institute of Technology Indore
  - **link:** https://arxiv.org/pdf/2512.17262
  - **Simple LLM Summary:** The paper proposes SHARP-QoS, a unified model for joint QoS prediction that uses hyperbolic convolution to extract hierarchical features, an adaptive feature-sharing mechanism with gated fusion, and an EMA-based loss balancing strategy. It demonstrates superior performance over single- and multi-task baselines across multiple datasets, effectively handling sparsity, outliers, and cold-start scenarios with moderate computational cost.

- **[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes**
  - **tags:** [ai], [reinforcement learning], [bisimulation metric, generalized bisimulation metric, Markov decision process, policy transfer, state aggregation, sampling-based estimation]
  - **authors:** Zhenyu Tao, Wei Xu, Xiaohu You
  - **institution:** Southeast University, Purple Mountain Laboratories
  - **link:** https://arxiv.org/pdf/2512.17265
  - **Simple LLM Summary:** This paper introduces a Generalized Bisimulation Metric (GBSM) to measure state similarity between different Markov Decision Processes, establishing its fundamental mathematical properties. The authors leverage GBSM to theoretically analyze tasks like policy transfer and state aggregation, obtaining tighter performance bounds than previous methods. Numerical results validate the effectiveness of GBSM for multi-MDP scenarios.

- **[arXiv251222] Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems**
  - **tags:** [mlsys], [llm inference], [audit agents, attestation protocols, constrained reasoning, cryptographic attestation, symbolic methods, benchmark suite, verifiability]
  - **authors:** Abhivansh Gupta
  - **institution:** Indian Institute of Technology, Roorkee
  - **link:** https://arxiv.org/pdf/2512.17259
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a Verifiability-First architecture for LLM-based agents, integrating runtime attestations, lightweight audit agents for continuous verification, and challenge-response protocols for high-risk operations. It introduces the OPERA benchmark to evaluate the detectability and speed of remediation for misaligned behavior, shifting the focus from measuring the propensity for misalignment to ensuring reliable detection and control.

- **[arXiv251222] Alzheimer's Disease Brain Network Mining**
  - **tags:** [ai], [medical AI], [semi-supervised learning, optimal transport, label propagation, graph neural networks, Wasserstein distance]
  - **authors:** Alireza Moayedikia, Sara Fin
  - **institution:** Swinburne University of Technology, Monash University
  - **link:** https://arxiv.org/pdf/2512.17276
  - **Simple LLM Summary:** This paper introduces MATCH-AD, a semi-supervised framework that combines deep representation learning, graph-based label propagation, and optimal transport theory to diagnose Alzheimer's disease from neuroimaging data with limited labeled samples. The method achieves near-perfect diagnostic accuracy on a large dataset, demonstrating that semi-supervised learning can effectively leverage partially annotated data for clinical deployment.

- **[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory**
  - **tags:** [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]
  - **authors:** Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li
  - **institution:** Wuhan University, Tongyi Lab, Zhongguancun Academy
  - **link:** https://arxiv.org/pdf/2512.17270
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.

- **[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics**
  - **tags:** [mlsys], [others], [Kolmogorov-Arnold Networks (KANs), multilayer perceptron networks (MLPs), Physics-informed Neural Networks, nonlocal consistency loss, integro-differential equations (IDEs), fractional PDEs]
  - **authors:** Farinaz Mostajeran, Aruzhan Tleubek, Salah A Faroughi
  - **institution:** University of Utah
  - **link:** https://arxiv.org/pdf/2512.17273
  - **Simple LLM Summary:** The paper introduces MINPO, a unified neural framework that learns nonlocal operators and their inverses using KANs or MLPs to solve integro-differential equations. It enforces coherence between the learned operator and solution via a nonlocal consistency loss. The method is shown to be accurate and robust across diverse kernel types and dimensionalities, generalizing beyond problem-specific formulations.

- **[arXiv251222] Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest**
  - **tags:** [mlsys], [others], [residual connection, score regularization, manifold mixup]
  - **authors:** Saeed Ebrahimi, Weijie Jiang, Jaewon Yang, Olafur Gudmundsson, Yucheng Tu, Huizhong Duan
  - **institution:** Pinterest
  - **link:** https://arxiv.org/pdf/2512.17277
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a cost-efficient strategy to improve cold-start recommendations by introducing lightweight techniques: a residual connection for non-historical features, a score regularization term, and manifold mixup for data sparsity. These methods collectively increased fresh content engagement by 10% without harming overall engagement or cost, and have been deployed at Pinterest.

- **[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection**
  - **tags:** [ai], [speech processing], [voice activity detection, vision transformer, MFCC, Gammatone filter bank cepstral coefficients, dataset augmentation, out-of-distribution evaluation]
  - **authors:** Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan
  - **institution:** Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17281
  - **Simple LLM Summary:** This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.

- **[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge**
  - **tags:** [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]
  - **authors:** Abdullah M. Zyarah, Dhireesha Kudithipudi
  - **institution:** University of Texas at San Antonio, University of Baghdad
  - **link:** https://arxiv.org/pdf/2512.17299
  - **Simple LLM Summary:** This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.

- **[arXiv251222] Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability**
  - **tags:** [ai], [explainable AI (XAI)], [graph theory, model decomposition, hypothesis-evidence structure, Cox proportional hazards model, PREDICT]
  - **authors:** Michael Merry, Pat Riddle, Jim Warren
  - **institution:** University of Auckland
  - **link:** https://arxiv.org/pdf/2512.17316
  - **Simple LLM Summary:** This paper proposes a formal, testable criterion for inherent explainability in AI, using graph theory to decompose models into verifiable structure-local explanations called annotations. The method is applied to demonstrate the inherent explainability of a clinical cardiovascular risk model (PREDICT). The work provides a rigorous foundation for regulators and formalizes the distinction between an explainable model and an explained one.

- **[arXiv251222] Task Schema and Binding: A Double Dissociation Study of In-Context Learning**
  - **tags:** [ai], [in-context learning], [activation patching, double dissociation, task schema, binding, transformer, mamba]
  - **authors:** Chaeha Kim
  - **institution:** Changwon National University
  - **link:** https://arxiv.org/pdf/2512.17325
  - **Simple LLM Summary:** This paper uses activation patching experiments across multiple Transformer models and Mamba to causally dissect in-context learning. It concludes that ICL decomposes into two separable mechanisms: Task Schema (abstract task recognition) and Binding (specific input-output associations), with their reliance governed by a trade-off with the model's prior knowledge.

- **[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs**
  - **tags:** [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]
  - **authors:** Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas
  - **institution:** University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17352
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.

- **[arXiv251222] Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach**
  - **tags:** [mlsys], [others], [ensemble learning, adversarial training, Bayesian inference, LLM-based sample generation, weight assignment]
  - **authors:** Yidong Chai, Yi Liu, Mohammadreza Ebrahimi, Weifeng Li, Balaji Padmanabhan
  - **institution:** Hefei University of Technology, University of South Florida, University of Georgia, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.17367
  - **Simple LLM Summary:** This paper proposes a novel framework called LLM-SGA and instantiates a detector named ARHOCD, which uses an ensemble of base detectors, a dynamic Bayesian weight assignment method, and an iterative adversarial training strategy to improve robustness. The results show that ARHOCD achieves strong generalizability and improves detection accuracy for harmful online content under adversarial conditions.

- **[arXiv251222] AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens**
  - **tags:** [mlsys], [post-training], [adversarial control tokens, beam-search exploration, last-layer logit gap, LoRA-based adversarial training, reward hacking]
  - **authors:** Tung-Ling Li, Yuhao Wu, Hongliang Liu
  - **institution:** Palo Alto Networks
  - **link:** https://arxiv.org/pdf/2512.17375
  - **Simple LLM Summary:** The paper introduces AdvJudge-Zero, a method that uses beam-search on a model's next-token distribution to discover short, low-perplexity control token sequences that can flip the binary decisions of LLM-as-a-Judge systems from "No" to "Yes". It concludes that these tokens represent a realistic reward-hacking vulnerability in post-training pipelines, and shows that adversarial training can mitigate the issue while preserving evaluation quality.

- **[arXiv251222] Timely Information Updating for Mobile Devices Without and With ML Advice**
  - **tags:** [sys], [online scheduling], [competitive online algorithm, age of information, consistency-robustness trade-off, ML-augmented algorithm, adversarial environment]
  - **authors:** Yu-Pin Hsu, Yi-Hsuan Tseng
  - **institution:** National Taipei University
  - **link:** https://arxiv.org/pdf/2512.17381
  - **Simple LLM Summary:** This paper proposes an online algorithm for a mobile device to decide when to send status updates to an access point, balancing information timeliness and update cost. The algorithm achieves an optimal competitive ratio against adversarial uncertainties and, when augmented with machine learning advice, attains an optimal consistency-robustness trade-off. The main conclusion is that an optimal competitive algorithm exhibits a threshold-like response to ML advice, either fully trusting or completely ignoring it.

- **[arXiv251222] DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference**
  - **tags:** [mlsys], [others], [Private Inference, ReLU sharing, DReLU, cryptographic protocols, activation sharing]
  - **authors:** Yonathan Bornfeld, Shai Avidan
  - **institution:** Tel Aviv University
  - **link:** https://arxiv.org/pdf/2512.17398
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DeepShare, a method for efficient Private Inference (PI) that reduces computational costs by sharing the DReLU (the non-linear step function of ReLU) across channels and layers within a neural network. It achieves state-of-the-art results by drastically decreasing the number of expensive DReLU operations while maintaining model performance on tasks like classification and segmentation.

- **[arXiv251222] meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis**
  - **tags:** [mlsys], [others], [stratified analysis, statistical methods, bias assessment, performance metrics, multiple comparisons correction, intersectional analysis]
  - **authors:** Dishantkumar Sutariya, Eike Petersen
  - **institution:** Fraunhofer Institute for Digital Medicine MEVIS
  - **link:** https://arxiv.org/pdf/2512.17409
  - **Simple LLM Summary:** The paper presents a statistical toolbox called 'meval' designed for rigorous, fine-grained analysis of machine learning model performance across different subgroups. It addresses challenges like metric selection, uncertainty estimation, and multiple comparisons to identify performance disparities. The main conclusion is that this toolbox enables practitioners to easily detect potential biases and failure modes, particularly in medical imaging applications.

- **[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories**
  - **tags:** [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]
  - **authors:** Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe
  - **institution:** Turing
  - **link:** https://arxiv.org/pdf/2512.17419
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements.

- **[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]
  - **authors:** Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni
  - **institution:** Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University
  - **link:** https://arxiv.org/pdf/2512.17444
  - **Simple LLM Summary:** This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.

- **[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation**
  - **tags:** [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]
  - **authors:** Jon Muhovič, Janez Perš
  - **institution:** University of Ljubljana
  - **link:** https://arxiv.org/pdf/2512.17450
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.

- **[arXiv251222] When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction**
  - **tags:** [ai], [software defect prediction], [Explainable Boosting Machines, stratified interaction analysis, class imbalance, class overlap, irrelevant features, attribute noise, outliers]
  - **authors:** Emmanuel Charleson Dapaah, Jens Grabowski
  - **institution:** University of Göttingen
  - **link:** https://arxiv.org/pdf/2512.17460
  - **Simple LLM Summary:** This paper conducts a large-scale empirical study using Explainable Boosting Machines and stratified interaction analysis to examine five co-occurring data quality issues in software defect prediction across 374 datasets. It finds that co-occurrence is nearly universal, identifies tipping points for issues like class overlap and imbalance, and reveals context-dependent effects, concluding that no single model performs best under all conditions.

- **[arXiv251222] Learning What to Write: Write-Gated KV for Efficient Long-Context Inference**
  - **tags:** [mlsys], [llm inference], [KV cache management, Write-Gated KV, KV Admission, KV cache eviction, KV cache selection, FlashAttention, paged-KV systems]
  - **authors:** Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu
  - **institution:** National Taiwan University, Academia Sinica
  - **link:** https://arxiv.org/pdf/2512.17452
  - **Simple LLM Summary:** This paper introduces Write-Gated KV, a learnable KV Admission mechanism that predicts token utility before it enters the KV cache to reduce memory usage and speed up inference. By filtering low-utility tokens early and maintaining a compact global cache, the method significantly reduces memory usage and improves prefill and decode speeds for long-context LLMs with minimal accuracy loss. The results demonstrate that proactive KV cache management is a practical solution for efficient long-context inference.

- **[arXiv251222] A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting**
  - **tags:** [ai], [time series forecasting], [spatial-temporal graph neural network, trend-seasonal decomposition, low-rank Top-K adjacency learning, horizon-wise gating, linear baseline]
  - **authors:** Henok Tenaw Moges, Deshendran Moodley
  - **institution:** University of Cape Town
  - **link:** https://arxiv.org/pdf/2512.17453
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Lite-STGNN, a lightweight model combining decomposition-based linear temporal modeling with a learnable sparse graph module for spatial corrections. It achieves state-of-the-art accuracy on long-term multivariate forecasting benchmarks while being parameter-efficient and faster than transformer-based methods. The learned adjacency matrices also provide interpretable insights into domain-specific variable interactions.

- **[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application**
  - **tags:** [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]
  - **authors:** Olivier Jeunen, Schaun Wheeler
  - **institution:** aampe
  - **link:** https://arxiv.org/pdf/2512.17462
  - **Simple LLM Summary:** This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.

- **[arXiv251222] Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks**
  - **tags:** [mlsys], [others], [transformer, linear attention, supervised learning, user-centric clustering, power optimization]
  - **authors:** Irched Chafaa, Giacomo Bacci, Luca Sanguinetti
  - **institution:** University of Pisa
  - **link:** https://arxiv.org/pdf/2512.17466
  - **Simple LLM Summary:** The paper proposes a lightweight transformer model with a customized linear attention mechanism to jointly predict access point clusters and transmission powers in user-centric cell-free massive MIMO networks, using only spatial coordinates. This approach eliminates channel estimation overhead and pilot contamination while ensuring linear scalability with the number of users. Numerical results show the model provides near-optimal performance in maximizing minimum spectral efficiency and is adaptable to dynamic network scenarios.

- **[arXiv251222] Translating the Rashomon Effect to Sequential Decision-Making Tasks**
  - **tags:** [ai], [sequential decision-making], [Rashomon effect, formal verification, policy ensembles, behavioral cloning, permissive policies]
  - **authors:** Dennis Gross, Jørn Eirik Betten, Helge Spieker
  - **institution:** University of Oslo
  - **link:** https://arxiv.org/pdf/2512.17470
  - **Simple LLM Summary:** This paper translates the Rashomon effect from classification to sequential decision-making by defining it for policies that behave identically but have different internal structures. It uses formal verification methods to compare the complete probabilistic behavior of policies in stochastic environments. The study concludes that the effect exists in this domain and that ensembles from the Rashomon set are more robust to distribution shifts.

- **[arXiv251222] Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study**
  - **tags:** [mlsys], [others], [BiLSTM-Variational Autoencoder, BiLSTM-Transformer, surrogate modelling, Norton creep law, self-attention, probabilistic prediction, deterministic prediction]
  - **authors:** Shubham Das, Kaushal Singhania, Amit Sadhu, Suprabhat Das, Arghya Nandi
  - **institution:** Jadavpur University
  - **link:** https://arxiv.org/pdf/2512.17477
  - **Simple LLM Summary:** This paper proposes deep learning-based surrogate models, specifically a BiLSTM-Variational Autoencoder and a BiLSTM-Transformer, to rapidly predict creep strain in Inconel 625, replacing computationally expensive finite-element simulations. The models, trained on ANSYS-generated data, achieve high accuracy and provide predictions within seconds compared to the 30-40 minutes required by traditional simulations, enabling faster design optimization and structural health monitoring for high-temperature alloys.

- **[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis**
  - **tags:** [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]
  - **authors:** Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu
  - **institution:** Towson University
  - **link:** https://arxiv.org/pdf/2512.17488
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.

- **[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology**
  - **tags:** [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]
  - **authors:** Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers
  - **institution:** Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center
  - **link:** https://arxiv.org/pdf/2512.17517
  - **Simple LLM Summary:** This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.

- **[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals**
  - **tags:** [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]
  - **authors:** Muhammad Haris Khan
  - **institution:** University of Copenhagen
  - **link:** https://arxiv.org/pdf/2512.17527
  - **Simple LLM Summary:** This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at ≤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.

- **[arXiv251222] NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks**
  - **tags:** [ai], [neural network training algorithms], [Forward-Forward algorithm, Collaborative Forward-Forward, inter-layer cooperation, goodness function, neuromorphic computing]
  - **authors:** Salar Beigzad
  - **institution:** University of St. Thomas, Minnesota
  - **link:** https://arxiv.org/pdf/2512.17531
  - **Simple LLM Summary:** This paper introduces Collaborative Forward-Forward (CFF) learning, an enhancement to the Forward-Forward algorithm that addresses inter-layer isolation by incorporating weighted contributions from all layers into a collaborative goodness function. It proposes two variants, Fixed CFF and Adaptive CFF, which enable coordinated feature learning while preserving forward-only computation. The method demonstrates significant performance improvements on benchmark datasets, establishing inter-layer collaboration as a fundamental enhancement for biologically plausible and memory-efficient neural network training.

- **[arXiv251222] Bayesian Optimisation: Which Constraints Matter?**
  - **tags:** [ai], [Bayesian optimisation], [Bayesian optimisation, Knowledge Gradient, Gaussian Processes, decoupled constraints, acquisition functions]
  - **authors:** Xietao Wang Lin, Juan Ungredda, Max Butler, James Town, Alma Rahat, Hemant Singh, Juergen Branke
  - **institution:** University of Warwick, ESTECO SpA, Swansea University, University of New South Wales
  - **link:** https://arxiv.org/pdf/2512.17569
  - **Simple LLM Summary:** This paper proposes new Bayesian optimisation variants of the Knowledge Gradient acquisition function for problems with decoupled black-box constraints, focusing on evaluating only the most relevant constraints. The methods are empirically benchmarked and shown to be superior to existing state-of-the-art approaches.

- **[arXiv251222] When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems**
  - **tags:** [mlsys], [others], [MetricGAN-plus-voicebank, semantic WER, noise robustness, speech enhancement]
  - **authors:** Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati
  - **institution:** EkaCare (Orbi Health Private Limited)
  - **link:** https://arxiv.org/pdf/2512.17562
  - **Simple LLM Summary:** This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.

- **[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping**
  - **tags:** [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]
  - **authors:** Yikang Yue, Yishu Yin, Xuehai Qian
  - **institution:** Tsinghua University, University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17570
  - **Simple LLM Summary:** The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.

- **[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing**
  - **tags:** [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]
  - **authors:** Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng
  - **institution:** Wuhan University
  - **link:** https://arxiv.org/pdf/2512.17574
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.

- **[arXiv251222] Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models**
  - **tags:** [mlsys], [others], [federated learning, asynchronous collaboration, model stitching, ensembles, multi-objective learning]
  - **authors:** Arthur Guijt, Dirk Thierens, Ellen Kerkhof, Jan Wiersma, Tanja Alderliesten, Peter A.N. Bosman
  - **institution:** Not specified (inferred from author names only; no affiliations or email domains provided)
  - **link:** https://arxiv.org/pdf/2512.17592
  - **Simple LLM Summary:** This paper proposes using model stitching to combine disjointly trained deep learning models as an asynchronous alternative to federated learning, allowing collaboration without sharing raw data. The method inserts stitching layers to merge intermediate representations, improving generalization across different parties' datasets while maintaining competitive performance on each party's own data. The results show that asynchronous collaboration through stitching can yield competitive performance without requiring synchronous training or data exchange.

- **[arXiv251222] Machine Learning for Static and Single-Event Dynamic Complex Network Analysis**
  - **tags:** [ai], [graph representation learning], [latent space models, latent distance model, graph representation learning, network embeddings, static networks, dynamic networks]
  - **authors:** Nikolaos Nakis
  - **institution:** arXiv
  - **link:** https://arxiv.org/pdf/2512.17577
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a74e4c69d21da3d9d39474c8e185aec564c37a98f7b8fa85563d3b895bd7a484_w640_q70.webp
  - **Simple LLM Summary:** This thesis develops novel algorithmic approaches for graph representation learning using latent space models, specifically the latent distance model, to capture network characteristics like homophily and transitivity. It aims to create structural-aware network representations for tasks such as community characterization and impact dynamics quantification in temporal networks. The methods are designed as unified learning processes to avoid heuristics and multi-stage post-processing, advancing towards comprehensive and powerful network embeddings.

- **[arXiv251222] A Unified Representation of Neural Networks Architectures**
  - **tags:** [ai], [neural networks theory], [continuous neural networks, neural ODEs, distributed parameter systems, approximation error, discretization]
  - **authors:** Christophe Prieur, Mircea Lazar, Bogdan Robu
  - **institution:** Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Eindhoven University of Technology
  - **link:** https://arxiv.org/pdf/2512.17593
  - **Simple LLM Summary:** This paper proposes a unified representation of neural network architectures called Distributed Parameter neural Network (DiPaNet), derived by considering the limiting case where the number of neurons and layers tends to infinity. The method merges integral infinite-width representations with neural ODEs using homogenization and discretization techniques. The main conclusion is that most existing finite and infinite-dimensional neural network architectures can be related through this unified DiPaNet framework.

- **[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification**
  - **tags:** [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]
  - **authors:** Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli
  - **institution:** The University of Texas at El Paso, University of North Carolina
  - **link:** https://arxiv.org/pdf/2512.17594
  - **Simple LLM Summary:** This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.

- **[arXiv251222] A Systems-Theoretic View on the Convergence of Algorithms under Disturbances**
  - **tags:** [mlsys], [others], [converse Lyapunov theorems, stability bounds, convergence rates, distributed learning, noise injection, disturbance modeling]
  - **authors:** Guner Dilsad Er, Sebastian Trimpe, Michael Muehlebach
  - **institution:** Max Planck Institute for Intelligent Systems, RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.17598
  - **Simple LLM Summary:** This paper uses converse Lyapunov theorems to derive stability bounds and convergence rates for iterative algorithms operating under disturbances. It provides a unifying framework to quantify how noise and interconnections affect algorithmic performance, with applications in distributed learning and privacy-preserving computation.

- **[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations**
  - **tags:** [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]
  - **authors:** Mahesh Keswani, Raunak Bhattacharyya
  - **institution:** Indian Institute of Technology Delhi
  - **link:** https://arxiv.org/pdf/2512.17586
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.

- **[arXiv251222] More Consistent Accuracy PINN via Alternating Easy-Hard Training**
  - **tags:** [ai], [scientific machine learning], [physics-informed neural networks, easy-hard prioritization, hybrid training strategy, alternating scheme]
  - **authors:** Zhaoqian Gao, Min Yanga
  - **institution:** Yantai University
  - **link:** https://arxiv.org/pdf/2512.17607
  - **Simple LLM Summary:** This paper proposes a hybrid training strategy for Physics-Informed Neural Networks (PINNs) that alternates between easy and hard prioritization to improve performance. The method achieves consistently high accuracy on challenging PDEs, significantly outperforming baseline approaches. The work demonstrates that this alternating scheme enhances the robustness and reliability of PINNs across diverse problem types.

- **[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions**
  - **tags:** [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]
  - **authors:** Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt
  - **institution:** KU Leuven, Technical University of Munich (TUM)
  - **link:** https://arxiv.org/pdf/2512.17629
  - **Simple LLM Summary:** The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.

- **[arXiv251222] Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection**
  - **tags:** [ai], [natural language processing], [ensemble learning, weighted voting, Condorcet’s Jury Theorem, fine-tuning, transformer models]
  - **authors:** Menna Elgabry, Ali Hamdi
  - **institution:** MSA University
  - **link:** https://arxiv.org/pdf/2512.17630
  - **Simple LLM Summary:** This paper proposes a confidence- and credibility-weighted ensemble framework using diverse small transformer models (BERT, RoBERTa, etc.) for emotion detection. The method combines global validation performance and instance-level confidence to weight model votes. The ensemble achieves a 93.5% macro F1-score on the DAIR-AI dataset, outperforming larger LLMs while being more parameter-efficient.

- **[arXiv251222] Trust-Region Adaptive Policy Optimization**
  - **tags:** [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]
  - **authors:** Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang
  - **institution:** Tsinghua University, Ant Group
  - **link:** https://arxiv.org/pdf/2512.17636
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.

- **[arXiv251222] Estimating Spatially Resolved Radiation Fields Using Neural Networks**
  - **tags:** [mlsys], [others], [Monte-Carlo Simulation, Geant4, Convolutional Neural Networks, Fully Connected Neural Networks, U-Net, FCNN, Dataset Generation]
  - **authors:** Felix Lehner, Pasquale Lombardo, Susana Castillo, Oliver Hupe, Marcus Magnor
  - **institution:** Physikalisch-Technische Bundesanstalt (PTB), Technical University Braunschweig, Belgian Nuclear Research Centre (SCK CEN), University of New Mexico, Leibniz University Hannover
  - **link:** https://arxiv.org/pdf/2512.17654
  - **Simple LLM Summary:** This paper uses neural networks, including convolutional and fully connected architectures, to estimate spatial radiation fields for medical dosimetry, trained on synthetic datasets generated via Monte-Carlo simulations with Geant4. It evaluates design decisions for reconstructing fluence and spectra distributions, concluding with open-source release of datasets and training pipelines.

- **[arXiv251222] Polyharmonic Cascade**
  - **tags:** [ai], [deep learning architecture], [polyharmonic cascade, polyharmonic splines, random functions, principles of indifference, global linear system, GPU matrix operations]
  - **authors:** Yuriy N. Bakhvalov
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17671
  - **Simple LLM Summary:** The paper introduces the "polyharmonic cascade," a deep learning architecture built from packages of polyharmonic splines, derived from random function theory and principles of indifference. It proposes a training method that solves a global linear system per batch instead of using gradient descent, enabling synchronized layer updates and efficient GPU computation. The method demonstrates fast learning without overfitting on the MNIST dataset.

- **[arXiv251222] Vidarc: Embodied Video Diffusion Model for Closed-loop Control**
  - **tags:** [mlsys], [diffusion inference], [video diffusion, autoregressive generation, masked inverse dynamics model, closed-loop control, cross-embodiment pre-training, KV cache]
  - **authors:** Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu
  - **institution:** Tsinghua University
  - **link:** https://arxiv.org/pdf/2512.17661
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines.

- **[arXiv251222] You Only Train Once: Differentiable Subset Selection for Omics Data**
  - **tags:** [ai], [bioinformatics], [differentiable subset selection, multi-task learning, end-to-end training, sparsity, single-cell RNA-seq]
  - **authors:** Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt
  - **institution:** ETH Zurich, University Children’s Hospital Zurich
  - **link:** https://arxiv.org/pdf/2512.17678
  - **Simple LLM Summary:** The paper introduces YOTO, an end-to-end framework that jointly selects discrete gene subsets and performs prediction in a single differentiable model, using sparsity and multi-task learning. It demonstrates improved predictive performance and yields compact, meaningful gene subsets on single-cell RNA-seq datasets, advancing biomarker discovery.

- **[arXiv251222] Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting**
  - **tags:** [ai], [spatio-temporal forecasting], [transformer, self-attention, geostatistical covariance, Gaussian processes, spatial decay parameters]
  - **authors:** Yuri Calleo
  - **institution:** Universitas Mercatorum
  - **link:** https://arxiv.org/pdf/2512.17696
  - **Simple LLM Summary:** This paper proposes a spatially-informed transformer that injects a learnable geostatistical covariance kernel into the self-attention mechanism to incorporate spatial distance information for spatio-temporal forecasting. The method decomposes attention into a stationary physical prior and a data-driven residual, allowing the network to recover spatial decay parameters. Experiments show it outperforms graph neural networks and provides well-calibrated probabilistic forecasts.

- **[arXiv251222] Mitigating Forgetting in Low Rank Adaptation**
  - **tags:** [mlsys], [post-training], [LoRA, Laplace approximation, catastrophic forgetting, regularization, parameter-efficient fine-tuning]
  - **authors:** Joanna Sliwa, Frank Schneider, Philipp Hennig, Jose Miguel Hernandez-Lobato
  - **institution:** University of Tübingen, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.17720
  - **Simple LLM Summary:** This paper introduces LaLoRA, a method that applies a Laplace approximation to LoRA weights to estimate parameter confidence and regularize updates, mitigating catastrophic forgetting during fine-tuning. The approach improves the learning-forgetting trade-off for large language models, as demonstrated by fine-tuning a Llama model for mathematical reasoning, while remaining computationally lightweight.

- **[arXiv251222] Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents**
  - **tags:** [ai], [reinforcement learning], [federated learning, SARSA, linear function approximation, convergence analysis, heterogeneous agents, local training]
  - **authors:** Paul Mangold, Eloïse Berthier, Eric Moulines
  - **institution:** CNRS, École polytechnique, Institut Polytechnique de Paris, ENSTA, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)
  - **link:** https://arxiv.org/pdf/2512.17688
  - **Simple LLM Summary:** This paper presents a theoretical analysis of Federated SARSA (FedSARSA), an on-policy reinforcement learning algorithm with linear function approximation and local training across heterogeneous agents. It establishes the first convergence guarantees and complexity bounds for this setting, showing that FedSARSA achieves linear speed-up with the number of agents despite heterogeneity in transitions and rewards.

- **[arXiv251222] Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation**
  - **tags:** [ai], [graph neural networks], [ECHO benchmark, long-range propagation, message-passing, over-smoothing, over-squashing, synthetic graph tasks, molecular property prediction]
  - **authors:** Luca Miglior, Matteo Tolloso, Alessio Gravina, Davide Bacciu
  - **institution:** University of Pisa
  - **link:** https://arxiv.org/pdf/2512.17762
  - **Simple LLM Summary:** The paper introduces ECHO, a benchmark designed to evaluate the ability of Graph Neural Networks (GNNs) to handle long-range information propagation. It includes synthetic tasks and real-world molecular datasets to test GNNs on challenging, long-distance dependencies. The benchmarking reveals significant performance gaps in existing GNNs, highlighting the difficulty of long-range propagation and the need for improved architectural designs.

- **[arXiv251222] Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments**
  - **tags:** [mlsys], [llm training], [Easy Adaptation, Parameter-Efficient Fine-Tuning, LoRA, Specific Small Models, task adaptation, resource-constrained]
  - **authors:** Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo
  - **institution:** Not explicitly provided in the given text.
  - **link:** https://arxiv.org/pdf/2512.17771
  - **Simple LLM Summary:** The paper proposes Easy Adaptation (EA), a method that uses Specific Small Models (SSMs) to complement the data distribution for Large Models, enabling task adaptation without accessing the LM's internal parameters. This approach matches the performance of Parameter-Efficient Fine-Tuning (PEFT) like LoRA on diverse tasks while requiring only minimal computational resources, making it suitable for resource-constrained environments.

- **[arXiv251222] Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation**
  - **tags:** [ai], [sequential recommendation], [ensembling, ID embeddings, text embeddings, modality features]
  - **authors:** Liam Collins, Bhuvesh Kumar, Clark Mingxuan Ju, Tong Zhao, Donald Loveland, Leonardo Neves, Neil Shah
  - **institution:** Snap Inc.
  - **link:** https://arxiv.org/pdf/2512.17820
  - **Simple LLM Summary:** The paper proposes a sequential recommendation method that independently trains ID-based and text-based models and then combines them via a simple ensembling strategy. It demonstrates that ID and text features learn complementary signals. The main conclusion is that both feature types are necessary for state-of-the-art performance, but complex fusion architectures are not required.

- **[arXiv251222] Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning**
  - **tags:** [ai], [weakly supervised learning], [multi-instance partial-label learning, calibratable disambiguation loss, model calibration]
  - **authors:** Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang
  - **institution:** Southeast University, The University of Newcastle
  - **link:** https://arxiv.org/pdf/2512.17788
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec09016ca3c7c620b69a1f11fd5eac67d6cf0e41b74b5709b9e32e919c36ac8f_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a plug-and-play Calibratable Disambiguation Loss (CDL) to improve classification accuracy and model calibration in Multi-Instance Partial-Label Learning. The loss has two instantiations that calibrate predictions using candidate label probabilities or both candidate and non-candidate sets. Experimental results show that CDL significantly enhances both classification and calibration performance over conventional methods.

- **[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy**
  - **tags:** [mlsys], [others], [imitation learning, distributionally robust control, layered control architecture, Taylor Series Imitation Learning (TaSIL), L1-Distributionally Robust Adaptive Control (L1-DRAC), certifiable autonomy]
  - **authors:** Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon
  - **institution:** University of Illinois Urbana-Champaign, University of Pennsylvania, California Institute of Technology, Massachusetts Institute of Technology, Lockheed Martin
  - **link:** https://arxiv.org/pdf/2512.17899
  - **Simple LLM Summary:** This paper proposes a Distributionally Robust Imitation Policy (DRIP) architecture, a layered control framework that integrates Taylor Series Imitation Learning (TaSIL) and L1-Distributionally Robust Adaptive Control (L1-DRAC) to address different sources of distribution shift. The main conclusion is that this integration enables the design of certifiable autonomy pipelines by guaranteeing performance certificates for the entire control system, combining learning-based components with model-based decision-making.

- **[arXiv251222] Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow**
  - **tags:** [ai], [generative modeling], [Wasserstein-Fisher-Rao gradient flow, weighted stochastic differential equations, Feynman-Kac representation, score-based diffusion models, Langevin dynamics]
  - **authors:** Herlock Rahimi
  - **institution:** Yale University
  - **link:** https://arxiv.org/pdf/2512.17878
  - **Simple LLM Summary:** The paper proposes a new sampling method for generative modeling by implementing Wasserstein-Fisher-Rao gradient flow via weighted stochastic differential equations, using the Feynman-Kac representation. This approach aims to overcome the slow mixing rates of traditional diffusion models in non-log-concave, multimodal target distributions by incorporating controlled mass reweighting. The study provides a rigorous geometric and operator-theoretic foundation for future developments in this area.

- **[arXiv251222] Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space**
  - **tags:** [ai], [operator learning], [random Fourier features, Tikhonov regularization, finite element reconstruction, Student's t distribution]
  - **authors:** Xinyue Yu, Hayden Schaeffer
  - **institution:** University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.17884
  - **Simple LLM Summary:** The paper proposes a regularized random Fourier feature method combined with finite element reconstruction (RRFF-FEM) for learning operators from noisy data, using Student's t-distributed random features and frequency-weighted regularization. It proves theoretical guarantees on conditioning and generalization when features scale appropriately. Numerical experiments on PDE problems show the method is robust to noise, faster to train, and maintains competitive accuracy compared to kernel and neural operator baselines.

- **[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile**
  - **tags:** [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]
  - **authors:** Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa
  - **institution:** UC Berkeley
  - **link:** https://arxiv.org/pdf/2512.17875
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp
  - **Simple LLM Summary:** The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.

- **[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras**
  - **tags:** [mlsys], [multi-modal inference], [diffusion model, bird's-eye-view, radar cross section, Doppler, point cloud generation, foundation models]
  - **authors:** Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany
  - **institution:** Technion, MIT, NVIDIA, University of Toronto, Vector Institute
  - **link:** https://arxiv.org/pdf/2512.17897
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp
  - **Simple LLM Summary:** RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.

- **[arXiv251222] Disentangled representations via score-based variational autoencoders**
  - **tags:** [ai], [representation learning], [variational autoencoder, diffusion models, score-based guidance, disentanglement, evidence lower bound]
  - **authors:** Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin
  - **institution:** New York University, Flatiron Institute
  - **link:** https://arxiv.org/pdf/2512.17127
  - **Simple LLM Summary:** This paper introduces SAMI, a method that combines diffusion models and variational autoencoders into a unified framework for unsupervised representation learning. It uses score-based guidance to learn disentangled, semantically meaningful latent representations from data. The results show that this approach can make implicit structural information in diffusion models explicit and interpretable.

- **[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification**
  - **tags:** [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]
  - **authors:** Faisal Ahmed
  - **institution:** Embry-Riddle Aeronautical University
  - **link:** https://arxiv.org/pdf/2512.16964
  - **Simple LLM Summary:** This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer's disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.

- **[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting**
  - **tags:** [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]
  - **authors:** Ananta R. Bhattarai, Helge Rhodin
  - **institution:** Bielefeld University
  - **link:** https://arxiv.org/pdf/2512.17908
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model's priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.

- **[arXiv251222] Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings**
  - **tags:** [mlsys], [others], [graph attention networks, electroencephalography, spatio-temporal graphs, edge analysis, low-cost hardware, RaspberryPi]
  - **authors:** Szymon Mazurek, Stephen Moore, Alessandro Crimi
  - **institution:** AGH University of Krakow, University of Cape Coast
  - **link:** https://arxiv.org/pdf/2507.15118
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a graph attention network (GAT) framework that models EEG signals as spatio-temporal graphs to detect epilepsy, with a focus on low-cost hardware for deployment in low-resource settings. The method adapts GATs to analyze edge connectivity for biomarker identification and is designed for lightweight training and deployment. The results demonstrate promising classification performance and highlight the potential for scalable, accessible diagnostic support in underserved regions.

- **[arXiv251222] Application of machine learning to predict food processing level using Open Food Facts**
  - **tags:** [ai], [food science and nutrition informatics], [LightGBM, Random Forest, CatBoost, NOVA classification, nutrient concentration data]
  - **authors:** Nalin Arora, Aviral Chauhan, Siddhant Rana, Mahansh Aditya, Sumit Bhagat, Aditya Kumar, Akash Kumar, Akanksh Semar, Ayush Vikram Singh, Ganesh Bagler
  - **institution:** Indraprastha Institute of Information Technology Delhi (IIIT-Delhi), Infosys Center for Artificial Intelligence, Center of Excellence in Healthcare, Foodoscope Technologies Pvt Ltd
  - **link:** https://arxiv.org/pdf/2512.17169
  - **Simple LLM Summary:** This study applies machine learning models, including LightGBM, Random Forest, and CatBoost, to classify food processing levels (NOVA) using nutrient data from the Open Food Facts dataset. The best-performing model, LightGBM, achieved 80-85% accuracy and effectively distinguished minimally from ultra-processed foods. The research concludes that higher processing levels are strongly associated with poorer nutritional quality, greater environmental impact, and common allergens like gluten and milk.

- **[arXiv251222] Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning**
  - **tags:** [mlsys], [others], [multi-layer graph, GNN, temporal GNN, logistic regression, Random Forest, correlation-based, systemic risk]
  - **authors:** Sandeep Neela
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.17185
  - **Simple LLM Summary:** This paper introduces the Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility. It demonstrates that graph-derived features from this model provide useful early-warning signals for market crashes, outperforming standard feature-based models.

- **[arXiv251222] Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease**
  - **tags:** [ai], [fair machine learning], [penalized regression, cost-sensitive classification, true positive rate disparity penalties]
  - **authors:** Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.17340
  - **Simple LLM Summary:** The paper proposes a penalized fair regression framework using unfairness penalties for multiple groups, implemented via reduction to cost-sensitive classification. The method is applied to predict end-stage renal disease in a chronic kidney disease study, showing substantial fairness improvements for multiple race and ethnicity groups without appreciable loss in overall model fit.

- **[arXiv251222] Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions**
  - **tags:** [ai], [optimization algorithms], [alternating direction method of multipliers, nonlinear matrix decomposition, rectified linear unit, component-wise square, MinMax transform]
  - **authors:** Atharva Awari, Nicolas Gillis, Arnaud Vandaele
  - **institution:** University of Mons
  - **link:** https://arxiv.org/pdf/2512.17473
  - **Simple LLM Summary:** The paper proposes an ADMM-based algorithm for solving nonlinear matrix decompositions, where a matrix is approximated via a nonlinear function applied to a low-rank product. It demonstrates flexibility across various nonlinear models and loss functions, showing applicability to real-world datasets in areas like sparse data approximation and recommender systems.

- **[arXiv251222] Resource-efficient medical image classification for edge devices**
  - **tags:** [mlsys], [post-training], [quantization, quantization-aware training, post-training quantization, convolutional neural networks]
  - **authors:** Mahsa Lavaei, Zahra Abadi, Salar Beigzad, Alireza Maleki
  - **institution:** University of Tehran, Tehran University, University of St. Thomas, Minnesota
  - **link:** https://arxiv.org/pdf/2512.17515
  - **Simple LLM Summary:** This paper investigates a resource-efficient approach for medical image classification on edge devices using model quantization techniques, specifically quantization-aware training (QAT) and post-training quantization (PTQ). The study demonstrates that quantized models achieve significant reductions in model size and inference latency while maintaining clinically acceptable diagnostic accuracy, enabling real-time processing in resource-limited settings.

- **[arXiv251222] Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function**
  - **tags:** [mlsys], [others], [wavelet-transform radial distribution function (WT-RDF), parameter optimization, machine learning, RBF, LSTM]
  - **authors:** Deriyan Senjaya, Stephen Ekaputra Limantoro
  - **institution:** National Tsing Hua University, National Yang Ming Chiao Tung University
  - **link:** https://arxiv.org/pdf/2512.17245
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0fcb546d51f53548e0556b6cedd431012b18af3f725ab1ca0c73a63485285ea_w640_q70.webp
  - **Simple LLM Summary:** This paper enhances the wavelet-transform radial distribution function (WT-RDF) for analyzing amorphous materials by using machine learning to optimize its parameters, creating the WT-RDF+ framework. The improved model provides more accurate peak predictions and outperforms benchmark ML models like RBF and LSTM, demonstrating its robustness for characterizing Ge-Se amorphous systems and aiding in the design of phase-change thin films.

- **[arXiv251222] Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing**
  - **tags:** [ai], [compressed sensing, sparse signal reconstruction], [SCAD penalty, approximate message passing, replica symmetry breaking, state evolution, nonconvexity control]
  - **authors:** Xiaosi Gu, Ayaka Sakata, Tomoyuki Obuchi
  - **institution:** Kyoto University, RIKEN center for AIP, Ochanomizu University
  - **link:** https://arxiv.org/pdf/2512.17426
  - **Simple LLM Summary:** This paper develops a one-step replica-symmetry-breaking extension of approximate message passing (1RSB-AMP) for sparse signal reconstruction using the SCAD penalty. The authors propose a new criterion for selecting the Parisi parameter and combine it with nonconvexity control, which improves the algorithmic limit for perfect reconstruction compared to the replica-symmetric AMP, though the gain is modest and remains below the Bayes-optimal threshold.

- **[arXiv251222] Sharp Structure-Agnostic Lower Bounds for General Functional Estimation**
  - **tags:** [ai], [causal inference], [structure-agnostic estimation, double machine learning, debiased learning, doubly robust learning, nuisance functions, functional estimation]
  - **authors:** Jikai Jin, Vasilis Syrgkanis
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.17341
  - **Simple LLM Summary:** This paper establishes sharp lower bounds for structure-agnostic functional estimation, analyzing the optimal error rates achievable without imposing structural priors. It shows that doubly robust learning and double machine learning (DML) are optimal for a general class of functionals, including the average treatment effect (ATE), across different regimes of double robustness. The results provide theoretical validation for first-order debiasing methods and guidance for practitioners in the absence of strong structural assumptions.

- **[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics**
  - **tags:** [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]
  - **authors:** Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton
  - **institution:** University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17534
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.

- **[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design**
  - **tags:** [ai], [molecular design], [Bayesian optimization, generative models, multi-objective optimization, acquisition function, qPMHI, Monte Carlo sampling]
  - **authors:** Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson
  - **institution:** University of Wisconsin–Madison, The Ohio State University, The Dow Chemical Company
  - **link:** https://arxiv.org/pdf/2512.17659
  - **Simple LLM Summary:** This paper introduces a "generate-then-optimize" framework for de novo molecular design, which uses a generative model to create candidate molecules and a novel acquisition function called qPMHI to efficiently select batches for evaluation. The method demonstrates significant improvements over existing approaches in sample efficiency and performance, as shown in benchmarks and a case study on designing organic cathode materials for batteries.

- **[arXiv251222] Imputation Uncertainty in Interpretable Machine Learning Methods**
  - **tags:** [ai], [interpretable machine learning], [permutation feature importance, partial dependence plots, Shapley values, single imputation, multiple imputation, imputation uncertainty, confidence intervals]
  - **authors:** Pegah Golchian, Marvin N. Wright
  - **institution:** Leibniz Institute for Prevention Research & Epidemiology – BIPS, University of Bremen
  - **link:** https://arxiv.org/pdf/2512.17689
  - **Simple LLM Summary:** This paper investigates the impact of missing data imputation methods on the uncertainty quantification of interpretable machine learning (IML) methods. It compares single and multiple imputation, showing that single imputation underestimates variance in IML explanations, while multiple imputation provides confidence interval coverage closer to the nominal level.

- **[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis**
  - **tags:** [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]
  - **authors:** N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain
  - **institution:** Alliance University, Østfold University College
  - **link:** https://arxiv.org/pdf/2512.17585
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.

- **[arXiv251222] Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study**
  - **tags:** [ai], [computational physics], [neural network variational Monte Carlo, quantum Monte Carlo, deep neural network wave function, constant pressure ensemble, density functional theory, group-theoretical analysis]
  - **authors:** Shengdu Chai, Chen Lin, Xinyang Dong, Yuqiang Li, Wanli Ouyang, Lei Wang, X.C. Xie
  - **institution:** Fudan University, Shanghai Artificial Intelligence Laboratory, University of Oxford, Chinese Academy of Sciences, Peking University, Hefei National Laboratory
  - **link:** https://arxiv.org/pdf/2512.17703
  - **Simple LLM Summary:** The paper develops a neural network variational Monte Carlo method to treat electrons and nuclei quantum mechanically, revealing a new Cmcm crystal structure candidate for high-pressure solid hydrogen. This structure matches experimental data but is unstable in static DFT calculations, highlighting the need for full quantum many-body treatment.

- **[arXiv251222] Domain-Aware Quantum Circuit for QML**
  - **tags:** [ai], [quantum machine learning], [parameterized quantum circuits, domain-aware encoding, locality-preserving entanglement, barren plateau mitigation, DCT-style zigzag windows, encode-entangle-train cycles]
  - **authors:** Gurinder Singh, Thaddeus Pellegrini, Kenneth M. Merz Jr
  - **institution:** Cleveland Clinic, IBM Quantum
  - **link:** https://arxiv.org/pdf/2512.17800
  - **Simple LLM Summary:** This paper introduces a Domain-Aware Quantum Circuit (DAQC) for quantum machine learning, which uses image priors to guide locality-preserving encoding and entanglement via non-overlapping zigzag windows. The method mitigates barren plateaus and hardware noise, achieving performance on real quantum hardware competitive with strong classical neural networks for image classification tasks.

- **[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data**
  - **tags:** [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]
  - **authors:** Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen
  - **institution:** University of Nottingham, Nottingham City Hospital
  - **link:** https://arxiv.org/pdf/2512.17759
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.

- **[arXiv251222] Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines**
  - **tags:** [mlsys], [others], [quantum annealing, restricted boltzmann machine, qubo, quantum-assisted machine learning, fraud detection]
  - **authors:** João Marcos Cavalcanti de Albuquerque Neto, Gustavo Castro do Amaral, Guilherme Penello Temporão
  - **institution:** Pontifícia Universidade Católica do Rio de Janeiro, The Netherlands Organization for Applied Scientific Research (TNO)
  - **link:** https://arxiv.org/pdf/2512.17660
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43a1f86f2b2d05435580200ee936d437f042be18dc15953d8144f949af07568c_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the use of Quantum-Assisted Restricted Boltzmann Machines (RBMs) for credit card fraud detection, applying quantum annealing to solve the QUBO problem derived from the RBM's energy function. The method was tested on a real-world dataset of 145 million transactions. The results indicate that the quantum-assisted approach achieves superior performance compared to classical methods, even on current noisy quantum hardware.

- **[arXiv251222] Learning vertical coordinates via automatic differentiation of a dynamical core**
  - **tags:** [mlsys], [others], [automatic differentiation, neural network, terrain-following coordinates, differentiable dynamical core, NEUVE, Arakawa C-grid, non-hydrostatic Euler equations]
  - **authors:** Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo
  - **institution:** Université du Québec à Montréal, University of Saskatchewan, Memorial University of Newfoundland
  - **link:** https://arxiv.org/pdf/2512.17877
  - **Simple LLM Summary:** The paper proposes a framework to learn vertical coordinates by integrating a parametric, neural network-based coordinate system (NEUVE) into a differentiable dynamical core for atmospheric modeling. Using automatic differentiation to compute exact geometric terms, the method optimizes the grid structure for physics and numerics. The learned coordinates reduce errors in benchmarks and eliminate spurious velocity patterns over steep topography.

- **[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation**
  - **tags:** [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]
  - **authors:** Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein
  - **institution:** German Cancer Research Center (DKFZ), Heidelberg University
  - **link:** https://arxiv.org/pdf/2512.17774
  - **Simple LLM Summary:** This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network's architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.

## 2025-12-23

- **[arXiv251223] Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models**
  - **tags:** TBD
  - **authors:** Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17911
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp
  - **Simple LLM Summary:** Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models

- **[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments**
  - **tags:** TBD
  - **authors:** Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17918
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp
  - **Simple LLM Summary:** QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments

- **[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA**
  - **tags:** TBD
  - **authors:** Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17910
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp
  - **Simple LLM Summary:** Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA

- **[arXiv251223] Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset**
  - **tags:** TBD
  - **authors:** Nick Rossenbach, Robin Schmitt, Tina Raissi, Simon Berger, Larissa Kleppel, Ralf Schlüter
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17915
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp
  - **Simple LLM Summary:** Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset

- **[arXiv251223] chatter: a Python library for applying information theory and AI/ML models to animal communication**
  - **tags:** TBD
  - **authors:** Mason Youngblood
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17935
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp
  - **Simple LLM Summary:** chatter: a Python library for applying information theory and AI/ML models to animal communication

- **[arXiv251223] What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD**
  - **tags:** TBD
  - **authors:** Petr Koklev
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17945
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/564ec84544e92a29e57fb456987e278879803262a1ad7f11b5331621181314ae_w640_q70.webp
  - **Simple LLM Summary:** What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD

- **[arXiv251223] Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States**
  - **tags:** TBD
  - **authors:** Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp
  - **Simple LLM Summary:** Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States

- **[arXiv251223] SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries**
  - **tags:** TBD
  - **authors:** Bin Wang, Fadi Dornaika
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17954
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp
  - **Simple LLM Summary:** SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries

- **[arXiv251223] CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs**
  - **tags:** TBD
  - **authors:** Gunho Park, Jeongin Bae, Byeongwook Kim, Baeseong park, Jiwon Ryu, Hoseung Kim, Se Jung Kwon, Dongsoo Lee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17970
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp
  - **Simple LLM Summary:** CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs

- **[arXiv251223] Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models**
  - **tags:** TBD
  - **authors:** Irina Seregina, Philippe Lalanda, German Vega
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17983
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp
  - **Simple LLM Summary:** Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models

- **[arXiv251223] Convolutional-neural-operator-based transfer learning for solving PDEs**
  - **tags:** TBD
  - **authors:** Peng Fan, Guofei Pang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17969
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp
  - **Simple LLM Summary:** Convolutional-neural-operator-based transfer learning for solving PDEs

- **[arXiv251223] MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements**
  - **tags:** TBD
  - **authors:** Ruichen Tan, Jiawei Xue, Kota Tsubouchi, Takahiro Yabe, Satish V. Ukkusuri
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17985
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a4ee269c2652d9f53e2d00acd67fb791427f4c088062380a3d842683837aff9_w640_q70.webp
  - **Simple LLM Summary:** MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements

- **[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India**
  - **tags:** TBD
  - **authors:** Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18014
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp
  - **Simple LLM Summary:** ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India

- **[arXiv251223] FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability**
  - **tags:** TBD
  - **authors:** S M Ruhul Kabir Howlader, Xiao Chen, Yifei Xie, Lu Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17986
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e5e22c4360871dd251f35903ee0f2af43d4bf37326ef9aa7f4dba13f94916b_w640_q70.webp
  - **Simple LLM Summary:** FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability

- **[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models**
  - **tags:** TBD
  - **authors:** Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp
  - **Simple LLM Summary:** Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models

- **[arXiv251223] Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization**
  - **tags:** TBD
  - **authors:** Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17987
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp
  - **Simple LLM Summary:** Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization

- **[arXiv251223] A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients**
  - **tags:** TBD
  - **authors:** Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18031
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp
  - **Simple LLM Summary:** A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients

- **[arXiv251223] Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models**
  - **tags:** TBD
  - **authors:** Wei Qian, Chenxu Zhao, Yangyi Li, Mengdi Huai
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18035
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aaf9e957a407bb93f6576826db847a43f0675e8b765d8c6f38b7c40e06953c3_w640_q70.webp
  - **Simple LLM Summary:** Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models

- **[arXiv251223] A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations**
  - **tags:** TBD
  - **authors:** Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17984
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp
  - **Simple LLM Summary:** A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations

- **[arXiv251223] NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging**
  - **tags:** TBD
  - **authors:** Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18038
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp
  - **Simple LLM Summary:** NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging

- **[arXiv251223] Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts**
  - **tags:** TBD
  - **authors:** Roger A. Finger, Eduardo G. Cortes, Sandro J. Rigo, Gabriel de O. Ramos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18041
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp
  - **Simple LLM Summary:** Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts

- **[arXiv251223] Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics**
  - **tags:** TBD
  - **authors:** Daniel David
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18056
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12301f0dbb41071430f34a9014134b625a48eecd1e6b2f0d69403405addc6d7d_w640_q70.webp
  - **Simple LLM Summary:** Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics

- **[arXiv251223] FOODER: Real-time Facial Authentication and Expression Recognition**
  - **tags:** TBD
  - **authors:** Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18057
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp
  - **Simple LLM Summary:** FOODER: Real-time Facial Authentication and Expression Recognition

- **[arXiv251223] Approximation and learning with compositional tensor trains**
  - **tags:** TBD
  - **authors:** Martin Eigel, Charles Miranda, Anthony Nouy, David Sommer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18059
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91f6bb6272d1b93753642e31936e179de5889b19456b73cd0d2d98f6cb87c979_w640_q70.webp
  - **Simple LLM Summary:** Approximation and learning with compositional tensor trains

- **[arXiv251223] Graph-based Nearest Neighbors with Dynamic Updates via Random Walks**
  - **tags:** TBD
  - **authors:** Nina Mishra, Yonatan Naamad, Tal Wagner, Lichen Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18060
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac9385682bd6e4b07b769afe016fb50f71be5c2b6656f027de335845a16aed_w640_q70.webp
  - **Simple LLM Summary:** Graph-based Nearest Neighbors with Dynamic Updates via Random Walks

- **[arXiv251223] Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability**
  - **tags:** TBD
  - **authors:** Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18092
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp
  - **Simple LLM Summary:** Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability

- **[arXiv251223] From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines**
  - **tags:** TBD
  - **authors:** Kishan Kumar Ganguly, Tim Menzies
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18102
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/509a927741960d2c63062790fd8bebe6a6a22769c29f6985c39fb001039d9fab_w640_q70.webp
  - **Simple LLM Summary:** From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines

- **[arXiv251223] Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins**
  - **tags:** TBD
  - **authors:** Andreas E. Robertson, Samuel B. Inman, Ashley T. Lenau, Ricardo A. Lebensohn, Dongil Shin, Brad L. Boyce, Remi M. Dingreville
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18104
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4c8fb93b00e9d3df0d57fa728955a837f9e46dce9a1fe1000acf82c6b0a1f2_w640_q70.webp
  - **Simple LLM Summary:** Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins

- **[arXiv251223] TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates**
  - **tags:** TBD
  - **authors:** Maxmillan Ries, Sohan Seth
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18129
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a81b46d02b46e72f2027e054c761a825748caf485414255423ed130f0887b330_w640_q70.webp
  - **Simple LLM Summary:** TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates

- **[arXiv251223] Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection**
  - **tags:** TBD
  - **authors:** Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18133
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp
  - **Simple LLM Summary:** Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection

- **[arXiv251223] Learning Generalizable Neural Operators for Inverse Problems**
  - **tags:** TBD
  - **authors:** Adam J. Thorpe, Stepan Tretiakov, Dibakar Roy Sarkar, Krishna Kumar, Ufuk Topcu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18120
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e045cf5e1b4d1841db31e3e3cbb272846f23e43ba557e1455491e138f9aa045_w640_q70.webp
  - **Simple LLM Summary:** Learning Generalizable Neural Operators for Inverse Problems

- **[arXiv251223] Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs**
  - **tags:** TBD
  - **authors:** Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18134
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp
  - **Simple LLM Summary:** Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs

- **[arXiv251223] Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation**
  - **tags:** TBD
  - **authors:** Lena Libon, Meghana Bhange, Rushabh Solanki, Elliot Creager, Ulrich Aïvodji
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18174
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp
  - **Simple LLM Summary:** Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation

- **[arXiv251223] FairExpand: Individual Fairness on Graphs with Partial Similarity Information**
  - **tags:** TBD
  - **authors:** Rebecca Salganik, Yibin Wang, Guillaume Salha-Galvan, Jian Kang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18180
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865ccd9cabcf2b700230e9c10d5db5bbd94993cb8f81acdca83db374c99156b8_w640_q70.webp
  - **Simple LLM Summary:** FairExpand: Individual Fairness on Graphs with Partial Similarity Information

- **[arXiv251223] External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning**
  - **tags:** TBD
  - **authors:** Jian Yan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18190
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp
  - **Simple LLM Summary:** External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning

- **[arXiv251223] When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics**
  - **tags:** TBD
  - **authors:** Yizhou Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18209
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp
  - **Simple LLM Summary:** When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics

- **[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning**
  - **tags:** TBD
  - **authors:** Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18215
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp
  - **Simple LLM Summary:** Stable and Efficient Single-Rollout RL for Multimodal Reasoning

- **[arXiv251223] Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization**
  - **tags:** TBD
  - **authors:** Lichen Yang, Qiang Wang, Zhonghao Yang, Daojing He, Yu Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18228
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9987a54aec669e601a71ba8b0ee7d5434ba0c73d08a66e20538cf347a09669cd_w640_q70.webp
  - **Simple LLM Summary:** Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization

- **[arXiv251223] AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation**
  - **tags:** TBD
  - **authors:** Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18232
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp
  - **Simple LLM Summary:** AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation

- **[arXiv251223] Offline Behavioral Data Selection**
  - **tags:** TBD
  - **authors:** Shiye Lei, Zhihao Cheng, Dacheng Tao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18246
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp
  - **Simple LLM Summary:** Offline Behavioral Data Selection

- **[arXiv251223] Dimensionality Reduction Considered Harmful (Some of the Time)**
  - **tags:** TBD
  - **authors:** Hyeon Jeon
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18230
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp
  - **Simple LLM Summary:** Dimensionality Reduction Considered Harmful (Some of the Time)

- **[arXiv251223] On the Convergence Rate of LoRA Gradient Descent**
  - **tags:** TBD
  - **authors:** Siqiao Mu, Diego Klabjan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18248
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8cfafc1ac3ad3a56d892c47d02b17599c178471d68de77c5fdb73874a8fc10_w640_q70.webp
  - **Simple LLM Summary:** On the Convergence Rate of LoRA Gradient Descent

- **[arXiv251223] LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform**
  - **tags:** TBD
  - **authors:** Lizhi Ma, Yi-Xiang Hu, Yuke Wang, Yifang Zhao, Yihui Ren, Jian-Xiang Liao, Feng Wu, Xiang-Yang Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18266
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0debec5ee9990221746ff49e0aa9313a4766307624a21ded5312ce8127844d_w640_q70.webp
  - **Simple LLM Summary:** LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform

- **[arXiv251223] FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation**
  - **tags:** TBD
  - **authors:** Runze You, Shi Pu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18275
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/075e8765f18ed2a89f21c4b5c0db9ea968dad78f6fb6aca6d0ea6c6135aadcde_w640_q70.webp
  - **Simple LLM Summary:** FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation

- **[arXiv251223] AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning**
  - **tags:** TBD
  - **authors:** Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18295
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp
  - **Simple LLM Summary:** AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning

- **[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings**
  - **tags:** TBD
  - **authors:** Harsh Rathva, Ojas Srivastava, Pruthwik Mishra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18309
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp
  - **Simple LLM Summary:** Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings

- **[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems**
  - **tags:** TBD
  - **authors:** Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18317
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp
  - **Simple LLM Summary:** Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems

- **[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)**
  - **tags:** TBD
  - **authors:** Youssef Mahran, Zeyad Gamal, Ayman El-Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18333
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp
  - **Simple LLM Summary:** Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)

- **[arXiv251223] A two-stream network with global-local feature fusion for bone age assessment**
  - **tags:** TBD
  - **authors:** Qiong Lou, Han Yang, Fang Lu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18331
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp
  - **Simple LLM Summary:** A two-stream network with global-local feature fusion for bone age assessment

- **[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism**
  - **tags:** TBD
  - **authors:** Youssef Mahran, Zeyad Gamal, Ayman El-Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18336
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp
  - **Simple LLM Summary:** Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism

- **[arXiv251223] Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale**
  - **tags:** TBD
  - **authors:** Ansh Nagwekar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18373
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a52d6d3b760cd69f9655054febc798829d4e703cf29f84c34244775c0311631_w640_q70.webp
  - **Simple LLM Summary:** Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale

- **[arXiv251223] Neural Proofs for Sound Verification and Control of Complex Systems**
  - **tags:** TBD
  - **authors:** Alessandro Abate
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18389
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp
  - **Simple LLM Summary:** Neural Proofs for Sound Verification and Control of Complex Systems

- **[arXiv251223] The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?**
  - **tags:** TBD
  - **authors:** Vassilis Digalakis Jr, Christophe Pérignon, Sébastien Saurin, Flore Sentenac
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18390
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c5a36d8f48f4ca89c4a34600dea33569d95e61a50916599a5cc161e183185a_w640_q70.webp
  - **Simple LLM Summary:** The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?

- **[arXiv251223] Automated Mosaic Tesserae Segmentation via Deep Learning Techniques**
  - **tags:** TBD
  - **authors:** Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18406
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp
  - **Simple LLM Summary:** Automated Mosaic Tesserae Segmentation via Deep Learning Techniques

- **[arXiv251223] Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem**
  - **tags:** TBD
  - **authors:** Vikram Krishnamurthy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18409
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb98e65517a8f8628860576c4228301198ae634fca16d11c5f0a252aee09a842_w640_q70.webp
  - **Simple LLM Summary:** Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem

- **[arXiv251223] Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance**
  - **tags:** TBD
  - **authors:** Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18365
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp
  - **Simple LLM Summary:** Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance

- **[arXiv251223] MoE Pathfinder: Trajectory-driven Expert Pruning**
  - **tags:** TBD
  - **authors:** Xican Yang, Yuanhe Tian, Yan Song
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18425
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c828ca8ec3754ef48c892048aa8de845b7afe7c78f324073b3d3bb8afcdfc08_w640_q70.webp
  - **Simple LLM Summary:** MoE Pathfinder: Trajectory-driven Expert Pruning

- **[arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough?**
  - **tags:** TBD
  - **authors:** Amirreza Abbasi, Mohsen Hooshmand
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18445
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp
  - **Simple LLM Summary:** On the Universality of Transformer Architectures; How Much Attention Is Enough?

- **[arXiv251223] NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic**
  - **tags:** TBD
  - **authors:** Jayant Lohia
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18453
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp
  - **Simple LLM Summary:** NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic

- **[arXiv251223] Secret mixtures of experts inside your LLM**
  - **tags:** TBD
  - **authors:** Enric Boix-Adsera
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18452
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp
  - **Simple LLM Summary:** Secret mixtures of experts inside your LLM

- **[arXiv251223] Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs**
  - **tags:** TBD
  - **authors:** David Graber, Victor Armegioiu, Rebecca Buller, Siddhartha Mishra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18454
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2eb80d49254700f260ff3967d335e1d51ecb254bd1a0020cef4b670cd5e329_w640_q70.webp
  - **Simple LLM Summary:** Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs

- **[arXiv251223] Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling**
  - **tags:** TBD
  - **authors:** Christopher Román Jaimes
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18462
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp
  - **Simple LLM Summary:** Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling

- **[arXiv251223] Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review**
  - **tags:** TBD
  - **authors:** Oraib Almegdadi, João Marcelino, Sarah Fakhreddine, João Manso, Nuno C. Marques
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18466
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp
  - **Simple LLM Summary:** Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review

- **[arXiv251223] The Geometry of Abstraction: Continual Learning via Recursive Quotienting**
  - **tags:** TBD
  - **authors:** Xin Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18471
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/946bc88c1851af1d701b7d19272c910b4e964e6d83ae04fa6fbdbf0f215f6e75_w640_q70.webp
  - **Simple LLM Summary:** The Geometry of Abstraction: Continual Learning via Recursive Quotienting

- **[arXiv251223] Research on a hybrid LSTM-CNN-Attention model for text-based web content classification**
  - **tags:** TBD
  - **authors:** Mykola Kuz, Ihor Lazarovych, Mykola Kozlenko, Mykola Pikuliak, Andrii Kvasniuk
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18475
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp
  - **Simple LLM Summary:** Research on a hybrid LSTM-CNN-Attention model for text-based web content classification

- **[arXiv251223] APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification**
  - **tags:** TBD
  - **authors:** Khaled Berkani
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18473
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/917648447e1450eccba6ff12938452166cbb307d588b3036ddd72e80601da793_w640_q70.webp
  - **Simple LLM Summary:** APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification

- **[arXiv251223] PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs**
  - **tags:** TBD
  - **authors:** Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18500
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp
  - **Simple LLM Summary:** PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs

- **[arXiv251223] NASTaR: NovaSAR Automated Ship Target Recognition Dataset**
  - **tags:** TBD
  - **authors:** Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18503
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp
  - **Simple LLM Summary:** NASTaR: NovaSAR Automated Ship Target Recognition Dataset

- **[arXiv251223] Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts**
  - **tags:** TBD
  - **authors:** Hatim M. E. Geli, Islam Omar, Mona Y. Elshinawy, David W. DuBios, Lara Prehodko, Kelly H Smith, Abdel-Hameed A. Badawy
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18522
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp
  - **Simple LLM Summary:** Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts

- **[arXiv251223] Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study**
  - **tags:** TBD
  - **authors:** Janek Dyer, Jagdeep Ahluwalia, Javad Zarrin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18524
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08031a1b54db55f30489d768a136dda2ef1e6ae4c24a0e92ea56eefd2fef4532_w640_q70.webp
  - **Simple LLM Summary:** Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study

- **[arXiv251223] Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset**
  - **tags:** TBD
  - **authors:** S Mahmudul Hasan, Shaily Roy, Akib Jawad Nafis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18533
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp
  - **Simple LLM Summary:** Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset

- **[arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies**
  - **tags:** TBD
  - **authors:** John Cao, Luca Furieri
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18540
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp
  - **Simple LLM Summary:** Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies

- **[arXiv251223] SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models**
  - **tags:** TBD
  - **authors:** Scott Thornton
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18542
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp
  - **Simple LLM Summary:** SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models

- **[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL**
  - **tags:** TBD
  - **authors:** Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18552
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp
  - **Simple LLM Summary:** Toward Training Superintelligent Software Agents through Self-Play SWE-RL

- **[arXiv251223] Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing**
  - **tags:** TBD
  - **authors:** Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18575
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp
  - **Simple LLM Summary:** Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing

- **[arXiv251223] Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model**
  - **tags:** TBD
  - **authors:** Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18573
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp
  - **Simple LLM Summary:** Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model

- **[arXiv251223] SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models**
  - **tags:** TBD
  - **authors:** Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18583
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp
  - **Simple LLM Summary:** SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models

- **[arXiv251223] Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment**
  - **tags:** TBD
  - **authors:** Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18566
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17e1231f344e9051e9a9124b095324f59b3035dd2a93ac7e3bdf19d86c6eec28_w640_q70.webp
  - **Simple LLM Summary:** Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment

- **[arXiv251223] EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture**
  - **tags:** TBD
  - **authors:** Quanxi Zhou, Wencan Mao, Yilei Liang, Manabu Tsukada, Yunling Liu, Jon Crowcroft
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18596
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856f913f2cb31f98d005287343fc242bd36ba508c56e6d8fbbbb68c073df2a30_w640_q70.webp
  - **Simple LLM Summary:** EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture

- **[arXiv251223] Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows**
  - **tags:** TBD
  - **authors:** Runze Mao, Rui Zhang, Xuan Bai, Tianhao Wu, Teng Zhang, Zhenyi Chen, Minqi Lin, Bocheng Zeng, Yangchen Xu, Yingxuan Xiang, Haoze Zhang, Shubham Goswami, Pierre A. Dawe, Yifan Xu, Zhenhua An, Mengtao Yan, Xiaoyi Lu, Yi Wang, Rongbo Bai, Haobu Gao, Xiaohang Fang, Han Li, Hao Sun, Zhi X. Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18595
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54fa7968484f2312fe5e5490bcb221fdffbad35b3d30834c98660d094a86bdd2_w640_q70.webp
  - **Simple LLM Summary:** Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows

- **[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning**
  - **tags:** TBD
  - **authors:** Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18604
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp
  - **Simple LLM Summary:** Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning

- **[arXiv251223] The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss**
  - **tags:** TBD
  - **authors:** Rongyao Cai, Yuxi Wan, Kexin Zhang, Ming Jin, Hao Wang, Zhiqiang Ge, Daoyi Dong, Yong Liu, Qingsong Wen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18610
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a15caebdaadc9b61bcc30ac1fa7534d1efe4cc5ec0d9274e6b2bd1e89b75d13_w640_q70.webp
  - **Simple LLM Summary:** The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss

- **[arXiv251223] The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation**
  - **tags:** TBD
  - **authors:** Huiqi Deng, Qihan Ren, Zhuofan Chen, Zhenyuan Cui, Wen Shen, Peng Zhang, Hongbin Pei, Quanshi Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18607
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp
  - **Simple LLM Summary:** The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation

- **[arXiv251223] ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs**
  - **tags:** TBD
  - **authors:** Han-Seul Jeong, Youngjoon Park, Hyungseok Song, Woohyung Lim
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18633
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp
  - **Simple LLM Summary:** ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs

- **[arXiv251223] From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers**
  - **tags:** TBD
  - **authors:** Ryotaro Kawata, Yujin Song, Alberto Bietti, Naoki Nishikawa, Taiji Suzuki, Samuel Vaiter, Denny Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18634
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e54fb36cb8dc15a87cb11d651f2fd0a36426f91acc811072a8e23c04544cd8_w640_q70.webp
  - **Simple LLM Summary:** From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers

- **[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval**
  - **tags:** TBD
  - **authors:** Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18660
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp
  - **Simple LLM Summary:** PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval

- **[arXiv251223] Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs**
  - **tags:** TBD
  - **authors:** Ning Lyu, Junjie Jiang, Lu Chang, Chihui Shao, Feng Chen, Chong Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18673
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c16637936bf2055d30084813c6afcafc16bc9cf66165235e01a57ad34447eb0_w640_q70.webp
  - **Simple LLM Summary:** Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs

- **[arXiv251223] Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis**
  - **tags:** TBD
  - **authors:** Pengchao Feng, Yao Xiao, Ziyang Ma, Zhikang Niu, Shuai Fan, Yao Li, Sheng Wang, Xie Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18699
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp
  - **Simple LLM Summary:** Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis

- **[arXiv251223] Generating Risky Samples with Conformity Constraints via Diffusion Models**
  - **tags:** TBD
  - **authors:** Han Yu, Hao Zou, Xingxuan Zhang, Zhengyi Wang, Yue He, Kehan Li, Peng Cui
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18722
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccd2af87861ffba1a1a1172552bec8a5a2c9fb4db73b173801ff6c5db0f1734b_w640_q70.webp
  - **Simple LLM Summary:** Generating Risky Samples with Conformity Constraints via Diffusion Models

- **[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments**
  - **tags:** TBD
  - **authors:** Xue Yang, Michael Schukat, Junlin Lu, Patrick Mannion, Karl Mason, Enda Howley
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18670
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp
  - **Simple LLM Summary:** Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments

- **[arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models**
  - **tags:** TBD
  - **authors:** Zhiquan Tan, Yinrong Hong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18730
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp
  - **Simple LLM Summary:** A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models

- **[arXiv251223] ML Inference Scheduling with Predictable Latency**
  - **tags:** TBD
  - **authors:** Haidong Zhao, Nikolaos Georgantas
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18725
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a8d1548e228be441a72232fe96db7a46d1c28ad7f5a73b47e37580f3b42dadf_w640_q70.webp
  - **Simple LLM Summary:** ML Inference Scheduling with Predictable Latency

- **[arXiv251223] Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding**
  - **tags:** TBD
  - **authors:** Xiangrui Cai, Shaocheng Ma, Lei Cao, Jie Li, Tianyu Liu, Yilin Dong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18689
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp
  - **Simple LLM Summary:** Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding

- **[arXiv251223] PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation**
  - **tags:** TBD
  - **authors:** Zichuan Lin, Xiaokai Huang, Jiate Liu, Yuxuan Han, Jia Chen, Xiapeng Wu, Deheng Ye
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18737
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp
  - **Simple LLM Summary:** PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation

- **[arXiv251223] Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth**
  - **tags:** TBD
  - **authors:** Chainarong Amornbunchornvej
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18732
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp
  - **Simple LLM Summary:** Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth

- **[arXiv251223] Is Your Conditional Diffusion Model Actually Denoising?**
  - **tags:** TBD
  - **authors:** Daniel Pfrommer, Zehao Dou, Christopher Scarvelis, Max Simchowitz, Ali Jadbabaie
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18736
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856ef1fcb2e79aa2ab4b2a022b1c3d13580d56e426a05e9b3d88850160ea2eac_w640_q70.webp
  - **Simple LLM Summary:** Is Your Conditional Diffusion Model Actually Denoising?

- **[arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Minh Vu, Konstantinos Slavakis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18763
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp
  - **Simple LLM Summary:** Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning

- **[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search**
  - **tags:** TBD
  - **authors:** Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18745
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp
  - **Simple LLM Summary:** InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search

- **[arXiv251223] Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers**
  - **tags:** TBD
  - **authors:** Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18784
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp
  - **Simple LLM Summary:** Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers

- **[arXiv251223] Label-Informed Outlier Detection Based on Granule Density**
  - **tags:** TBD
  - **authors:** Baiyang Chen, Zhong Yuan, Dezhong Peng, Hongmei Chen, Xiaomin Song, Huiming Zheng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18774
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27f829438adaa9d5eb3ec20d666226593c7dc2a412469b585dd40c6517ab3e3_w640_q70.webp
  - **Simple LLM Summary:** Label-Informed Outlier Detection Based on Granule Density

- **[arXiv251223] Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection**
  - **tags:** TBD
  - **authors:** Souhail Abdelmouaiz Sadat, Mohamed Yacine Touahria Miliani, Khadidja Hab El Hames, Hamida Seba, Mohammed Haddad
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18826
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp
  - **Simple LLM Summary:** Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection

- **[arXiv251223] Controllable Probabilistic Forecasting with Stochastic Decomposition Layers**
  - **tags:** TBD
  - **authors:** John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne II, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18815
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp
  - **Simple LLM Summary:** Controllable Probabilistic Forecasting with Stochastic Decomposition Layers

- **[arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator**
  - **tags:** TBD
  - **authors:** Yuanchao Xu, Fengyi Li, Masahiro Fujisawa, Youssef Marzouk, Isao Ishikawa
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18837
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp
  - **Simple LLM Summary:** Generative Modeling through Spectral Analysis of Koopman Operator

- **[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning**
  - **tags:** TBD
  - **authors:** Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18857
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp
  - **Simple LLM Summary:** CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning

- **[arXiv251223] Application of deep learning approaches for medieval historical documents transcription**
  - **tags:** TBD
  - **authors:** Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18865
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp
  - **Simple LLM Summary:** Application of deep learning approaches for medieval historical documents transcription

- **[arXiv251223] Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models**
  - **tags:** TBD
  - **authors:** Gökdeniz Gülmez
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18901
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp
  - **Simple LLM Summary:** Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models

- **[arXiv251223] The Ensemble Schr\{ö\}dinger Bridge filter for Nonlinear Data Assimilation**
  - **tags:** TBD
  - **authors:** Feng Bao, Hui Sun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18928
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b1984f127a0ea77e013fd30f9e55a2d1893fc429cb8128fbd4e89c1fefcb6_w640_q70.webp
  - **Simple LLM Summary:** The Ensemble Schr\{ö\}dinger Bridge filter for Nonlinear Data Assimilation

- **[arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets**
  - **tags:** TBD
  - **authors:** Andrew Polar, Michael Poluektov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18921
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp
  - **Simple LLM Summary:** Merging of Kolmogorov-Arnold networks trained on disjoint datasets

- **[arXiv251223] DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems**
  - **tags:** TBD
  - **authors:** Sarwan Ali
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18932
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c243588ebf5b62b4dd8e1854c41bcdec7ca5861520351edb4b3b12455148d30_w640_q70.webp
  - **Simple LLM Summary:** DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems

- **[arXiv251223] When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models**
  - **tags:** TBD
  - **authors:** Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp
  - **Simple LLM Summary:** When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models

- **[arXiv251223] Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement**
  - **tags:** TBD
  - **authors:** Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18950
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp
  - **Simple LLM Summary:** Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement

- **[arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation**
  - **tags:** TBD
  - **authors:** Debamita Ghosh, George K. Atia, Yue Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18957
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp
  - **Simple LLM Summary:** Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation

- **[arXiv251223] Learning Through Little Eyes: Attribute Discrimination Beyond Objects**
  - **tags:** TBD
  - **authors:** Patrick Batsell, Tsutsui Satoshi, Bihan Wen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18951
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e80b7c71db93c26589e4d25cfffe472d3de86a058a86caaabd4d81fb9bc2c38_w640_q70.webp
  - **Simple LLM Summary:** Learning Through Little Eyes: Attribute Discrimination Beyond Objects

- **[arXiv251223] Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling**
  - **tags:** TBD
  - **authors:** Sutashu Tomonaga, Kenji Doya, Noboru Murata
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18965
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49d3230921a0fe478c29d46346dffb80acdac27624660b8dd8316094a5be88aa_w640_q70.webp
  - **Simple LLM Summary:** Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling

- **[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection**
  - **tags:** TBD
  - **authors:** Yizhi Wang, Linan Yue, Min-Ling Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18956
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp
  - **Simple LLM Summary:** Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection

- **[arXiv251223] Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets**
  - **tags:** TBD
  - **authors:** Baiyang Chen, Zhong Yuan, Dezhong Peng, Xiaoliang Chen, Hongmei Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18977
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f137793e37d26cff00a3715c8212e2ad3fa2487bcba70f9d1f1f23ec3f06ea9_w640_q70.webp
  - **Simple LLM Summary:** Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets

- **[arXiv251223] Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy**
  - **tags:** TBD
  - **authors:** Baiyang Chen, Zhong Yuan, Zheng Liu, Dezhong Peng, Yongxiang Li, Chang Liu, Guiduo Duan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18978
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58b9c746816307fee96e3cf245632387dbf3ca0eaff52acdf2fdc9465315f8e9_w640_q70.webp
  - **Simple LLM Summary:** Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy

- **[arXiv251223] OPBO: Order-Preserving Bayesian Optimization**
  - **tags:** TBD
  - **authors:** Wei Peng, Jianchen Hu, Kang Liu, Qiaozhu Zhai
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18980
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b75b70c8d757c498d035de7add4278dda4866bb8a7052d89525a499956b1847_w640_q70.webp
  - **Simple LLM Summary:** OPBO: Order-Preserving Bayesian Optimization

- **[arXiv251223] R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression**
  - **tags:** TBD
  - **authors:** Kun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu, Pengfei Gu, Chenghua Lin, Paul M. Thompson, Alex Leow, Heng Huang, Lifang He, Liang Zhan, Haoteng Tang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18986
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp
  - **Simple LLM Summary:** R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression

- **[arXiv251223] Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline**
  - **tags:** TBD
  - **authors:** Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19011
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp
  - **Simple LLM Summary:** Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline

- **[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management**
  - **tags:** TBD
  - **authors:** Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19001
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp
  - **Simple LLM Summary:** ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management

- **[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models**
  - **tags:** TBD
  - **authors:** Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp
  - **Simple LLM Summary:** Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models

- **[arXiv251223] The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results**
  - **tags:** TBD
  - **authors:** Konstantin Kaulen, Tobias Ladner, Stanley Bak, Christopher Brix, Hai Duong, Thomas Flinkow, Taylor T. Johnson, Lukas Koller, Edoardo Manino, ThanhVu H Nguyen, Haoze Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19007
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp
  - **Simple LLM Summary:** The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results

- **[arXiv251223] Optimizer Dynamics at the Edge of Stability with Differential Privacy**
  - **tags:** TBD
  - **authors:** Ayana Hussain, Ricky Fang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19019
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe869fb966bc8e432b82a891d4042c2fd2899dd38d8367f5952e66e27a733144_w640_q70.webp
  - **Simple LLM Summary:** Optimizer Dynamics at the Edge of Stability with Differential Privacy

- **[arXiv251223] CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization**
  - **tags:** TBD
  - **authors:** Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19020
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp
  - **Simple LLM Summary:** CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization

- **[arXiv251223] The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation**
  - **tags:** TBD
  - **authors:** Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19025
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp
  - **Simple LLM Summary:** The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation

- **[arXiv251223] Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building**
  - **tags:** TBD
  - **authors:** Liping Sun, Yucheng Guo, Siliang Lu, Zhenzhen Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19038
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac72029d8bfa1d8aa93562d8cd7cf4c8a3ea568788647c1533d43f1e70bc9335_w640_q70.webp
  - **Simple LLM Summary:** Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building

- **[arXiv251223] Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms**
  - **tags:** TBD
  - **authors:** Md Minhazul Islam Munna, Md Mahbubur Rahman, Jaroslav Frnda, Muhammad Shahid Anwar, Alpamis Kutlimuratov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19037
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f65bbdc2e7a521d46fdaeebb4c5a652347737b90cb0eb72dc48b2bfbbff2789_w640_q70.webp
  - **Simple LLM Summary:** Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms

- **[arXiv251223] Recontextualization Mitigates Specification Gaming without Modifying the Specification**
  - **tags:** TBD
  - **authors:** Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19027
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp
  - **Simple LLM Summary:** Recontextualization Mitigates Specification Gaming without Modifying the Specification

- **[arXiv251223] On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation**
  - **tags:** TBD
  - **authors:** George Vershinin, Asaf Cohen, Omer Gurewitz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19067
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/395cdf02cbfcdfcb8cd487bcc7bb40a760166e8635105ec2f48357e1a0a09bbb_w640_q70.webp
  - **Simple LLM Summary:** On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation

- **[arXiv251223] Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation**
  - **tags:** TBD
  - **authors:** Chi Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19061
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp
  - **Simple LLM Summary:** Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation

- **[arXiv251223] Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges**
  - **tags:** TBD
  - **authors:** Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19091
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp
  - **Simple LLM Summary:** Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges

- **[arXiv251223] Efficient Personalization of Generative Models via Optimal Experimental Design**
  - **tags:** TBD
  - **authors:** Guy Schacht, Ziyad Sheebaelhamd, Riccardo De Santi, Mojmír Mutný, Andreas Krause
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19057
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcef3113c71e842238e399994571c9492cf534afd3c06e6241febfcda1354e87_w640_q70.webp
  - **Simple LLM Summary:** Efficient Personalization of Generative Models via Optimal Experimental Design

- **[arXiv251223] Dual Model Deep Learning for Alzheimer Prognostication**
  - **tags:** TBD
  - **authors:** Alireza Moayedikia, Sara Fin, Uffe Kock Wiil
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19099
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57c6707a8cb9bbf9189cedc3fc04aab23c29bbec6c15b1c12163211decc70869_w640_q70.webp
  - **Simple LLM Summary:** Dual Model Deep Learning for Alzheimer Prognostication

- **[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning**
  - **tags:** TBD
  - **authors:** Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19103
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp
  - **Simple LLM Summary:** Timely Parameter Updating in Over-the-Air Federated Learning

- **[arXiv251223] DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale**
  - **tags:** TBD
  - **authors:** Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19097
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp
  - **Simple LLM Summary:** DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale

- **[arXiv251223] A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development**
  - **tags:** TBD
  - **authors:** Yuan Fang, Fabian Waschkowski, Maximilian Reissmann, Richard D. Sandberg, Takuo Oda, Koichi Tanimoto
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19031
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6630d808b10ee83956b0b8d3975fc127ddbef17b5014ced381ff234a9441900d_w640_q70.webp
  - **Simple LLM Summary:** A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development

- **[arXiv251223] A Composable Channel-Adaptive Architecture for Seizure Classification**
  - **tags:** TBD
  - **authors:** Francesco Carzaniga, Michael Hersche, Kaspar Schindler, Abbas Rahimi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19123
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80ba764ee64b005cd983a9e6b724f69e3e08628a8e048027385de0b6ea62e65d_w640_q70.webp
  - **Simple LLM Summary:** A Composable Channel-Adaptive Architecture for Seizure Classification

- **[arXiv251223] SAP: Syntactic Attention Pruning for Transformer-based Language Models**
  - **tags:** TBD
  - **authors:** Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19125
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp
  - **Simple LLM Summary:** SAP: Syntactic Attention Pruning for Transformer-based Language Models

- **[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT**
  - **tags:** TBD
  - **authors:** Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19131
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp
  - **Simple LLM Summary:** Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT

- **[arXiv251223] HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction**
  - **tags:** TBD
  - **authors:** Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin, Wei Zhong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19114
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp
  - **Simple LLM Summary:** HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction

- **[arXiv251223] A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage**
  - **tags:** TBD
  - **authors:** Francis Bach
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19142
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4a93fce488552329133c465e13cb55215345c0def3d3e8109e9ab58f1388fad_w640_q70.webp
  - **Simple LLM Summary:** A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage

- **[arXiv251223] RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling**
  - **tags:** TBD
  - **authors:** Haoran Yang, Yinan Zhang, Wenjie Zhang, Dongxia Wang, Peiyu Liu, Yuqi Ye, Kexin Chen, Wenhai Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19147
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d3125cc08ed130beab248cba1a7473ff408ca14e82e124e77351d681c62aec5_w640_q70.webp
  - **Simple LLM Summary:** RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling

- **[arXiv251223] Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments**
  - **tags:** TBD
  - **authors:** Geraud Nangue Tasse, Matthew Riemer, Benjamin Rosman, Tim Klinger
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19154
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp
  - **Simple LLM Summary:** Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments

- **[arXiv251223] Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning**
  - **tags:** TBD
  - **authors:** Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19184
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp
  - **Simple LLM Summary:** Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning

- **[arXiv251223] Practical Quantum-Classical Feature Fusion for complex data Classification**
  - **tags:** TBD
  - **authors:** Azadeh Alavi, Fatemeh Kouchmeshki, Abdolrahman Alavi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19180
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp
  - **Simple LLM Summary:** Practical Quantum-Classical Feature Fusion for complex data Classification

- **[arXiv251223] Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction**
  - **tags:** TBD
  - **authors:** Leming Zhou, Zuo Wang, Zhigang Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19194
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6faf26f0cc652e06ad561a1f1b33d9cf5c741015ebbded034f635a578186206_w640_q70.webp
  - **Simple LLM Summary:** Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction

- **[arXiv251223] On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning**
  - **tags:** TBD
  - **authors:** Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19199
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp
  - **Simple LLM Summary:** On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning

- **[arXiv251223] PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements**
  - **tags:** TBD
  - **authors:** Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19190
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp
  - **Simple LLM Summary:** PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements

- **[arXiv251223] MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning**
  - **tags:** TBD
  - **authors:** Tao Zhang, Ziqian Zeng, Hao Peng, Huiping Zhuang, Cen Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19206
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp
  - **Simple LLM Summary:** MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning

- **[arXiv251223] Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation**
  - **tags:** TBD
  - **authors:** Anna-Maria Gueorguieva, Aylin Caliskan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19238
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp
  - **Simple LLM Summary:** Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation

- **[arXiv251223] Phase-space entropy at acquisition reflects downstream learnability**
  - **tags:** TBD
  - **authors:** Xiu-Cheng Wang, Jun-Jie Zhanga, Nan Cheng, Long-Gang Pang, Taijiao Du, Deyu Meng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19223
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp
  - **Simple LLM Summary:** Phase-space entropy at acquisition reflects downstream learnability

- **[arXiv251223] From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis**
  - **tags:** TBD
  - **authors:** Moncef Garouani, Ayah Barhrhouj
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19246
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edaee8be894e0b2aabf965adc9c2da7a8499fac8b0b1b7437890bac0428efa6_w640_q70.webp
  - **Simple LLM Summary:** From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis

- **[arXiv251223] Regression generation adversarial network based on dual data evaluation strategy for industrial application**
  - **tags:** TBD
  - **authors:** Zesen Wang, Yonggang Li, Lijuan Lan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19232
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0ad96ff9f5740075b9f98eecc3bf3e0d3e6ae902f7528d0c14e4cef5572f55_w640_q70.webp
  - **Simple LLM Summary:** Regression generation adversarial network based on dual data evaluation strategy for industrial application

- **[arXiv251223] Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems**
  - **tags:** TBD
  - **authors:** Prathamesh Devadiga
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19250
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp
  - **Simple LLM Summary:** Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems

- **[arXiv251223] Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study**
  - **tags:** TBD
  - **authors:** Carla Crivoi, Radu Tudor Ionescu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19253
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp
  - **Simple LLM Summary:** Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study

- **[arXiv251223] Translating Flow to Policy via Hindsight Online Imitation**
  - **tags:** TBD
  - **authors:** Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19269
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp
  - **Simple LLM Summary:** Translating Flow to Policy via Hindsight Online Imitation

- **[arXiv251223] GShield: Mitigating Poisoning Attacks in Federated Learning**
  - **tags:** TBD
  - **authors:** Sameera K. M., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19286
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c43d719deb3b65a2681b1c10fbb4647b0e73aa0efbe80e20c06d9f27f59d1058_w640_q70.webp
  - **Simple LLM Summary:** GShield: Mitigating Poisoning Attacks in Federated Learning

- **[arXiv251223] Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring**
  - **tags:** TBD
  - **authors:** Keivan Faghih Niresi, Jun Qing, Mengjie Zhao, Olga Fink
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19309
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e839b4b1adb5610adcd939ece8fb2b9f58ed8b25b96c321b897b8bac816ca350_w640_q70.webp
  - **Simple LLM Summary:** Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring

- **[arXiv251223] Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals**
  - **tags:** TBD
  - **authors:** Chang Dong, Jianfeng Tao, Chengliang Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19280
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23dae36b800575da09218251c1aa3582a2ce5ca30c8c9988566be10e83f43e38_w640_q70.webp
  - **Simple LLM Summary:** Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals

- **[arXiv251223] Alternative positional encoding functions for neural transformers**
  - **tags:** TBD
  - **authors:** Ezequiel Lopez-Rubio, Macoris Decena-Gimenez, Rafael Marcos Luque-Baena
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19323
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a8d61d0d13038271b6545ad7d265494ffd5b3cce79e03046ec542b879d17a8_w640_q70.webp
  - **Simple LLM Summary:** Alternative positional encoding functions for neural transformers

- **[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration**
  - **tags:** TBD
  - **authors:** Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19320
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp
  - **Simple LLM Summary:** MAGIC: Achieving Superior Model Merging via Magnitude Calibration

- **[arXiv251223] A Logical View of GNN-Style Computation and the Role of Activation Functions**
  - **tags:** TBD
  - **authors:** Pablo Barceló, Floris Geerts, Matthias Lanzinger, Klara Pakhomenko, Jan Van den Bussche
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19332
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6cc724cbd8fd52ec250ad8e7e8c8c76440bef31c5f1eae060dbb796cd6e916_w640_q70.webp
  - **Simple LLM Summary:** A Logical View of GNN-Style Computation and the Role of Activation Functions

- **[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives**
  - **tags:** TBD
  - **authors:** Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19342
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp
  - **Simple LLM Summary:** Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives

- **[arXiv251223] Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models**
  - **tags:** TBD
  - **authors:** Haohua Chen, Songbin Liu, Junjie Ma
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19334
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72f86b8d15772f627ff898121603174c39243f416963a0e15e5c8ec2eec03c72_w640_q70.webp
  - **Simple LLM Summary:** Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models

- **[arXiv251223] VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop**
  - **tags:** TBD
  - **authors:** JiaWei Zhu, ZiHeng Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19349
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0b686fca34f155b782afa1f7cabe09de9b7bcb967ccfba8215a39405da5ba99_w640_q70.webp
  - **Simple LLM Summary:** VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop

- **[arXiv251223] Learning General Policies with Policy Gradient Methods**
  - **tags:** TBD
  - **authors:** Simon Ståhlberg, Blai Bonet, Hector Geffner
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19366
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp
  - **Simple LLM Summary:** Learning General Policies with Policy Gradient Methods

- **[arXiv251223] From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples**
  - **tags:** TBD
  - **authors:** Canran Xiao, Jiabao Dou, Zhiming Lin, Zong Ke, Liwei Hou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19363
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc75c95e9b775b7503c7e61fa819d446488d179c11fba8ca58a1b86470bc4a23_w640_q70.webp
  - **Simple LLM Summary:** From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples

- **[arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation**
  - **tags:** TBD
  - **authors:** Isshaan Singh, Divyansh Chawla, Anshu Garg, Shivin Mangal, Pallavi Gupta, Khushi Agarwal, Nimrat Singh Khalsa, Nandan Patel
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19361
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afec297ffe8c1a2ba4e79439bf065a0920537f7decb882056013d6e7740f8e9_w640_q70.webp
  - **Simple LLM Summary:** Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation

- **[arXiv251223] Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture**
  - **tags:** TBD
  - **authors:** Christian Hägg, Kathlén Kohn, Giovanni Luca Marchetti, Boris Shapiro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19367
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa84d344152205d90f463c26b0bb9356b71dde7f412bdd03d7fd7f036a0b845_w640_q70.webp
  - **Simple LLM Summary:** Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture

- **[arXiv251223] Real-Time Machine Learning for Embedded Anomaly Detection**
  - **tags:** TBD
  - **authors:** Abdelmadjid Benmachiche, Khadija Rais, Hamda Slimi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19383
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d97ca2384a1b645ebcb761e5a1b2985732f10d8ceeea842160ea8a4c2445fca_w640_q70.webp
  - **Simple LLM Summary:** Real-Time Machine Learning for Embedded Anomaly Detection

- **[arXiv251223] OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation**
  - **tags:** TBD
  - **authors:** Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19379
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp
  - **Simple LLM Summary:** OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation

- **[arXiv251223] Brain-Grounded Axes for Reading and Steering LLM States**
  - **tags:** TBD
  - **authors:** Sandro Andric
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19399
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01206bf10e0d62a8473930c796f72a63c7683f7ede540cfc495bca18c3dae148_w640_q70.webp
  - **Simple LLM Summary:** Brain-Grounded Axes for Reading and Steering LLM States

- **[arXiv251223] Research Program: Theory of Learning in Dynamical Systems**
  - **tags:** TBD
  - **authors:** Elad Hazan, Shai Shalev Shwartz, Nathan Srebro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19410
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/879026bb2ccb2b89c867729f9f077e32888b82173c20d950c0ed5feda6519aa9_w640_q70.webp
  - **Simple LLM Summary:** Research Program: Theory of Learning in Dynamical Systems

- **[arXiv251223] Symplectic Reservoir Representation of Legendre Dynamics**
  - **tags:** TBD
  - **authors:** Robert Simon Fong, Gouhei Tanaka, Kazuyuki Aihara
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19409
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2e2995c2a2a446a77a09ae653f133649b499ee9a5ae92858d90f48c38a521f4_w640_q70.webp
  - **Simple LLM Summary:** Symplectic Reservoir Representation of Legendre Dynamics

- **[arXiv251223] Attention Is Not What You Need**
  - **tags:** TBD
  - **authors:** Zhang Chong
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19428
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbe1f1a463179312610994fd34a110ca4bfc5b56928e49a6749dd43948421e91_w640_q70.webp
  - **Simple LLM Summary:** Attention Is Not What You Need

- **[arXiv251223] An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning**
  - **tags:** TBD
  - **authors:** Rixin Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19439
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e313ba9b9f91e7572ee520f356be411e0bf445d35d515381b2d04e8c77cfcf_w640_q70.webp
  - **Simple LLM Summary:** An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning

- **[arXiv251223] Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm**
  - **tags:** TBD
  - **authors:** Antonio Consolo, Andrea Manno, Edoardo Amaldi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19440
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7c0b3cd4456cb3f28896dad50578a59f6ee4373475817c1d203ce6b2eda4d60_w640_q70.webp
  - **Simple LLM Summary:** Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm

- **[arXiv251223] GLUE: Generative Latent Unification of Expertise-Informed Engineering Models**
  - **tags:** TBD
  - **authors:** Tim Aebersold, Soheyl Massoudi, Mark D. Fuge
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19469
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e870c1241b521f8cd85e2b95b902b04bfcabd5e890393790f05186cc61348532_w640_q70.webp
  - **Simple LLM Summary:** GLUE: Generative Latent Unification of Expertise-Informed Engineering Models

- **[arXiv251223] Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications**
  - **tags:** TBD
  - **authors:** Lorenzo Capelli, Leandro de Souza Rosa, Gianluca Setti, Mauro Mangia, Riccardo Rovatti
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19472
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40e0a2de9bc90f1d512acba9f8178e3caeb11f59b899b803bcea54b876c14e2e_w640_q70.webp
  - **Simple LLM Summary:** Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications

- **[arXiv251223] Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset**
  - **tags:** TBD
  - **authors:** Nikita Volzhin, Soowhan Yoon
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19494
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d0c1cdb2dc4e7f555b6151e7dd057ebb961c3137ad65952a50508e3d588bb4a_w640_q70.webp
  - **Simple LLM Summary:** Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset

- **[arXiv251223] Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks**
  - **tags:** TBD
  - **authors:** Hafsa Benaddi, Mohammed Jouhari, Nouha Laamech, Anas Motii, Khalil Ibrahimi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19488
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a32520398008c395d68627f40b4f8898b363f28acb9b801f7e86c375bececdb_w640_q70.webp
  - **Simple LLM Summary:** Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks

- **[arXiv251223] Toward Scalable and Valid Conditional Independence Testing with Spectral Representations**
  - **tags:** TBD
  - **authors:** Alek Frohlich, Vladimir Kostic, Karim Lounici, Daniel Perazzo, Massimiliano Pontil
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19510
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67736573115d067bb1bcf8e6be20a765add92537af45beed64b6a3f2b096ef5c_w640_q70.webp
  - **Simple LLM Summary:** Toward Scalable and Valid Conditional Independence Testing with Spectral Representations

- **[arXiv251223] Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico**
  - **tags:** TBD
  - **authors:** Martí Medina-Hern ández, Janos Kertész, Mihály Fazekas
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19491
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4c4c038ee2a9e249f2f33259b933475235132c1cbefe5a1ac37223ea8fc2367_w640_q70.webp
  - **Simple LLM Summary:** Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico

- **[arXiv251223] DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast**
  - **tags:** TBD
  - **authors:** Hongliang Li, Nong Zhang, Zhewen Xu, Xiang Li, Changzheng Liu, Chongbo Zhao, Jie Wu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19506
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ab91ff5a253b798aeb4b7fa1cee40fb5a1319f0697a2094d06ad95557270ff9_w640_q70.webp
  - **Simple LLM Summary:** DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast

- **[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Xueming Yan, Bo Yin, Yaochu Jin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19516
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp
  - **Simple LLM Summary:** LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning

- **[arXiv251223] Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions**
  - **tags:** TBD
  - **authors:** Diego Hitzges, Guillaume Sagnol
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19527
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9be7dc2a227e30f57b54fae513beec5b2448e4823ad54422005ffd50328df87_w640_q70.webp
  - **Simple LLM Summary:** Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions

- **[arXiv251223] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement**
  - **tags:** TBD
  - **authors:** Hongsheng Xing, Qiuxin Si
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19530
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a398a1c7e656e700ae7d2e4b360c6e2a84d8a4f125de94406eb2fcbd1174cabf_w640_q70.webp
  - **Simple LLM Summary:** Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement

- **[arXiv251223] Initialization of a Polyharmonic Cascade, Launch and Testing**
  - **tags:** TBD
  - **authors:** Yuriy N. Bakhvalov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19524
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8a226c40c413901a8fba890ca183141879ee006dd07d881c31b02c4c795c952_w640_q70.webp
  - **Simple LLM Summary:** Initialization of a Polyharmonic Cascade, Launch and Testing

- **[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal**
  - **tags:** TBD
  - **authors:** Yongxin Wang, Zhicheng Yang, Meng Cao, Mingfei Han, Haokun Lin, Yingying Zhu, Xiaojun Chang, Xiaodan Liang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19554
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp
  - **Simple LLM Summary:** CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal

- **[arXiv251223] DFORD: Directional Feedback based Online Ordinal Regression Learning**
  - **tags:** TBD
  - **authors:** Naresh Manwani, M Elamparithy, Tanish Taneja
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19550
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbc721b57e400a0a633cbed7c7bd41446b344d459c761659f224385919cc0b0_w640_q70.webp
  - **Simple LLM Summary:** DFORD: Directional Feedback based Online Ordinal Regression Learning

- **[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller**
  - **tags:** TBD
  - **authors:** Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19576
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp
  - **Simple LLM Summary:** LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller

- **[arXiv251223] KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning**
  - **tags:** TBD
  - **authors:** Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp
  - **Simple LLM Summary:** KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning

- **[arXiv251223] The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference**
  - **tags:** TBD
  - **authors:** Rajyasri Roy, Dibyajyoti Nayak, Somdatta Goswami
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19643
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de5e312b39c5b4c9d5e5ba414dbf1be28f3eefa665a2062af2d2753365d7ba18_w640_q70.webp
  - **Simple LLM Summary:** The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference

- **[arXiv251223] Deep Legendre Transform**
  - **tags:** TBD
  - **authors:** Aleksey Minabutdinov, Patrick Cheridito
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19649
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6556ccc5250396e2d156a9c4ace9606ae9e710cca90dde9ad9a791f5bf821cd7_w640_q70.webp
  - **Simple LLM Summary:** Deep Legendre Transform

- **[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies**
  - **tags:** TBD
  - **authors:** Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19673
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp
  - **Simple LLM Summary:** Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies

- **[arXiv251223] Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing**
  - **tags:** TBD
  - **authors:** Christopher Regan, Ying Xie
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17923
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0972c0f33f9a898aad22d9d466edb2d113b1f06ae271519a0310fbe4deb8326_w640_q70.webp
  - **Simple LLM Summary:** Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing

- **[arXiv251223] A curated UK rain radar data set for training and benchmarking nowcasting models**
  - **tags:** TBD
  - **authors:** Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17924
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp
  - **Simple LLM Summary:** A curated UK rain radar data set for training and benchmarking nowcasting models

- **[arXiv251223] Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach**
  - **tags:** TBD
  - **authors:** Dongdong Yang, Bin Li, Jiguang He, Yicheng Yan, Xiaoyu Zhang, Chongwen Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17928
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52bfb16cf6da30f529d6affbb7e78493e8d297701e513afd78f81efa7c4804bd_w640_q70.webp
  - **Simple LLM Summary:** Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach

- **[arXiv251223] Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning**
  - **tags:** TBD
  - **authors:** Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19687
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp
  - **Simple LLM Summary:** Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning

- **[arXiv251223] Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making**
  - **tags:** TBD
  - **authors:** Safiye Turgay, Serkan Erdoğan, Željko Stević, Orhan Emre Elma, Tevfik Eren, Zhiyuan Wang, Mahmut Baydaş
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17936
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f5144fba33f969e0e0ef3789e33db059938702b3b5d79afd93befb1670b02b_w640_q70.webp
  - **Simple LLM Summary:** Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making

- **[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods**
  - **tags:** TBD
  - **authors:** Sheryl Chen, Tony Wang, Kyle Feinstein
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17929
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp
  - **Simple LLM Summary:** Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods

- **[arXiv251223] Disentangled representations via score-based variational autoencoders**
  - **tags:** TBD
  - **authors:** Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17127
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp
  - **Simple LLM Summary:** Disentangled representations via score-based variational autoencoders

- **[arXiv251223] MEGState: Phoneme Decoding from Magnetoencephalography Signals**
  - **tags:** TBD
  - **authors:** Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17978
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82b8623a222c1e415238f07bfd00f186b0fd97345fe3a78288d0e4e33530c69c_w640_q70.webp
  - **Simple LLM Summary:** MEGState: Phoneme Decoding from Magnetoencephalography Signals

- **[arXiv251223] Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler**
  - **tags:** TBD
  - **authors:** Holden Lee, Matheau Santana-Gijzen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.17977
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb197c6366f5076ec8d83592cce39cf5b5b65c76bc5090ea04ea14c30cdf6873_w640_q70.webp
  - **Simple LLM Summary:** Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler

- **[arXiv251223] Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models**
  - **tags:** TBD
  - **authors:** Fabian Kreppel, Reza Salkhordeh, Ferdinand Schmidt-Kaler, André Brinkmann
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18021
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0bdf00e2aa0c745b05913e16f268fd8bf9934eeb7b22056c328f85c5e744b2d_w640_q70.webp
  - **Simple LLM Summary:** Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models

- **[arXiv251223] Long-range electrostatics for machine learning interatomic potentials is easier than we thought**
  - **tags:** TBD
  - **authors:** Dongjin Kim, Bingqing Cheng
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18029
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7547e964d0dd16907fbafcc9b548ddcc192b748a763449bd7c5098fb4f79beb3_w640_q70.webp
  - **Simple LLM Summary:** Long-range electrostatics for machine learning interatomic potentials is easier than we thought

- **[arXiv251223] Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty**
  - **tags:** TBD
  - **authors:** Ashley Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18083
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9e0e031bdf1e2542517c9521c36614dd83cab06c75d8cba77aef133d3bbf5e4_w640_q70.webp
  - **Simple LLM Summary:** Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty

- **[arXiv251223] Estimating Solvation Free Energies with Boltzmann Generators**
  - **tags:** TBD
  - **authors:** Maximilian Schebek, Nikolas M. Froböse, Bettina G. Keller, Jutta Rogal
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18147
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be00925063782c2d33d3147328ee4619cc5235ab4e60dcbb70ee97bf3dcb2f8f_w640_q70.webp
  - **Simple LLM Summary:** Estimating Solvation Free Energies with Boltzmann Generators

- **[arXiv251223] Exploring polymer classification with a hybrid single-photon quantum approach**
  - **tags:** TBD
  - **authors:** Alexandrina Stoyanova, Bogdan Penkovsky
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18125
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6aacb9f15cccaa0ab48dbf4d0906a45d2c5bb03514111402f33ab0cb75853841_w640_q70.webp
  - **Simple LLM Summary:** Exploring polymer classification with a hybrid single-photon quantum approach

- **[arXiv251223] CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction**
  - **tags:** TBD
  - **authors:** Zhendong Cao, Shigang Ou, Lei Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18251
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8cf6c41ccf7d3886a9f73f6faa6ec65ab26be6a067ce7c99f7b8336c5475e658_w640_q70.webp
  - **Simple LLM Summary:** CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction

- **[arXiv251223] PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior**
  - **tags:** TBD
  - **authors:** Wenhan Guo, Jinglun Yu, Yaning Wang, Jin U. Kang, Yu Sun
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18367
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98dc2d7a9538276a946338546d48fae623eb87f67d0a306a36ac1560cd23d715_w640_q70.webp
  - **Simple LLM Summary:** PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior

- **[arXiv251223] TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition**
  - **tags:** TBD
  - **authors:** Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18263
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp
  - **Simple LLM Summary:** TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition

- **[arXiv251223] Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence**
  - **tags:** TBD
  - **authors:** Frank Hu, Jonathan M. Tubb, Dimitris Argyropoulos, Sergey Golotvin, Mikhail Elyashberg, Grant M. Rotskoff, Matthew W. Kanan, Thomas E. Markland
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18531
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08aee7a9e45a9f29af6d103a3e940fa95a91c99015126260b7c777d51567ce83_w640_q70.webp
  - **Simple LLM Summary:** Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence

- **[arXiv251223] Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning**
  - **tags:** TBD
  - **authors:** Feng Yu, MD Saifur Rahman Mazumder, Ying Su, Oscar Contreras Velasco
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18720
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0032f6c7f335886e0c863fb5416f2765511b0ba1ca06f1073d2eab5626b6c3f_w640_q70.webp
  - **Simple LLM Summary:** Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning

- **[arXiv251223] RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization**
  - **tags:** TBD
  - **authors:** George C. Alexandropoulos, Kostantinos D. Katsanos, George Stamatelis, Ioannis Gavras
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18788
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02ac819b22b566bf908c19d15c96b16e6d173f4f375e5123bcf427c7b9405730_w640_q70.webp
  - **Simple LLM Summary:** RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization

- **[arXiv251223] On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction**
  - **tags:** TBD
  - **authors:** Shuntuo Xu, Zhou Yu, Jian Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18971
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7664dca196bdcb61dc392090fb8c53996afeb07a245afa8d3b648e0385bd7f5d_w640_q70.webp
  - **Simple LLM Summary:** On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction

- **[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics**
  - **tags:** TBD
  - **authors:** Yucheng Yang, Chiyuan Wang, Andreas Schaab, Benjamin Moll
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18892
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp
  - **Simple LLM Summary:** Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics

- **[arXiv251223] Finite-sample guarantees for data-driven forward-backward operator methods**
  - **tags:** TBD
  - **authors:** Filippo Fabiani, Barbara Franci
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19172
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8222a9719b756b6dad7e034a173dde65aa924d9d7b0789fd4e5882f3dfe97b3_w640_q70.webp
  - **Simple LLM Summary:** Finite-sample guarantees for data-driven forward-backward operator methods

- **[arXiv251223] Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations**
  - **tags:** TBD
  - **authors:** Xiaolong Wu, Qifeng Liao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19196
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3780b800013ebe275e2c8ef8b8f7ba281c86d44d100ae51b95eeb7d19999391_w640_q70.webp
  - **Simple LLM Summary:** Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations

- **[arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions**
  - **tags:** TBD
  - **authors:** Haishan Ye
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19104
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e1de9f54709e8dfd7512b0bf65bde1fbf69bbf9510338a82e721bb9ab43307b_w640_q70.webp
  - **Simple LLM Summary:** Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions

- **[arXiv251223] Cluster-Based Generalized Additive Models Informed by Random Fourier Features**
  - **tags:** TBD
  - **authors:** Xin Huang, Jia Li, Jun Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19373
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f54a5120ec11e3d3fee124d496a72d9ed6944487b104802da2d971b4720063e2_w640_q70.webp
  - **Simple LLM Summary:** Cluster-Based Generalized Additive Models Informed by Random Fourier Features

- **[arXiv251223] A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows**
  - **tags:** TBD
  - **authors:** Eneko Lazpita, Andrés Bell-Navas, Jesús Garicano-Mena, Petros Koumoutsakos, Soledad Le Clainche
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19376
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed5b488d452940ef72b893f7e49bf3407f54d6a1ce8a3642724d7bf4ddcad601_w640_q70.webp
  - **Simple LLM Summary:** A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows

- **[arXiv251223] Real-Time Streamable Generative Speech Restoration with Flow Matching**
  - **tags:** TBD
  - **authors:** Simon Welker, Bunlong Lay, Maris Hillemann, Tal Peer, Timo Gerkmann
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19442
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50f384e81e75842e398563da395c9a42cafd7dfeb79c90bc7c4489b17d033102_w640_q70.webp
  - **Simple LLM Summary:** Real-Time Streamable Generative Speech Restoration with Flow Matching

- **[arXiv251223] Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence**
  - **tags:** TBD
  - **authors:** Adrian A. Moazzam, Anindya Ghoshroy, Breeanne Heusdens, Durdu O. Guney, Roohollah Askari
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19540
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aefac8efeaa58dab809884787824024dfad8ec6cfb7ca6a8de7663af53c553bc_w640_q70.webp
  - **Simple LLM Summary:** Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence

## 2025-12-24

- **[arXiv251224] QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19696
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp
  - **Simple LLM Summary:** QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning

- **[arXiv251224] PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility**
  - **tags:** TBD
  - **authors:** Md Nahid Hasan Shuvo, Moinul Hossain
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19711
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp
  - **Simple LLM Summary:** PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility

- **[arXiv251224] Large Language Models for EDA Cloud Job Resource and Lifetime Prediction**
  - **tags:** TBD
  - **authors:** Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19701
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp
  - **Simple LLM Summary:** Large Language Models for EDA Cloud Job Resource and Lifetime Prediction

- **[arXiv251224] Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data**
  - **tags:** TBD
  - **authors:** Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19716
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp
  - **Simple LLM Summary:** Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data

- **[arXiv251224] Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance**
  - **tags:** TBD
  - **authors:** James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19707
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp
  - **Simple LLM Summary:** Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance

- **[arXiv251224] Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches**
  - **tags:** TBD
  - **authors:** Taoran Sheng, Manfred Huber
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19713
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp
  - **Simple LLM Summary:** Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches

- **[arXiv251224] Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism**
  - **tags:** TBD
  - **authors:** Alessandro Casadei, Clemens Grupp, Sreyoshi Bhaduri, Lu Guo, Wilson Fung, Rohit Malshe, Raj Ratan, Ankush Pole, Arkajit Rakshit
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19722
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5fc3669c6e3b3729a8ceeab7556b8b190a71237342736f2d91f0e9e99de3dc0_w640_q70.webp
  - **Simple LLM Summary:** Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism

- **[arXiv251224] Per-Axis Weight Deltas for Frequent Model Updates**
  - **tags:** TBD
  - **authors:** Stefan Kuyumdzhiev, Radostin Cholakov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19720
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fced4b263f86d7aa4d12f96b832ddc3447334c926ee67499537f6d38e8e740d_w640_q70.webp
  - **Simple LLM Summary:** Per-Axis Weight Deltas for Frequent Model Updates

- **[arXiv251224] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference**
  - **tags:** TBD
  - **authors:** Zhan Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19717
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp
  - **Simple LLM Summary:** Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference

- **[arXiv251224] Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals**
  - **tags:** TBD
  - **authors:** Vineet Yadav
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19721
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/476a71567ad37a78f3007d1e492eb010eed6a7f8bed8a187d46ae8e80465f4f5_w640_q70.webp
  - **Simple LLM Summary:** Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals

- **[arXiv251224] Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries**
  - **tags:** TBD
  - **authors:** Zihao Lv, Siqi Ai, Yanbin Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19719
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp
  - **Simple LLM Summary:** Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries

- **[arXiv251224] Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data**
  - **tags:** TBD
  - **authors:** Vasileios C. Pezoulas, Nikolaos S. Tachos, Eleni Georga, Kostas Marias, Manolis Tsiknakis, Dimitrios I. Fotiadis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19718
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp
  - **Simple LLM Summary:** Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data

- **[arXiv251224] Tiny, On-Device Decision Makers with the MiniConv Library**
  - **tags:** TBD
  - **authors:** Carlos Purves
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19726
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp
  - **Simple LLM Summary:** Tiny, On-Device Decision Makers with the MiniConv Library

- **[arXiv251224] Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking**
  - **tags:** TBD
  - **authors:** Srishti Gupta, Riccardo Balia, Daniele Angioni, Fabio Brau, Maura Pintor, Ambra Demontis, Alessandro Sebastian, Salvatore Mario Carta, Fabio Roli, Battista Biggio
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19725
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a6b157cb48d9bcd740b085c12288bed5c0e95c01a7146a5e7c2e50b7d77787d_w640_q70.webp
  - **Simple LLM Summary:** Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking

- **[arXiv251224] Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels**
  - **tags:** TBD
  - **authors:** Peng-Hung Tsai, Daniel Berleant
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19727
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2c9404cbdd59c5c56c246473b4a6996ca55f6051700faa6c654323d6990a290_w640_q70.webp
  - **Simple LLM Summary:** Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels

- **[arXiv251224] End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment**
  - **tags:** TBD
  - **authors:** Firas Bayram, Bestoun S. Ahmed, Erik Hallin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19723
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e87a73b8e332e90e3cd2839f1faa8882cdeef6cb35e274b072b141a0cabb8572_w640_q70.webp
  - **Simple LLM Summary:** End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment

- **[arXiv251224] Hard Negative Sample-Augmented DPO Post-Training for Small Language Models**
  - **tags:** TBD
  - **authors:** Haocheng Lu, Minjun Zhu, Henry Yu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19728
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp
  - **Simple LLM Summary:** Hard Negative Sample-Augmented DPO Post-Training for Small Language Models

- **[arXiv251224] ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures**
  - **tags:** TBD
  - **authors:** Zhonghao Yang, Cheng Luo, Daojing He, Yiming Li, Yu Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19730
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66bbe64f6f3cac5df3dfd69a018806ee7d1973ac071d8af57c13752826c622fe_w640_q70.webp
  - **Simple LLM Summary:** ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures

- **[arXiv251224] High-Performance Self-Supervised Learning by Joint Training of Flow Matching**
  - **tags:** TBD
  - **authors:** Kosuke Ukita, Tsuyoshi Okita
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19729
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp
  - **Simple LLM Summary:** High-Performance Self-Supervised Learning by Joint Training of Flow Matching

- **[arXiv251224] Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis**
  - **tags:** TBD
  - **authors:** Gaurav Kumar Sharma
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19732
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53da428992d5e5cae28642416139ec0d148864a5c060b683ac9e143fe97079ee_w640_q70.webp
  - **Simple LLM Summary:** Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis

- **[arXiv251224] Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems**
  - **tags:** TBD
  - **authors:** Xiangzhong Luo, Weichen Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19731
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp
  - **Simple LLM Summary:** Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems

- **[arXiv251224] OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting**
  - **tags:** TBD
  - **authors:** Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19738
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp
  - **Simple LLM Summary:** OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting

- **[arXiv251224] The Deleuzian Representation Hypothesis**
  - **tags:** TBD
  - **authors:** Clément Cornet, Romaric Besançon, Hervé Le Borgne
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19734
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/877f862cfd6b85ad4699167cc9b9bc17de797ef85a7ede6911637da4967d6121_w640_q70.webp
  - **Simple LLM Summary:** The Deleuzian Representation Hypothesis

- **[arXiv251224] Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach**
  - **tags:** TBD
  - **authors:** Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19737
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp
  - **Simple LLM Summary:** Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach

- **[arXiv251224] CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology**
  - **tags:** TBD
  - **authors:** Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19736
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp
  - **Simple LLM Summary:** CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology

- **[arXiv251224] Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction**
  - **tags:** TBD
  - **authors:** Gangxiong Zhang, Yongchao Long
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19735
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffac075f9a08ac05f63c8e47026ade8aa961ca7bcc7071d314dbbcf27a110f66_w640_q70.webp
  - **Simple LLM Summary:** Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction

- **[arXiv251224] EdgeFlex-Transformer: Transformer Inference for Edge Devices**
  - **tags:** TBD
  - **authors:** Shoaib Mohammad, Guanqun Song, Ting Zhu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19741
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp
  - **Simple LLM Summary:** EdgeFlex-Transformer: Transformer Inference for Edge Devices

- **[arXiv251224] Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics**
  - **tags:** TBD
  - **authors:** Kousar Raza, Faizan Ali
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19740
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f547c21d7d60b5ce4f92522c83aa7c297fc051a77b6eddf69ef405348fda9d8_w640_q70.webp
  - **Simple LLM Summary:** Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics

- **[arXiv251224] On-device Large Multi-modal Agent for Human Activity Recognition**
  - **tags:** TBD
  - **authors:** Md Shakhrul Iman Siam, Ishtiaque Ahmed Showmik, Guanqun Song, Ting Zhu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19742
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3cc1dde2877d131c87748e2a0e8975b217b5776f2152e724bf632e2fa6532f_w640_q70.webp
  - **Simple LLM Summary:** On-device Large Multi-modal Agent for Human Activity Recognition

- **[arXiv251224] From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning**
  - **tags:** TBD
  - **authors:** Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19743
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp
  - **Simple LLM Summary:** From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning

- **[arXiv251224] OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting**
  - **tags:** TBD
  - **authors:** Soumen Garai, Suman Samui
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19739
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp
  - **Simple LLM Summary:** OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting

- **[arXiv251224] DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation**
  - **tags:** TBD
  - **authors:** Gustavo Coelho Haase, Paulo Henrique Dourado da Silva
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19744
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bccd562ab93d21bfa941fe354c60f5ea1f28b2e74751c8d34532405481aeeca_w640_q70.webp
  - **Simple LLM Summary:** DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation

- **[arXiv251224] How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts**
  - **tags:** TBD
  - **authors:** Sumin Park, Noseong Park
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19765
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp
  - **Simple LLM Summary:** How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts

- **[arXiv251224] Learning to Design City-scale Transit Routes**
  - **tags:** TBD
  - **authors:** Bibek Poudel, Weizi Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19767
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp
  - **Simple LLM Summary:** Learning to Design City-scale Transit Routes

- **[arXiv251224] A K-Means, Ward and DBSCAN repeatability study**
  - **tags:** TBD
  - **authors:** Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19772
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp
  - **Simple LLM Summary:** A K-Means, Ward and DBSCAN repeatability study

- **[arXiv251224] Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy**
  - **tags:** TBD
  - **authors:** Deepit Sapru
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19805
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb04e43886d4319737a1d917f74a55c539bf9fd5290a700f1e160de279d18cef_w640_q70.webp
  - **Simple LLM Summary:** Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy

- **[arXiv251224] Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling**
  - **tags:** TBD
  - **authors:** Shane X. Coffing, John Tipton, Arvind T. Mohan, Darren Engwirda
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19804
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e689d3fa5a81d3d8eba58c25d7cb4a0158b1806b3118990d514118edc1fa566_w640_q70.webp
  - **Simple LLM Summary:** Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling

- **[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication**
  - **tags:** TBD
  - **authors:** Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19849
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp
  - **Simple LLM Summary:** UCCL-EP: Portable Expert-Parallel Communication

- **[arXiv251224] Fine-Tuned In-Context Learners for Efficient Adaptation**
  - **tags:** TBD
  - **authors:** Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19879
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp
  - **Simple LLM Summary:** Fine-Tuned In-Context Learners for Efficient Adaptation

- **[arXiv251224] Detecting cyberbullying in Spanish texts through deep learning techniques**
  - **tags:** TBD
  - **authors:** Paúl Cumba-Armijos, Diego Riofrío-Luzcando, Verónica Rodríguez-Arboleda, Joe Carrión-Jumbo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19899
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp
  - **Simple LLM Summary:** Detecting cyberbullying in Spanish texts through deep learning techniques

- **[arXiv251224] Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19920
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp
  - **Simple LLM Summary:** Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning

- **[arXiv251224] Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling**
  - **tags:** TBD
  - **authors:** Indranil Halder, Cengiz Pehlevan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19905
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp
  - **Simple LLM Summary:** Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling

- **[arXiv251224] Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra**
  - **tags:** TBD
  - **authors:** Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19909
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp
  - **Simple LLM Summary:** Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra

- **[arXiv251224] The Seismic Wavefield Common Task Framework**
  - **tags:** TBD
  - **authors:** Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19927
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59879c33166ae9f8c44c6fb1768cff1db7963674dc0f8347a443a44df80bf19_w640_q70.webp
  - **Simple LLM Summary:** The Seismic Wavefield Common Task Framework

- **[arXiv251224] Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress**
  - **tags:** TBD
  - **authors:** Samruddhi Baviskar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19935
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp
  - **Simple LLM Summary:** Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress

- **[arXiv251224] Vehicle-centric Perception via Multimodal Structured Pre-training**
  - **tags:** TBD
  - **authors:** Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19934
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp
  - **Simple LLM Summary:** Vehicle-centric Perception via Multimodal Structured Pre-training

- **[arXiv251224] Block-Recurrent Dynamics in Vision Transformers**
  - **tags:** TBD
  - **authors:** Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19941
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp
  - **Simple LLM Summary:** Block-Recurrent Dynamics in Vision Transformers

- **[arXiv251224] Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis**
  - **tags:** TBD
  - **authors:** Surya Jayakumar, Kieran Sullivan, John McLaughlin, Christine O'Meara, Indrakshi Dey
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19970
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04d93c1a47d24166acae7ed5012d760c9a02de04651f40a319ded305b7aaad13_w640_q70.webp
  - **Simple LLM Summary:** Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis

- **[arXiv251224] Bloom Filter Encoding for Machine Learning**
  - **tags:** TBD
  - **authors:** John Cartmell, Mihaela Cardei, Ionut Cardei
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19991
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e677d5a3d466425f205097a14f528d2211f9cb2642b715b0647694a1db34a243_w640_q70.webp
  - **Simple LLM Summary:** Bloom Filter Encoding for Machine Learning

- **[arXiv251224] A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection**
  - **tags:** TBD
  - **authors:** Tamim Ahasan Rijon, Yeasin Arafath
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19989
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp
  - **Simple LLM Summary:** A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection

- **[arXiv251224] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models**
  - **tags:** TBD
  - **authors:** Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19995
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp
  - **Simple LLM Summary:** Schoenfeld's Anatomy of Mathematical Reasoning by Language Models

- **[arXiv251224] Control Variate Score Matching for Diffusion Models**
  - **tags:** TBD
  - **authors:** Khaled Kahouli, Romuald Elie, Klaus-Robert Müller, Quentin Berthet, Oliver T. Unke, Arnaud Doucet
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20003
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99c1fcd9cfbbb5a241296c106e6f4311b324493691659f27747af21f56a722fe_w640_q70.webp
  - **Simple LLM Summary:** Control Variate Score Matching for Diffusion Models

- **[arXiv251224] Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance**
  - **tags:** TBD
  - **authors:** Sukumar Kishanthan, Asela Hevapathige
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20006
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/767f4e4dcb2738c000b0ea66f1109ff56b696feb519040f06df4fdd1a29c343a_w640_q70.webp
  - **Simple LLM Summary:** Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance

- **[arXiv251224] IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense**
  - **tags:** TBD
  - **authors:** Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20004
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp
  - **Simple LLM Summary:** IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense

- **[arXiv251224] LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models**
  - **tags:** TBD
  - **authors:** Jiacheng You, Jingcheng Yang, Yuhang Xie, Zhongxuan Wu, Xiucheng Li, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, Xinyang Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20002
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3eef74ab05c46cb25ce1372769f7b8dc5bc12b1c381a6076e807a4bdde96f36_w640_q70.webp
  - **Simple LLM Summary:** LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models

- **[arXiv251224] DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics**
  - **tags:** TBD
  - **authors:** Yuan Gao, Zhenguo Dong, Xuelong Wang, Zhiqiang Wang, Yong Zhang, Shaofan Wang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20028
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp
  - **Simple LLM Summary:** DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics

- **[arXiv251224] Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems**
  - **tags:** TBD
  - **authors:** H. Li, J. Sun, Z. Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20058
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3aee744984f754345f38a1211552354aa6d648480458cd66df06732c2b2624_w640_q70.webp
  - **Simple LLM Summary:** Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems

- **[arXiv251224] An Optimal Policy for Learning Controllable Dynamics by Exploration**
  - **tags:** TBD
  - **authors:** Peter N. Loxley
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20053
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp
  - **Simple LLM Summary:** An Optimal Policy for Learning Controllable Dynamics by Exploration

- **[arXiv251224] DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion**
  - **tags:** TBD
  - **authors:** Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20059
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp
  - **Simple LLM Summary:** DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion

- **[arXiv251224] PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models**
  - **tags:** TBD
  - **authors:** Mingue Park, Jisung Hwang, Seungwoo Yoo, Kyeongmin Yeo, Minhyuk Sung
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20063
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp
  - **Simple LLM Summary:** PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models

- **[arXiv251224] Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection**
  - **tags:** TBD
  - **authors:** Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20086
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp
  - **Simple LLM Summary:** Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection

- **[arXiv251224] QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption**
  - **tags:** TBD
  - **authors:** Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20084
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp
  - **Simple LLM Summary:** QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption

- **[arXiv251224] Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning**
  - **tags:** TBD
  - **authors:** Zuo Wang, Ye Yuan
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20094
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8ebe19b950699158b26962ef207ee20be9d3fd1a9e78b063e5d97cd6c18f0a_w640_q70.webp
  - **Simple LLM Summary:** Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning

- **[arXiv251224] Information-directed sampling for bandits: a primer**
  - **tags:** TBD
  - **authors:** Annika Hirling, Giorgio Nicoletti, Antonio Celani
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20096
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp
  - **Simple LLM Summary:** Information-directed sampling for bandits: a primer

- **[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language**
  - **tags:** TBD
  - **authors:** Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20111
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp
  - **Simple LLM Summary:** ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language

- **[arXiv251224] Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering**
  - **tags:** TBD
  - **authors:** Yuanhao Chen, Qi Liu, Pengbin Chen, Zhongjian Qiao, Yanjie Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20115
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp
  - **Simple LLM Summary:** Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering

- **[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models**
  - **tags:** TBD
  - **authors:** Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20145
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp
  - **Simple LLM Summary:** Retrieval-augmented Prompt Learning for Pre-trained Foundation Models

- **[arXiv251224] Learning to Reason in LLMs by Expectation Maximization**
  - **tags:** TBD
  - **authors:** Junghyun Lee, Branislav Kveton, Sunav Choudhary, Subhojyoti Mukherjee, Anup Rao, Ryan A. Rossi, Alexa Siu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20169
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp
  - **Simple LLM Summary:** Learning to Reason in LLMs by Expectation Maximization

- **[arXiv251224] Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography**
  - **tags:** TBD
  - **authors:** Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20168
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp
  - **Simple LLM Summary:** Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography

- **[arXiv251224] NeuralCrop: Combining physics and machine learning for improved crop yield predictions**
  - **tags:** TBD
  - **authors:** Yunan Lin, Sebastian Bathiany, Maha Badri, Maximilian Gelbrecht, Philipp Hess, Brian Groenke, Jens Heinke, Christoph Müller, Niklas Boers
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20177
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11ac7dee2967ebac7497c5b7c4ac412a47aa4080e36cf0d94605e0611bcf487_w640_q70.webp
  - **Simple LLM Summary:** NeuralCrop: Combining physics and machine learning for improved crop yield predictions

- **[arXiv251224] Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning**
  - **tags:** TBD
  - **authors:** Kausthubh Manda, Raghuram Bharadwaj Diddigi
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20220
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp
  - **Simple LLM Summary:** Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning

- **[arXiv251224] Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud**
  - **tags:** TBD
  - **authors:** Jixiao Yang, Jinyu Chen, Zixiao Huang, Chengda Xu, Chi Zhang, Sijia Li
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20218
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfa99879bd1a1c5083fc9e9363cb2903b42568a0726a124b591c9da1ed744c4f_w640_q70.webp
  - **Simple LLM Summary:** Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud

- **[arXiv251224] Adaptive Multi-task Learning for Probabilistic Load Forecasting**
  - **tags:** TBD
  - **authors:** Onintze Zaballa, Verónica Álvarez, Santiago Mazuelas
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20232
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f105781b741c5e961d80700aae99a9a5f5491196f3957723a0ad4a9d0ceee4f_w640_q70.webp
  - **Simple LLM Summary:** Adaptive Multi-task Learning for Probabilistic Load Forecasting

- **[arXiv251224] How I Met Your Bias: Investigating Bias Amplification in Diffusion Models**
  - **tags:** TBD
  - **authors:** Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20233
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp
  - **Simple LLM Summary:** How I Met Your Bias: Investigating Bias Amplification in Diffusion Models

- **[arXiv251224] Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion**
  - **tags:** TBD
  - **authors:** Xuanyu Hu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20249
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp
  - **Simple LLM Summary:** Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion

- **[arXiv251224] HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training**
  - **tags:** TBD
  - **authors:** Yuanjian Xu, Yuan Shuai, Jianing Hao, Guang Zhang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20272
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336f6868f026371e14a0b1bb308e5d34193071d2aae6293400dd9ee01e404c90_w640_q70.webp
  - **Simple LLM Summary:** HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training

- **[arXiv251224] Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity**
  - **tags:** TBD
  - **authors:** Yuxing Gan, Ziyu Lei
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20291
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b419d6d6ce75f3399c7716ea75c97138302ae8a2a1e0c9b1547fa29a97bde7_w640_q70.webp
  - **Simple LLM Summary:** Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity

- **[arXiv251224] DeepONet-accelerated Bayesian inversion for moving boundary problems**
  - **tags:** TBD
  - **authors:** Marco A. Iglesias, Michael. E. Causon, Mikhail Y. Matveev, Andreas Endruweit, Michael .V. Tretyakov
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20268
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp
  - **Simple LLM Summary:** DeepONet-accelerated Bayesian inversion for moving boundary problems

- **[arXiv251224] Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology**
  - **tags:** TBD
  - **authors:** Yoshihiro Maruyama
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20311
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862ac961bd55b0b8dc1a2ea1c32580f402c570473c4f0ee0635c937b87e810e8_w640_q70.webp
  - **Simple LLM Summary:** Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology

- **[arXiv251224] TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20312
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp
  - **Simple LLM Summary:** TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning

- **[arXiv251224] FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning**
  - **tags:** TBD
  - **authors:** Mrinmay Sen, Subhrajit Nag
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20329
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp
  - **Simple LLM Summary:** FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning

- **[arXiv251224] Toward Explaining Large Language Models in Software Engineering Tasks**
  - **tags:** TBD
  - **authors:** Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20328
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp
  - **Simple LLM Summary:** Toward Explaining Large Language Models in Software Engineering Tasks

- **[arXiv251224] Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability**
  - **tags:** TBD
  - **authors:** Yoshihiro Maruyama
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20325
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d819bb4159585b74aeaf35355e16ef7a49b64ef54f670ff861804098262da1c5_w640_q70.webp
  - **Simple LLM Summary:** Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability

- **[arXiv251224] Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation**
  - **tags:** TBD
  - **authors:** Emilia Majerz, Witold Dzwinel, Jacek Kitowski
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20346
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp
  - **Simple LLM Summary:** Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation

- **[arXiv251224] Physics-guided Neural Network-based Shaft Power Prediction for Vessels**
  - **tags:** TBD
  - **authors:** Dogan Altan, Hamza Haruna Mohammed, Glenn Terje Lines, Dusica Marijan, Arnbjørn Maressa
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20348
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2fe8932d1c82e06128e6863fee4baaa3d988167af43502e32d83bf972eb356_w640_q70.webp
  - **Simple LLM Summary:** Physics-guided Neural Network-based Shaft Power Prediction for Vessels

- **[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning**
  - **tags:** TBD
  - **authors:** Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20363
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp
  - **Simple LLM Summary:** Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning

- **[arXiv251224] Field-Space Attention for Structure-Preserving Earth System Transformers**
  - **tags:** TBD
  - **authors:** Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20350
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp
  - **Simple LLM Summary:** Field-Space Attention for Structure-Preserving Earth System Transformers

- **[arXiv251224] BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples**
  - **tags:** TBD
  - **authors:** Xuan-An Le, Minh-Nam Tran, Son Nguyen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20403
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61964964aad2cf8cac29992fe364ccde3a687f3d0e6b42b8148ce3b0f96dee99_w640_q70.webp
  - **Simple LLM Summary:** BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples

- **[arXiv251224] GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer**
  - **tags:** TBD
  - **authors:** Corey Adams, Rishikesh Ranade, Ram Cherukuri, Sanjay Choudhry
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20399
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6966b4d62e10f76b2120afc54fcd95e1e717c9aef6840968117642fb0eb9df42_w640_q70.webp
  - **Simple LLM Summary:** GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer

- **[arXiv251224] AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition**
  - **tags:** TBD
  - **authors:** Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20407
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp
  - **Simple LLM Summary:** AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition

- **[arXiv251224] Simplifying Multi-Task Architectures Through Task-Specific Normalization**
  - **tags:** TBD
  - **authors:** Mihai Suteu, Ovidiu Serban
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20420
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp
  - **Simple LLM Summary:** Simplifying Multi-Task Architectures Through Task-Specific Normalization

- **[arXiv251224] Machine Learning to Predict Digital Frustration from Clickstream Data**
  - **tags:** TBD
  - **authors:** Jibin Joseph
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20438
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54b66cbccae694b64cb30bccff552dc3cd9c19b7ad622d4169444f7f1f67fdcb_w640_q70.webp
  - **Simple LLM Summary:** Machine Learning to Predict Digital Frustration from Clickstream Data

- **[arXiv251224] Explainable time-series forecasting with sampling-free SHAP for Transformers**
  - **tags:** TBD
  - **authors:** Matthias Hertel, Sebastian Pütz, Ralf Mikut, Veit Hagenmeyer, Benjamin Schäfer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20514
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce0ac2e96e47ae16fe2a238adc470cf7f3968e7b03d41a218d1483b207e0e532_w640_q70.webp
  - **Simple LLM Summary:** Explainable time-series forecasting with sampling-free SHAP for Transformers

- **[arXiv251224] Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow**
  - **tags:** TBD
  - **authors:** Tyler Clark, Christine Evers, Jonathon Hare
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20513
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp
  - **Simple LLM Summary:** Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow

- **[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving**
  - **tags:** TBD
  - **authors:** Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20563
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp
  - **Simple LLM Summary:** LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving

- **[arXiv251224] Improving ML Training Data with Gold-Standard Quality Metrics**
  - **tags:** TBD
  - **authors:** Leslie Barrett, Michael W. Sherman
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20577
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcda9dded34420c31f0b16ebfbeec41a79ad04607b8b4862e82f7a5504ae0ad_w640_q70.webp
  - **Simple LLM Summary:** Improving ML Training Data with Gold-Standard Quality Metrics

- **[arXiv251224] Performative Policy Gradient: Optimality in Performative Reinforcement Learning**
  - **tags:** TBD
  - **authors:** Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20576
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp
  - **Simple LLM Summary:** Performative Policy Gradient: Optimality in Performative Reinforcement Learning

- **[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs**
  - **tags:** TBD
  - **authors:** Rui Pan, Zhuofu Chen, Ravi Netravali
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20573
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp
  - **Simple LLM Summary:** Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs

- **[arXiv251224] Relu and softplus neural nets as zero-sum turn-based games**
  - **tags:** TBD
  - **authors:** Stephane Gaubert, Yiannis Vlassopoulos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20582
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab02d27a8e6adea8ae6003cffa24e9fc6b3b0ee9d8f7ab828d024af5b17df0d7_w640_q70.webp
  - **Simple LLM Summary:** Relu and softplus neural nets as zero-sum turn-based games

- **[arXiv251224] Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures**
  - **tags:** TBD
  - **authors:** Yedi Zhang, Andrew Saxe, Peter E. Latham
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20607
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e73c03581900a982dea4784fdd454e704c3d3b120fb84735e5f76640eb67bc86_w640_q70.webp
  - **Simple LLM Summary:** Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures

- **[arXiv251224] FedPOD: the deployable units of training for federated learning**
  - **tags:** TBD
  - **authors:** Daewoon Kim, Si Young Yie, Jae Sung Lee
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20610
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp
  - **Simple LLM Summary:** FedPOD: the deployable units of training for federated learning

- **[arXiv251224] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning**
  - **tags:** TBD
  - **authors:** Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20605
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp
  - **Simple LLM Summary:** Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

- **[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos**
  - **tags:** TBD
  - **authors:** Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20618
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp
  - **Simple LLM Summary:** LongVideoAgent: Multi-Agent Reasoning with Long Videos

- **[arXiv251224] ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval**
  - **tags:** TBD
  - **authors:** Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19703
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp
  - **Simple LLM Summary:** ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval

- **[arXiv251224] NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra**
  - **tags:** TBD
  - **authors:** Federico Ottomano, Yingzhen Li, Alex M. Ganose
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19733
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1b9a0fd1ed2cc7769e92111592f07cc1a8189724bd03924f8083fce8cf7a5b_w640_q70.webp
  - **Simple LLM Summary:** NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra

- **[arXiv251224] Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise**
  - **tags:** TBD
  - **authors:** Joonsung Kang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19746
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5763bddf883e47ffbd39188c2d91d52b8f64bdca5101b8f91c97d4995c41b17_w640_q70.webp
  - **Simple LLM Summary:** Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise

- **[arXiv251224] Fundamentals of quantum Boltzmann machine learning with visible and hidden units**
  - **tags:** TBD
  - **authors:** Mark M. Wilde
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19819
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/109d3ad52a1d76b25a8f6b53d84a7219f99f5f6c4a1f4d3cfefd214d0a937f8f_w640_q70.webp
  - **Simple LLM Summary:** Fundamentals of quantum Boltzmann machine learning with visible and hidden units

- **[arXiv251224] Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization**
  - **tags:** TBD
  - **authors:** Habibollah Safari, Mona Bavarian
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19715
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db0de246828ee3f9ea683f9e4205d984e45250d6966e0c692c62d8ded1fbecb0_w640_q70.webp
  - **Simple LLM Summary:** Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization

- **[arXiv251224] Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization**
  - **tags:** TBD
  - **authors:** Nikolaos Iliopoulos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19986
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f49f51d4e93dfaa76b5d85049c4b769868d3a8661c7cc58b3f721255548f39_w640_q70.webp
  - **Simple LLM Summary:** Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization

- **[arXiv251224] Efficient Learning of Lattice Gauge Theories with Fermions**
  - **tags:** TBD
  - **authors:** Shreya Shukla, Yukari Yamauchi, Andrey Y. Lokhov, Scott Lawrence, Abhijith Jayakumar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19891
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/006eec86d07c59f9bdf92e4095e9b3bdfcd0d6d88a3ff992c9ae1bcaa42efa8d_w640_q70.webp
  - **Simple LLM Summary:** Efficient Learning of Lattice Gauge Theories with Fermions

- **[arXiv251224] Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function**
  - **tags:** TBD
  - **authors:** Matthew Drnevich, Stephen Jiggins, Kyle Cranmer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19913
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae42de4f7e707842fef18d1ebed0d4af2b3ff8eda273d229ca4aeaac9e765c90_w640_q70.webp
  - **Simple LLM Summary:** Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function

- **[arXiv251224] Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing**
  - **tags:** TBD
  - **authors:** Zhihan Huang, Ziang Niu
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20007
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ac47ca7745f3ae745307a996bf9615b4c854438f5673b23f72f2db0c17c0052_w640_q70.webp
  - **Simple LLM Summary:** Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing

- **[arXiv251224] Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems**
  - **tags:** TBD
  - **authors:** Qiushuo Hou, Sangwoo Park, Matteo Zecchin, Yunlong Cai, Guanding Yu, Osvaldo Simeone, Tommaso Melodia
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20012
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9dd4c42fc11f6f1f9e179a396845d6d9c90f18f4b2a0968536d1bbe730aaa182_w640_q70.webp
  - **Simple LLM Summary:** Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems

- **[arXiv251224] GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics**
  - **tags:** TBD
  - **authors:** Suguru Shiratori, Elham Kiyani, Khemraj Shukla, George Em Karniadakis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19936
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/045354376dff81526b71609d7a09c5a83b94cf3ff400ab3fe060114398f02961_w640_q70.webp
  - **Simple LLM Summary:** GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics

- **[arXiv251224] Optimal Anytime-Valid Tests for Composite Nulls**
  - **tags:** TBD
  - **authors:** Shubhanshu Shekhar
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20039
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6951c37e90b9cc41d18c9ac7d45b558a0fb6668f2f64b9527e2985cfe68c8bce_w640_q70.webp
  - **Simple LLM Summary:** Optimal Anytime-Valid Tests for Composite Nulls

- **[arXiv251224] Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models**
  - **tags:** TBD
  - **authors:** Anna R. Flowers, Christopher T. Franck, Robert B. Gramacy, Justin A. Krometis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20021
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50cf659ea746173ff874243a1ed23ec74a9a603abf5c8d8333c36a4bf579ec63_w640_q70.webp
  - **Simple LLM Summary:** Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models

- **[arXiv251224] Optimality-Informed Neural Networks for Solving Parametric Optimization Problems**
  - **tags:** TBD
  - **authors:** Matthias K. Hoffmann, Amine Othmane, Kathrin Flaßkamp
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20270
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e060736a8b15ff91b7a61c98f5f036e0acad7f1f487053768b143db60163709_w640_q70.webp
  - **Simple LLM Summary:** Optimality-Informed Neural Networks for Solving Parametric Optimization Problems

- **[arXiv251224] KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis**
  - **tags:** TBD
  - **authors:** Mebin Jose, Jisha Francis, Sudheesh Kumar Kattumannil
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20305
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp
  - **Simple LLM Summary:** KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis

- **[arXiv251224] Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability**
  - **tags:** TBD
  - **authors:** Samya Praharaj, Koulik Khamaru
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20368
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/912d43c0875266e42e18f18ddc2bcf11aaf3e8607a61856f82ebf3a2932bf5e1_w640_q70.webp
  - **Simple LLM Summary:** Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability

- **[arXiv251224] The Aligned Economic Index & The State Switching Model**
  - **tags:** TBD
  - **authors:** Ilias Aarab
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20460
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac7ffd2530ee7539902991cd05061cc86de381735b4620a6937d0821e9cb5403_w640_q70.webp
  - **Simple LLM Summary:** The Aligned Economic Index & The State Switching Model

- **[arXiv251224] ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification**
  - **tags:** TBD
  - **authors:** Masahiro Kato
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20523
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca45318d031a0b7e00633a6c1f048fa9feead63ea3447f2257acc9011d26b6c_w640_q70.webp
  - **Simple LLM Summary:** ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification

- **[arXiv251224] Over-the-Air Goal-Oriented Communications**
  - **tags:** TBD
  - **authors:** Kyriakos Stylianopoulos, Paolo Di Lorenzo, George C. Alexandropoulos
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20533
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cf38466c85fbc6d90f4a4b0d12601a4d1aa3d13a53fc9c4c997edc35e26a213_w640_q70.webp
  - **Simple LLM Summary:** Over-the-Air Goal-Oriented Communications

- **[arXiv251224] Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention**
  - **tags:** TBD
  - **authors:** Yingzhen Yang
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20562
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/126073a8d27aee52f27e9654f39d510e34be1f6aaec1e9c80cd8b6d23342281e_w640_q70.webp
  - **Simple LLM Summary:** Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention

## 2025-12-25

- **[arXiv251225] Parameter-Efficient Neural CDEs via Implicit Function Jacobians**
  - **tags:** [ai], [time series analysis], [Neural Controlled Differential Equations, parameter efficiency, implicit function Jacobians, continuous RNN]
  - **authors:** Ilya Kuleshov, Alexey Zaytsev
  - **institution:** Applied AI Institute
  - **link:** https://arxiv.org/pdf/2512.20625
  - **contributions:** 1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a "Continuous RNN," aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a "Continuous RNN" and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Parameter-Efficient Neural CDEs via Implicit Function Jacobians] --> B[核心问题/Problem: NCDEs require many parameters]
    A --> C[主要方法/Method: Parameter-efficient formulation via implicit Jacobians, "Continuous RNN" analogy]
    A --> D[关键结果/Results: Achieves similar performance with far fewer parameters]
    ```

- **[arXiv251225] Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps**
  - **tags:** [ai], [brain-computer interface], [EEG, cybersickness, interpretability maps, convolutional neural networks, event-related potentials]
  - **authors:** Jacqueline Yau, Katherine J. Mimnaugh, Evan G. Center, Timo Ojala, Steven M. LaValle, Wenzhen Yuan, Nancy Amato, Minje Kim, Kara Federmeier
  - **institution:** University of Illinois Urbana-Champaign, University of Oulu
  - **link:** https://arxiv.org/pdf/2512.20620
  - **contributions:** 1. Introduced a method using CNNs and transformers with interpretability maps (integrated gradients and class activation) to identify EEG features for cybersickness classification. 2. Identified a consistent and surprising pattern: amplitudes near the left prefrontal cortex electrode are important for cybersickness classification. 3. Proposed using the identified scalp location as a tagged feature for better real-time cybersickness classification with EEG.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of detecting cybersickness from EEG data by using event-related potentials to isolate sickness-related brain activity from visual stimulus confounds. The authors employ trained convolutional neural networks and transformer models with interpretability maps to identify key EEG features. The main finding is that amplitudes recorded near the left prefrontal cortex are consistently important for classification, suggesting this location as a valuable feature for real-time detection.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps] --> B(核心问题/Problem: Cybersickness detection in VR using EEG is confounded by visual stimulus processing.)
        A --> C(主要方法/Method: Use ERPs, CNNs/Transformers, and interpretability maps (integrated gradients/class activation) to analyze EEG data.)
        A --> D(关键结果/Results: Left prefrontal cortex electrode amplitudes are consistently important for cybersickness classification.)
    ```

- **[arXiv251225] Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning**
  - **tags:** [mlsys], [agent system], [multi-agent language systems, latent strategy evolution, reinforcement feedback, external latent vectors, dual-loop architecture]
  - **authors:** Wenlong Tang
  - **institution:** Independent Researcher (No institutional affiliation inferred from provided content)
  - **link:** https://arxiv.org/pdf/2512.20629
  - **code:** https://github.com/wltang-dev/Latent-Strategy-RL-Agent
  - **contributions:** 1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model's parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning] --> B[核心问题/Problem: Static semantic representations in LLMs cannot evolve with experience.]
    A --> C[主要方法/Method: Dual-loop architecture (Behavior Loop & Language Loop) updates external latent vectors via reinforcement and reflection.]
    A --> D[关键结果/Results: Agents develop stable, disentangled strategies; latent spaces show convergence and emergent adaptation.]
    ```

- **[arXiv251225] Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams**
  - **tags:** [nlp], [sentiment analysis], [temporal drift, zero-training detection, transformer models, social media streams, model instability]
  - **authors:** Aayam Bansal, Ishaan Gangwani
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2512.20631
  - **contributions:**  1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --> B[核心问题/Problem: Transformer模型在动态事件期间的行为不稳定/Transformer model instability during dynamic events]
    A --> C[主要方法/Method: 零训练检测框架与四个新指标/Zero-training detection framework with four novel metrics]
    A --> D[关键结果/Results: 在真实数据上验证，准确率下降达23.4%，检测能力强/Validated on authentic data, 23.4% accuracy drop, strong detection capability]
    ```

- **[arXiv251225] Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models**
  - **tags:** [ai], [clinical prediction], [large language models, semantic feature engineering, multi-modal data integration, goal-oriented knowledge curator, treatment outcome prediction]
  - **authors:** MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong
  - **institution:** Mayo Clinic
  - **link:** https://arxiv.org/pdf/2512.20633
  - **contributions:** 1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Enhancing Lung Cancer Treatment Outcome Prediction<br>增强肺癌治疗结果预测] --> B(核心问题/Problem: Sparse, heterogeneous clinical data<br>稀疏、异构的临床数据)
    A --> C(主要方法/Method: LLMs as Goal-oriented Knowledge Curators<br>LLMs作为目标导向知识策展器)
    A --> D(关键结果/Results: Superior AUROC 0.803, outperforms baselines<br>优异的AUROC 0.803，超越基线)
    ```

- **[arXiv251225] Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning**
  - **tags:** [mlsys], [llm training], [catastrophic forgetting, spurious forgetting, shallow alignment, deep alignment, task alignment depth]
  - **authors:** Weiwei Wang
  - **institution:** Shenzhen Sunline Tech Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.20634
  - **contributions:** 1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to "spurious forgetting" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>虚假遗忘的实时检测与定量分析] --> B[核心问题/Problem: Catastrophic forgetting from shallow task alignment<br/>由浅层任务对齐导致的灾难性遗忘]
    A --> C[主要方法/Method: Quantitative metrics & real-time detection for alignment depth<br/>对齐深度的量化指标与实时检测]
    A --> D[关键结果/Results: High identification accuracy & improved robustness<br/>高识别准确率与提升的鲁棒性]
    ```

- **[arXiv251225] SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [structured pruning, attention head, expert attention, dynamic routing, inference latency]
  - **authors:** Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang
  - **institution:** Minzu University of China, Shanghai Jiao Tong University, Peking University
  - **link:** https://arxiv.org/pdf/2512.20635
  - **contributions:** 1. Proposes SHRP, a novel structured pruning framework that treats attention heads as independent experts and uses a unified Top-1 usage-driven mechanism for dynamic routing and deterministic pruning. 2. Introduces Expert Attention, a modular design with a lightweight shared expander feed-forward network to refine outputs after head selection. 3. Demonstrates significant compression on BERT-base, achieving high parameter/FLOP reduction with minimal accuracy loss, enabling practical deployment for latency-sensitive services.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee1b632660555a4b466586c0f3f7e064778c9087cde3160efa73cb5e0bf7723_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high inference latency and memory consumption of Transformer encoders by proposing SHRP, a structured pruning framework that identifies and removes redundant attention heads. The method uses an Expert Attention module and a unified routing mechanism to compress the model while preserving accuracy. Experiments show SHRP can reduce BERT-base's parameters by 48% with 93% accuracy retained, and achieve a 4.2x throughput gain under extreme compression.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[SHRP: Specialized Head Routing and Pruning] --> B(核心问题/Problem: Transformer编码器推理延迟高、内存消耗大/High inference latency & memory consumption of Transformer encoders)
    A --> C(主要方法/Method: 专家注意力与动态路由/Expert Attention & dynamic routing for structured pruning)
    A --> D(关键结果/Results: 高压缩比与精度保持/High compression ratio & accuracy preservation)
    ```

- **[arXiv251225] Data-Free Pruning of Self-Attention Layers in LLMs**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [attention pruning, data-free pruning, Gate-Norm, inference acceleration, attention suppression hypothesis]
  - **authors:** Dhananjay Saikumar, Blesson Varghese
  - **institution:** University of St Andrews
  - **link:** https://arxiv.org/pdf/2512.20636
  - **contributions:** 1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Data-Free Pruning of Self-Attention Layers in LLMs] --> B[核心问题/Problem: LLM推理成本高，注意力层是瓶颈/High LLM inference cost, attention layers are bottleneck]
    A --> C[主要方法/Method: 提出Gate-Norm，基于权重无数据剪枝/Propose Gate-Norm, weight-only data-free pruning]
    A --> D[关键结果/Results: 推理速度提升1.30倍，精度损失<2%，速度快1000倍/1.30x faster inference, <2% accuracy drop, 1000x faster scoring]
    ```

- **[arXiv251225] Uncovering Competency Gaps in Large Language Models and Their Benchmarks**
  - **tags:** [nlp], [llm evaluation], [sparse autoencoders, benchmark gaps, model gaps, concept activations, competency gaps]
  - **authors:** Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan
  - **institution:** Stanford University, Google DeepMind
  - **link:** https://arxiv.org/pdf/2512.20638
  - **code:** competency-gaps.github.io
  - **contributions:** 1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Uncovering Competency Gaps<br/>揭示能力差距] --> B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>问题：聚合指标掩盖模型/基准差距]
        A --> C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>方法：使用稀疏自编码器进行概念级分解]
        A --> D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>结果：发现非谄媚、安全方面的差距；基准过度代表服从性]
    ```

- **[arXiv251225] Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations**
  - **tags:** [ai], [scientific machine learning], [Neural Ordinary Differential Equations, Universal Differential Equations, forecasting breakdown point, n-body problem, Julia]
  - **authors:** Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat
  - **institution:** Vizuara AI Labs
  - **link:** https://arxiv.org/pdf/2512.20643
  - **contributions:** 1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the "forecasting breakdown point" to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp
  - **Simple LLM Summary:** This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a "forecasting breakdown point" to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Forecasting N-Body Dynamics<br/>N体动力学预测] --> B[核心问题/Problem<br/>传统黑盒模型忽略物理定律<br/>Traditional black-box models ignore physics]
    A --> C[主要方法/Method<br/>使用科学机器学习框架<br/>Use Scientific ML frameworks (NODEs, UDEs)]
    A --> D[关键结果/Results<br/>UDE数据效率更高<br/>UDE is more data-efficient]
    ```

- **[arXiv251225] MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing**
  - **tags:** [mlsys], [others], [mask optimization, optical proximity correction, inverse lithography technique, deep learning, benchmark dataset]
  - **authors:** Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam
  - **institution:** University at Buffalo, IBM Research
  - **link:** https://arxiv.org/pdf/2512.20655
  - **contributions:** 1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp
  - **Simple LLM Summary:** The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];
    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];
    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];
    ```

- **[arXiv251225] Q-RUN: Quantum-Inspired Data Re-uploading Networks**
  - **tags:** [ai], [quantum machine learning], [data re-uploading, Fourier-expressive, quantum-inspired, neural network layer, parameter efficiency]
  - **authors:** Wenbo Qiao, Shuaixian Wang, Peng Zhang, Yan Ming, Jiaming Zhao
  - **institution:** Tianjin University
  - **link:** https://arxiv.org/pdf/2512.20654
  - **contributions:** 1. Proposes Q-RUN, a novel quantum-inspired neural network layer that translates the mathematical paradigm of Data Re-uploading Quantum Circuits (DRQC) into a classical model, retaining their Fourier-expressive power without requiring quantum hardware. 2. Demonstrates that Q-RUN significantly outperforms standard fully connected layers and other state-of-the-art layers, reducing error by 1-3 orders of magnitude on certain tasks while using fewer parameters. 3. Shows that Q-RUN can serve as a versatile, drop-in replacement for fully connected layers, improving performance across a wide range of neural architectures and illustrating how quantum ML principles can enhance classical AI design.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0762ab538e009e0b634d9be449e9107606ff0182095b377b357ae3a8796d291b_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Q-RUN, a quantum-inspired neural network layer based on data re-uploading principles, designed to capture high-frequency functions efficiently without quantum hardware. It demonstrates superior performance and parameter efficiency compared to standard layers across various modeling tasks. The work shows how quantum machine learning concepts can guide the development of more expressive classical AI models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Q-RUN: Quantum-Inspired Data Re-uploading Networks] --> B[核心问题/Problem: DRQC量子模型受限于硬件可扩展性<br>DRQC quantum models are limited by hardware scalability]
    A --> C[主要方法/Method: 提出经典量子启发式数据重上传网络<br>Propose classical quantum-inspired data re-uploading network (Q-RUN)]
    A --> D[关键结果/Results: 性能更优，参数更少，可作为全连接层替代<br>Superior performance, fewer parameters, drop-in replacement for FC layers]
    ```

- **[arXiv251225] Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering**
  - **tags:** [mlsys], [agent system], [dual-state architecture, atomic action pairs, guard functions, neuro-symbolic systems, code generation]
  - **authors:** Matthew Thompson
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.20660
  - **contributions:** 1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Managing the Stochastic<br>管理随机性] --> B[Problem: LLM-based agents prone to stochastic failures<br>问题: 基于LLM的智能体易受随机性故障影响]
    A --> C[Method: Dual-State Architecture, Atomic Action Pairs, Guard Functions<br>方法: 双态架构, 原子动作对, 守卫函数]
    A --> D[Results: Improved success rates, architectural constraints can substitute for scale<br>结果: 成功率提升, 架构约束可替代模型规模]
    ```

- **[arXiv251225] Graph Neural Networks for Source Detection: A Review and Benchmark Study**
  - **tags:** [ai], [graph neural networks], [Graph Neural Networks, Epidemic Source Detection, Rumor Centrality, Benchmark Study, Single-Source Problems]
  - **authors:** Martin Sterchi, Nathan Brack, Lorenz Hilfiker
  - **institution:** University of Applied Sciences and Arts Northwestern Switzerland FHNW, University of Zürich
  - **link:** https://arxiv.org/pdf/2512.20657
  - **contributions:** 1. A comprehensive review of existing GNN-based methods for source detection, clarifying their settings and models. 2. Proposal of a principled GNN architecture specifically tailored for the source detection task. 3. A systematic benchmark study demonstrating that GNNs substantially outperform traditional source detection methods across various network types, and advocating for epidemic source detection as a benchmark task for evaluating GNN architectures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3b943e72216650945e413f1e6dcd8c0978e12972df42eff52dc2deb20fcbe59_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews Graph Neural Network (GNN) approaches for detecting the source of an epidemic in a network, proposes a new GNN architecture for the task, and conducts a benchmark study. The experiments show that GNNs significantly outperform traditional methods like rumor centrality, establishing them as highly effective for source detection and suggesting the task as a standard benchmark for GNN evaluation.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Graph Neural Networks for Source Detection: A Review and Benchmark Study] --> B[核心问题/Problem: 识别网络中流行病传播的源头/Identify the source of an epidemic in a network]
        A --> C[主要方法/Method: 回顾GNN方法并提出新的GNN架构/Review GNN methods and propose a new GNN architecture]
        A --> D[关键结果/Results: GNN显著优于传统方法，提议作为基准任务/GNNs substantially outperform traditional methods, propose as a benchmark task]
    ```

- **[arXiv251225] Dominating vs. Dominated: Generative Collapse in Diffusion Models**
  - **tags:** [cv], [text-to-image generation], [diffusion models, cross-attention, generative collapse, multi-concept generation, attention dynamics]
  - **authors:** Hayeon Jeong, Jong-Seok Lee
  - **institution:** Yonsei University
  - **link:** https://arxiv.org/pdf/2512.20666
  - **contributions:** 1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation & distributed head mechanisms) perspectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the "Dominant-vs-Dominated" (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Dominating vs. Dominated<br/>支配 vs. 被支配] --> B[核心问题/Problem<br/>Multi-concept prompt generation imbalance<br/>多概念提示生成失衡];
        A --> C[主要方法/Method<br/>Introduce DominanceBench & analyze causes<br/>引入DominanceBench并分析原因];
        A --> D[关键结果/Results<br/>Data diversity & attention dynamics cause DvD<br/>数据多样性和注意力动态导致DvD];
    ```

- **[arXiv251225] Forward Only Learning for Orthogonal Neural Networks of any Depth**
  - **tags:** [ai], [neural network training algorithms], [forward-only learning, orthogonal neural networks, backpropagation alternative, FOTON, PEPITA]
  - **authors:** Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen
  - **institution:** Université Paris-Dauphine - PSL, ESPCI PSL
  - **link:** https://arxiv.org/pdf/2512.20668
  - **code:** https://github.com/ (URL mentioned as "this https URL" and "open-sourced on github" in the abstract/first page)
  - **contributions:** 1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs>
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Forward Only Learning for Orthogonal Neural Networks<br>前向传播学习用于正交神经网络] --> B[Problem: Backpropagation is computationally expensive<br>问题: 反向传播计算成本高]
    A --> C[Method: FOTON - Forward-Only Training with modulated forward pass<br>方法: FOTON - 使用调制前向传播的前向训练]
    A --> D[Results: Trains networks of any depth, outperforms PEPITA<br>结果: 可训练任意深度网络，性能优于PEPITA]
    ```

- **[arXiv251225] Improving Cardiac Risk Prediction Using Data Generation Techniques**
  - **tags:** [ai], [generative models], [Conditional Variational Autoencoder, synthetic data generation, cardiac risk prediction, data augmentation, clinical records]
  - **authors:** Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama
  - **institution:** Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela
  - **link:** https://arxiv.org/pdf/2512.20669
  - **contributions:** 1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Improving Cardiac Risk Prediction Using Data Generation Techniques] --> B(核心问题/Problem: 真实医疗数据稀缺、不完整且存在缺失值/Real-world medical data is scarce, incomplete, and has missing values)
    A --> C(主要方法/Method: 基于条件变分自编码器的架构生成合成临床记录/CVAE-based architecture for synthetic clinical record generation)
    A --> D(关键结果/Results: 生成的数据提高了心脏风险预测分类器的准确性/Generated data improves cardiac risk prediction classifier accuracy)
    ```

- **[arXiv251225] Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection**
  - **tags:** [ai], [multimodal fake news detection], [inconsistency detection, feature disentanglement, conflict-consensus mechanism, physics-inspired dynamics, cross-modal discrepancy]
  - **authors:** Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang
  - **institution:** Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano
  - **link:** https://arxiv.org/pdf/2512.20670
  - **contributions:** 1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection] --> B[核心问题/Problem: 主流一致性融合将关键跨模态差异误判为噪声，稀释了伪造证据]
    A --> C[主要方法/Method: 提出DCCF框架，解耦事实与情感，利用物理启发的动力学主动放大矛盾]
    A --> D[关键结果/Results: 在三个真实数据集上超越SOTA，平均准确率提升3.52%]
    ```

- **[arXiv251225] Revisiting the Learning Objectives of Vision-Language Reward Models**
  - **tags:** [ai], [reinforcement learning], [reward modeling, vision-language models, triplet loss, Meta-World, contrastive learning]
  - **authors:** Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome
  - **institution:** Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université
  - **link:** https://arxiv.org/pdf/2512.20675
  - **contributions:** 1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Revisiting VLM Reward Models] --> B(核心问题/Problem: 难以比较不同奖励模型目标/Difficulty in comparing reward model objectives)
        A --> C(主要方法/Method: 统一框架评估/Unified framework evaluation)
        A --> D(关键结果/Results: 三元组损失更优/Triplet loss outperforms SOTA)
    ```

- **[arXiv251225] HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model**
  - **tags:** [mlsys], [multi-modal training], [Low-Rank Adaptation (LoRA), parameter-efficient fine-tuning, rank adaptation, mobile vision language model, dynamic scheduling]
  - **authors:** Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour
  - **institution:** Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen
  - **link:** https://arxiv.org/pdf/2512.20674
  - **contributions:** 1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]
    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]
    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]
    ```

- **[arXiv251225] Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems**
  - **tags:** [ai], [multi-agent systems], [Differentiable Price Mechanism, Dominant Strategy Incentive Compatibility, VCG-equivalent incentive, Dec-POMDPs, Bayesian Incentive Compatibility]
  - **authors:** Stefano Grassi
  - **institution:** None (No affiliation or email domain provided in the given content)
  - **link:** https://arxiv.org/pdf/2512.20688
  - **contributions:** 1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems] --> B[核心问题/Problem: Hayekian Information Problem & Hurwiczian Incentive Problem]
    A --> C[主要方法/Method: Differentiable Price Mechanism (DPM) & Bayesian Extension]
    A --> D[关键结果/Results: DSIC/BIC Guarantee, Linear Scaling, 50x Faster than Model-Free RL]
    ```

- **[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation**
  - **tags:** [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]
  - **authors:** Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai
  - **institution:** Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University
  - **link:** https://arxiv.org/pdf/2512.20687
  - **contributions:** 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]
    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]
    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]
    ```

- **[arXiv251225] Real-World Adversarial Attacks on RF-Based Drone Detectors**
  - **tags:** [sec], [adversarial machine learning], [adversarial attack, drone detection, radio frequency (RF), over-the-air (OTA), I/Q perturbation]
  - **authors:** Omer Gazit, Yael Itzhakev, Yuval Elovici, Asaf Shabtai
  - **institution:** Ben-Gurion University of the Negev
  - **link:** https://arxiv.org/pdf/2512.20712
  - **contributions:** 1. The first physical adversarial attack targeting image-based object detection models for RF-based drone detection. 2. A novel method that optimizes adversarial perturbations directly in the complex baseband (I/Q) domain for over-the-air transmission. 3. Demonstration of the attack's effectiveness and hardware compatibility through both digital and physical (OTA) evaluations with multiple drone types.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384206c86ac50ef9b9d711e77f0b2e691640ea7831fb12c1517a40575d8c07f3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes the first physical adversarial attack against RF-based drone detectors. Instead of modifying digital spectrogram images, the method generates and transmits optimized I/Q perturbation waveforms alongside legitimate drone signals. The results show these structured perturbations are compatible with standard RF hardware and reliably reduce target drone detection while maintaining detection of legitimate ones.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Real-World Adversarial Attacks on RF-Based Drone Detectors] --> B(核心问题/Problem: Digital RF attacks are hard to implement over-the-air)
    A --> C(主要方法/Method: Generate & transmit I/Q perturbation waveforms)
    A --> D(关键结果/Results: Perturbations reduce target detection, preserve others, are hardware-compatible)
    ```

- **[arXiv251225] FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs**
  - **tags:** [mlsys], [llm inference], [Finite Element Method (FEM), Code Generation, LLM Benchmark, Computational Mechanics, Scientific Machine Learning]
  - **authors:** Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune
  - **institution:** Boston University, Move37 Labs
  - **link:** https://arxiv.org/pdf/2512.20732
  - **contributions:** 1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs' ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a lack of benchmarks for evaluating LLMs' scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[FEM-Bench Paper] --> B[核心问题/Problem: 缺乏评估LLM生成科学物理模型代码能力的基准/Lack of benchmark for evaluating LLMs' ability to generate scientifically valid physical model code]
        A --> C[主要方法/Method: 提出基于计算力学和有限元法的结构化基准/Proposes a structured benchmark based on computational mechanics and the Finite Element Method]
        A --> D[关键结果/Results: 先进LLM无法可靠解决所有基准任务，为跟踪进展奠定基础/State-of-the-art LLMs cannot reliably solve all benchmark tasks, establishing a foundation for tracking progress]
    ```

- **[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent**
  - **tags:** [mlsys], [agent system], [tool-augmented agent, agentic reinforcement learning, supervised fine-tuning (SFT), request-level asynchronous rollout, prefix-aware load balancing]
  - **authors:** Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang
  - **institution:** Tsinghua University, Tencent Hunyuan
  - **link:** https://arxiv.org/pdf/2512.20745
  - **contributions:** 1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AgentMath] --> B[核心问题/Problem: LRMs are inefficient and inaccurate for complex math]
    A --> C[主要方法/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]
    A --> D[关键结果/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]
    ```

- **[arXiv251225] AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication**
  - **tags:** [sys], [wireless networks], [Deep Reinforcement Learning (DRL), Reconfigurable Intelligent Surfaces (RIS), Energy Harvesting (EH)]
  - **authors:** Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur
  - **institution:** Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)
  - **link:** https://arxiv.org/pdf/2512.20739
  - **contributions:** 1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AI-Driven Green CRNs for 6G] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[频谱稀缺与高能耗/Spectrum Scarcity & High Energy Consumption]
    C --> C1[AI驱动框架/AI-Driven Framework]
    C1 --> C2[集成DRL, TL, EH, RIS/Integrates DRL, TL, EH, RIS]
    D --> D1[节能25-30%/25-30% Energy Saving]
    D --> D2[AUC>0.90, PDR提升/AUC>0.90, PDR Improved]
    ```

- **[arXiv251225] TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection**
  - **tags:** [mlsys], [on-device ai], [neural architecture search, hardware-aware search, edge detection, TinyML, waste detection]
  - **authors:** Tony Tran, Bin Hu
  - **institution:** University of Houston
  - **link:** https://arxiv.org/pdf/2512.20746
  - **contributions:** 1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[TrashDet] --> B[核心问题/Problem: 边缘设备垃圾检测<br>TinyML Constraints];
        A --> C[主要方法/Method: 迭代硬件感知NAS<br>Iterative Hardware-aware NAS];
        A --> D[关键结果/Results: 高效TrashDet家族<br>Efficient TrashDet Family];
        B --> D;
        C --> D;
    ```

- **[arXiv251225] Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies**
  - **tags:** [mlsys], [multi-modal training], [Lipschitz Continuity, Attention Mechanism, Aggregation Methods, Training Stability, Multimodal Autoencoders]
  - **authors:** Diyar Altinses, Andreas Schwung
  - **institution:** South Westphalia University of Applied Sciences
  - **link:** https://arxiv.org/pdf/2512.20749
  - **contributions:** 1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method's superior performance in consistency, convergence speed, and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Stabilizing Multimodal Autoencoders<br/>稳定多模态自编码器] --> B(核心问题/Problem: Training Stability & Robustness<br/>训练稳定性与鲁棒性)
    A --> C(主要方法/Method: Theoretical Lipschitz Analysis & Regularized Attention Fusion<br/>理论Lipschitz分析与正则化注意力融合)
    A --> D(关键结果/Results: Improved Consistency, Convergence, Accuracy<br/>提升的一致性、收敛速度与精度)
    ```

- **[arXiv251225] Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits**
  - **tags:** [mlsys], [others], [formal verification, neural network robustness, early exits, adversarial perturbations, off-the-shelf solvers]
  - **authors:** Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz
  - **institution:** The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside
  - **link:** https://arxiv.org/pdf/2512.20755
  - **contributions:** 1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title<br>Bridging Efficiency and Safety] --> B(核心问题 / Problem<br>Verifying Early Exit Networks);
    A --> C(主要方法 / Method<br>Tailored Robustness Property & Enhanced Algorithm);
    A --> D(关键结果 / Results<br>Improved Verifiability & Efficiency);
    ```

- **[arXiv251225] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior**
  - **tags:** [nlp], [tokenization], [tokenizer, language models, benchmark, subword segmentation, BPE]
  - **authors:** Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel
  - **institution:** University of Toronto, Vector Institute, Google DeepMind, McGill University, Mila - Quebec AI Institute, University of Cambridge, Hugging Face
  - **link:** https://arxiv.org/pdf/2512.20757
  - **code:** https://github.com/r-three/Tokenizers
  - **contributions:** 1. Introduces TokSuite, a collection of fourteen language models that are identical except for their tokenizers, enabling isolated study of tokenizer impact. 2. Curates and releases a new benchmark designed to measure model performance under real-world text perturbations that affect tokenization. 3. Provides a robust framework that supports novel findings on the benefits and shortcomings of various popular tokenizers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of isolating the impact of tokenizer choice on language model behavior. It proposes TokSuite, a suite of models with different tokenizers but identical other components, along with a specialized benchmark. The work enables systematic analysis and reveals new insights into how different tokenizers affect model performance.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[TokSuite: Measuring Tokenizer Impact] --> B[核心问题/Problem: Tokenization's role in LM performance is poorly understood]
        A --> C[主要方法/Method: TokSuite - Identical models with different tokenizers + new benchmark]
        A --> D[关键结果/Results: Novel findings on tokenizer benefits and shortcomings]
    ```

- **[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed**
  - **tags:** [ai], [reinforcement learning], [RLVR, causal reasoning, generalization, supervised fine-tuning, large language models]
  - **authors:** Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei
  - **institution:** Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago
  - **link:** https://arxiv.org/pdf/2512.20760
  - **contributions:** 1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.
  - **Mindmap:**

    ```mermaid
    graph LR
    A["Generalization of RLVR Using Causal Reasoning as a Testbed<br>以因果推理为测试平台的RLVR泛化研究"] --> B["核心问题/Problem<br>RLVR何时能实现鲁棒泛化？<br>When does RLVR yield robust generalization?"]
    A --> C["主要方法/Method<br>在因果图模型上实证研究RLVR与SFT<br>Empirical study of RLVR vs SFT on causal graphical models"]
    A --> D["关键结果/Results<br>RLVR泛化更强，但依赖模型规模与初始能力<br>RLVR yields stronger generalization but depends on model size & initial competence"]
    ```

- **[arXiv251225] TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform**
  - **tags:** [mlsys], [others], [time series foundation models, live forecasting, pre-registration, information leakage, temporal split]
  - **authors:** Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller
  - **institution:** Paderborn University
  - **link:** https://arxiv.org/pdf/2512.20761
  - **code:** https://huggingface.co/spaces/DAG-UPB/TS-Arena
  - **contributions:** 1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[TS-Arena Technical Report] --> B[核心问题/Problem: TSFM评估危机 / TSFM Evaluation Crisis]
    A --> C[主要方法/Method: 预注册实时预测平台 / Pre-registered Live Forecasting Platform]
    A --> D[关键结果/Results: 防止历史污染，真实评估泛化 / Prevents Historical Contamination, Authentic Generalization Assessment]
    B --> E[信息泄露与数据重用 / Information Leakage & Data Reuse]
    C --> F[实时数据流与严格时间分割 / Live Data Streams & Strict Temporal Split]
    D --> G[可持续的基准测试基础设施 / Sustainable Benchmarking Infrastructure]
    ```

- **[arXiv251225] Subgroup Discovery with the Cox Model**
  - **tags:** [ai], [survival analysis], [subgroup discovery, Cox proportional hazards model, expected prediction entropy, conditional rank statistics, interpretable machine learning]
  - **authors:** Zachary Izzo, Iain Melvin
  - **institution:** NEC Labs America
  - **link:** https://arxiv.org/pdf/2512.20762
  - **contributions:** 1. Introduced two novel metrics for evaluating survival models in the context of subgroup discovery: the Expected Prediction Entropy (EPE) and the Conditional Rank Statistics (CRS). 2. Proposed eight algorithms for the Cox subgroup discovery problem, with a main algorithm that leverages both EPE and CRS and has theoretical correctness guarantees. 3. Demonstrated the effectiveness of the methods through empirical evaluation on synthetic and real data, including a case study on NASA jet engine simulation data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa075e21de650db93aab9430f1fc0b8df1921ae52ac2d9c0e9c3dadc8ba3c5f7_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of subgroup discovery for survival analysis, aiming to find interpretable data subsets where the Cox model is highly accurate. The authors propose two new evaluation metrics (EPE and CRS) and eight algorithms to solve this problem. Their methods successfully recover ground-truth subgroups and improve model fit compared to fitting a Cox model on the entire dataset, as validated on synthetic, real, and NASA case study data.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Subgroup Discovery with the Cox Model] --> B(核心问题/Problem: Find interpretable subsets where Cox model is accurate)
    A --> C(主要方法/Method: Introduce EPE & CRS metrics; Propose 8 algorithms)
    A --> D(关键结果/Results: Recovers ground-truth subgroups; Better model fit; Validated on NASA data)
    ```

- **[arXiv251225] Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer**
  - **tags:** [mlsys], [gpu kernels], [matrix exponential, scaling and squaring, Taylor series, generative AI, numerical stability]
  - **authors:** Jorge Sastre, Daniel Faronbi, José Miguel Alonso, Peter Traver, Javier Ibáñez, Nuria Lloret
  - **institution:** Universitat Politècnica de València, New York University
  - **link:** https://arxiv.org/pdf/2512.20777
  - **contributions:** 1. An optimized Taylor-based algorithm for matrix exponential designed for high-throughput generative AI flows. 2. A rigorous error analysis and a dynamic selection strategy for Taylor order and scaling factor to minimize computation under error tolerance. 3. Extensive numerical experiments demonstrating significant acceleration and high numerical stability compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes an optimized Taylor-based algorithm for computing the matrix exponential, a key operation in generative AI. The method improves upon classical techniques like Paterson-Stockmeyer and includes a dynamic strategy to balance accuracy and speed. Experiments show it offers significant acceleration while maintaining high numerical stability for large-scale generative modeling.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Improving Matrix Exponential for Generative AI Flows<br/>生成式AI流程的矩阵指数改进] --> B(Problem: Standard methods (Padé) may be inefficient for high-throughput AI<br/>问题: 标准方法对高吞吐AI效率不足)
        A --> C(Method: Optimized Taylor-based algorithm with dynamic parameter selection<br/>方法: 优化的基于泰勒级数的算法与动态参数选择)
        A --> D(Results: Significant acceleration & high numerical stability demonstrated<br/>结果: 显著加速并保持高数值稳定性)
    ```

- **[arXiv251225] NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts**
  - **tags:** [cv], [medical image segmentation], [nullable prompts, mixed-supervision, vision-language models, breast ultrasound segmentation]
  - **authors:** Raja Mallina, Bryar Shareef
  - **institution:** University of Nevada, Las Vegas
  - **link:** https://arxiv.org/pdf/2512.20783
  - **contributions:** 1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]
    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]
    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]
    ```

- **[arXiv251225] Symbolic regression for defect interactions in 2D materials**
  - **tags:** [ai], [symbolic regression], [Symbolic Regression, SEGVAE, Graph Neural Networks, 2D Materials, Interpretability]
  - **authors:** Mikhail Lazarev, Andrey Ustyuzhanin
  - **institution:** HSE University, Institute for Functional Intelligent Materials (National University of Singapore), Constructor University
  - **link:** https://arxiv.org/pdf/2512.20785
  - **contributions:** 1. Applied the deep symbolic regression algorithm SEGVAE to model defect interactions in 2D materials. 2. Demonstrated that symbolic regression can achieve performance comparable to state-of-the-art graph neural network methods. 3. Discussed the broader applicability and advantages (e.g., interpretability) of symbolic regression methods in natural sciences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e61aacf1f171adb632fccf66c94fbeaf1d24724489a5e5c0d3c2a23d60bf7d16_w640_q70.webp
  - **Simple LLM Summary:** This paper applies the SEGVAE deep symbolic regression algorithm to discover analytical equations describing defect interactions in 2D materials. The results show that this interpretable method achieves performance comparable to state-of-the-art graph neural networks, highlighting its potential for scientific discovery.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Symbolic regression for defect interactions in 2D materials] --> B(核心问题/Problem: ML models lack interpretability for scientific data)
    A --> C(主要方法/Method: Apply SEGVAE for symbolic regression)
    A --> D(关键结果/Results: Comparable performance to GNNs, offers interpretability)
    ```

- **[arXiv251225] FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative**
  - **tags:** [mlsys], [federated learning], [gradient compression, directional derivative, privacy preservation, communication efficiency, low-rank projection]
  - **authors:** Mohammadreza Rostami, Solmaz S. Kia
  - **institution:** University of California Irvine
  - **link:** https://arxiv.org/pdf/2512.20814
  - **contributions:** 1. Proposes FedMPDD, a novel FL algorithm that compresses high-dimensional gradients into low-dimensional messages via multi-projected directional derivatives, reducing uplink cost from O(d) to O(m)., 2. Provides theoretical convergence analysis showing FedMPDD achieves an O(1/√K) convergence rate, matching FedSGD, by averaging multiple projections to overcome single-projection limitations., 3. Demonstrates the method offers inherent privacy against gradient inversion attacks due to geometric properties of low-rank projections, providing a tunable privacy-utility trade-off.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/026756c87a289335823e9350b3b3cf6b10435dec5873ca0fe832db069efa00b9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high communication cost and privacy risks in Federated Learning. It proposes FedMPDD, which compresses client gradients by computing their directional derivatives along multiple random vectors, significantly reducing bandwidth usage while providing inherent privacy. Theoretical and experimental results show the method maintains convergence performance comparable to FedSGD and offers a tunable privacy-utility trade-off.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[FedMPDD] --> B[核心问题/Problem: High Communication Cost & Privacy Risk in FL]
    A --> C[主要方法/Method: Multi-Projected Directional Derivative for Gradient Compression]
    A --> D[关键结果/Results: O(1/√K) Convergence, O(m) Communication, Inherent Privacy]
    ```

- **[arXiv251225] GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface**
  - **tags:** [ai], [physics-informed machine learning], [graph neural networks, XGBoost, AlphaEarth embeddings, contagion dynamics, ensemble learning]
  - **authors:** Miguel Esparza, Vamshi Battal, Ali Mostafavi
  - **institution:** Texas A&M University
  - **link:** https://arxiv.org/pdf/2512.20813
  - **contributions:** 1. A novel dual-specialist ensemble framework that disentangles wildfire vulnerability into distinct environmental contagion and structural fragility vectors. 2. Integration of a physics-informed Graph Neural Network (GNN) for neighborhood-scale contagion modeling with an XGBoost model for asset-level structural resilience. 3. Generation of a diagnostic risk topology enabling targeted mitigation strategies, such as vegetation management for high-connectivity clusters and structural hardening for vulnerable nodes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3a0fb230e974a7c6219e056bcb5e5d527990414b399512769ab5745bdc847_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes GraphFire-X, a dual-specialist ensemble framework combining a physics-informed Graph Neural Network and XGBoost to model building-scale wildfire risk at the Wildland-Urban Interface. The method disentangles environmental contagion from structural fragility, revealing that neighborhood-scale environmental pressure dominates propagation pathways while eaves are a key micro-scale vulnerability. The ensemble provides a diagnostic risk map to guide precise, proactive mitigation strategies.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[GraphFire-X] --> B[核心问题/Problem: 传统模型无法捕捉WUI非线性蔓延动态/Traditional models fail to capture non-linear contagion dynamics at WUI]
    A --> C[主要方法/Method: 双专家集成框架/Dual-Specialist Ensemble: 物理信息GNN(环境) + XGBoost(结构)/Physics-informed GNN (Environment) + XGBoost (Structure)]
    A --> D[关键结果/Results: 环境压力主导蔓延路径，屋檐是主要入侵点，生成诊断风险拓扑/Environmental pressure dominates pathways, eaves are key ingress, diagnostic risk topology generated]
    ```

- **[arXiv251225] Defending against adversarial attacks using mixture of experts**
  - **tags:** [sec], [Adversarial Machine Learning], [Mixture of Experts, Adversarial Training, Robustness, Ensemble Learning, Evasion Attacks]
  - **authors:** Mohammad Meymani, Roozbeh Razavi-Far
  - **institution:** University of New Brunswick
  - **link:** https://arxiv.org/pdf/2512.20821
  - **contributions:** 1. Proposes a defense system that integrates an adversarial training module within a Mixture of Experts (MoE) architecture. 2. Employs nine pre-trained experts with ResNet-18 backbones and jointly optimizes both the expert parameters and the gating mechanism during end-to-end training. 3. Demonstrates that the proposed system outperforms state-of-the-art defense systems and plain classifiers, even those with more complex architectures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0952838b3d0745a7f0396318a91c52e0ef0a7e273730cc1cf1a311f7bfecf079_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the vulnerability of machine learning models to adversarial attacks. It proposes a defense system that combines adversarial training with a Mixture of Experts architecture, using nine pre-trained ResNet-18 models. The system is shown to be more robust than existing defenses and standard classifiers.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Defending against adversarial attacks using mixture of experts] --> B(核心问题/Problem: ML模型易受对抗性攻击/ML Models Vulnerable to Adversarial Attacks)
    A --> C(主要方法/Method: 对抗训练与专家混合体结合/Adversarial Training within Mixture-of-Experts)
    A --> D(关键结果/Results: 优于现有防御与分类器/Outperforms State-of-the-art Defenses & Classifiers)
    ```

- **[arXiv251225] Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions**
  - **tags:** [ai], [reinforcement learning], [parameterized actions, state abstraction, action abstraction, TD(λ), sample efficiency]
  - **authors:** Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava
  - **institution:** Arizona State University, Brown University
  - **link:** https://arxiv.org/pdf/2512.20831
  - **code:** https://github.com/AAIR-lab/PEARL.git
  - **contributions:** 1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Context-Sensitive Abstractions for RL with Parameterized Actions] --> B(核心问题/Problem: RL for Parameterized Actions)
    A --> C(主要方法/Method: Learn & Refine State/Action Abstractions)
    A --> D(关键结果/Results: Higher Sample Efficiency for TD(λ))
    ```

- **[arXiv251225] CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images**
  - **tags:** [cv], [bioimage analysis], [multi-channel microscopy, cellular morphology, pre-training dataset, channel-adaptive models, heterogeneous data]
  - **authors:** Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo
  - **institution:** Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard
  - **link:** https://arxiv.org/pdf/2512.20833
  - **contributions:** 1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[CHAMMI-75: Pre-training Multi-channel Models] --> B[核心问题/Problem: Specialized models cannot be reused across studies]
        A --> C[主要方法/Method: Curate CHAMMI-75, a heterogeneous multi-channel dataset]
        A --> D[关键结果/Results: Training with CHAMMI-75 improves performance via diversity]
    ```

- **[arXiv251225] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning**
  - **tags:** [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, agentic reasoning, sparse activation, long context]
  - **authors:** NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.20848
  - **contributions:** 1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp
  - **Simple LLM Summary:** This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Nemotron 3 Nano<br>论文标题/Paper Title] --> B[构建高效、能进行智能体推理的大模型<br>核心问题/Problem];
    A --> C[混合MoE与Mamba-Transformer架构，稀疏激活参数<br>主要方法/Method];
    A --> D[更高推理吞吐与精度，支持100万令牌上下文<br>关键结果/Results];
    ```

- **[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence**
  - **tags:** [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, LatentMoE, NVFP4, multi-environment reinforcement learning]
  - **authors:** NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia
  - **institution:** NVIDIA
  - **link:** https://arxiv.org/pdf/2512.20856
  - **contributions:** 1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[NVIDIA Nemotron 3] --> B[核心问题/Problem: Efficient and open intelligence for agentic applications]
    A --> C[主要方法/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]
    A --> D[关键结果/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]
    ```

- **[arXiv251225] Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs**
  - **tags:** [mlsys], [llm inference], [block low-rank (BLR), Triton kernels, memory-bound optimization, Jetson Orin Nano, roofline analysis]
  - **authors:** Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim
  - **institution:** University of Michigan
  - **link:** https://arxiv.org/pdf/2512.20861
  - **code:** https://github.com/pabillam/mem-efficient-blr
  - **contributions:** 1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Memory-Efficient Acceleration of Block Low-Rank Foundation Models] --> B[核心问题/Problem: BLR模型多token推理存在内存墙/Multi-token inference for BLR models is memory-bound]
    A --> C[主要方法/Method: 定制Triton内核与内存优化/Custom Triton kernels with memory optimizations]
    A --> D[关键结果/Results: 显著加速与模型压缩/Significant speedup & model compression]
    ```

- **[arXiv251225] Robustness Certificates for Neural Networks against Adversarial Attacks**
  - **tags:** [sec], [adversarial robustness], [barrier certificates, data poisoning, formal verification, scenario convex program, robustness certification]
  - **authors:** Sara Taheri, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Majid Zamani
  - **institution:** LMU Munich, TUM Munich, University of Colorado Boulder
  - **link:** https://arxiv.org/pdf/2512.20865
  - **contributions:** 1. Introduces a formal robustness certification framework by modeling gradient-based training as a discrete-time dynamical system and formulating poisoning robustness as a safety verification problem., 2. Adapts barrier certificates from control theory to derive sufficient conditions for certifying a robust radius against worst-case ℓp-norm poisoning, and makes it practical by parameterizing BCs as neural networks., 3. Derives PAC bounds via a scenario convex program to provide a confidence lower bound on the certified robustness radius, and extends the unified framework to also certify against test-time attacks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5a85af42ea34173637ac88fc6859bba1d4a68d3161f2fcc2567276ca7d37b5b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of formal guarantees in defending neural networks against data poisoning attacks. It proposes a certification framework that models training as a dynamical system, uses barrier certificates for verification, and provides PAC-bounded robustness radii. Experiments show the approach certifies non-trivial perturbation budgets without needing prior attack knowledge.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Robustness Certificates for Neural Networks against Adversarial Attacks] --> B[核心问题/Problem: Lack of formal guarantees for neural networks against data poisoning attacks]
    A --> C[主要方法/Method: Model training as dt-DS, use Barrier Certificates & Scenario Convex Program for verification]
    A --> D[关键结果/Results: Certifies non-trivial robustness radii on MNIST/SVHN/CIFAR-10, model-agnostic, no prior attack knowledge needed]
    ```

- **[arXiv251225] Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification**
  - **tags:** [sec], [malware detection], [function call graphs, Android malware, graph-based classification, APK, dataset]
  - **authors:** Jakir Hossain, Gurvinder Singh, Lukasz Ziarek, Ahmet Erdem Sarıyüce
  - **institution:** University at Buffalo
  - **link:** https://arxiv.org/pdf/2512.20872
  - **code:** https://erdemub.github.io/BCG-dataset
  - **contributions:** 1. Introduces the Better Call Graphs (BCG) dataset, a new large-scale collection of unique and recent Android FCGs for malware classification., 2. Addresses limitations of existing datasets (outdated, redundant, small graphs) to prevent overfitting and enable reliable evaluation., 3. Demonstrates the necessity and value of the BCG dataset through extensive experiments with baseline classifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87c9a8db7055364694bd0257a161b2a24a77226da1ab49504cd20679d8699222_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of high-quality datasets for Android malware classification using function call graphs (FCGs). It introduces the Better Call Graphs (BCG) dataset, which contains large, unique, and recent FCGs from Android APKs. Experiments show BCG is necessary for reliable evaluation and helps overcome overfitting issues present with older datasets.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification] --> B(核心问题/Problem: Lack of large-scale, high-quality Android FCG datasets hinders malware classification research)
    A --> C(主要方法/Method: Introduce BCG dataset with large, unique, recent Android APK FCGs)
    A --> D(关键结果/Results: BCG enables more reliable evaluation and addresses overfitting compared to existing datasets)
    ```

- **[arXiv251225] Architectural Trade-offs in Small Language Models Under Compute Constraints**
  - **tags:** [nlp], [language modeling], [small language models, compute constraints, architectural trade-offs, rotary positional embeddings, transformer]
  - **authors:** Shivraj Singh Bhatti
  - **institution:** University of Massachusetts Amherst
  - **link:** https://arxiv.org/pdf/2512.20877
  - **contributions:** 1. A systematic empirical study of architectural choices (from linear predictors to transformers) for small language models under strict compute constraints. 2. An analysis showing attention-based models are more FLOP-efficient than MLPs even at small scale, and that increasing depth/context without sufficient optimization can hurt performance. 3. An investigation revealing that techniques like Rotary Positional Embeddings (RoPE), successful in large models, do not necessarily transfer effectively to the small-model regime.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically studies how architectural choices affect small language model performance under limited compute. The method involves progressively building from linear predictors to multi-layer transformers and evaluating them on character and word-level datasets. The main conclusion is that attention is more efficient than MLPs per FLOP at small scales, but scaling depth or applying large-model techniques like RoPE can be detrimental without careful optimization.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Architectural Trade-offs in Small Language Models<br>小型语言模型的架构权衡] --> B[核心问题/Problem<br>How do architectural choices affect performance under compute constraints?<br>计算约束下架构选择如何影响性能？]
    A --> C[主要方法/Method<br>Progressive architectural study from linear to transformer models<br>从线性到Transformer模型的渐进式架构研究]
    A --> D[关键结果/Results<br>Attention > MLPs in per-FLOP efficiency; RoPE may not transfer<br>注意力机制单位FLOP效率优于MLP；RoPE可能不适用于小模型]
    ```

- **[arXiv251225] Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks**
  - **tags:** [ai], [adversarial robustness], [adversarial robustness, deep neural networks, time-efficient, red-blue adversarial framework, adversarial evaluation]
  - **authors:** Runqi Lin
  - **institution:** The University of Sydney
  - **link:** https://arxiv.org/pdf/2512.20893
  - **contributions:** 1. Proposes time-efficient methods for evaluating adversarial robustness in DNNs, 2. Proposes time-efficient methods for enhancing adversarial robustness in DNNs, 3. Aims to overcome the computational intensity limitation of existing approaches for large-scale models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c027984dda35366287528822de7264568bdef6dac0b744de35e5554d0dc77b9_w640_q70.webp
  - **Simple LLM Summary:** This thesis addresses the computational inefficiency of existing methods for evaluating and improving the adversarial robustness of deep neural networks. It proposes new time-efficient techniques for both identifying vulnerabilities (red team) and mitigating them (blue team). The main goal is to make adversarial robustness assessment and enhancement more applicable to large-scale models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks] --> B[核心问题/Problem: 现有对抗鲁棒性评估与增强方法计算成本高 / Existing adversarial robustness evaluation and enhancement methods are computationally intensive.]
    A --> C[主要方法/Method: 为红队（评估）和蓝队（增强）提供时间高效的方法 / Provides time-efficient methods for the red team (evaluation) and blue team (enhancement).]
    A --> D[关键结果/Results: 旨在克服大规模模型的应用限制 / Aims to overcome the applicability limitation for large-scale models.]
    ```

- **[arXiv251225] From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction**
  - **tags:** [mlsys], [communication & networking], [Graph Neural Networks, Kolmogorov-Arnold Networks, Symbolic Regression, Attention, Message Passing]
  - **authors:** Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard
  - **institution:** Université Jean Monnet Saint-Étienne, CNRS, Institut d'Optique Graduate School, Laboratoire Hubert Curien, Institut Universitaire de France (IUF), Inria
  - **link:** https://arxiv.org/pdf/2512.20885
  - **contributions:** 1. A heterogeneous Graph Neural Network with attention-based message passing as a strong baseline for flow delay prediction. 2. FlowKANet, a fully KAN-based GNN architecture that integrates KAN operators into message-passing and attention computation for efficiency and interpretability. 3. A symbolic distillation of FlowKANet via block-wise regression to produce lightweight, transparent closed-form surrogate models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96bffa46ff987475631fa980227e1a967ebbe523f541ff36441600d02001b3_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles flow delay prediction in communication networks by proposing a progression of models. It introduces FlowKANet, a GNN that replaces standard MLP layers with Kolmogorov-Arnold Networks (KANs) for better efficiency and interpretability, and further distills it into symbolic surrogate models. The results show that KANs offer a good efficiency-accuracy trade-off, and the symbolic surrogates enable lightweight, transparent deployment.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[From GNNs to Symbolic Surrogates via KANs for Delay Prediction] --> B[核心问题/Problem: Accurate flow delay prediction for network optimization]
    A --> C[主要方法/Method: Propose FlowKANet (KAN-based GNN) and distill it into symbolic surrogates]
    A --> D[关键结果/Results: KANs balance efficiency & accuracy; surrogates enable lightweight, transparent deployment]
    ```

- **[arXiv251225] DiEC: Diffusion Embedded Clustering**
  - **tags:** [ai], [deep clustering], [diffusion models, representation selection, self-training, graph regularization, denoising consistency]
  - **authors:** Haidong Hu
  - **institution:** Not explicitly provided in the given content.
  - **link:** https://arxiv.org/pdf/2512.20905
  - **contributions:** 1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model's internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[DiEC: Diffusion Embedded Clustering] --> B[核心问题/Problem: Single fixed embedding ignores varying clusterability in diffusion model's internal trajectory];
    A --> C[主要方法/Method: Two-stage search (CML & OTS) on layer×timestep, regularized self-training with denoising-consistency];
    A --> D[关键结果/Results: Achieves competitive clustering performance on multiple benchmarks];
    ```

- **[arXiv251225] RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks**
  - **tags:** [mlsys], [llm training], [reversible networks, memory-efficient fine-tuning, mixture-of-experts, full-parameter fine-tuning, activation recomputation]
  - **authors:** Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.20920
  - **contributions:** 1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model's expressive power.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[RevFFN: Memory-Efficient Fine-Tuning] --> B[核心问题/Problem: Full fine-tuning memory overhead高]
    A --> C[主要方法/Method: 使用可逆Transformer块/Use reversible Transformer blocks]
    A --> D[关键结果/Results: 单GPU高效全参数微调/Efficient full fine-tuning on single GPU]
    ```

- **[arXiv251225] Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining**
  - **tags:** [ai], [combinatorial optimization], [graph features, hardness prediction, association rule mining, maximum clique problem, machine learning classification]
  - **authors:** Bharat Sharman, Elkafi Hassini
  - **institution:** (Inferred from author names and arXiv submission; specific institution not provided in abstract. Could be a university research group.)
  - **link:** https://arxiv.org/pdf/2512.20915
  - **contributions:** 1. Proposes GCO-HPIF, a general two-stage ML framework for predicting and explaining the hardness of graph-based combinatorial optimization problems. 2. Demonstrates the framework's effectiveness by applying it to the maximum clique problem using diverse algorithms (exact and GNN-based) and achieving high prediction accuracy with few features. 3. Introduces the use of association rule mining (FP-Growth) to generate human-interpretable explanations for the hardness predictions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6637eddd49fc539b03c9e2248f2656c238346a34b80db77df78bdd41cc252f_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GCO-HPIF, a machine learning framework that predicts and explains the computational hardness of graph-based combinatorial optimization problems by first classifying instances using graph features and then explaining the predictions via association rule mining. The framework was validated on a large dataset of maximum clique problem instances, achieving excellent prediction performance and generating interpretable rules. The results show the framework's potential for both accurate hardness forecasting and providing insights into problem difficulty.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Towards a General Framework...<br/>预测与解释图组合优化问题难度的通用框架] --> B(核心问题/Problem: Predicting computational hardness of graph-based combinatorial optimization problems<br/>预测图组合优化问题的计算难度)
    A --> C(主要方法/Method: Two-stage ML framework (GCO-HPIF)<br/>两阶段机器学习框架)
    C --> C1(Stage 1: Classification using graph features<br/>阶段1: 使用图特征的分类)
    C --> C2(Stage 2: Explanation using Association Rule Mining (FP-Growth)<br/>阶段2: 使用关联规则挖掘进行解释)
    A --> D(关键结果/Results: High prediction accuracy (F1=0.9921), interpretable rules, low error for time prediction<br/>高预测精度, 可解释规则, 低时间预测误差)
    ```

- **[arXiv251225] Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy**
  - **tags:** [ai], [revenue management], [constrained optimization, Bayesian hierarchical modeling, Monte Carlo simulation, price elasticity, churn prediction]
  - **authors:** Deepit Sapru
  - **institution:** University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.20932
  - **contributions:** 1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Guardrailed Elasticity Pricing] --> B[核心问题/Problem: Static pricing fails to balance revenue & retention];
    A --> C[主要方法/Method: Forecast + Elasticity + Churn model with constrained optimization];
    A --> D[关键结果/Results: Outperforms static pricing, protects customers, enables durable growth];
    ```

- **[arXiv251225] A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate**
  - **tags:** [mlsys], [others], [graph neural network, surrogate model, multi-fidelity dataset, scaling laws, aerodynamic field prediction]
  - **authors:** Yiren Shen, Juan J. Alonso
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.20941
  - **contributions:** 1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[论文标题 / Paper Title<br>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws] --> B(核心问题 / Problem<br>缺乏开源多保真数据集与数据规模对模型性能影响的实证指导 / Lack of open-source multi-fidelity datasets and empirical guidelines on data scaling)
        A --> C(主要方法 / Method<br>发布数据集并进行缩放研究 / Release dataset and conduct scaling study)
        B --> D(关键结果 / Results<br>误差随数据量呈幂律下降 / Test error decreases with data size via power law)
        C --> D
    ```

- **[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences**
  - **tags:** [mlsys], [communication & networking], [4D Gaussian Splatting, video streaming, integer linear programming, pruning, keyframe selection]
  - **authors:** Zhe Wang, Jinghang Li, Yifei Zhu
  - **institution:** Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.20943
  - **contributions:** 1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]
    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]
    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]
    ```

- **[arXiv251225] MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment**
  - **tags:** [nlp], [crosslingual information retrieval], [dual-encoder, contrastive learning, hard negative sampling, data augmentation, multi-source alignment]
  - **authors:** Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah
  - **institution:** Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is "MultiMind".
  - **link:** https://arxiv.org/pdf/2512.20950
  - **contributions:** 1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --> B(核心问题/Problem: Rapid spread of multilingual misinformation);
        A --> C(主要方法/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);
        A --> D(关键结果/Results: Improved retrieval accuracy on benchmarks);
    ```

- **[arXiv251225] Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations**
  - **tags:** [ai], [operator learning], [Gaussian processes, functional renormalization group, functional PDEs, operator learning, non-perturbative methods]
  - **authors:** Xianjin Yang, Matthieu Darcy, Matthew Hudes, Francis J. Alexander, Gregory Eyink, Houman Owhadi
  - **institution:** Caltech
  - **link:** https://arxiv.org/pdf/2512.20956
  - **contributions:** 1. Proposes a Gaussian process-based operator learning framework for solving functional PDEs directly in function space, independent of specific discretizations. 2. Demonstrates the method's application to non-perturbative functional renormalization group equations (e.g., Wetterich, Wilson-Polchinski), achieving performance equal to or better than traditional approximations like the local-potential approximation. 3. Shows the framework's flexibility in handling non-constant fields and incorporating physical priors, making it promising for studying complex field configurations like instantons.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18c810a016d418e9edb4adddfcb5d19e76d457adf7e311d4c5ca1b064e1fa145_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a Gaussian process operator learning framework to solve functional partial differential equations, specifically targeting non-perturbative functional renormalization group equations. The method directly represents functionals in function space, offering flexibility and independence from equation-specific discretizations. The results demonstrate that it matches or outperforms traditional approximations like the local-potential approximation and can handle complex, non-constant field configurations.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Solving Functional PDEs with Gaussian Processes] --> B(核心问题/Problem: Solving functional renormalization group equations)
    A --> C(主要方法/Method: Gaussian process operator learning in function space)
    A --> D(关键结果/Results: Matches or beats traditional approximations, handles non-constant fields)
    ```

- **[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design**
  - **tags:** [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), ChemBERTa, ESM-2, reaction-template, de novo drug design]
  - **authors:** R Yadunandan, Nimisha Ghosh
  - **institution:** Department of Computer Science and Engineering, Shiv Nadar University Chennai
  - **link:** https://arxiv.org/pdf/2512.20958
  - **code:** https://github.com/YadunandanRaman/ReACT-Drug/
  - **contributions:** 1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[ReACT-Drug] --> B[核心问题/Problem: Navigating vast chemical space for synthesizable, high-affinity drugs];
        A --> C[主要方法/Method: RL + Protein Embeddings + Reaction-Template Actions];
        A --> D[关键结果/Results: Novel, valid, synthetically accessible candidates];
    ```

- **[arXiv251225] Can Agentic AI Match the Performance of Human Data Scientists?**
  - **tags:** [ai], [automated data science], [agentic AI, domain knowledge, synthetic data, large language models, human-AI teaming]
  - **authors:** An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding
  - **institution:** University of Minnesota, University of Chicago, Cisco Research
  - **link:** https://arxiv.org/pdf/2512.20959
  - **contributions:** 1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp
  - **Simple LLM Summary:** The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Can Agentic AI Match Human Data Scientists?] --> B[核心问题/Problem: Can agentic AI match human performance using domain knowledge?];
    A --> C[主要方法/Method: Design task with latent variable in images, use synthetic insurance data];
    A --> D[关键结果/Results: Agentic AI with generic workflow falls short; highlights need for domain-aware AI];
    ```

- **[arXiv251225] Generalization of Diffusion Models Arises with a Balanced Representation Space**
  - **tags:** [ai], [diffusion models], [representation learning, denoising autoencoder, memorization detection, representation steering, generalization]
  - **authors:** Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu
  - **institution:** University of Michigan, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.20963
  - **code:** https://deepthink-umich.github.io
  - **contributions:** 1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures ("spiky" vs. "balanced") using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to "spiky" representations while generalization yields "balanced" ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Generalization of Diffusion Models Arises with a Balanced Representation Space] --> B(核心问题/Problem: Diffusion models risk memorizing training data)
    A --> C(主要方法/Method: Analyze via representation learning using a two-layer ReLU DAE)
    A --> D(关键结果/Results: Memorization yields spiky representations, generalization yields balanced ones; Enables detection and editing techniques)
    ```

- **[arXiv251225] Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions**
  - **tags:** [mlsys], [llm training], [spot instance, online scheduling, deadline-aware, LoRA, integer programming]
  - **authors:** Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu
  - **institution:** Fudan University, University of Oregon, Inria
  - **link:** https://arxiv.org/pdf/2512.20967
  - **contributions:** 1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --> B(核心问题/Problem: Expensive LLM fine-tuning with volatile spot instances)
    A --> C(主要方法/Method: Mixed instance scheduling with prediction & online policy selection)
    A --> D(关键结果/Results: O(√T) regret, up to 54.8% utility gain)
    ```

- **[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions**
  - **tags:** [ai], [reinforcement learning], [Bayesian Reinforcement Learning, Meta-Reinforcement Learning, Generalised Linear Models, Learnable Basis Functions, Variational Inference]
  - **authors:** Jingyang You, Hanna Kurniawati
  - **institution:** Australian National University
  - **link:** https://arxiv.org/pdf/2512.20974
  - **contributions:** 1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]
    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]
    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]
    ```

- **[arXiv251225] Automatic Replication of LLM Mistakes in Medical Conversations**
  - **tags:** [nlp], [llm evaluation], [medical conversation, mistake replication, benchmark creation, llm judges, single-shot qa]
  - **authors:** Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu
  - **institution:** Lumos AI
  - **link:** https://arxiv.org/pdf/2512.20983
  - **contributions:** 1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Automatic Replication of LLM Mistakes in Medical Conversations] --> B(核心问题/Problem: LLM错误难以在其他模型中复现/Mistakes hard to replicate across LLMs)
    A --> C(主要方法/Method: MedMistake自动管道/MedMistake automatic pipeline)
    A --> D(关键结果/Results: 发布基准并评估12个LLM/Released benchmark & evaluated 12 LLMs)
    C --> C1(生成对话/Generate conversations)
    C --> C2(LLM委员会评估/LLM committee evaluation)
    C --> C3(创建单轮QA对/Create single-shot QA pairs)
    D --> D1(MedMistake-All数据集/MedMistake-All dataset)
    D --> D2(MedMistake-Bench验证子集/MedMistake-Bench validated subset)
    D --> D3(GPT/Claude/Grok表现最佳/GPT/Claude/Grok performed best)
    ```

- **[arXiv251225] CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices**
  - **tags:** [ai], [correlation matrix segmentation], [correlation matrices, segmentation algorithms, metaheuristic optimization, overlapping technique, window difference metric]
  - **authors:** Alberto. Palomo-Alonso, David Casillas-Perez, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, Sancho Salcedo-Sanz
  - **institution:** Department of Signal Processing and Communications, Universidad de Alcalá (Spain)
  - **link:** https://arxiv.org/pdf/2512.21000
  - **contributions:** 1. Proposes CoSeNet, a novel four-layer algorithmic architecture for optimal segmentation of noisy correlation matrices. 2. Introduces a method using a heuristic algorithm to optimize the re-scaling layer parameters based on a Window Difference-based fitness metric. 3. Utilizes an overlapping technique and pre-trained ML algorithms to enhance robustness and generalizability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a9a31a49fc5fbd3e09151d607fb89a66d2e15d7c89ecd0f67b8be73bebac208_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces CoSeNet, a novel four-layer model for optimally segmenting correlated groups within noisy correlation matrices. The method uses an overlapping technique, pre-trained ML algorithms, and a heuristic to optimize its re-scaling parameters. The output is a clean, binary segmentation matrix, offering a balanced solution in terms of efficiency, memory, and speed.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[CoSeNet: 相关矩阵分割] --> B[核心问题: 噪声相关矩阵中的相关段识别 / Problem: Identifying correlated segments in noisy correlation matrices]
    A --> C[主要方法: 四层架构与启发式优化 / Method: Four-layer architecture with heuristic optimization]
    A --> D[关键结果: 生成无噪声二值分割矩阵 / Results: Generates noise-free binary segmentation matrix]
    C --> C1[输入层 / Input Layer]
    C --> C2[格式化层 / Formatting Layer]
    C --> C3[重缩放层 / Re-scaling Layer]
    C --> C4[分割层 / Segmentation Layer]
    C3 --> C3a[启发式参数优化 / Heuristic Parameter Optimization]
    C4 --> C4a[重叠技术与预训练ML / Overlapping Technique & Pre-trained ML]
    ```

- **[arXiv251225] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics**
  - **tags:** [ai], [llm evaluation], [Competitive Swiss-System Dynamics, Expected Win Score, Failure Sensitivity Analysis, Monte Carlo Simulation, risk appetite]
  - **authors:** Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong
  - **institution:** ByteDance Seed, Carnegie Mellon University, Columbia University
  - **link:** https://arxiv.org/pdf/2512.21010
  - **contributions:** 1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]
    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]
    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]
    ```

- **[arXiv251225] Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces**
  - **tags:** [nlp], [text embeddings], [Matryoshka Representation Learning, role-specific prefixes, purchase-driven fine-tuning]
  - **authors:** Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel
  - **institution:** Mercari, Inc.
  - **link:** https://arxiv.org/pdf/2512.21021
  - **contributions:** 1. Proposed a domain-aware Japanese text-embedding model fine-tuned on purchase-driven query-title pairs for C2C marketplace search. 2. Introduced the use of role-specific prefixes to model the query-item asymmetry inherent in search tasks. 3. Applied Matryoshka Representation Learning to create compact, truncation-robust embeddings that meet production latency and throughput constraints.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of improving search relevance in noisy, user-generated C2C marketplaces by fine-tuning a Japanese text-embedding model with role-specific prefixes and Matryoshka Representation Learning. The method produces compact embeddings that are robust to truncation for efficiency. Offline and online evaluations show significant improvements in retrieval quality and business metrics, providing a practical foundation for enhanced search experiences.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces] --> B[核心问题/Problem: C2C搜索挑战<br>Short queries, Noisy listings, Production constraints]
    A --> C[主要方法/Method: 领域感知嵌入<br>Domain-aware embeddings via fine-tuning, Role prefixes, Matryoshka Learning]
    A --> D[关键结果/Results: 离线与在线提升<br>Offline gains, Online revenue & efficiency improvements]
    ```

- **[arXiv251225] Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection**
  - **tags:** [nlp], [misinformation detection], [multi-persona agent, LLM-SLM synergy, evidence-grounded, multimodal fusion, credibility fusion]
  - **authors:** Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak
  - **institution:** Indian Institute of Technology (Indore, Patna)
  - **link:** https://arxiv.org/pdf/2512.21039
  - **contributions:** 1. Proposed AMPEND-LS, an agentic multi-persona framework that integrates textual, visual, and contextual evidence through a structured LLM reasoning pipeline. 2. Introduced a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context to improve reliability. 3. Designed a complementary SLM classifier to mitigate LLM uncertainty and hallucinations, enhancing robustness and explainability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of multimodal fake news detection by proposing AMPEND-LS, a framework that synergizes LLMs and SLMs within a multi-persona agent structure to reason over diverse evidence. It demonstrates superior performance over state-of-the-art baselines in accuracy and robustness across multiple datasets. The work contributes to developing more adaptive and explainable systems for combating online misinformation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AMPEND-LS] --> B[核心问题/Problem: 虚假新闻检测的挑战/Fake News Detection Challenges];
    A --> C[主要方法/Method: 多角色智能体与证据融合/Agentic Multi-Persona & Evidence Fusion];
    A --> D[关键结果/Results: 性能超越基线/Outperforms SOTA Baselines];
    B --> B1[多模态内容/Multimodal Content];
    B --> B2[领域泛化/Domain Generalization];
    B --> B3[可解释性/Explainability];
    C --> C1[LLM-SLM协同/LLM-SLM Synergy];
    C --> C2[可信度融合/Credibility Fusion];
    C --> C3[结构化推理/Structured Reasoning];
    D --> D1[高准确率与F1/High Accuracy & F1];
    D --> D2[强鲁棒性/Robustness];
    D --> D3[透明推理/Transparent Reasoning];
    ```

- **[arXiv251225] zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy**
  - **tags:** [mlsys], [federated learning], [zero-knowledge proofs, trusted execution environments, blockchain, medical AI, verifiable aggregation]
  - **authors:** Savvy Sharma, George Petrovic, Sarthak Kaushik
  - **institution:** George Brown Polytechnic
  - **link:** https://arxiv.org/pdf/2512.21048
  - **contributions:** 1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[zkFL-Health] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[隐私泄露与聚合器信任/Privacy Leakage & Aggregator Trust]
    C --> C1[FL+ZKP+TEE/FL+ZKP+TEE]
    C --> C2[链上验证/On-chain Verification]
    D --> D1[可验证的隐私保护/Verifiable Privacy]
    D --> D2[审计与合规/Auditability & Compliance]
    ```

- **[arXiv251225] DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors**
  - **tags:** [cv], [3D human pose estimation], [3D sign language reconstruction, biomechanical accuracy, hand and body pose priors, monocular video, SMPL-X]
  - **authors:** Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov
  - **institution:** Monash University, TCS Research
  - **link:** https://arxiv.org/pdf/2512.21054
  - **code:** https://github.com/kaustesseract/DexAvatar
  - **contributions:** 1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]
        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]
        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]
    ```

- **[arXiv251225] Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics**
  - **tags:** [ai], [deep learning theory], [scaling laws, feature learning, infinite-depth limit, ResNets, hyperparameter transfer]
  - **authors:** Zihan Yao, Ruoyu Wu, Tianxiang Gao
  - **institution:** DePaul University, Iowa State University
  - **link:** https://arxiv.org/pdf/2512.21075
  - **contributions:** 1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Understanding Scaling Laws via Feature Learning Dynamics] --> B[核心问题/Problem: Scaling laws describe success but not when/why scaling fails]
    A --> C[主要方法/Method: Derive Neural Feature Dynamics (NFD) in infinite-width & depth limit]
    A --> D[关键结果/Results: Explains diminishing returns, proposes depth-aware LR correction]
    ```

- **[arXiv251225] Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions**
  - **tags:** [nlp], [text classification], [hierarchical genre classification, zero-shot semantic alignment, dual-path graph convolution, label co-occurrence graph, blurb-refined inference]
  - **authors:** Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick
  - **institution:** Indian Institute of Technology Indore, Indian Institute of Technology Patna
  - **link:** https://arxiv.org/pdf/2512.21076
  - **contributions:** 1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[HiGeMine: Blurb-Refined Inference from Crowdsourced Book Reviews] --> B[核心问题/Problem: Noisy reviews & flat genre classification degrade reliability]
    A --> C[主要方法/Method: Two-phase framework: 1. Zero-shot review filtering 2. Dual-path graph classification]
    A --> D[关键结果/Results: Outperforms baselines on new hierarchical dataset]
    ```

- **[arXiv251225] LLM Personas as a Substitute for Field Experiments in Method Benchmarking**
  - **tags:** [ai], [algorithmic fairness & evaluation], [field experiments, A/B testing, LLM personas, algorithmic benchmarking, information-theoretic bounds]
  - **authors:** Enoch Hyunwook Kang
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.21080
  - **contributions:** 1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LLM Personas as a Substitute for Field Experiments] --> B[核心问题/Problem: Field experiments are costly and slow, hindering iterative method development.]
    A --> C[主要方法/Method: Use LLM-based persona simulation as a cheap synthetic benchmark under specific conditions.]
    A --> D[关键结果/Results: Formal validity conditions proven; sample-size bounds derived for decision-relevance.]
    ```

- **[arXiv251225] Hierarchical Modeling Approach to Fast and Accurate Table Recognition**
  - **tags:** [cv], [document analysis], [table recognition, multi-task learning, non-causal attention, parallel inference, hierarchical modeling]
  - **authors:** Takaya Kawakatsu
  - **institution:** Preferred Networks, Inc.
  - **link:** https://arxiv.org/pdf/2512.21083
  - **contributions:** 1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Hierarchical Modeling Approach to Fast and Accurate Table Recognition] --> B(核心问题/Problem: 现有表格识别模型推理慢且有效性未充分解释/Existing models are slow and their effectiveness is not fully explained)
    A --> C(主要方法/Method: 使用非因果注意力的多任务模型与并行推理算法/Novel multi-task model with non-causal attention & parallel inference algorithm)
    A --> D(关键结果/Results: 在大型公开数据集上实现视觉与统计上的优越性/Superiority demonstrated visually and statistically on two large public datasets)
    ```

- **[arXiv251225] Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics**
  - **tags:** [ai], [reinforcement learning], [Dyna-Style RL, SINDy, TD3, Model-based RL, Bi-rotor Control]
  - **authors:** Karim Abdelsalam, Zeyad Gamal, Ayman El-Badawy
  - **institution:** German University in Cairo
  - **link:** https://arxiv.org/pdf/2512.21081
  - **contributions:** 1. Proposes a Dyna-Style RL framework that integrates SINDy for data-driven dynamics modeling with TD3 for policy learning., 2. Introduces a method to periodically inject synthetic rollouts from the learned SINDy model into the RL replay buffer to improve sample efficiency., 3. Demonstrates the framework's effectiveness on a bi-rotor system, showing superior accuracy and robustness in stabilization and trajectory tracking compared to direct model-free RL.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hybrid control framework combining Sparse Identification of Nonlinear Dynamics (SINDy) and Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning to efficiently control nonlinear systems. The SINDy model generates synthetic data to augment real-world training, improving sample efficiency. The method is validated on a bi-rotor system, showing better performance than direct model-free RL.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Dyna-Style RL Modeling and Control of Non-linear Dynamics] --> B[核心问题/Problem<br>控制复杂非线性系统<br>Controlling Complex Nonlinear Systems]
    A --> C[主要方法/Method<br>SINDy + TD3 混合框架<br>SINDy + TD3 Hybrid Framework]
    A --> D[关键结果/Results<br>在双旋翼系统上性能更优<br>Superior Performance on Bi-rotor]
    B --> E[样本效率低<br>Sample Inefficiency]
    C --> F[SINDy识别模型<br>SINDy Identifies Model]
    C --> G[生成合成数据<br>Generates Synthetic Rollouts]
    C --> H[TD3学习策略<br>TD3 Learns Policy]
    D --> I[精度与鲁棒性提升<br>Improved Accuracy & Robustness]
    ```

- **[arXiv251225] Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends**
  - **tags:** [mlsys], [others], [multi-task time series forecasting, shared representation learning, cloud-native systems, resource contention, dynamic adjustment mechanism]
  - **authors:** Zixiao Huang, Jixiao Yang, Sijia Li, Chi Zhang, Jinyu Chen, Chengda Xu
  - **institution:** University of Washington, Westcliff University, University of Michigan, Northeastern University, University of Virginia
  - **link:** https://arxiv.org/pdf/2512.21102
  - **contributions:** 1. A unified forecasting framework with a shared encoding structure for high-dimensional, multi-task time series in cloud-native backends. 2. A cross-task structural propagation module to model complex dependencies among nodes caused by resource contention and service topology changes. 3. A dynamic adjustment mechanism that regulates internal feature flows to adapt to non-stationary behaviors like sudden load shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9838b10e22d58d1ccca2c68a210133b154701b0c5ff14deadba418ccdbdb92f2_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a unified forecasting framework for high-dimensional multi-task time series in cloud-native backend systems. The method uses shared representation learning, a structural propagation module, and a dynamic adjustment mechanism to model complex dependencies and adapt to non-stationary behaviors. Experimental results show the framework achieves superior performance and provides reliable predictive capability for dynamic cloud environments.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[高维多任务时序预测/High-Dimensional Multi-Task Forecasting]
    B --> B2[云原生后端动态环境/Cloud-Native Backend Dynamics]
    C --> C1[共享编码结构/Shared Encoding Structure]
    C --> C2[跨任务结构传播/Cross-Task Structural Propagation]
    C --> C3[动态调整机制/Dynamic Adjustment Mechanism]
    D --> D1[性能优越/Superior Performance]
    D --> D2[可靠预测能力/Reliable Predictive Capability]
    ```

- **[arXiv251225] Semi-Supervised Learning for Large Language Models Safety and Content Moderation**
  - **tags:** [nlp], [content moderation], [semi-supervised learning, data augmentation, safety classifiers, LLM safety, prompt harmfulness]
  - **authors:** Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea
  - **institution:** National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.21107
  - **contributions:** 1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --> B[核心问题 / Problem<br>依赖大量标注数据 / Reliance on large labeled data]
    A --> C[主要方法 / Method<br>半监督学习与任务特定增强 / SSL & Task-Specific Augmentation]
    A --> D[关键结果 / Results<br>性能显著提升 / Significant Performance Improvement]
    ```

- **[arXiv251225] Semantic Refinement with LLMs for Graph Representations**
  - **tags:** [ai], [graph representation learning], [graph neural network, large language model, semantic refinement, structure-semantics heterogeneity, data-centric adaptation]
  - **authors:** Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang
  - **institution:** University of Connecticut, University of Notre Dame
  - **link:** https://arxiv.org/pdf/2512.21106
  - **contributions:** 1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Semantic Refinement with LLMs for Graph Representations] --> B(核心问题/Problem: Graph structure-semantics heterogeneity 图的结构-语义异质性)
    A --> C(主要方法/Method: Data-Adaptive Semantic Refinement (DAS) framework 数据自适应语义精炼框架)
    A --> D(关键结果/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs 提升结构主导图性能，在语义丰富图上保持竞争力)
    ```

- **[arXiv251225] STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting**
  - **tags:** [ai], [diffusion models], [precipitation nowcasting, latent diffusion model, spatio-temporal prediction, variational autoencoder, conditioning network]
  - **authors:** Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong
  - **institution:** The Hong Kong University of Science and Technology, Hong Kong Observatory
  - **link:** https://arxiv.org/pdf/2512.21118
  - **code:** https://github.com/sqfoo/stldm_official
  - **contributions:** 1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]
    A --> C[主要方法/Method: 两阶段潜扩散模型]
    A --> D[关键结果/Results: 性能优越，推理高效]
    ```

- **[arXiv251225] A Mechanistic Analysis of Transformers for Dynamical Systems**
  - **tags:** [ai], [dynamical systems modeling], [transformers, attention mechanism, dynamical systems, representational analysis, delay embedding]
  - **authors:** Gregory Duthé, Nikolaos Evangelou, Wei Liu, Ioannis G. Kevrekidis, Eleni Chatzi
  - **institution:** ETH Zürich, Johns Hopkins University, Singapore-ETH Centre
  - **link:** https://arxiv.org/pdf/2512.21113
  - **contributions:** 1. Provides a mechanistic interpretation of causal self-attention in Transformers as a linear, history-dependent recurrence from a dynamical systems perspective. 2. Demonstrates that the softmax attention's convexity constraint fundamentally limits the representation of certain linear dynamics, leading to oversmoothing. 3. Shows that for nonlinear partially observable systems, attention acts as an adaptive delay-embedding mechanism for state reconstruction.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad3a459d499a20015cb02bd1e25bf788476d675d70043dd137b07a4b0f35b55_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the representational capabilities of single-layer Transformers for modeling dynamical systems, interpreting attention as a history-dependent recurrence. Through linear and nonlinear case studies, it finds that softmax attention restricts representable linear dynamics but can enable state reconstruction in nonlinear systems via adaptive delay embedding. The work bridges empirical Transformer performance with classical dynamical systems theory.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题/Paper Title: A Mechanistic Analysis of Transformers for Dynamical Systems] --> B[核心问题/Problem: Transformers as black boxes for time-series, lack of dynamical systems theory understanding];
    A --> C[主要方法/Method: Interpret causal self-attention as linear recurrence, analyze via linear/nonlinear case studies];
    A --> D[关键结果/Results: Softmax restricts linear dynamics (oversmoothing), attention enables nonlinear state reconstruction via delay embedding];
    ```

- **[arXiv251225] AutoBaxBuilder: Bootstrapping Code Security Benchmarking**
  - **tags:** [sec], [code security evaluation], [automated benchmarking, LLM-generated code, security vulnerabilities, exploit generation, plausibility checks]
  - **authors:** Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev
  - **institution:** ETH Zurich, Snyk, INSAIT (Sofia University "St. Kliment Ohridski")
  - **link:** https://arxiv.org/pdf/2512.21132
  - **code:** https://github.com/eth-sri/autobaxbuilder
  - **contributions:** 1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]
    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]
    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]
    ```

- **[arXiv251225] MODE: Multi-Objective Adaptive Coreset Selection**
  - **tags:** [mlsys], [others], [coreset selection, submodular maximization, data efficiency, adaptive weighting, multi-objective optimization]
  - **authors:** Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui
  - **institution:** CRIL, Université d'Artois
  - **link:** https://arxiv.org/pdf/2512.21152
  - **contributions:** 1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MODE: Multi-Objective Adaptive Coreset Selection] --> B[核心问题/Problem: Static coreset selection methods cannot adapt to changing data utility during training.]
    A --> C[主要方法/Method: Dynamic, multi-objective framework that adaptively weights selection strategies based on training phase.]
    A --> D[关键结果/Results: (1-1/e)-approximation, O(n log n) complexity, reduced memory, interpretable insights.]
    ```

- **[arXiv251225] ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update**
  - **tags:** [mlsys], [on-device ai], [spiking neural network, self-supervised learning, structured sparsity, activity-dependent update, event-driven processing]
  - **authors:** Zhe Su, Giacomo Indiveri
  - **institution:** Institute of Neuroinformatics, University of Zurich and ETH Zurich
  - **link:** https://arxiv.org/pdf/2512.21153
  - **contributions:** 1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[ElfCore] --> B[核心问题/Problem]
    A --> C[主要方法/Method]
    A --> D[关键结果/Results]
    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]
    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]
    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]
    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]
    D --> D1[功耗降低16倍/16x Lower Power]
    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]
    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]
    ```

- **[arXiv251225] BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft**
  - **tags:** [sys], [distributed consensus], [Raft, election timeout, contextual bandits, LinUCB, fault tolerance]
  - **authors:** Qizhi Wang
  - **institution:** PingCAP, Data & AI-Innovation Lab
  - **link:** https://arxiv.org/pdf/2512.21165
  - **contributions:** 1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft] --> B(核心问题/Problem: Brittle randomized timeouts under long-tail latency & jitter)
    A --> C(主要方法/Method: Lightweight online adaptation with contextual bandits & safe exploration)
    A --> D(关键结果/Results: Reduces recovery/unwritable time in WAN, competitive in stable settings)
    ```

- **[arXiv251225] A Community-Enhanced Graph Representation Model for Link Prediction**
  - **tags:** [ai], [graph representation learning], [Graph Neural Networks, Link Prediction, Community Detection, Structure Enhancement]
  - **authors:** Lei Wang, Darong Lai
  - **institution:** Southeast University
  - **link:** https://arxiv.org/pdf/2512.21166
  - **contributions:** 1. Proposes a Community-Enhanced Link Prediction (CELP) framework that integrates community structure to jointly model local and global graph topology. 2. Introduces a graph enhancement technique via community-aware, confidence-guided edge completion and pruning. 3. Integrates multi-scale structural features to improve link prediction accuracy, with experimental validation showing superior performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154686865f18f6411728ca7ec41c4f9a507cd6a00a6b336c6fbfca553149d9b1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitation of Graph Neural Networks (GNNs) in link prediction, where they often underperform traditional heuristic methods due to over-reliance on local information. It proposes the CELP framework, which incorporates community structure to enhance the graph and capture multi-scale features. Experimental results show CELP achieves superior performance, validating the importance of community structure for accurate link prediction.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[A Community-Enhanced Graph Representation Model for Link Prediction] --> B[核心问题/Problem: GNNs for link prediction underperform traditional heuristics due to local focus and over-smoothing.]
        A --> C[主要方法/Method: Proposes CELP framework integrating community structure for graph enhancement and multi-scale feature learning.]
        A --> D[关键结果/Results: Superior performance on benchmarks, validating the role of community structure.]
    ```

- **[arXiv251225] A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine**
  - **tags:** [ai], [machine learning for healthcare], [Universum learning, GEPSVM, EEG classification]
  - **authors:** Yogesh Kumar, Vrushank Ahire, M. A. Ganaie
  - **institution:** Indian Institute of Technology Ropar
  - **link:** https://arxiv.org/pdf/2512.21170
  - **contributions:** 1. Proposes U-GEPSVM, a novel classifier integrating Universum constraints into the GEPSVM framework via a ratio-based objective function. 2. Introduces IU-GEPSVM, an improved variant with a weighted difference-based formulation for enhanced stability and independent control over class separation and Universum alignment. 3. Demonstrates the effectiveness of the proposed models for EEG seizure detection, achieving superior accuracy on the Bonn University dataset and validating improvements with rigorous statistical tests.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ebfba0923a4428be5bd05b82efd0f6ff72c162a8ee710189c4ccb146c2374f3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes two novel classifiers, U-GEPSVM and IU-GEPSVM, which combine the computational efficiency of generalized eigenvalue decomposition with Universum learning to improve EEG seizure detection. The models are designed to handle challenges like non-stationarity and limited labeled data in EEG signals. Evaluation on the Bonn University dataset shows that IU-GEPSVM outperforms baseline methods, providing an efficient and reliable solution for neurological diagnosis.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[论文标题 / Paper Title: A Unified Framework for EEG Seizure Detection] --> B(核心问题 / Problem: EEG信号分类挑战 / EEG Signal Classification Challenges)
    A --> C(主要方法 / Method: 提出U-GEPSVM和IU-GEPSVM / Propose U-GEPSVM and IU-GEPSVM)
    A --> D(关键结果 / Results: IU-GEPSVM取得更高准确率 / IU-GEPSVM Achieves Higher Accuracy)
    B --> B1(非平稳性, 低信噪比, 标记数据有限 / Non-stationarity, Low SNR, Limited Labeled Data)
    C --> C1(结合通用集学习和广义特征值分解 / Integrates Universum Learning & GEPSVM)
    D --> D1(O vs S: 85%峰值准确率 / O vs S: 85% Peak Accuracy)
    D --> D2(Z vs S: 80%峰值准确率 / Z vs S: 80% Peak Accuracy)
    ```

- **[arXiv251225] Analytic and Variational Stability of Deep Learning Systems**
  - **tags:** [ai], [learning theory], [Learning Stability Profile, Lyapunov methods, Clarke generalized derivatives, stability exponents, energy-dissipative systems]
  - **authors:** Ronald Katende
  - **institution:** Kabale University
  - **link:** https://arxiv.org/pdf/2512.21208
  - **contributions:** 1. Proposes a unified analytic and variational framework for studying stability in deep learning systems via a central object called the Learning Stability Profile (LSP). 2. Proves a Fundamental Analytic Stability Theorem linking uniform boundedness of stability signatures to the existence of a dissipative Lyapunov-type energy. 3. Extends the theory to non-smooth learning systems (e.g., ReLU networks, proximal updates) using Clarke generalized derivatives and variational Lyapunov functionals.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43ea5a1a8d6262f5e2056565c57706a69bbe8f558d9c82e401fa498ff92d5daa_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a unified framework to analyze the stability of deep learning systems by viewing them as coupled representation-parameter dynamics. The core method introduces a Learning Stability Profile and proves a theorem linking bounded stability signatures to a dissipative Lyapunov energy, which is extended to non-smooth cases using generalized derivatives. The main conclusion is that this provides a single, foundational dynamical description of stability that unifies and explains robustness across various architectures and optimization methods.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Analytic and Variational Stability of Deep Learning Systems] --> B[核心问题/Problem: 缺乏统一的深度学习稳定性数学描述 / Lack of unified mathematical description of stability in deep learning]
    A --> C[主要方法/Method: 提出学习稳定性剖面与解析-变分框架 / Proposes Learning Stability Profile and analytic-variational framework]
    A --> D[关键结果/Results: 证明基本稳定性定理，统一平滑与非平滑系统的稳定性分析 / Proves Fundamental Stability Theorem, unifying stability analysis for smooth and non-smooth systems]
    ```

- **[arXiv251225] MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models**
  - **tags:** [ai], [reinforcement learning], [latent solvability, mid-stage scientific training, chemical reasoning, rule-based rewards, symbolic competence]
  - **authors:** Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller
  - **institution:** École Polytechnique Fédérale de Lausanne (EPFL), University of New South Wales (UNSW), Green Dynamics, Shanghai Artificial Intelligence Laboratory
  - **link:** https://arxiv.org/pdf/2512.21231
  - **contributions:** 1. Identifies two necessary conditions for RL-based chemical reasoning: symbolic competence and latent chemical knowledge. 2. Proposes MiST (mid-stage scientific training), a set of techniques including data-mixing with SMILES/CIF-aware pre-processing and continued pre-training. 3. Demonstrates that MiST significantly improves latent solvability and enables RL to achieve large accuracy gains on challenging chemical tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that reinforcement learning for chemical reasoning fails unless the base model already has some latent solvability. The authors propose MiST, a mid-stage training pipeline involving data pre-processing and continued pre-training, to build the necessary prerequisites. Their method substantially boosts model performance on tasks like organic reaction naming and inorganic material generation, establishing clear prerequisites for training chemical reasoning models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[MiST: 化学推理模型中的中期科学训练 / MiST: Mid-Stage Scientific Training for Chemical Reasoning] --> B[核心问题/Problem: RL需要潜在可解性 / RL requires latent solvability]
    A --> C[主要方法/Method: 中期科学训练 / Mid-stage scientific training (MiST)]
    A --> D[关键结果/Results: 准确率大幅提升 / Accuracy significantly improved]
    B --> B1[条件: 符号能力与潜在知识 / Conditions: Symbolic competence & latent knowledge]
    C --> C1[技术: 数据混合与持续预训练 / Techniques: Data-mixing & continued pre-training]
    D --> D1[有机反应命名: 10.9% -> 63.9% / Organic reaction naming: 10.9% -> 63.9%]
    D --> D2[无机材料生成: 40.6% -> 67.4% / Inorganic material generation: 40.6% -> 67.4%]
    ```

- **[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks**
  - **tags:** [sec], [adversarial attacks], [hard-label black-box attacks, query efficiency, ray search optimization, Nesterov's Accelerated Gradient, momentum-based optimization]
  - **authors:** Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma
  - **institution:** Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments
  - **link:** https://arxiv.org/pdf/2512.21241
  - **code:** https://github.com/machanic/hard_label_attacks
  - **contributions:** 1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]
    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]
    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]
    ```

- **[arXiv251225] LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation**
  - **tags:** [ai], [embodied ai], [scene graph, vision language model, dynamic planning, memory graph, graph augmentation]
  - **authors:** Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov
  - **institution:** MIRAI, Cognitive AI Systems Lab
  - **link:** https://arxiv.org/pdf/2512.21243
  - **code:** https://lookplangraph.github.io/
  - **contributions:** 1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];
    A --> C[主要方法/Method: Dynamic graph update via VLM observation];
    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];
    ```

- **[arXiv251225] Assessing the Software Security Comprehension of Large Language Models**
  - **tags:** [sec], [software security assessment], [Bloom's Taxonomy, knowledge boundary, misconception patterns]
  - **authors:** Mohammed Latif Siddiq, Natalie Sekerak, Antonio Karam, Maria Leal, Arvin Islam-Gomes, Joanna C. S. Santos
  - **institution:** University of Notre Dame
  - **link:** https://arxiv.org/pdf/2512.21238
  - **contributions:** 1. Introduced a systematic evaluation framework using Bloom's Taxonomy to assess LLMs' software security comprehension across six cognitive levels. 2. Proposed the concept of a "software security knowledge boundary" to identify the highest reliable cognitive performance level for an LLM. 3. Identified and documented 51 recurring misconception patterns made by LLMs in software security tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3809be1ff4cae9ad7acb47676829cd58ca3ea614efa57d9121857f85d97fc7_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically evaluates the software security comprehension of five leading LLMs using Bloom's Taxonomy as a framework across diverse datasets. The results show that while LLMs perform well on lower-level cognitive tasks like recalling facts, their performance significantly degrades on higher-order tasks requiring reasoning and secure system creation. The study introduces a knowledge boundary to quantify reliable performance limits and identifies common misconception patterns.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Assessing LLM Software Security Comprehension<br/>评估LLM软件安全理解] --> B{核心问题/Problem};
    A --> C{主要方法/Method};
    A --> D{关键结果/Results};
    B --> B1[LLMs' Security Expertise Unclear<br/>LLM安全专业知识不明];
    C --> C1[Framework: Bloom's Taxonomy<br/>框架: 布鲁姆分类法];
    C --> C2[Datasets: MCQs, Code, Courses, Case Studies<br/>数据集: 选择题, 代码, 课程, 案例];
    D --> D1[Good on Low-Level Tasks<br/>低级任务表现好];
    D --> D2[Poor on High-Order Reasoning<br/>高阶推理表现差];
    D --> D3[Knowledge Boundary & Misconceptions<br/>知识边界与误解模式];
    ```

- **[arXiv251225] Model Merging via Multi-Teacher Knowledge Distillation**
  - **tags:** [ai], [model merging], [model merging, knowledge distillation, PAC-Bayes, sharpness-aware minimization, multi-task learning]
  - **authors:** Seyed Arshan Dalili, Mehrdad Mahdavi
  - **institution:** The Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2512.21288
  - **code:** https://github.com/arshandalili/SAMerging
  - **contributions:** 1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a "cross-task heterogeneity" term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Model Merging via Multi-Teacher Knowledge Distillation] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[缺乏理论保证/Lack of Theoretical Guarantees]
    B --> B2[启发式方法不稳定/Heuristic Methods are Brittle]
    C --> C1[理论: 平坦性感知PAC-Bayes边界/Theory: Flatness-aware PAC-Bayes Bound]
    C --> C2[框架: 多教师知识蒸馏/Framework: Multi-Teacher Knowledge Distillation]
    C --> C3[方法: SAMerging/Method: SAMerging]
    D --> D1[新SOTA/New SOTA]
    D --> D2[高数据效率/High Data Efficiency]
    ```

- **[arXiv251225] Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering**
  - **tags:** [ai], [computational drug design], [metaheuristic assembly, de novo drug generation, transcriptomics, molecular docking, multi-objective optimization]
  - **authors:** Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H.M. Abou-El-Enien
  - **institution:** Faculty of Computers and Artificial Intelligence, Cairo University
  - **link:** https://arxiv.org/pdf/2512.21301
  - **contributions:** 1. A novel end-to-end computational framework that integrates patient transcriptomics with de novo drug generation for personalized AML therapy. 2. Development of a reaction-first evolutionary metaheuristic algorithm for assembling novel ligands guided by target-specific structural hotspots. 3. Validation of generated drug candidates through in-silico ADMET profiling and molecular docking, demonstrating pharmacologically viable leads.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8733d98ab11f787e5d221faf5b5b5383a6af0c23974a0e3febf13cb0aa80b4f3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a computational framework for personalized drug discovery in Acute Myeloid Leukemia (AML). It uses patient transcriptome data to identify biomarkers, models their 3D structures, and employs a novel metaheuristic algorithm to generate new drug-like molecules targeting these biomarkers. The results show the framework can produce viable drug candidates, offering a scalable approach for precision oncology.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Transcriptome-Conditioned Personalized De Novo Drug Generation] --> B[核心问题/Problem: AML分子异质性与复发率高/AML's molecular heterogeneity & high relapse]
        A --> C[主要方法/Method: 元启发式组装与靶向过滤/Metaheuristic Assembly & Target-Driven Filtering]
        A --> D[关键结果/Results: 生成具有药理活性的先导化合物/Generated pharmacologically viable leads]
    ```

- **[arXiv251225] Learning to Solve PDEs on Neural Shape Representations**
  - **tags:** [cv], [geometry processing], [neural shape representations, mesh-free PDE solver, geometry-conditioned operator, surface PDEs, differentiable pipeline]
  - **authors:** Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra
  - **institution:** University College London, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.21311
  - **contributions:** 1. A novel, mesh-free formulation that learns a local update operator conditioned on neural shape attributes to solve surface PDEs directly on neural representations. 2. A method that is trained once on a single shape and generalizes across shape and topology variations, enabling fast inference without per-instance optimization. 3. The first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations, preserving differentiability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed9eac0c574821f5dc10188c61bf1bd40ede7fca32ee8f98accf90b52993315f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the mismatch between traditional mesh-based PDE solvers and modern neural shape representations by proposing a learned, mesh-free operator. This operator, trained once, solves surface PDEs directly on neural data, generalizing across shapes and preserving differentiability. The method performs comparably to classical solvers and enables the first end-to-end PDE-solving pipeline for neural surfaces.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Learning to Solve PDEs on Neural Shape Representations] --> B(核心问题/Problem: Mismatch between mesh-based solvers and neural shape data)
    A --> C(主要方法/Method: Mesh-free, learned local operator conditioned on shape)
    A --> D(关键结果/Results: Generalizes across shapes, close to FEM, enables end-to-end pipeline)
    ```

- **[arXiv251225] Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks**
  - **tags:** [ai], [information theory, statistical learning], [data processing inequality, Bayes classifier, low-level processing, classification accuracy, finite sample analysis]
  - **authors:** Roy Turgeman, Tom Tirer
  - **institution:** Bar-Ilan University
  - **link:** https://arxiv.org/pdf/2512.21315
  - **contributions:** 1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Does the Data Processing Inequality Reflect Practice?<br/>数据加工不等式反映实践吗?] --> B(核心问题/Problem: Low-level processing is common in practice but seems to contradict the Data Processing Inequality.<br/>实践中的低级处理似乎与数据加工不等式矛盾)
    A --> C(主要方法/Method: Theoretical study of a binary classifier + empirical validation on deep networks.<br/>二元分类器的理论研究+深度网络的实证验证)
    A --> D(关键结果/Results: For finite samples, pre-processing can improve accuracy; gain depends on dataset properties.<br/>对于有限样本，预处理可提高精度；收益取决于数据集属性)
    ```

- **[arXiv251225] Parallel Token Prediction for Language Models**
  - **tags:** [mlsys], [llm inference], [parallel token prediction, speculative decoding, autoregressive decoding, transformer inference, latency optimization]
  - **authors:** Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt
  - **institution:** University of California, Irvine, Chan-Zuckerberg Initiative, Pyramidal AI
  - **link:** https://arxiv.org/pdf/2512.21323
  - **contributions:** 1. Proposes Parallel Token Prediction (PTP), a universal framework for parallel sequence generation that jointly predicts multiple dependent tokens in a single transformer call. 2. Proves that PTP can represent arbitrary autoregressive sequence distributions, avoiding the restrictive independence assumptions of prior multi-token prediction methods. 3. Demonstrates state-of-the-art speculative decoding performance, accepting over four tokens per step on Spec-Bench with Vicuna-7B, showing parallel long-sequence generation is feasible without losing modeling power.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high latency of autoregressive decoding in large language models by proposing Parallel Token Prediction (PTP), a framework that predicts multiple dependent tokens in parallel within a single transformer call. It proves PTP's universality in representing autoregressive distributions and shows it achieves superior speculative decoding performance, enabling faster text generation without sacrificing quality.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Parallel Token Prediction for Language Models] --> B[核心问题/Problem: Autoregressive decoding latency bottleneck]
    A --> C[主要方法/Method: Parallel Token Prediction (PTP), joint prediction of dependent tokens]
    A --> D[关键结果/Results: State-of-the-art speculative decoding, >4 tokens/step, universal framework]
    ```

- **[arXiv251225] Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation**
  - **tags:** [other], [scientific computing], [first-order system least squares, reduced basis neural operator, a posteriori error estimation]
  - **authors:** Yuan Qiu, Wolfgang Dahmen, Peng Chen
  - **institution:** Georgia Institute of Technology, University of South Carolina
  - **link:** https://arxiv.org/pdf/2512.21319
  - **contributions:** 1. Proposes a variationally correct operator learning framework using First-Order System Least-Squares (FOSLS) objectives that are provably equivalent to solution error in PDE-induced norms. 2. Introduces a Reduced Basis Neural Operator (RBNO) that predicts coefficients for a pre-computed, conforming reduced basis to ensure function space conformity and variational stability. 3. Provides a rigorous convergence analysis that decomposes the total error into discretization bias, basis truncation error, neural network approximation error, and statistical estimation errors, with the residual serving as a reliable a posteriori error estimator.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a56bc0fe5da5167f8e01f84491e1f4cb6dfb3c8d8926f4a22fe5ff594010bf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the issue of "variational correctness" in neural operators for solving PDEs, where standard residual losses do not guarantee small solution errors. The authors propose a new framework using a First-Order System Least-Squares (FOSLS) loss and a Reduced Basis Neural Operator (RBNO) to ensure stability and norm equivalence. The method provides a rigorous error bound and demonstrates superior accuracy in PDE-compliant norms, with the residual acting as a reliable error estimator.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Variationally Correct Operator Learning<br>变分正确的算子学习] --> B[核心问题/Problem<br>Standard PDE-residual losses lack variational correctness<br>标准PDE残差损失缺乏变分正确性]
    A --> C[主要方法/Method<br>FOSLS objective + Reduced Basis Neural Operator (RBNO)<br>FOSLS目标 + 约简基神经算子]
    A --> D[关键结果/Results<br>Provable error equivalence & reliable a posteriori estimator<br>可证明的误差等价性 & 可靠的后验估计器]
    ```

- **[arXiv251225] Measuring all the noises of LLM Evals**
  - **tags:** [mlsys], [llm inference], [LLM evaluation, statistical noise, paired analysis, prediction variance, data variance]
  - **authors:** Sida Wang
  - **institution:** FAIR at Meta
  - **link:** https://arxiv.org/pdf/2512.21326
  - **contributions:** 1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the "all-pairs paired method" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an "all-pairs paired method" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Measuring all the noises of LLM Evals] --> B(核心问题/Problem: LLM评估中的统计噪声/Separating signal from noise in LLM evals)
    A --> C(主要方法/Method: 全配对分析法/All-pairs paired method)
    A --> D(关键结果/Results: 可预测的总噪声与主导的预测噪声/Predictable total noise & dominant prediction noise)
    ```

- **[arXiv251225] Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty**
  - **tags:** [mlsys], [diffusion models], [Denoising Entropy, Masked Diffusion Models, decoding path optimization, predictive uncertainty, non-autoregressive generation]
  - **authors:** Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin
  - **institution:** Zhejiang University, Westlake University, University of Chicago
  - **link:** https://arxiv.org/pdf/2512.21336
  - **code:** https://github.com/LINs-lab/DenoisingEntropy
  - **contributions:** 1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>通过量化不确定性优化掩码扩散模型的解码路径] --> B(核心问题/Problem: MDMs生成质量对解码顺序敏感<br/>MDM output quality is sensitive to decoding order)
        A --> C(主要方法/Method: 提出去噪熵和路径优化算法<br/>Propose Denoising Entropy & path optimization algorithms)
        A --> D(关键结果/Results: 熵引导方法提升生成质量<br/>Entropy-guided methods improve generation quality)
    ```

- **[arXiv251225] Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding**
  - **tags:** [ai], [optimization], [least absolute deviations, piecewise affine lower-bounding, linear programming, robust regression, exact algorithm]
  - **authors:** Stefan Volz, Martin Storath, Andreas Weinmann
  - **institution:** Technische Hochschule Würzburg-Schweinfurt
  - **link:** https://arxiv.org/pdf/2512.20682
  - **contributions:** 1. Proposes the Piecewise Affine Lower-Bounding (PALB) method, a novel exact algorithm for LAD line fitting. 2. Provides theoretical proof of correctness and bounds on the number of iterations for the algorithm. 3. Demonstrates empirical log-linear scaling and superior speed compared to existing LP-based and IRLS-based solvers on synthetic and real-world data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b21c32d0f735b0e43e7203101aa47a6f9a1a7a2ca1eed65c3391f51147bd5e6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational challenge of exact Least Absolute Deviations (LAD) line fitting, a robust regression method. It proposes the PALB algorithm, which uses piecewise-affine lower bounds and a subdivision scheme to find the exact solution efficiently. The method is proven correct, shows log-linear empirical scaling, and is faster than existing practical solvers like linear programming and iteratively reweighted least squares.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Fast and Exact LAD Line Fitting via PALB] --> B[核心问题/Problem: LAD拟合计算复杂，现有高效算法难实现/LAD fitting is computationally involved, existing efficient algorithms are hard to implement]
    A --> C[主要方法/Method: 使用分段仿射下界和细分方案/Use piecewise affine lower-bounding and a subdivision scheme]
    A --> D[关键结果/Results: 算法精确、对数线性扩展、速度快于LP和IRLS/Algorithm is exact, log-linear scaling, faster than LP & IRLS]
    ```

- **[arXiv251225] Diffusion Models in Simulation-Based Inference: A Tutorial Review**
  - **tags:** [ai], [diffusion models], [simulation-based inference, score-based models, parameter estimation, generative modeling]
  - **authors:** Jonas Arruda, Niels Bracher, Ullrich Köthe, Jan Hasenauer, Stefan T. Radev
  - **institution:** University of Bonn, Rensselaer Polytechnic Institute, Heidelberg University
  - **link:** https://arxiv.org/pdf/2512.20685
  - **contributions:** 1. Provides a comprehensive tutorial review synthesizing recent developments on diffusion models for Simulation-Based Inference (SBI)., 2. Highlights and discusses key design choices and concepts affecting SBI performance, such as guidance, score composition, flow matching, and noise schedules., 3. Illustrates the application of diffusion models in SBI through case studies and outlines open questions for future research.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/024277637e6ce2f35e35619e320a14456fb6904af22070de1a30be768a5a2475_w640_q70.webp
  - **Simple LLM Summary:** This tutorial review synthesizes how diffusion models, through their score-based formulation, are used for fast and accurate parameter estimation in Simulation-Based Inference (SBI). It covers key design choices in training and inference, discusses factors affecting efficiency and accuracy, and presents case studies to illustrate the concepts.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[Diffusion Models in SBI: A Tutorial Review] --> B[核心问题/Problem: 如何从模拟和真实数据中推断潜在参数/How to infer latent parameters from simulated and real data?];
        A --> C[主要方法/Method: 使用基于分数的扩散模型学习条件或联合分布/Using score-based diffusion models to learn conditional or joint distributions];
        A --> D[关键结果/Results: 综述了设计选择、概念与应用，并指出未来方向/Reviewed design choices, concepts, applications, and outlined future directions];
    ```

- **[arXiv251225] Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights**
  - **tags:** [ai], [classification evaluation], [Matthews Correlation Coefficient, weighted performance measure, multiclass classification, robustness analysis, confusion matrix]
  - **authors:** Rommel Cortez, Bala Krishnamoorthy
  - **institution:** Washington State University
  - **link:** https://arxiv.org/pdf/2512.20811
  - **contributions:** 1. Proposes a new weighted version of the Pearson-Matthews Correlation Coefficient (MCC) for binary and multiclass classification that is sensitive to individual observation weights. 2. Proves the robustness of the proposed weighted measures, showing that changes in weights by at most ε lead to bounded changes in the measure value (by a factor of ε for binary and ε² for multiclass). 3. Demonstrates empirically that the weighted measures can effectively distinguish classifiers based on their performance on highly weighted observations, unlike standard unweighted measures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/277843f9180deb3e0b651c6e34535bdaba884d63d88920a3f85e89a8483c52d1_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of performance measures for classification that account for individually weighted observations. It proposes a weighted version of the Matthews Correlation Coefficient (MCC) for binary and multiclass tasks, which is proven to be robust to weight perturbations. The results show that this new measure successfully identifies classifiers that perform well on more important, highly weighted data points.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Weighted MCC] --> B[核心问题/Problem: 现有分类评估指标不适用于带权重的观测值/Existing measures ignore individual observation weights]
    A --> C[主要方法/Method: 提出加权MCC及其多分类扩展/Propose weighted MCC and multiclass extensions]
    A --> D[关键结果/Results: 指标对权重变化鲁棒，能识别在重要样本上性能好的分类器/Measures are robust and identify classifiers good on high-weight observations]
    ```

- **[arXiv251225] A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations**
  - **tags:** [other], [astro-physics, solar physics, magnetohydrodynamics], [Physics-Informed Neural Network (PINN), MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT), solar active regions (ARs), toroidal bands (toroids), state-vector reconstruction]
  - **authors:** Subhamoy Chatterjee, Mausumi Dikpati
  - **institution:** Southwest Research Institute, High Altitude Observatory (NSF-NCAR)
  - **link:** https://arxiv.org/pdf/2512.20747
  - **contributions:** 1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method's ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.
  - **Mindmap:**

    ```mermaid
    graph LR
        A[PINNBARDS: 从全球活动区观测推导MHD状态向量 / PINNBARDS: Deriving MHD State Vectors From Global Active Regions Observations] --> B(核心问题/Problem: 表面磁图仅提供活动区分布的几何形状，无法提供初始化MHD模型所需的自洽状态向量 / Problem: Surface magnetograms only provide geometric shape of AR distribution, not the self-consistent state-vector needed to initialize MHD models.)
        A --> C(主要方法/Method: 开发PINNBARDS，一个基于物理信息神经网络(PINN)的模拟器，使用观测到的环形带和MHD-SWT方程来推导初始状态向量 / Method: Develop PINNBARDS, a PINN-based simulator using observed toroids and MHD-SWT equations to derive the initial state-vector.)
        A --> D(关键结果/Results: PINN收敛到物理一致解，与观测匹配；最佳参数为20-30 kG环形场和~10度带宽 / Results: PINN converges to physically consistent solutions matching observations; optimal parameters are 20-30 kG toroidal field and ~10 degree bandwidth.)
    ```

- **[arXiv251225] GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model**
  - **tags:** [ai], [speech separation], [target speaker extraction, generative language model, coarse-to-fine, exposure bias, direct preference optimization]
  - **authors:** Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng
  - **institution:** Nanyang Technological University, Southeast University
  - **link:** https://arxiv.org/pdf/2512.20978
  - **contributions:** 1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model's outputs with human perceptual preferences.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[GenTSE] --> B[核心问题/Problem: TSE generalization & fidelity];
    A --> C[主要方法/Method: Two-stage generative LM, FLC, DPO];
    A --> D[关键结果/Results: Surpasses prior LM-based systems];
    ```

- **[arXiv251225] Enhancing diffusion models with Gaussianization preprocessing**
  - **tags:** [mlsys], [diffusion models], [Gaussianization, bifurcation, sampling efficiency, generative models, data preprocessing]
  - **authors:** Li Cunzhi, Louis Kang, Hideaki Shimazaki
  - **institution:** Kyoto University, RIKEN Center for Brain Science
  - **link:** https://arxiv.org/pdf/2512.21020
  - **contributions:** 1. Proposes a novel Gaussianization preprocessing step for training data to align the target distribution with the initial Gaussian noise in diffusion models. 2. Aims to mitigate the bifurcation-related sampling inefficiency, particularly improving early-stage reconstruction quality. 3. Enables more stable and efficient sampling, especially beneficial for small-scale network architectures.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1994287172f7f096232171fd21b57b4b1d23f670ef89d86fdbadc32dceabbf46_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the slow sampling problem in diffusion models, which is caused by a delay before trajectory bifurcation. The authors propose applying Gaussianization preprocessing to the training data to make it more closely resemble the initial Gaussian noise, simplifying the model's learning task. This method improves generation quality in the early reconstruction stages and enables more efficient sampling.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Enhancing diffusion models with Gaussianization preprocessing<br>增强扩散模型的Gaussianization预处理] --> B[Problem: Slow sampling due to bifurcation delay<br>问题: 分岔延迟导致采样慢]
    A --> C[Method: Gaussianization preprocessing of training data<br>方法: 训练数据的Gaussianization预处理]
    A --> D[Results: Improved early-stage quality & efficient sampling<br>结果: 提升早期生成质量与高效采样]
    ```

- **[arXiv251225] Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization**
  - **tags:** [ai], [optimization theory], [Euclidean distance degree, overparametrization, algebraic geometry, degenerate metrics, critical points]
  - **authors:** Giovanni Luca Marchetti, Erin Connelly, Paul Breiding, Kathlén Kohn
  - **institution:** KTH Royal Institute of Technology, University of Osnabrück, Digital Futures
  - **link:** https://arxiv.org/pdf/2512.21029
  - **contributions:** 1. Relates degenerate quadratic optimization problems to nondegenerate ones via a projection, revealing the role of the projection's ramification locus. 2. Provides tools for counting the number of critical points over projective varieties in the degenerate setting. 3. Extends the theory of Euclidean distance degree (EDD) to the overparametrized regime, bridging algebraic geometry with machine learning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a41be3cd2e9b4a085f8c60d76c30d977b51dcb4e0c2ab2a8f0cb584bc2b6a827_w640_q70.webp
  - **Simple LLM Summary:** This paper studies the critical points of a degenerate quadratic objective over an algebraic variety, a scenario arising in overparametrized machine learning. The main method connects the degenerate problem to a nondegenerate one via a projection, highlighting the importance of the projection's ramification locus. The work extends the Euclidean distance degree framework to the degenerate setting and provides tools for analyzing optimization landscapes in overparametrized models.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Critical Points of Degenerate Metrics<br>退化度量临界点] --> B[核心问题/Problem<br>Overparametrized quadratic optimization<br>过参数化二次优化];
    A --> C[主要方法/Method<br>Projection to nondegenerate problem<br>投影至非退化问题];
    A --> D[关键结果/Results<br>Ramification locus role & EDD extension<br>分歧轨迹作用与EDD扩展];
    ```

- **[arXiv251225] Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments**
  - **tags:** [ai], [Bayesian nonparametric models], [Poisson Hierarchical Indian Buffet Process, sparse count data, infectious disease prediction, Bayesian machine learning, microbiome unseen species problems]
  - **authors:** Edwin Fong, Lancelot F. James, Juho Lee
  - **institution:** The University of Hong Kong (HKU), The Hong Kong University of Science and Technology (HKUST), Korea Advanced Institute of Science and Technology (KAIST)
  - **link:** https://arxiv.org/pdf/2512.21005
  - **contributions:** 1. Introduces the PHIBP, a Bayesian nonparametric model, for robustly modeling sparse count data in epidemiology. 2. Demonstrates the model's ability to borrow statistical strength from related regions to predict outbreaks in areas with zero historical cases. 3. Provides a unified framework that yields accurate predictions and meaningful epidemiological insights (e.g., alpha/beta diversity) in data-sparse environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71507d2ce74cd2738ff8811f5abfd9a141d92f2c2fbc07f24c76785e9d864c9e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of predicting infectious disease outbreaks in regions with historically zero reported cases. It proposes using the Poisson Hierarchical Indian Buffet Process (PHIBP), a Bayesian model that leverages information from neighboring areas to handle sparse count data. The experiments show this approach provides accurate outbreak forecasts and useful epidemiological insights in data-scarce settings.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Learning from Neighbors with PHIBP<br>基于PHIBP向邻居学习] --> B[核心问题/Problem<br>Predicting outbreaks in regions with zero historical cases<br>预测零病例地区的疫情]
    A --> C[主要方法/Method<br>Poisson Hierarchical Indian Buffet Process (PHIBP)<br>泊松分层印度自助餐过程]
    A --> D[关键结果/Results<br>Accurate predictions & epidemiological insights in sparse data<br>稀疏数据下的准确预测与流行病学洞见]
    ```

- **[arXiv251225] Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks**
  - **tags:** [ai], [cheminformatics], [Clever Hans, shortcut learning, activity prediction, author-disjoint splits, CHEMBL]
  - **authors:** Andrew D. Blevins, Ian K. Quigley
  - **institution:** (Institution not explicitly stated in the provided content. Based on the authors' names and arXiv submission, it cannot be reliably inferred without full paper affiliations.)
  - **link:** https://arxiv.org/pdf/2512.20924
  - **contributions:** 1. Demonstrates that machine learning models can predict the author of a molecule from its structure alone with high accuracy, revealing distinctive "chemist style" signals in public datasets. 2. Shows that an activity prediction model using only inferred author probabilities (and a protein ID) performs comparably to a structure-based baseline, exposing a "Clever Hans" failure mode where models exploit chemist intent rather than learning causal chemistry. 3. Analyzes the sources of this data leakage and proposes author-disjoint dataset splits as a mitigation strategy to decouple chemist intent from biological outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0714bbaf21a9c4a2f8221248ca78fbc74ff052dffe780f1607a9187bc7047174_w640_q70.webp
  - **Simple LLM Summary:** The paper investigates whether machine learning models for molecular activity prediction exploit "chemist style" signals rather than learning causal chemistry. By training a classifier to predict molecule authors from structure and then using only the author probabilities to predict bioactivity, the authors show that models can achieve competitive performance without direct structural input. This reveals a shortcut learning problem in cheminformatics benchmarks, prompting recommendations for author-disjoint data splits to mitigate intent leakage.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Clever Hans in Chemistry<br>化学中的汉斯效应] --> B{核心问题/Problem};
    A --> C{主要方法/Method};
    A --> D{关键结果/Results};
    B --> B1[Can models identify chemist from molecule?<br>模型能从分子识别化学家吗?];
    B --> B2[Do models exploit intent, not chemistry?<br>模型利用意图而非化学原理吗?];
    C --> C1[Link CHEMBL assays to authors<br>关联CHEMBL实验与作者];
    C --> C2[Train author classifier from fingerprints<br>从指纹训练作者分类器];
    C --> C3[Train activity model on author probabilities<br>基于作者概率训练活性模型];
    D --> D1[Author prediction is accurate<br>作者预测准确];
    D --> D2[Author-only model matches baseline<br>仅作者模型媲美基线];
    D --> D3[Reveals Clever Hans failure<br>揭示汉斯效应失败模式];
    ```

- **[arXiv251225] Causal-driven attribution (CDA): Estimating channel influence without user-level data**
  - **tags:** [ai], [causal inference], [PCMCI, Structural Causal Model, privacy-first analytics, marketing attribution, temporal causal discovery]
  - **authors:** Georgios Filippou, Boi Mai Quach, Diana Lenghel, Arthur White, Ashish Kumar Jha
  - **institution:** Trinity College Dublin
  - **link:** https://arxiv.org/pdf/2512.21211
  - **contributions:** 1. Proposes a Causal-Driven Attribution (CDA) framework that estimates channel influence using only aggregated impression-level data, eliminating the need for user-level path data. 2. Integrates temporal causal discovery (PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions. 3. Demonstrates the framework's accuracy and robustness on synthetic data, showing it captures cross-channel interdependencies while providing a privacy-preserving, scalable alternative to traditional models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/375f379a2c16450357d40be6e36b5fe98344711a4e78be19f7e0ad3d106a71cf_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Causal-Driven Attribution (CDA), a framework that uses aggregated impression data and causal inference techniques to estimate marketing channel influence without user-level tracking. It combines temporal causal discovery (PCMCI) with structural causal modeling to quantify channel contributions to conversions. The method shows strong accuracy in synthetic experiments and offers a privacy-preserving, scalable solution for attribution modeling.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Causal-driven attribution (CDA): Estimating channel influence without user-level data] --> B[核心问题/Problem: Attribution models rely on inaccessible user-level data due to privacy regulations.]
    A --> C[主要方法/Method: Integrates PCMCI for temporal causal discovery and Structural Causal Model for effect estimation using aggregated data.]
    A --> D[关键结果/Results: Achieves low relative RMSE (9.50%-24.23%), captures cross-channel effects, and provides privacy-preserving attribution.]
    ```

- **[arXiv251225] Autonomous Uncertainty Quantification for Computational Point-of-care Sensors**
  - **tags:** [mlsys], [on-device ai], [Monte Carlo dropout, uncertainty quantification, computational point-of-care sensors, vertical flow assays, neural networks]
  - **authors:** Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan
  - **institution:** University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.21335
  - **contributions:** 1. Proposed an autonomous uncertainty quantification technique for computational point-of-care diagnostics to mitigate erroneous neural network predictions. 2. Integrated a Monte Carlo dropout-based method into a diagnostic pipeline to identify and exclude high-uncertainty predictions without needing ground truth. 3. Demonstrated the method's effectiveness on a Lyme disease diagnostic platform, significantly improving sensitivity from 88.2% to 95.7% in blinded testing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5869aed4bf3e962c5ac24490891076a87e2bc2b992a51fbf1ce76fcce3ce32b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of erroneous predictions from neural networks in computational point-of-care diagnostic sensors. The authors propose an autonomous uncertainty quantification method using Monte Carlo dropout to identify and exclude unreliable predictions. Their approach, tested on a Lyme disease diagnostic platform, significantly improved diagnostic sensitivity, demonstrating enhanced robustness for neural network-driven sensing systems.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[Autonomous Uncertainty Quantification for Computational Point-of-care Sensors] --> B[核心问题/Problem: Neural network hallucinations cause misdiagnosis in POC sensors]
    A --> C[主要方法/Method: Integrate Monte Carlo dropout for autonomous uncertainty quantification]
    A --> D[关键结果/Results: Diagnostic sensitivity increased from 88.2% to 95.7%]
    ```
