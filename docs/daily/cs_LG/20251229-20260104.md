---
slug: /daily/cslg/20251229-20260104
---
# 20251229-20260104 (cs.LG)

## 2025-12-29

- **[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum**
  - **tags:** [mlsys], [on-device ai], [data spaces, cloud-edge continuum, containerized microservices, edge AI, intelligent infrastructure monitoring]
  - **authors:** Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda
  - **institution:** Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.
  - **link:** https://arxiv.org/pdf/2512.21340
  - **contributions:** 1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Harnessing Data Spaces for Smart City Infrastructures] --> B[核心问题/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]
        A --> C[主要方法/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]
        A --> D[关键结果/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]
    ```

- **[arXiv251229] Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems**
  - **tags:** [ai], [physics-informed machine learning], [physics-informed neural networks, Floquet-Bloch eigenvalue problem, honeycomb lattice, band structure, transfer learning]
  - **authors:** Haaris Mian
  - **institution:** Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)
  - **link:** https://arxiv.org/pdf/2512.21349
  - **contributions:** 1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schrödinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp
  - **Simple LLM Summary:** This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[求解二维周期势中的Floquet-Bloch本征值问题/Solve Floquet-Bloch eigenvalue problem in 2D periodic potentials]
        B --> B2[关注石墨烯等材料的蜂窝晶格和能带拓扑/Focus on honeycomb lattice & band topology for materials like graphene]
        C --> C1[使用神经网络同时学习布洛赫函数和本征值/Use neural networks to learn Bloch functions & eigenvalues]
        C --> C2[通过复合损失函数强制执行薛定谔方程和周期性/Enforce Schrödinger eq. & periodicity via composite loss]
        C --> C3[在布里渊区训练并探索迁移学习/Train over Brillouin zone & explore transfer learning]
        D --> D1[数值验证与传统方法一致/Numerical validation against traditional methods]
        D --> D2[迁移学习捕捉能带拓扑变化/Transfer learning captures changes in band topology]
    ```

- **[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation**
  - **tags:** [ai], [reinforcement learning], [synthetic data generation, reinforcement learning, proximal policy optimization, privacy, biomedical data]
  - **authors:** Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin
  - **institution:** Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis
  - **link:** https://arxiv.org/pdf/2512.21395
  - **contributions:** 1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Reinforcement Learning Approach to Synthetic Data Generation] --> B
        A --> C
        A --> D
        B[核心问题/Problem: State-of-the-art generative models need large datasets and complex training, limiting use in small-sample settings.]
        C[主要方法/Method: Reframe SDG as RL; introduce RLSyn (stochastic policy optimized via PPO with discriminator rewards).]
        D[关键结果/Results: RLSyn performs comparably to/better than GANs & diffusion models, especially on smaller datasets.]
    ```

- **[arXiv251229] Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme**
  - **tags:** [sys], [storage systems], [constrained coding, LOCO codes, linear programming, code reconfiguration, two-dimensional magnetic recording (TDMR)]
  - **authors:** Doğukan Özbayrak, Ahmed Hareedy
  - **institution:** Middle East Technical University
  - **link:** https://arxiv.org/pdf/2512.21396
  - **contributions:** 1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("设备老化需要不同等级的数据保护/Device aging requires different levels of data protection")
        Problem --> P2("基于预定时间戳的重配置忽略实际状态/Reconfiguration based on timestamps neglects actual device status")
        Method --> M1("提出离线和在线学习方法/Propose offline and online learning methods")
        Method --> M2("将训练数据拟合为多项式方程/Fit training data to polynomial equations")
        Method --> M3("将重配置建模为线性规划问题/Model reconfiguration as a linear programming problem")
        Results --> R1("解决方案是全局最优的/Solution is globally optimal")
        Results --> R2("实验证明在TDMR系统中有效/Experiments demonstrate effectiveness in TDMR systems")
    ```

- **[arXiv251229] kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning**
  - **tags:** [ai], [dynamical systems learning], [Koopman operator, transfer operator, spectral decomposition, scikit-learn API, reduced-order models]
  - **authors:** Giacomo Turri, Grégoire Pacreau, Giacomo Meanti, Timothée Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli
  - **institution:** Italian Institute of Technology, École Polytechnique, Inria, University College London
  - **link:** https://arxiv.org/pdf/2512.21409
  - **code:** https://github.com/Machine-Learning-Dynamical-Systems/kooplearn
  - **contributions:** 1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[kooplearn: A Scikit-Learn Compatible Library] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Need for a unified, easy-to-use library for learning evolution operators from dynamical systems data] --> Problem_Sub1[应用/Application: Analyze systems, forecast, reduce model dimension]
        Method[主要方法/Method: Implement linear, kernel, and deep-learning estimators] --> Method_Sub1[接口/Interface: Scikit-learn compatible API]
        Method --> Method_Sub2[模型/Model: Discrete-time (Koopman/Transfer) & continuous-time operators]
        Results[关键结果/Results: kooplearn library released] --> Results_Sub1[特性/Features: Includes benchmark datasets for fair comparison]
        Results --> Results_Sub2[目标/Goal: Facilitate integration, experimentation, and reproducibility]
    ```

- **[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding**
  - **tags:** [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]
  - **authors:** Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli
  - **institution:** California Institute of Technology, Stanford University
  - **link:** https://arxiv.org/pdf/2512.21414
  - **code:** https://github.com/christinaliu2020/tool-bottleneck-framework
  - **contributions:** 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]
        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]
        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]
    ```

- **[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning**
  - **tags:** [sys], [wireless networking], [Age of Information (AoI), reinforcement learning, freshness optimization, wireless networks, multi-agent systems]
  - **authors:** Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng
  - **institution:** Illinois Institute of Technology, California State University, San Bernardino
  - **link:** https://arxiv.org/pdf/2512.21412
  - **contributions:** 1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp
  - **Simple LLM Summary:** This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有综述的不足: 经典AoI与泛化RL研究分离 / Gap: Classical AoI vs. Broad RL]
        C --> C1[提出以AoI为中心的RL综述框架 / Propose AoI-centric RL Survey Framework]
        C --> C2[构建AoI变体分类与策略中心分类法 / Build AoI Variant & Policy-Centric Taxonomies]
        D --> D1[为B5G/6G建立学习式新鲜度优化的统一基础 / Establish Unified Foundation for Learning-based Freshness Optimization]
        D --> D2[识别开放挑战: 延迟决策、随机性、跨层设计 / Identify Open Challenges: Delayed Decisions, Stochasticity, Cross-layer]
    ```

- **[arXiv251229] Scalable Deep Subspace Clustering Network**
  - **tags:** [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]
  - **authors:** Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami
  - **institution:** University of Quebec at Montreal
  - **link:** https://arxiv.org/pdf/2512.21434
  - **contributions:** 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Deep Subspace Clustering Network"] --> Problem["核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity"]
        Root --> Method["主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization"]
        Root --> Results["关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance"]
    ```

- **[arXiv251229] Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors**
  - **tags:** [se], [software testing], [runtime error detection, coverage-guided testing, multi-agent reasoning, large language models, static analysis]
  - **authors:** Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.21431
  - **contributions:** 1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors] --> B(核心问题/Problem: Detecting runtime errors in code snippets without execution is crucial for software safety.)
        A --> C(主要方法/Method: Uses LLMs for execution-free, coverage-guided testing with a two-phase feedback loop.)
        A --> D(关键结果/Results: Outperforms conventional and learning-based frameworks by generating high-coverage tests and finding more errors.)
    ```

- **[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]
  - **authors:** Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello
  - **institution:** University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2512.21433
  - **contributions:** 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]
        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]
        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]
        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]
        D --> D2[显著优于现有方法/Significantly outperforms existing methods]
    ```

- **[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models**
  - **tags:** [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]
  - **authors:** Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi
  - **institution:** Institute of Intelligent Systems and Robotics, Sorbonne University
  - **link:** https://arxiv.org/pdf/2512.21439
  - **contributions:** 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]
        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]
        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]
    ```

- **[arXiv251229] Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing**
  - **tags:** [se], [fuzz testing], [initial corpus generation, large language models, multi-agent framework, predictive code coverage, mutation-based fuzzing]
  - **authors:** Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.21440
  - **contributions:** 1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FuzzWise: Intelligent Initial Corpus Generation for Fuzzing] --> B[核心问题/Problem: 为模糊测试生成高质量的初始种子语料库/Generating high-quality initial seed corpus for fuzzing]
        A --> C[主要方法/Method: 基于LLM的多智能体框架，集成生成与预测性覆盖评估/LLM-based multi-agent framework integrating generation and predictive coverage assessment]
        A --> D[关键结果/Results: 用更少的测试用例实现更高的代码覆盖率和错误发现率/Achieves higher code coverage and bug detection with fewer test cases]
    ```

- **[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning**
  - **tags:** [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]
  - **authors:** Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu
  - **institution:** University of Washington, University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.21446
  - **contributions:** 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards "diffusion supremacy" over autoregressive models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]
        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]
        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]
        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]
        D --> D2[迈向"扩散霸权"/Moving towards "diffusion supremacy"]
    ```

- **[arXiv251229] An Equivariance Toolbox for Learning Dynamics**
  - **tags:** [ai], [learning theory], [equivariance, Noether's theorem, Hessian constraints, learning dynamics, symmetry]
  - **authors:** Yongyi Yang, Liu Ziyin
  - **institution:** University of Michigan, Massachusetts Institute of Technology, NTT Research
  - **link:** https://arxiv.org/pdf/2512.21447
  - **contributions:** 1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp
  - **Simple LLM Summary:** The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[An Equivariance Toolbox for Learning Dynamics] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有分析是特定问题的/Existing analyses are problem-specific]
        Problem --> P2[二阶结构理解不足/Second-order structure less understood]
        Method[主要方法/Method] --> M1[构建等变性工具箱/Build general equivariance toolbox]
        Method --> M2[扩展诺特定理分析/Extend Noether-type analyses]
        Results[关键结果/Results] --> R1[统一一阶约束/Unify first-order constraints]
        Results --> R2[提供二阶结构预测/Provide second-order structural predictions]
        Results --> R3[连接变换结构与几何/Connect transformation structure to geometry]
    ```

- **[arXiv251229] RLLaVA: An RL-central Framework for Language and Vision Assistants**
  - **tags:** [mlsys], [multi-modal training], [reinforcement learning, vision-language models, Markov decision process, resource-efficient training, modular framework]
  - **authors:** Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang
  - **institution:** Beihang University
  - **link:** https://arxiv.org/pdf/2512.21450
  - **code:** https://github.com/TinyLoopX/RLLaVA
  - **contributions:** 1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RLLaVA: An RL-central Framework for Language and Vision Assistants] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[缺乏专门的多模态RL框架 / Lack of specialized multi-modal RL framework]
        B --> B2[现有框架资源消耗大 / Existing frameworks are resource-intensive]
        C --> C1[解耦RL逻辑与架构 / Decouple RL logic from architecture & execution]
        C --> C2[统一MDP建模 / Unified MDP formulation for VLMs]
        C --> C3[支持多种算法与模型 / Supports broad RL methods & VLMs]
        D --> D1[资源高效训练 / Resource-efficient training (e.g., 4B model on 24GB GPU)]
        D --> D2[性能提升 / Models show improved performance]
        D --> D3[任务可扩展性 / Task extensibility demonstrated]
    ```

- **[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection**
  - **tags:** [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]
  - **authors:** Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu
  - **institution:** Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC
  - **link:** https://arxiv.org/pdf/2512.21459
  - **code:** https://github.com/chloeqxq/CCAD
  - **contributions:** 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]
        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]
        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]
    ```

- **[arXiv251229] Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US**
  - **tags:** [ai], [time series forecasting], [LSTM, SARIMA, conformal prediction, counterfactual estimation, uncertainty quantification]
  - **authors:** Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder
  - **institution:** Harvard University, Boston Children's Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21456
  - **contributions:** 1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp
  - **Simple LLM Summary:** This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[估计疫情导致的药物过量超额死亡率/Estimating pandemic-attributable excess mortality]
        B --> B2[传统方法在结构性变化下可能失效/Traditional methods may fail under structural change]
        C --> C1[系统比较SARIMA与深度学习模型/Systematic comparison of SARIMA vs. DL models]
        C --> C2[使用CDC数据(2015-2019)进行训练/Using CDC data (2015-2019) for training]
        C --> C3[预测2020-2023年的反事实轨迹/Projecting counterfactual trajectories for 2020-2023]
        D --> D1[LSTM在点估计和不确定性校准上表现最佳/LSTM achieves best point estimation & uncertainty calibration]
        D --> D2[注意力模型因过拟合历史均值而表现不佳/Attention models underperform due to overfitting to historical means]
        D --> D3[提供可部署的开源框架/Providing a deployable open-source framework]
    ```

- **[arXiv251229] When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning**
  - **tags:** [ai], [tensor decomposition], [Bayesian tensor completion, multioutput Gaussian processes, variational inference, rank learning, functional universality]
  - **authors:** Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis
  - **institution:** Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C.
  - **link:** https://arxiv.org/pdf/2512.21486
  - **code:** https://github.com/OceanSTARLab/RR-FBTC
  - **contributions:** 1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法需已知张量秩/Existing methods require known tensor rank]
        B --> B2[确定最优秩是NP难问题/Determining optimal rank is NP-hard]
        C --> C1[提出RR-FBTC方法/Propose RR-FBTC method]
        C --> C2[使用多输出高斯过程建模/Model with multioutput Gaussian processes]
        C --> C3[变分推断与闭式更新/Variational inference with closed-form updates]
        D --> D1[证明泛函逼近能力/Prove functional universal approximation]
        D --> D2[实现自动秩学习/Achieve automatic rank learning]
        D --> D3[实验验证有效性/Experiments validate effectiveness]
    ```

- **[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding**
  - **tags:** [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]
  - **authors:** Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson
  - **institution:** Dartmouth College
  - **link:** https://arxiv.org/pdf/2512.21506
  - **contributions:** 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]
        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]
        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]
    ```

- **[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering**
  - **tags:** [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]
  - **authors:** Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu
  - **institution:** University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University
  - **link:** https://arxiv.org/pdf/2512.21510
  - **contributions:** 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --> Problem["核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]
        Root --> Method["主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]
        Root --> Results["关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness."]
    ```

- **[arXiv251229] First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions**
  - **tags:** [mlsys], [federated learning], [differential privacy, convergence guarantees, partial client participation, local updates, clipping bias]
  - **authors:** Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik
  - **institution:** King Abdullah University of Science and Technology (KAUST)
  - **link:** https://arxiv.org/pdf/2512.21521
  - **contributions:** 1. Introduces Fed-α-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-α-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[First Provable Guarantees for Practical Private FL] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[现有私有FL方法依赖不现实假设/Existing private FL relies on unrealistic assumptions]
        Problem --> P2[忽略本地更新与部分客户端参与/Neglects local updates & partial participation]
        Method[主要方法/Method] --> M1[提出Fed-α-NormEC框架/Propose Fed-α-NormEC framework]
        Method --> M2[集成本地更新与独立学习率/Integrates local updates & separate stepsizes]
        Method --> M3[支持部分客户端参与/Supports partial client participation]
        Results[关键结果/Results] --> R1[提供可证明的收敛与DP保证/Provides provable convergence & DP guarantees]
        Results --> R2[实验验证理论/Experiments corroborate theory]
    ```

- **[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training**
  - **tags:** [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]
  - **authors:** Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren
  - **institution:** Ant Group, Zhejiang University
  - **link:** https://arxiv.org/pdf/2512.21515
  - **contributions:** 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of "perplexity landscapes" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Perplexity-Aware Data Scaling Law<br>困惑度感知数据缩放定律] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[CPT中单纯增加数据收益递减<br>Diminishing returns from scaling data in CPT]
        C --> C1[提出基于困惑度景观的缩放定律<br>Propose perplexity-landscape-based scaling law]
        C1 --> C2[利用困惑度量化知识差距<br>Use perplexity to quantify knowledge gap]
        C2 --> C3[自适应选择高价值数据子集<br>Adaptively select high-utility data subsets]
        D --> D1[识别接近最优的训练子集<br>Identifies near-optimal training subsets]
        D1 --> D2[在领域基准上取得优越性能<br>Achieves superior performance on domain benchmarks]
    ```

- **[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data**
  - **tags:** [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]
  - **authors:** Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu
  - **institution:** Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University
  - **link:** https://arxiv.org/pdf/2512.21516
  - **contributions:** 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]
        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]
        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]
        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]
        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]
        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]
        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]
    ```

- **[arXiv251229] Generative Actor Critic**
  - **tags:** [ai], [reinforcement learning], [generative modeling, policy evaluation, latent plan, offline-to-online, actor-critic]
  - **authors:** Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie
  - **institution:** Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University
  - **link:** https://arxiv.org/pdf/2512.21527
  - **code:** github.com/qayqaq/Generative-Actor-Critic
  - **contributions:** 1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Actor Critic] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[传统RL在线改进离线预训练模型存在挑战/Challenges in refining offline models online]
        C --> C1[将策略评估重构为学习轨迹与回报的联合生成模型/Reframe policy evaluation as learning p(τ, y)]
        C --> C2[将策略改进重构为在模型上进行多样化推理/Reframe policy improvement as versatile inference]
        C --> C3[基于潜变量模型的实例化与新颖推理策略/Instantiation with latent plans & novel inference]
        D --> D1[离线性能强大/Strong offline performance]
        D --> D2[离线到在线改进显著/Enhanced offline-to-online improvement]
    ```

- **[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification**
  - **tags:** [ai], [bioinformatics], [adaptive gating mechanism, contrastive learning, transfer learning]
  - **authors:** Xinru Wen, Weizhong Lin, Xuan Xiao
  - **institution:** JCI (inferred from email domain `jci.edu.cn`)
  - **link:** https://arxiv.org/pdf/2512.21544
  - **contributions:** 1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AVP-Fusion: 抗病毒肽识别 / Antiviral Peptide Identification] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题 / Problem] --> P1[现有方法难以捕捉复杂序列依赖 / Current methods struggle with sequence dependencies]
        Problem --> P2[难以处理模糊样本 / Hard to handle ambiguous samples]
        Method[主要方法 / Method] --> M1[构建全景特征空间 / Construct panoramic feature space]
        Method --> M2[自适应门控机制融合特征 / Adaptive Gating Mechanism for feature fusion]
        Method --> M3[对比学习与数据增强 / Contrastive learning & data augmentation]
        Results[关键结果 / Results] --> R1[准确率0.9531, MCC 0.9064 / Accuracy 0.9531, MCC 0.9064]
        Results --> R2[优于现有方法 / Outperforms SOTA]
        Results --> R3[实现病毒家族亚类预测 / Enables viral family subclass prediction]
    ```

- **[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search**
  - **tags:** [ai], [sparse recovery], [neural architecture search, meta-learning, iterative shrinkage thresholding algorithm, sparse optimization, algorithm discovery]
  - **authors:** Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde
  - **institution:** New York University, University of California, Riverside
  - **link:** https://arxiv.org/pdf/2512.21563
  - **contributions:** 1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework's capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework's applicability to various data distributions and algorithms beyond ISTA/FISTA.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Discovering Sparse Recovery Algorithms Using Neural Architecture Search] --> B[核心问题/Problem: Automated discovery of sparse optimization algorithms is difficult and heuristic-driven]
    A --> C[主要方法/Method: Meta-learning framework using Neural Architecture Search (NAS) for algorithm rediscovery]
    A --> D[关键结果/Results: Rediscovered ISTA/FISTA elements; framework applies to various data and algorithms]
    ```

- **[arXiv251229] AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging**
  - **tags:** [ai], [spatio-temporal kriging], [graph neural networks, incremental learning, data stratification, anchor locations, incomplete features]
  - **authors:** Xiaobin Ren, Kaiqi Zhao, Katerina Taškova, Patricia Riddle
  - **institution:** University of Auckland, Harbin Institute of Technology, Shenzhen
  - **link:** https://arxiv.org/pdf/2512.21569
  - **code:** https://github.com/xren451/Spatial-interpolation
  - **contributions:** 1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging] --> B(核心问题/Problem: Sparse sensor distribution and incomplete features hinder accurate spatio-temporal kriging)
        A --> C(主要方法/Method: Anchor-based stratification and dual-view graph learning for incremental feature integration)
        A --> D(关键结果/Results: Outperforms state-of-the-art baselines on multiple benchmark datasets)
    ```

- **[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures**
  - **tags:** [mlsys], [compiler & ir], [e-graph, term rewriting, phase ordering, NUMA abstraction, auto vectorize]
  - **authors:** Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang
  - **institution:** Canaan Inc.
  - **link:** https://arxiv.org/pdf/2512.21571
  - **code:** https://github.com/kendryte/nncase
  - **contributions:** 1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a "compile once, adapt everywhere" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp
  - **Simple LLM Summary:** The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --> B[核心问题/Problem: LLM部署受限于内存架构异构性，传统编译器流程碎片化/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]
        A --> C[主要方法/Method: 基于e-graph的项重写引擎，统一NUMA抽象，集成自动向量化、分布、调度模块/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]
        A --> D[关键结果/Results: 性能超越MLC LLM和Intel IPEX，接近手工优化的llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]
    ```

- **[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid**
  - **tags:** [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]
  - **authors:** Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng
  - **institution:** Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers
  - **link:** https://arxiv.org/pdf/2512.21577
  - **contributions:** 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp
  - **Simple LLM Summary:** This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Problem["核心问题/Problem"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Method["主要方法/Method"]
        Root["A Unified Definition of Hallucination / 幻觉的统一定义"] --> Results["关键结果/Results"]
        Problem --> P1["Hallucination persists in LLMs / 幻觉在LLM中持续存在"]
        Method --> M1["Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模"]
        Method --> M2["Framework: reference world & conflict policy / 框架：参考世界与冲突策略"]
        Results --> R1["Clarifies evaluation & terminology / 澄清评估与术语"]
        Results --> R2["Proposes new benchmark plans / 提出新基准计划"]
    ```

- **[arXiv251229] RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models**
  - **tags:** [ai], [time series forecasting], [Schrödinger Bridge, Low-rank Adaptation, Time Series Foundation Models, Financial Forecasting, Generative Refinement]
  - **authors:** Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic
  - **institution:** Imperial College London
  - **link:** https://arxiv.org/pdf/2512.21572
  - **contributions:** 1. Proposes RefineBridge, a novel post-processing module built on a tractable Schrödinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schrödinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[TSFMs在金融预测中表现不佳<br/>TSFMs underperform in financial forecasting]
        B --> B2[LoRA等适应方法存在局限<br/>Limitations of adaptation methods like LoRA]
        C --> C1[提出基于薛定谔桥的RefineBridge模块<br/>Propose RefineBridge based on Schrödinger Bridge]
        C --> C2[将TSFM预测作为先验进行迭代优化<br/>Iteratively refine TSFM predictions as prior]
        D --> D1[在多个基准上提升TSFM性能<br/>Improves TSFM performance on multiple benchmarks]
        D --> D2[对不同预测范围均有效<br/>Effective across different prediction horizons]
    ```

- **[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations**
  - **tags:** [ai], [imitation learning], [behavior cloning, latent representation, self-supervised learning, sample efficiency]
  - **authors:** Xin Liu, Haoran Li, Dongbin Zhao
  - **institution:** Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21586
  - **contributions:** 1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1["从视频模仿学习的挑战 / Challenges of Imitation Learning from Videos"]
        C --> C1["BCV-LR框架 / BCV-LR Framework"]
        C1 --> C2["自监督提取潜在特征 / Self-supervised Latent Feature Extraction"]
        C1 --> C3["基于动态的潜在动作预测 / Dynamics-based Latent Action Prediction"]
        C1 --> C4["在线对齐与迭代策略改进 / Online Alignment & Iterative Policy Improvement"]
        D --> D1["高样本效率 / High Sample Efficiency"]
        D --> D2["超越SOTA方法 / Outperforms SOTA Baselines"]
        D --> D3["首次证明视频可作为高效监督 / First to Show Videos as Efficient Supervision"]
    ```

- **[arXiv251229] Quantitative Verification of Omega-regular Properties in Probabilistic Programming**
  - **tags:** [other], [probabilistic programming and verification], [temporal posterior inference, omega-regular properties, stochastic barrier certificates, Rabin automata, quantitative verification]
  - **authors:** Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong
  - **institution:** East China Normal University, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.21596
  - **contributions:** 1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Quantitative Verification of Omega-regular Properties in Probabilistic Programming") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("标准后验推断的局限/Limitation of Standard Posterior Inference")
        P1 --> P2("无法捕捉程序执行的时间演化/Fails to capture temporal evolution")
        Method --> M1("提出时间后验推断框架/Propose Temporal Posterior Inference (TPI)")
        M1 --> M2("统一概率编程与时序逻辑/Unifies Probabilistic Programming & Temporal Logic")
        M2 --> M3("基于随机屏障证书的定量验证方法/Quantitative Verification via Stochastic Barrier Certificates")
        Results --> R1("实现原型工具 TPInfer/Implement Prototype Tool TPInfer")
        Results --> R2("在基准测试中展示有效性与效率/Demonstrates Effectiveness & Efficiency on Benchmarks")
    ```

- **[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care**
  - **tags:** [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]
  - **authors:** Yusuf Brima, Marcellin Atemkeng
  - **institution:** Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)
  - **link:** https://arxiv.org/pdf/2512.21602
  - **contributions:** 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp
  - **Simple LLM Summary:** This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]
        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]
        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]
    ```

- **[arXiv251229] A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)**
  - **tags:** [ai], [predictive modeling], [XGBoost, SHAP analysis, K-Fold Cross-Validation, Isolation Forest, hyperparameter tuning]
  - **authors:** Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen
  - **institution:** Tongji University, University of Chittagong
  - **link:** https://arxiv.org/pdf/2512.21610
  - **contributions:** 1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["A Data-Driven Multi-Objective Approach for Predicting UHPC Properties<br>预测UHPC性能的数据驱动多目标方法"] --> B["Problem: Predicting UHPC mechanical performance, flowability, porosity<br>核心问题: 预测UHPC的力学性能、流动性和孔隙率"]
        A --> C["Method: Two-stage ML framework with XGBoost, data cleaning, SHAP<br>主要方法: 两阶段ML框架，使用XGBoost、数据清洗和SHAP"]
        A --> D["Results: High prediction accuracy, developed GUI for designers<br>关键结果: 高预测精度，为设计师开发了GUI"]
    ```

- **[arXiv251229] MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations**
  - **tags:** [ai], [scientific machine learning], [Neural Galerkin Method, meta-learning, parametric PDEs, space-time decoupling, randomized sparse updates]
  - **authors:** Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu
  - **institution:** Hunan University, Capital Normal University
  - **link:** https://arxiv.org/pdf/2512.21633
  - **contributions:** 1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[MAD-NG: Meta-Auto-Decoder Neural Galerkin Method<br>MAD-NG: 元自解码器神经伽辽金方法] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统神经求解器泛化与效率挑战<br>Traditional Neural Solvers' Generalization & Efficiency Challenges]
        C --> C1[空间-时间解耦与元学习<br>Space-Time Decoupling & Meta-Learning]
        C --> C2[随机稀疏更新<br>Randomized Sparse Updates]
        D --> D1[物理一致的长时预测<br>Physically Consistent Long-Horizon Predictions]
        D --> D2[较低计算开销<br>Lower Computational Overhead]
    ```

- **[arXiv251229] Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms**
  - **tags:** [ai], [materials informatics], [hybrid machine learning, SHAP analysis, uncertainty quantification, strength prediction, high-performance concrete]
  - **authors:** Jagaran Chakma, Zhiguang Zhou, Badhan Chakma
  - **institution:** Tongji University, Chongqing Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.21638
  - **contributions:** 1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp
  - **Simple LLM Summary:** This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Predict mechanical properties (CS, FS, TS) of fiber-reinforced HPC]
        C[主要方法/Method<br>Develop & evaluate hybrid ML models (ET-XGB, RF-LGBM, Transformer-XGB) with SHAP & uncertainty analysis]
        D[关键结果/Results<br>ET-XGB most accurate, RF-LGBM most stable for FS, key influential factors identified via SHAP]
    ```

- **[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search**
  - **tags:** [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]
  - **authors:** Maximilian Weichart
  - **institution:** University of Regensburg
  - **link:** https://arxiv.org/pdf/2512.21648
  - **code:** https://github.com/Max-We/inverse-rpo
  - **contributions:** 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]
        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]
        D[关键结果/Results: New policies outperform PUCT without extra cost]
    ```

- **[arXiv251229] Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation**
  - **tags:** [cv], [anomaly detection], [multimodal fusion, causal modeling, hierarchical modulation, sensor guidance, unsupervised learning]
  - **authors:** Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing
  - **institution:** Chongqing University
  - **link:** https://arxiv.org/pdf/2512.21650
  - **contributions:** 1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process→Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[因果盲区/Causal Blindness]
        B --> B2[模态异质性/Modality Heterogeneity]
        C --> C1[传感器引导调制/Sensor-Guided CHM Modulation]
        C --> C2[因果分层架构/Causal-Hierarchical Architecture]
        D --> D1[新基准/Weld-4M Benchmark]
        D --> D2[SOTA性能/SOTA I-AUROC 90.7%]
    ```

- **[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression**
  - **tags:** [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]
  - **authors:** Liuyang Bai, Weiyi Lu, Li Guo
  - **institution:** NYU Shanghai
  - **link:** https://arxiv.org/pdf/2512.21653
  - **contributions:** 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --> Problem["核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]
        Root --> Method["主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]
        Root --> Results["关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]
    ```

- **[arXiv251229] Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [1-bit quantization, post-training quantization, output alignment, activation error, large language models]
  - **authors:** Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do
  - **institution:** Monash University, University of Surrey
  - **link:** https://arxiv.org/pdf/2512.21651
  - **contributions:** 1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Rethinking Output Alignment For 1-bit PTQ of LLMs<br>重新思考大语言模型1比特训练后量化的输出对齐"] --> Problem
        Root --> Method
        Root --> Results
        Problem["1-bit PTQ causes significant performance drop<br>1比特训练后量化导致性能显著下降"] --> P1["Focus on weight alignment, not output<br>关注权重对齐而非输出"]
        Problem --> P2["Naive output-matching fails<br>简单的输出匹配方法失败"]
        Method["Propose a data-aware PTQ approach<br>提出一种数据感知的训练后量化方法"] --> M1["Accounts for activation error accumulation<br>考虑激活误差累积"]
        Method --> M2["Keeps optimization efficient<br>保持优化高效"]
        Results["Consistently outperforms existing 1-bit PTQ methods<br>持续优于现有1比特训练后量化方法"] --> R1["Minimal overhead<br>开销极小"]
    ```

- **[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds**
  - **tags:** [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]
  - **authors:** Subramanyam Sahoo, Jared Junkin
  - **institution:** University of California, Berkeley, Johns Hopkins University
  - **link:** https://arxiv.org/pdf/2512.21670
  - **code:** https://github.com/SubramanyamSahoo/The-Deepfake-Detective
  - **contributions:** 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the "black box" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]
        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]
        C --> C2[法证流形分析/Forensic Manifold Analysis]
        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]
        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]
    ```

- **[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles**
  - **tags:** [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]
  - **authors:** Jalal Khan
  - **institution:** United Arab Emirates University
  - **link:** https://arxiv.org/pdf/2512.21673
  - **contributions:** 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp
  - **Simple LLM Summary:** This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]
    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]
    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]
    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]
    ```

- **[arXiv251229] Dictionary-Transform Generative Adversarial Networks**
  - **tags:** [ai], [generative adversarial networks], [dictionary learning, transform learning, sparse modeling, adversarial learning, nash equilibrium]
  - **authors:** Angshul Majumdar
  - **institution:** Indraprastha Institute of Information Technology Delhi (IIIT-D)
  - **link:** https://arxiv.org/pdf/2512.21677
  - **contributions:** 1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Dictionary-Transform GANs] --> B[核心问题/Problem: Classical GANs are theoretically fragile, with ill-posed objectives and unstable training.]
        A --> C[主要方法/Method: Propose DT-GAN, a model-based framework with a sparse synthesis dictionary generator and an analysis transform discriminator.]
        A --> D[关键结果/Results: Game is well-posed with Nash equilibrium; solutions are identifiable and stable; framework is interpretable and provably correct.]
    ```

- **[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting**
  - **tags:** [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]
  - **authors:** Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan
  - **institution:** Beijing Jiaotong University, Aalborg University
  - **link:** https://arxiv.org/pdf/2512.21685
  - **contributions:** 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)
        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)
        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)
        C --> C2(主成分网络 / Principal Component Network)
        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)
    ```

- **[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities**
  - **tags:** [sys], [communication & networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]
  - **authors:** Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin
  - **institution:** Kyung Hee University, Ghent University
  - **link:** https://arxiv.org/pdf/2512.21717
  - **contributions:** 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation"]
        Method["主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning"]
        Results["关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off"]
    ```

- **[arXiv251229] An Information Theoretic Perspective on Agentic System Design**
  - **tags:** [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]
  - **authors:** Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2512.21720
  - **contributions:** 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1("Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance")
        C --> C1("提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator")
        D --> D1("更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate")
        D --> D2("扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors")
    ```

- **[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks**
  - **tags:** [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]
  - **authors:** Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov
  - **institution:** MIRAI, Cognitive AI Systems Lab
  - **link:** https://arxiv.org/pdf/2512.21723
  - **contributions:** 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]
        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]
        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]
    ```

- **[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data**
  - **tags:** [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]
  - **authors:** Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda
  - **institution:** Sony Computer Science Laboratories, Inc., Sony Corporation of America
  - **link:** https://arxiv.org/pdf/2512.21746
  - **contributions:** 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]
        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]
        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]
    ```

- **[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning**
  - **tags:** [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]
  - **authors:** Hengyi Wu, Zhenyi Wang, Heng Huang
  - **institution:** University of Maryland, College Park, University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.21743
  - **contributions:** 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]
        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]
        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]
        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]
        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]
        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]
    ```

- **[arXiv251229] Approximation Capabilities of Feedforward Neural Networks with GELU Activations**
  - **tags:** [ai], [neural network approximation theory], [GELU activation, feedforward neural networks, approximation error bounds, derivative approximation, constructive approximation]
  - **authors:** Konstantin Yakovlev, Nikita Puchkin
  - **institution:** HSE University
  - **link:** https://arxiv.org/pdf/2512.21749
  - **contributions:** 1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives.
  - **Mindmap:**

    ```mermaid
    graph TB
    Root("Approximation Capabilities of Feedforward Neural Networks with GELU Activations<br>GELU激活前馈神经网络的逼近能力") --> Problem("核心问题/Problem")
    Root --> Method("主要方法/Method")
    Root --> Results("关键结果/Results")
    Problem --> P1("逼近函数及其导数<br>Approximating functions and their derivatives")
    Method --> M1("构造性乘法逼近<br>Constructive multiplication approximation")
    Method --> M2("扩展到除法和指数<br>Extension to division and exponent")
    Results --> R1("同时误差界<br>Simultaneous error bounds")
    Results --> R2("全局有界导数<br>Globally bounded derivatives")
    Results --> R3("网络规模控制<br>Network size control")
    ```

- **[arXiv251229] Assessing the Effectiveness of Membership Inference on Generative Music**
  - **tags:** [sec], [membership inference attacks], [membership inference attack (MIA), generative music, MuseGAN, privacy, copyright]
  - **authors:** Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang
  - **institution:** University of California, Irvine
  - **link:** https://arxiv.org/pdf/2512.21762
  - **contributions:** 1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Assessing the Effectiveness of Membership Inference on Generative Music"] --> Problem["核心问题/Problem: Lack of MIA study on generative music, privacy & copyright concerns"]
        Root --> Method["主要方法/Method: Apply existing MIAs to MuseGAN model"]
        Root --> Results["关键结果/Results: Music data is resilient to known MIAs"]
    ```

- **[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization**
  - **tags:** [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]
  - **authors:** Evgeny Alves Limarenko, Anastasiia Studenikina
  - **institution:** Moscow Institute of Physics and Technology
  - **link:** https://arxiv.org/pdf/2512.21769
  - **contributions:** 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("BertsWin: 3D MAE优化") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE")
        Problem --> P2("破坏空间关系/Destroys Spatial Context")
        Method --> M1("BertsWin混合架构/BertsWin Hybrid Architecture")
        Method --> M2("完整3D令牌网格/Full 3D Token Grid")
        Method --> M3("Swin窗口 & 结构损失/Swin Windows & Structural Loss")
        Results --> R1("5.8x语义收敛加速/5.8x Faster Convergence")
        Results --> R2("15倍训练轮次减少/15x Fewer Epochs")
        Results --> R3("FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction")
    ```

- **[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents**
  - **tags:** [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]
  - **authors:** Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin
  - **institution:** Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard
  - **link:** https://arxiv.org/pdf/2512.21782
  - **contributions:** 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]
        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]
        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]
        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]
        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]
    ```

- **[arXiv251229] Synthetic Financial Data Generation for Enhanced Financial Modelling**
  - **tags:** [ai], [synthetic data generation], [synthetic financial data, TimeGAN, ARIMA-GARCH, VAE, Maximum Mean Discrepancy]
  - **authors:** Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana
  - **institution:** AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University
  - **link:** https://arxiv.org/pdf/2512.21791
  - **contributions:** 1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Synthetic Financial Data Generation for Enhanced Financial Modelling"] --> Problem
        Root --> Method
        Root --> Results
        Problem["数据稀缺与保密性<br/>Data Scarcity & Confidentiality"]
        Method["统一评估框架与三种生成模型<br/>Unified Evaluation Framework & Three Generative Models"]
        Results["TimeGAN最佳权衡与实用指南<br/>TimeGAN Best Trade-off & Practical Guidelines"]
    ```

- **[arXiv251229] Multi-agent Adaptive Mechanism Design**
  - **tags:** [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]
  - **authors:** Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao
  - **institution:** Massachusetts Institute of Technology, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.21794
  - **contributions:** 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\tilde\{O\}(\sqrt\{T\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]
        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]
        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\tilde{O}(\sqrt{T})$ regret]
    ```

- **[arXiv251229] VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance**
  - **tags:** [ai], [bioinformatics], [Set Attention Transformer, 1D-CNN, Multi-Path Network, Interpretable Machine Learning, Genomic Variant Analysis]
  - **authors:** Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud
  - **institution:** National School of Artificial Intelligence (ENSIA)
  - **link:** https://arxiv.org/pdf/2512.21786
  - **contributions:** 1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[VAMP-Net: Interpretable Multi-Path Framework] --> Problem
        Root --> Method
        Root --> Results
    
        Problem[核心问题/Problem<br>Challenges in MTB Drug Resistance Prediction] --> P1[复杂的上位性相互作用/Complex Epistatic Interactions]
        Problem --> P2[测序数据质量多变/Variable Sequencing Data Quality]
    
        Method[主要方法/Method<br>VAMP-Net Multi-Path Architecture] --> M1[路径1: 集合注意力变换器/Path-1: Set Attention Transformer]
        Method --> M2[路径2: 质量感知1D-CNN/Path-2: Quality-Aware 1D-CNN]
        Method --> M3[融合模块/Fusion Module]
        M1 --> M1_Detail[处理变异集合/Processes Variant Sets]
        M2 --> M2_Detail[分析质量指标/Analyzes Quality Metrics]
    
        Results[关键结果/Results<br>Superior Performance & Interpretability] --> R1[性能: >95% 准确率, ~97% AUC/Performance: >95% Acc, ~97% AUC]
        Results --> R2[可解释性: 双层面分析/Interpretability: Dual-Layer Analysis]
        R2 --> R2_1[注意力权重揭示上位网络/Attention Weights Reveal Epistatic Networks]
        R2 --> R2_2[梯度分析识别关键位点与质量指标/Gradient Analysis Identifies Key Loci & Quality Metrics]
    ```

- **[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers**
  - **tags:** [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]
  - **authors:** Krishna Chaitanya Sunkara, Rambabu Konakanchi
  - **institution:** Oracle, Charles Schwab
  - **link:** https://arxiv.org/pdf/2512.21801
  - **contributions:** 1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]
        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]
        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]
    ```

- **[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models**
  - **tags:** [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]
  - **authors:** Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang
  - **institution:** Australian National University, The University of Queensland, GE Research
  - **link:** https://arxiv.org/pdf/2512.21815
  - **contributions:** 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp
  - **Simple LLM Summary:** This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]
        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]
        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]
        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]
        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]
        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]
        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]
    ```

- **[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse**
  - **tags:** [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]
  - **authors:** Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen
  - **institution:** Not explicitly stated in the provided content. Affiliation information is not included.
  - **link:** https://arxiv.org/pdf/2512.21845
  - **code:** this https URLETF2
  - **contributions:** 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题 / Problem"] --> P1["过拟合新数据 / Overfitting to new data"]
        Problem --> P2["灾难性遗忘旧数据 / Catastrophic forgetting of old data"]
        Problem --> P3["特征差异与类别错位 / Feature difference & Class misalignment"]
    
        Method["主要方法 / Method"] --> M1["SCL-PNC方法 / SCL-PNC Method"]
        M1 --> M1_1["自适应层扩展主干 / Adapt-layer for backbone expansion"]
        M1 --> M1_2["动态参数化ETF分类器 / Dynamic Parametric ETF Classifier"]
        M1 --> M1_3["并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation"]
    
        Results["关键结果 / Results"] --> R1["高效处理类别增长 / Efficiently handles increasing categories"]
        Results --> R2["解决类别错位 / Addresses class misalignment"]
        Results --> R3["确保特征一致性 / Ensures feature consistency"]
    ```

- **[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs**
  - **tags:** [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]
  - **authors:** Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville
  - **institution:** Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR
  - **link:** https://arxiv.org/pdf/2512.21852
  - **contributions:** 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差"] --> P1["实践问题/Practical Issue<br>广泛使用但实现与目标不一致"]
        Problem --> P2["理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性"]
        Method["主要方法/Method<br>分析梯度偏差并进行实证验证"] --> M1["分析/Analysis<br>研究多种估计器配置的梯度"]
        Method --> M2["实验/Experiments<br>RL微调多个LLM并评估性能"]
        Results["关键结果/Results<br>无偏梯度配置带来更好性能"] --> R1["在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能"]
        Results --> R2["离线策略/Off-Policy<br>KL正则化有助于稳定异步训练"]
    ```

- **[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening**
  - **tags:** [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]
  - **authors:** Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan
  - **institution:** North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh
  - **link:** https://arxiv.org/pdf/2512.21861
  - **contributions:** 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]
        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]
        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]
    ```

- **[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation**
  - **tags:** [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]
  - **authors:** Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh
  - **institution:** Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University
  - **link:** https://arxiv.org/pdf/2512.21866
  - **contributions:** 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection")
        Problem --> P2("模型需要可解释性/Model needs explainability")
        Method --> M1("将随机森林转换为规则区域/Convert random forest to rule regions")
        Method --> M2("在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data")
        Results --> R1("数据量减少85-93%/Data volume reduced by 85-93%")
        Results --> R2("保持竞争性性能/Maintains competitive performance")
        Results --> R3("抵抗成员推理攻击/Resists membership-inference attacks")
    ```

- **[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction**
  - **tags:** [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]
  - **authors:** Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han
  - **institution:** Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University
  - **link:** https://arxiv.org/pdf/2512.21897
  - **contributions:** 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]
        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]
        Method --> M2[架构设计/Architecture Design]
        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]
        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]
        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]
        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]
        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]
        Results --> R2[消融研究<br>Ablation Studies]
        Results --> R3[概率校准<br>Probability Calibration]
    ```

- **[arXiv251229] GQ-VAE: A gated quantized VAE for learning variable length tokens**
  - **tags:** [nlp], [tokenization], [GQ-VAE, variable-length tokens, VQ-VAE, neural tokenizer, byte-pair encoding]
  - **authors:** Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener
  - **institution:** Kempner Institute, Harvard University
  - **link:** https://arxiv.org/pdf/2512.21913
  - **code:** https://github.com/Theo-Datta-115/gq-vae
  - **contributions:** 1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE's performance and, under equivalent compression, improving downstream language model learning.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[GQ-VAE: A gated quantized VAE for learning variable length tokens] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[传统分词器如BPE是确定性的/Traditional tokenizers like BPE are deterministic]
        Problem --> P2[神经分词器复杂且难集成/Neural tokenizers are complex and hard to integrate]
        Method[主要方法/Method] --> M1[提出GQ-VAE架构/Propose GQ-VAE architecture]
        Method --> M2[学习变长离散令牌/Learn variable-length discrete tokens]
        Method --> M3[可独立预训练/Can be independently pre-trained]
        Results[关键结果/Results] --> R1[压缩和语言建模性能接近BPE/Compression & LM performance approaches BPE]
        Results --> R2[在同等压缩下提升下游LM学习/Improves downstream LM learning at equivalent compression]
    ```

- **[arXiv251229] Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs**
  - **tags:** [mlsys], [others], [Tabular Data Generation, Large Language Models, Multi-Arm Bandit, Data Diversity, In-context Learning]
  - **authors:** Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang
  - **institution:** Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.21915
  - **code:** https://github.com/windblow32/DATE
  - **contributions:** 1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs"] --> Problem["核心问题/Problem: Real-world tabular data is heterogeneous, making universal generation models challenging"]
        Root --> Method["主要方法/Method: DATE framework partitions data, uses LLMs with decision tree feedback, and applies Multi-Arm Bandit for selection"]
        Root --> Results["关键结果/Results: Outperforms SOTA methods, reduces error rate by 23.75%, improves DPO and LLM reasoning"]
    ```

- **[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model**
  - **tags:** [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]
  - **authors:** Nathan Kallus
  - **institution:** Netflix, Cornell University
  - **link:** https://arxiv.org/pdf/2512.21917
  - **contributions:** 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型"] --> Problem
        Root --> Method
        Root --> Results
    
        Problem["核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment"]
        Method["主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model"]
        Results["关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees"]
    ```

- **[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization**
  - **tags:** [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]
  - **authors:** Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law
  - **institution:** JD.COM
  - **link:** https://arxiv.org/pdf/2512.21921
  - **code:** https://github.com/JD-GenX/AutoPP
  - **contributions:** 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]
        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]
        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]
        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]
        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]
        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]
        D --> D2[代码与数据集公开/Code & dataset released]
    ```

- **[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement**
  - **tags:** [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]
  - **authors:** Yiquan Gao, John See
  - **institution:** Heriot-Watt University
  - **link:** https://arxiv.org/pdf/2512.21944
  - **contributions:** 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")
        Method("主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")
        Results("关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")
    ```

- **[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms**
  - **tags:** [ai], [multi-armed bandits], [combinatorial multi-armed bandits, probabilistically triggered arms, hybrid learning, offline data, online interaction]
  - **authors:** Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong
  - **institution:** Southern University of Science and Technology, Microsoft Research
  - **link:** https://arxiv.org/pdf/2512.21925
  - **contributions:** 1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm's consistent advantage over purely online or offline baselines.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Hybrid CMAB-T<br>混合组合多臂老虎机") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("在线方法成本高、适应慢<br>Online: High Cost, Slow")
        Problem --> P2("离线方法受数据质量限制<br>Offline: Data Quality Limits")
        Method --> M1("提出混合CMAB-T框架<br>Propose Hybrid CMAB-T Framework")
        Method --> M2("设计混合CUCB算法<br>Design Hybrid CUCB Algorithm")
        M2 --> M2a("利用离线数据引导探索<br>Use Offline Data to Guide")
        M2 --> M2b("结合在线交互纠正偏差<br>Use Online to Correct Bias")
        Results --> R1("理论悔恨界保证<br>Theoretical Regret Guarantee")
        Results --> R2("实验显示一致优势<br>Empirical Consistent Advantage")
    ```

- **[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs**
  - **tags:** [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]
  - **authors:** Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He
  - **institution:** Chongqing University, Xinjiang University
  - **link:** https://arxiv.org/pdf/2512.21999
  - **code:** https://github.com/hujiayu1223/ALEAHallu
  - **contributions:** 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B
        A --> C
        A --> D
        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]
        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]
        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]
        C --> C1[激活数据集/Activation Dataset]
        C --> C2[定位关键参数/Locate Critical Parameters]
        C --> C3[对抗性编辑/Adversarial Editing]
    ```

- **[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction**
  - **tags:** [ai], [computational biology], [protein language model, ESM-2, dual-stream architecture, 1D CNN, transformer encoder]
  - **authors:** Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar
  - **institution:** National School of Artificial Intelligence (ENSIA)
  - **link:** https://arxiv.org/pdf/2512.22007
  - **contributions:** 1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DuaDeep-SeqAffinity: 序列抗原-抗体亲和力预测 / Sequence-Only Antigen-Antibody Affinity Prediction] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统方法依赖稀缺的3D结构 / Traditional methods rely on scarce 3D structures]
        C --> C1[双流混合架构 / Dual-Stream Hybrid Architecture]
        C1 --> C2[使用ESM-2嵌入 / Uses ESM-2 Embeddings]
        C1 --> C3[1D CNN检测局部模式 / 1D CNN for Local Motifs]
        C1 --> C4[Transformer编码全局上下文 / Transformer for Global Context]
        C1 --> C5[融合模块整合特征 / Fusion Module Integrates Features]
        D --> D1[性能超越SOTA / Outperforms SOTA]
        D --> D2[皮尔逊相关: 0.688 / Pearson: 0.688]
        D --> D3[AUC: 0.890]
        D --> D4[证明序列嵌入的有效性 / Proves Efficacy of Sequence Embeddings]
    ```

- **[arXiv251229] HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness**
  - **tags:** [ai], [graph neural networks], [hypergraph isomorphism network, hypergraph weisfeiler-lehman test, higher-order network robustness, hypergraph neural networks]
  - **authors:** Chengyu Tian, Wenbin Pei
  - **institution:** Not explicitly stated in the provided content.
  - **link:** https://arxiv.org/pdf/2512.22014
  - **contributions:** 1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model's expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HWL-HIN: Hypergraph-Level Hypergraph Isomorphism Network] --> B(核心问题/Problem: High computational cost of robustness assessment; Limited expressive power of HGNNs)
        A --> C(主要方法/Method: Propose HWL-HIN framework inspired by GIN; Prove expressive power equivalent to Hypergraph WL test)
        A --> D(关键结果/Results: Outperforms graph-based models and HGNNs; Maintains superior efficiency)
    ```

- **[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation**
  - **tags:** [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]
  - **authors:** Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni
  - **institution:** University College London, University of Urbino Carlo Bo
  - **link:** https://arxiv.org/pdf/2512.22031
  - **contributions:** 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Hit identification is resource-intensive/命中识别资源密集")
        Method --> M1("Propose tailored evaluation framework/提出定制评估框架")
        Method --> M2("Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型")
        Results --> R1("Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物")
        Results --> R2("Selected hits synthesized & confirmed active/选定命中物被合成并确认有效")
    ```

- **[arXiv251229] Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing**
  - **tags:** [other], [signal processing], [DOA estimation, sparse arrays, coarrays, spatial smoothing, MUSIC]
  - **authors:** Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt
  - **institution:** University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology
  - **link:** https://arxiv.org/pdf/2512.22024
  - **contributions:** 1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[DOA估计精度下降/DOA estimation accuracy degrades for correlated/coherent sources]
        C --> C1[可变窗口空间平滑/Variable Window Size Spatial Smoothing]
        C --> C2[压缩平滑孔径/Compressing the smoothing aperture]
        C --> C3[VWS-CA-MUSIC算法/VWS-CA-MUSIC algorithm]
        D --> D1[提高信噪子空间分离/Increased signal-noise subspace separation]
        D --> D2[性能提升与复杂度降低/Performance improvements and complexity savings]
    ```

- **[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning**
  - **tags:** [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]
  - **authors:** Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo
  - **institution:** Nanjing University, University of Wollongong, University of Rochester
  - **link:** https://arxiv.org/pdf/2512.22029
  - **code:** https://github.com/RL-VIG/LibContinual
  - **contributions:** 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]
        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]
        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]
        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]
        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]
        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]
        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]
    ```

- **[arXiv251229] Why Smooth Stability Assumptions Fail for ReLU Learning**
  - **tags:** [ai], [optimization theory], [ReLU networks, nonsmooth optimization, stability analysis, generalized derivatives, learning dynamics]
  - **authors:** Ronald Katende
  - **institution:** Kabale University
  - **link:** https://arxiv.org/pdf/2512.22055
  - **contributions:** 1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp
  - **Simple LLM Summary:** The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Why Smooth Stability Assumptions Fail for ReLU Learning] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Smooth stability assumptions are violated by ReLU networks.]
        C[主要方法/Method<br>Provide counterexample and identify minimal generalized derivative condition.]
        D[关键结果/Results<br>Classical bounds fail; stability can be restored under nonsmooth-aware condition.]
    ```

- **[arXiv251229] Scaling Adversarial Training via Data Selection**
  - **tags:** [ai], [adversarial robustness], [adversarial training, PGD, sample selection, gradient matching, margin-based sampling]
  - **authors:** Youran Ye, Dejin Wang, Ajinkya Bhandare
  - **institution:** Northeastern University
  - **link:** https://arxiv.org/pdf/2512.22069
  - **code:** https://github.com/youranye/Selective-Adversarial-Training
  - **contributions:** 1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Scaling Adversarial Training via Data Selection] --> B[核心问题/Problem: PGD计算成本高/High PGD Computational Cost]
        A --> C[主要方法/Method: 选择性对抗训练/Selective Adversarial Training]
        C --> D[选择标准1: 基于边界的采样/Margin-based Sampling]
        C --> E[选择标准2: 梯度匹配采样/Gradient-matching Sampling]
        A --> F[关键结果/Results: 鲁棒性相当，计算减少50%/Comparable Robustness, 50% Computation Reduction]
    ```

- **[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling**
  - **tags:** [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]
  - **authors:** Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras
  - **institution:** Uppsala University
  - **link:** https://arxiv.org/pdf/2512.22066
  - **contributions:** 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同"] --> Problem_Sub1["SRAM大小与频率如何影响能效？"]
        Problem --> Problem_Sub2["内存带宽如何限制性能？"]
        Method["主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法"] --> Method_Sub1["能耗建模/Energy Modeling"]
        Method --> Method_Sub2["延迟模拟/Latency Simulation"]
        Method --> Method_Sub3["操作强度分析/Operational Intensity"]
        Results["关键结果/Results"] --> Results_Sub1["总能耗主要由SRAM大小决定<br>大缓存增加静态能耗"]
        Results --> Results_Sub2["高频可降低总能耗<br>（减少静态能耗）"]
        Results --> Results_Sub3["最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)"]
    ```

- **[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law**
  - **tags:** [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]
  - **authors:** Chiwun Yang
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22088
  - **contributions:** 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\{-1/6\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp
  - **Simple LLM Summary:** This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]
        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]
        C --> C2[近似为核行为 / Approximate to kernel behaviors]
        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]
        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]
        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]
        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]
        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]
    ```

- **[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting**
  - **tags:** [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]
  - **authors:** Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang
  - **institution:** University of Minnesota
  - **link:** https://arxiv.org/pdf/2512.22101
  - **contributions:** 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp
  - **Simple LLM Summary:** This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[A2P-Vis] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]
        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]
        B1 --> B3[组装成专业报告/Assembling coherent professional report]
        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]
        C1 --> C2[数据分析器/Data Analyzer]
        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]
        C1 --> C4[报告呈现器/Presenter]
        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]
        D --> D1[端到端协同分析/End-to-end co-analysis]
        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]
    ```

- **[arXiv251229] Explainable Multimodal Regression via Information Decomposition**
  - **tags:** [ai], [multimodal machine learning], [Partial Information Decomposition (PID), multimodal regression, interpretability, Gaussianity assumption, conditional independence regularizer]
  - **authors:** Zhaozhao Ma, Shujian Yu
  - **institution:** Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway
  - **link:** https://arxiv.org/pdf/2512.22102
  - **code:** https://github.com/xxx/PIDReg (URL placeholder from abstract)
  - **contributions:** 1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID's underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Explainable Multimodal Regression via Information Decomposition<br/>可解释多模态回归与信息分解] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[现有方法缺乏量化模态贡献与交互的工具<br/>Existing methods lack tools to quantify modality contributions & interactions]
        C --> C1[基于PID分解模态信息<br/>Decompose modality info via PID]
        C --> C2[引入高斯性假设与正则化<br/>Introduce Gaussianity & regularizer]
        D --> D1[在6个数据集上超越SOTA<br/>Outperforms SOTA on 6 datasets]
        D --> D2[提升预测精度与可解释性<br/>Improves predictive accuracy & interpretability]
    ```

- **[arXiv251229] Sensitivity Analysis of the Consistency Assumption**
  - **tags:** [other], [causal inference], [consistency assumption, sensitivity analysis, hidden versions of treatment, partial identification, stable unit treatment value assumption (SUTVA)]
  - **authors:** Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank
  - **institution:** Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University
  - **link:** https://arxiv.org/pdf/2512.21379
  - **contributions:** 1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Sensitivity Analysis of the Consistency Assumption] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[一致性假设可能被违反/Consistency Assumption May Be Violated]
        B1 --> B2[存在隐藏的治疗版本/Hidden Versions of Treatment Exist]
        C --> C1[新颖的敏感性分析方法/Novel Sensitivity Analysis Method]
        C1 --> C2[专注于隐藏版本导致的混杂/Focus on Confounding by Hidden Versions]
        C2 --> C3[引入新的数学符号/Introduces New Mathematical Notation]
        D --> D1[提出新的敏感性参数/Proposes New Sensitivity Parameter]
        D1 --> D2[便于部分识别因果估计量/Facilitates Partial Identification of Causal Estimands]
    ```

- **[arXiv251229] Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)**
  - **tags:** [ai], [spatio-temporal forecasting], [dynamic attention, zero-inflated negative binomial, elastic-net gating]
  - **authors:** Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau
  - **institution:** Harvard University, Stockholm University, Malmö University, University of Catania, University of Toronto
  - **link:** https://arxiv.org/pdf/2512.21435
  - **contributions:** 1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[DynAttn: Interpretable Spatio-Temporal Forecasting] --> B(核心问题/Problem: Forecasting sparse, bursty conflict fatalities)
        A --> C(主要方法/Method: Dynamic attention, elastic-net gating, ZINB likelihood)
        A --> D(关键结果/Results: Higher accuracy, interpretable regional dynamics)
    ```

- **[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding**
  - **tags:** [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]
  - **authors:** Akshansh Mishra
  - **institution:** Politecnico di Milano, AI Fab Lab
  - **link:** https://arxiv.org/pdf/2512.21344
  - **contributions:** 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模"]
        Root --> Problem["准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW"]
        Root --> Method["使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction"]
        Root --> Results["模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation"]
    ```

- **[arXiv251229] An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry**
  - **tags:** [ai], [information geometry], [Fisher-Rao metric, non-parametric, G-entropy, Covariate Fisher Information Matrix (cFIM), intrinsic dimensionality]
  - **authors:** Bing Cheng, Howell Tong
  - **institution:** Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text.
  - **link:** https://arxiv.org/pdf/2512.21451
  - **contributions:** 1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cramér-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp
  - **Simple LLM Summary:** This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry] --> B(核心问题/Problem: Intractability of infinite-dimensional Fisher-Rao metric)
        A --> C(主要方法/Method: Orthogonal decomposition of tangent space to derive Covariate Fisher Information Matrix (cFIM))
        A --> D(关键结果/Results: Trace Theorem for G-entropy, link to CRLB, testable Manifold Hypothesis via cFIM rank)
    ```

- **[arXiv251229] Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases**
  - **tags:** [other], [biomedical sensing and diagnostics], [vertical flow assay, dual-mode detection, neural network-based quantification, multiplexed optical sensor, point-of-care diagnostics]
  - **authors:** Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan
  - **institution:** University of California, Los Angeles (UCLA)
  - **link:** https://arxiv.org/pdf/2512.21389
  - **contributions:** 1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson's r > 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 µL of serum.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp
  - **Simple LLM Summary:** This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Deep learning-enhanced dual-mode multiplexed optical sensor<br>深度学习增强的双模式多重光学传感器] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br>Current POC tests: long turnaround, narrow range, single-analyte<br>当前POC测试：耗时长、范围窄、单分析物]
        C[主要方法/Method<br>Dual-mode (colorimetric+chemiluminescent) xVFA + Neural Network<br>双模式(比色+化学发光)xVFA + 神经网络]
        D[关键结果/Results<br>Simultaneous 3-analyte quant in 23 min, wide dynamic range, r>0.96<br>23分钟同步3分析物定量，宽动态范围，r>0.96]
    ```

- **[arXiv251229] Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications**
  - **tags:** [other], [quantum signal processing], [quantum nondecimated wavelet transform, epsilon decimation, Hadamard test, quantum wavelet shrinkage, shift invariance]
  - **authors:** Brani Vidakovic
  - **institution:** Texas A&M University
  - **link:** https://arxiv.org/pdf/2512.21478
  - **contributions:** 1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Quantum Nondecimated Wavelet Transform<br/>量子非抽取小波变换] --> B(核心问题/Problem: How to embed classical NDWT's redundancy and shift invariance into quantum computation?<br/>如何将经典NDWT的冗余性和平移不变性嵌入量子计算？)
        A --> C(主要方法/Method: Two complementary quantum formulations.<br/>两种互补的量子形式。)
        C --> C1(Formulation 1: Epsilon-decimated, uses controlled circular shifts & wavelet unitary.<br/>方法一：基于ε抽取，使用受控循环移位和小波酉变换。)
        C --> C2(Formulation 2: Hadamard test, uses diagonal phase operators for interference.<br/>方法二：基于Hadamard测试，使用对角相位算子进行干涉。)
        A --> D(关键结果/Results: Quantum NDWTs enable coherent postprocessing (e.g., shrinkage) and direct access to scalograms/spectra for applications like denoising.<br/>量子NDWT支持相干后处理并可直接获取尺度图/频谱，用于去噪等应用。)
    ```

- **[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models**
  - **tags:** [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]
  - **authors:** Takuro Kutsuna
  - **institution:** Toyota Central R&D Labs., Inc.
  - **link:** https://arxiv.org/pdf/2512.21593
  - **contributions:** 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Residual Prior Diffusion (RPD) / 残差先验扩散模型"] --> Problem["核心问题/Problem"]
        Root --> Method["主要方法/Method"]
        Root --> Results["关键结果/Results"]
        Problem --> P1["单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details"]
        Method --> M1["两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model"]
        Method --> M2["概率模型与可处理ELBO / Probabilistic model with tractable ELBO"]
        Results --> R1["在合成数据上准确捕捉细节 / Accurately captures details on synthetic data"]
        Results --> R2["自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines"]
        Results --> R3["少步推理保持性能 / Maintains performance with few inference steps"]
    ```

- **[arXiv251229] Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine**
  - **tags:** [other], [photonic computing], [spatial photonic Ising machine, Hadamard product, amplitude-only modulation, rank-free coupling, incoherent light field]
  - **authors:** Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng
  - **institution:** Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences
  - **link:** https://arxiv.org/pdf/2512.21587
  - **contributions:** 1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation >0.98). 3. Shows the system's capability for ground-state search with <0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine<br>振幅调制空间光子伊辛机"]
        Root --> Problem["核心问题/Problem<br>Existing SPIMs sacrifice efficiency or scale to encode high-rank coupling and external fields.<br>现有SPIM编码高秩耦合和外场时牺牲效率或规模。"]
        Root --> Method["主要方法/Method<br>Reformulate Hamiltonian as sum of Hadamard products; map to incoherent light via amplitude modulators.<br>将哈密顿量重写为哈达玛积之和；通过振幅调制器映射到非相干光场。"]
        Root --> Results["关键结果/Results<br>797 spins, >0.98 correlation, <0.3% error rate, enables phase transition observation.<br>797个自旋，>0.98相关性，<0.3%错误率，支持相变观测。"]
    ```

- **[arXiv251229] Tilt Matching for Scalable Sampling and Fine-Tuning**
  - **tags:** [mlsys], [diffusion models], [Tilt Matching, stochastic interpolants, flow matching, unnormalized densities, fine-tuning]
  - **authors:** Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo
  - **institution:** Harvard University, University of Oxford, Kempner Institute, IAIFI
  - **link:** https://arxiv.org/pdf/2512.21829
  - **contributions:** 1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a "tilted" distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Tilt Matching for Scalable Sampling and Fine-Tuning] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Sampling from unnormalized densities and fine-tuning generative models] --> Problem_Detail[挑战/Challenges: Requires scalable, low-variance methods without reward gradients]
        Method[主要方法/Method: Tilt Matching] --> Method_Detail1[基础/Basis: Dynamical equation relating flow matching velocity to tilted distribution]
        Method_Detail1 --> Method_Detail2[特性/Properties: Implicitly solves stochastic optimal control, lower variance, no reward gradients needed]
        Results[关键结果/Results: Empirical Verification] --> Results_Detail1[应用/Applications: State-of-the-art on Lennard-Jones potentials, competitive on Stable Diffusion fine-tuning]
    ```

- **[arXiv251229] Modeling high dimensional point clouds with the spherical cluster model**
  - **tags:** [ai], [clustering], [spherical cluster model, high-dimensional median, non-smooth optimization, Clarke gradient, stratified cell complex]
  - **authors:** Frédéric Cazals, Antoine Commaret, Louis Goldenberg
  - **institution:** Université Côte d'Azur, Inria, Ecole Polytechnique
  - **link:** https://arxiv.org/pdf/2512.21960
  - **contributions:** 1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model's center behaves as a parameterized high-dimensional median.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model's center is found to act as a robust, parameterized median.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Modeling high dimensional point clouds with the spherical cluster model] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[为高维点云建模/Modeling high-dimensional point clouds]
        C --> C1[球形聚类模型/Spherical Cluster Model]
        C --> C2[精确求解器使用Clarke梯度/Exact solver using Clarke gradient]
        D --> D1[精确算法比BFGS快得多/Exact algorithm much faster than BFGS]
        D --> D2[中心表现为参数化高维中位数/Center acts as parameterized high-dimensional median]
    ```

- **[arXiv251229] A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models**
  - **tags:** [other], [dynamical systems, numerical linear algebra], [linear conservation laws, Frobenius norm, orthogonal projection, matrix correction, data-driven models]
  - **authors:** John M. Mango, Ronald Katende
  - **institution:** Makerere University, Kabale University
  - **link:** https://arxiv.org/pdf/2512.22084
  - **contributions:**  1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models"] --> Problem["核心问题/Problem: Learned linear models violate known linear conservation laws."]
        Root --> Method["主要方法/Method: Apply orthogonal projection A* = Â - C(CᵀC)⁻¹CᵀÂ to enforce CᵀA=0."]
        Root --> Results["关键结果/Results: Enforces exact conservation with minimal perturbation; correction is unique and low-rank."]
    ```
