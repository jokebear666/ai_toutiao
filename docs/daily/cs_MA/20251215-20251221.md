# 20251215-20251221 (cs.MA)

## 2025-12-18

- **[arXiv251218] SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis**
  - **tags:** [mlsys], [others], [multi-agent system, structural plasticity, variational free energy, metabolic lagrangian, stochastic thermodynamics, unsupervised anomaly detection]
  - **authors:** Mustapha Hamdi
  - **institution:** InnoDeep
  - **link:** https://arxiv.org/pdf/2512.14708
  - **Simple LLM Summary:** This paper introduces SGEMAS, a bio-inspired multi-agent system that uses agent birth/death and a variational free energy objective to achieve energy-efficient, unsupervised anomaly detection in physiological signals. It validates the approach on the MIT-BIH Arrhythmia Database, showing that this physics-based, energy-constrained model can detect anomalies in a zero-shot setting, outperforming a standard autoencoder baseline.

- **[arXiv251218] Epistemic diversity across language models mitigates knowledge collapse**
  - **tags:** [mlsys], [llm training], [model collapse, epistemic diversity, AI ecosystem, self-training, distributed training]
  - **authors:** Damian Hodel, Jevin D. West
  - **institution:** University of Washington
  - **link:** https://arxiv.org/pdf/2512.15011
  - **Simple LLM Summary:** The paper investigates whether diversity across language models (an "AI ecosystem") can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.
