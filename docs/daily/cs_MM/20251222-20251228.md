---
slug: /daily/csmm/20251222-20251228
---
# 20251222-20251228 (cs.MM)

## 2025-12-22

- **[arXiv251222] Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse**
  - **tags:** [mlsys], [multi-modal inference], [modular reuse, on-device execution, model decomposition, parallel execution, quantization]
  - **authors:** Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan
  - **institution:** University of Massachusetts Amherst, Adobe Research
  - **link:** https://arxiv.org/pdf/2512.17108
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27â€“33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.

- **[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs**
  - **tags:** [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]
  - **authors:** Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao
  - **institution:** Nanjing University
  - **link:** https://arxiv.org/pdf/2512.17319
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.

- **[arXiv251222] Region-Constraint In-Context Generation for Instructional Video Editing**
  - **tags:** [mlsys], [diffusion training], [in-context generation, video diffusion, latent regularization, attention regularization, region-constraint, joint denoising]
  - **authors:** Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei
  - **institution:** University of Science and Technology of China, HiDream.ai Inc.
  - **link:** https://arxiv.org/pdf/2512.17650
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp
  - **Simple LLM Summary:** The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks.