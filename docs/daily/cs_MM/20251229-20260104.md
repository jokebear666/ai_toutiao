---
slug: /daily/csmm/20251229-20260104
---
# 20251229-20260104 (cs.MM)

## 2025-12-29

- **[arXiv251229] Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding**
  - **tags:** [sec], [steganography], [raster domain steganography, glyph perturbation, deterministic rasterization, multimodal embedding, text-based data hiding]
  - **authors:** A V Uday Kiran Kandala
  - **institution:** Queen Mary University of London
  - **link:** https://arxiv.org/pdf/2512.21698
  - **contributions:** 1. Proposes a unified Glyph Perturbation Cardinality (GPC) framework for embedding heterogeneous data (text, images, audio, video) directly into the pixel space of rendered text glyphs. 2. Operates exclusively in the raster domain after font rendering, modifying bitmap pixels with minimal, visually imperceptible intensity increments for covert communication. 3. Introduces a decoding method based on re-rasterizing cover text, subtracting canonical glyph rasters, and recovering payload via pixel count analysis, leveraging deterministic raster behavior.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a raster domain steganography framework that embeds multimodal data into text by minimally perturbing the interior pixels of rendered glyphs. The method is visually imperceptible and computationally lightweight, enabling ordinary text to serve as a covert medium for secure data embedding. It generalizes beyond traditional linguistic steganography by operating directly on the deterministic bitmap output of text rendering pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding") --> Problem("核心问题/Problem: How to embed multimodal data covertly into ordinary text?")
        Root --> Method("主要方法/Method: Glyph Perturbation Cardinality (GPC) Framework")
        Root --> Results("关键结果/Results: Visually imperceptible, lightweight embedding in raster domain")
        Problem --> P1("传统方法局限/Limitations of linguistic & structural methods")
        Method --> M1("操作于栅格化后/Operates post-rasterization")
        Method --> M2("扰动字形内部像素/Perturbs interior ink pixels")
        Method --> M3("基于像素基数编码/Encodes via pixel cardinality")
        Results --> R1("支持多模态数据/Supports multimodal data")
        Results --> R2("解码稳定可靠/Stable & decodable signal")
    ```

- **[arXiv251229] Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion**
  - **tags:** [ai], [recommender systems], [frozen large video language models, micro-video recommendation, feature fusion, intermediate hidden states, dual feature fusion]
  - **authors:** Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du
  - **institution:** Nanjing University of Science and Technology, Singapore Management University, University of Wisconsin-Madison, GienTech Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21863
  - **contributions:** 1. Conducted the first systematic empirical study on integrating frozen LVLMs into micro-video recommendation, evaluating feature extraction paradigms (captions vs. hidden states) and integration strategies (replacement vs. fusion) with ID embeddings. 2. Derived three key principles: intermediate hidden states outperform captions, ID embeddings are irreplaceable (fusion > replacement), and the effectiveness of hidden states varies across layers. 3. Proposed the Dual Feature Fusion (DFF) Framework, a lightweight plug-and-play method that adaptively fuses multi-layer LVLM representations with ID embeddings, achieving state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically studies how to best integrate frozen Large Video-Language Models (LVLMs) as feature extractors for micro-video recommendation. It finds that using intermediate decoder hidden states and fusing them with item ID embeddings is superior to using generated captions or replacing IDs. Based on these insights, the authors propose the Dual Feature Fusion (DFF) framework, which achieves state-of-the-art results on benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Frozen LVLMs for Micro-Video Recommendation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LVLM集成缺乏系统评估/Lack of systematic evaluation for LVLM integration")
        Method --> M1("系统实证研究/Systematic empirical study")
        Method --> M2("提出DFF框架/Propose DFF Framework")
        M1 --> M1a("比较特征提取范式/Compare feature extraction paradigms")
        M1 --> M1b("比较ID集成策略/Compare ID integration strategies")
        M2 --> M2a("自适应融合多层特征/Adaptively fuse multi-layer features")
        M2 --> M2b("轻量级即插即用/Lightweight plug-and-play")
        Results --> R1("中间隐藏态优于描述/Intermediate hidden states > captions")
        Results --> R2("ID嵌入不可替代/Fusion > replacement")
        Results --> R3("DFF实现SOTA性能/DFF achieves SOTA performance")
    ```

- **[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement**
  - **tags:** [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]
  - **authors:** Yiquan Gao, John See
  - **institution:** Heriot-Watt University
  - **link:** https://arxiv.org/pdf/2512.21944
  - **contributions:** 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")
        Method("主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")
        Results("关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")
    ```

## 2025-12-30

- **[arXiv251230] Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark**
  - **tags:** [cv], [visual question answering], [signboard VQA, OCR-integrated VQA, multimodal dataset, Vietnamese, multi-agent framework]
  - **authors:** Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen
  - **institution:** University of Information Technology, Vietnam National University, Ho Chi Minh City
  - **link:** https://arxiv.org/pdf/2512.22218
  - **contributions:** 1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark"] --> B["核心问题/Problem: 理解自然场景中的招牌文本对于VQA的现实应用至关重要，但在低资源语言中仍未充分探索。"]
        A --> C["主要方法/Method: 1. 引入首个大规模越南语招牌VQA数据集ViSignVQA。 2. 通过集成越南语OCR和语言模型来适配SOTA VQA模型。 3. 提出结合感知与推理智能体及GPT-4的多智能体VQA框架。"]
        A --> D["关键结果/Results: 1. 添加OCR文本使F1分数提升高达209%。 2. 多智能体框架通过多数投票达到75.98%准确率。 3. 创建了首个捕获真实世界场景文本特征的大规模越南语多模态基准。"]
    ```

- **[arXiv251230] Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM**
  - **tags:** [other], [motion capture, wearable computing, human-computer interaction], [IMU, WebXR, SLAM, IoT, edge computing]
  - **authors:** Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidané, Tejaswi Gowda
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.22690
  - **contributions:** 1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5° joint-angle error) at approximately 5% of the cost, with low latency and high reliability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp
  - **Simple LLM Summary:** The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Motion capture is costly and complex, limiting accessibility] --> P1[昂贵且复杂/Expensive & Complex]
        Problem --> P2[局限于专业实验室/Limited to Specialized Labs]
        Method[主要方法/Method: Open-source, low-cost system using IoT sensors and web tech] --> M1[身体佩戴IMU传感器网络/Body-worn IMU Sensor Network]
        Method --> M2[智能手机WebXR SLAM定位/Smartphone WebXR SLAM for Positioning]
        Method --> M3[基于浏览器的应用/Web-Browser-Based Application]
        Results[关键结果/Results: Affordable, accurate, and real-time performance] --> R1[成本约为商业系统的5%/~5% Cost of Commercial System]
        Results --> R2[平均关节角度误差2-5度/Mean Joint-Angle Error 2-5°]
        Results --> R3[实时低延迟/Real-Time with Low Latency]
    ```

- **[arXiv251230] Federated Multi-Task Clustering**
  - **tags:** [mlsys], [federated learning], [federated clustering, spectral clustering, multi-task learning, tensor methods, ADMM]
  - **authors:** S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong
  - **institution:** South China University of Technology, Dalian University of Technology, Xidian University
  - **link:** https://arxiv.org/pdf/2512.22897
  - **contributions:** 1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Federated Multi-Task Clustering] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Centralized models inapplicable to decentralized environments/集中式模型不适用于去中心化环境]
        B --> B2[Poor generalization due to unreliable pseudo-labels/伪标签不可靠导致泛化性能差]
        B --> B3[Failure to capture latent client correlations/未能捕获客户端间的潜在关联]
        C --> C1[Client-side personalized clustering module/客户端个性化聚类模块]
        C --> C2[Server-side tensorial correlation module/服务器端张量关联模块]
        C --> C3[ADMM-based distributed algorithm/基于ADMM的分布式算法]
        D --> D1[Outperforms baselines and SOTA/性能优于基线和前沿方法]
        D --> D2[Validated on real-world datasets/在真实数据集上得到验证]
    ```

- **[arXiv251230] Bridging Your Imagination with Audio-Video Generation via a Unified Director**
  - **tags:** [cv], [video generation], [unified director model, Mixture-of-Transformers, interleaved concept learning, disentangled expert learning]
  - **authors:** Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen
  - **institution:** ByteDance Intelligent Creation, Nanyang Technological University
  - **link:** https://arxiv.org/pdf/2512.23222
  - **code:** https://kebii.github.io/UniMAGE
  - **contributions:** 1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a "first interleaving, then disentangling" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel "first interleaving, then disentangling" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有系统将脚本起草与关键镜头设计视为独立任务/Existing systems treat script drafting and key-shot design as disjoint tasks]
        C --> C1[提出统一导演模型UniMAGE/Propose unified director model UniMAGE]
        C --> C2[采用混合Transformer架构/Employ Mixture-of-Transformers architecture]
        C --> C3[引入"先交错，后解耦"训练范式/Introduce "first interleaving, then disentangling" training paradigm]
        D --> D1[在开源模型中达到SOTA性能/Achieves SOTA performance among open-source models]
        D --> D2[生成逻辑连贯的脚本和视觉一致的图像/Generates logically coherent scripts and visually consistent keyframes]
    ```

- **[arXiv251230] Multi Agents Semantic Emotion Aligned Music to Image Generation with Music Derived Captions**
  - **tags:** [cv], [multi-modal generation], [music-to-image generation, multi-agent system, valence-arousal alignment, diffusion models, CLIP fine-tuning]
  - **authors:** Junchang Shi, Gang Li
  - **institution:** University of Science and Technology Beijing
  - **link:** https://arxiv.org/pdf/2512.23320
  - **contributions:** 1. A multi-agent semantic-emotion aligned framework (MESA MIG) that uses cooperating agents to refine music-derived captions for image generation. 2. Integration of continuous Valence-Arousal regression from music and a CLIP-based visual emotion head to enforce emotional alignment between music and generated images. 3. Empirical demonstration that the framework outperforms caption-only and single-agent baselines in aesthetic quality, semantic consistency, and emotional alignment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf4833583da2d40740efcb2a747ee2d1334ebcef0ce1d2e3d3717470dea449e2_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of generating images from music by externalizing the visual imagery evoked by listening. The proposed MESA MIG framework uses a multi-agent system to produce and refine structured music captions, and enforces emotional alignment using Valence-Arousal regression. Experiments show it outperforms baselines in semantic and emotional consistency.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multi Agents Semantic Emotion Aligned Music to Image Generation<br>多智能体语义情感对齐的音乐到图像生成"] --> Problem["Externalize visual imagery from music<br>将音乐引发的视觉意象外显化"]
        Root --> Method["Multi-agent caption refinement & VA alignment<br>多智能体描述优化与情感唤醒度对齐"]
        Root --> Results["Outperforms baselines in quality & alignment<br>在质量与对齐上超越基线"]
    ```

- **[arXiv251230] RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction**
  - **tags:** [cv], [3D reconstruction], [benchmark, multi-view, physical degradation, neural radiance field, Gaussian splatting]
  - **authors:** Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada
  - **institution:** The University of Tokyo, NII, Tohoku University, University of Würzburg, RIKEN
  - **link:** https://arxiv.org/pdf/2512.23437
  - **contributions:** 1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[RealX3D: A Physically-Degraded 3D Benchmark] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[真实世界退化影响3D重建/Real-world degradations hinder 3D reconstruction]
        C --> C1[构建真实捕获基准/Build real-capture benchmark]
        C --> C2[四类退化, 多严重级别/Four corruption families, multiple severity levels]
        C --> C3[提供对齐的LQ/GT视图, RAW数据, 激光扫描/Provide aligned LQ/GT views, RAW, laser scans]
        D --> D1[当前方法质量显著下降/Current methods show substantial quality degradation]
        D --> D2[突出现实挑战性/Underscores fragility in challenging real environments]
    ```

- **[arXiv251230] Unlocking WebRTC for End User Driven Innovation**
  - **tags:** [sys], [web systems], [WebRTC, browser extension, API interception, real-time customization]
  - **authors:** Kundan Singh
  - **institution:** Intencity Cloud Technologies
  - **link:** https://arxiv.org/pdf/2512.23688
  - **contributions:** 1. A software architecture enabling end-user-driven customization of WebRTC applications via a browser extension. 2. A tool (RTC Helper) that intercepts and modifies WebRTC APIs in real-time to change web app behavior. 3. Support for rapid prototyping and over a hundred built-in examples across ten+ customization categories for novel communication use cases.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54e58edaa93822049a8bb88a45bce19b8d615d6c0c4f64e6c5e2c16cc6a757d3_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of vendor lock-in and limited innovation in WebRTC-based applications by proposing RTC Helper, a browser extension that intercepts WebRTC APIs to allow real-time customization by end-users and developers. This enables novel use cases and rapid prototyping without app redeployment. The conclusion is that this approach unlocks user-driven innovation in web multimedia communication.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Unlocking WebRTC for End User Driven Innovation] --> B[核心问题/Problem: Vendor lock-in hinders innovation in WebRTC apps]
        A --> C[主要方法/Method: RTC Helper browser extension intercepts & modifies WebRTC APIs]
        A --> D[关键结果/Results: Enables real-time customization & rapid prototyping with many examples]
    ```

- **[arXiv251230] SemCovert: Secure and Covert Video Transmission via Deep Semantic-Level Hiding**
  - **tags:** [sec], [semantic communication security], [semantic hiding, randomized embedding, secret semantic extractor]
  - **authors:** Zhihan Cao, Xiao Yang, Gaolei Li, Jun Wu, Jianhua Li, Yuchen Liu
  - **institution:** Shanghai Jiao Tong University, North Carolina State University
  - **link:** https://arxiv.org/pdf/2512.22233
  - **contributions:** 1. Proposes SemCovert, a deep semantic-level hiding framework for secure and covert video transmission, integrating co-designed hiding and extraction models into the semantic communication pipeline. 2. Introduces a randomized semantic hiding strategy to break embedding determinism and introduce unpredictable distribution patterns, improving resistance to analysis. 3. Demonstrates through experiments that the framework effectively mitigates eavesdropping/detection risks, reliably conceals secret videos with minor video quality degradation, preserving transmission fidelity.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b368b0e0e2b9ae1e6c7bf4629ba64b86e8e570fdf21089b033a631ae92af3c_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses privacy leakage in video semantic communication by proposing SemCovert, a framework that hides secret information at the semantic level using co-designed models and a randomized hiding strategy. The method enables authorized recovery while remaining imperceptible to regular users and resistant to statistical analysis. Experimental results confirm its effectiveness for secure and covert transmission without significantly compromising video quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[SemCovert: Secure and Covert Video Transmission] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[隐私泄露风险/Privacy Leakage Risk]
        B --> B2[传统安全技术不适用/Traditional Security Inapplicable]
        C --> C1[语义隐藏模型与提取器/Semantic Hiding Model & Extractor]
        C --> C2[随机化语义隐藏策略/Randomized Semantic Hiding Strategy]
        D --> D1[有效缓解窃听与检测风险/Effectively Mitigates Eavesdropping & Detection]
        D --> D2[可靠隐藏秘密视频/Reliably Conceals Secret Videos]
        D --> D3[视频质量轻微下降/Minor Video Quality Degradation]
    ```
