---
slug: /daily/csmm/20251229-20260104
---
# 20251229-20260104 (cs.MM)

## 2025-12-29

- **[arXiv251229] Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding**
  - **tags:** [sec], [steganography], [raster domain steganography, glyph perturbation, deterministic rasterization, multimodal embedding, text-based data hiding]
  - **authors:** A V Uday Kiran Kandala
  - **institution:** Queen Mary University of London
  - **link:** https://arxiv.org/pdf/2512.21698
  - **contributions:** 1. Proposes a unified Glyph Perturbation Cardinality (GPC) framework for embedding heterogeneous data (text, images, audio, video) directly into the pixel space of rendered text glyphs. 2. Operates exclusively in the raster domain after font rendering, modifying bitmap pixels with minimal, visually imperceptible intensity increments for covert communication. 3. Introduces a decoding method based on re-rasterizing cover text, subtracting canonical glyph rasters, and recovering payload via pixel count analysis, leveraging deterministic raster behavior.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces a raster domain steganography framework that embeds multimodal data into text by minimally perturbing the interior pixels of rendered glyphs. The method is visually imperceptible and computationally lightweight, enabling ordinary text to serve as a covert medium for secure data embedding. It generalizes beyond traditional linguistic steganography by operating directly on the deterministic bitmap output of text rendering pipelines.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding") --> Problem("核心问题/Problem: How to embed multimodal data covertly into ordinary text?")
        Root --> Method("主要方法/Method: Glyph Perturbation Cardinality (GPC) Framework")
        Root --> Results("关键结果/Results: Visually imperceptible, lightweight embedding in raster domain")
        Problem --> P1("传统方法局限/Limitations of linguistic & structural methods")
        Method --> M1("操作于栅格化后/Operates post-rasterization")
        Method --> M2("扰动字形内部像素/Perturbs interior ink pixels")
        Method --> M3("基于像素基数编码/Encodes via pixel cardinality")
        Results --> R1("支持多模态数据/Supports multimodal data")
        Results --> R2("解码稳定可靠/Stable & decodable signal")
    ```

- **[arXiv251229] Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion**
  - **tags:** [ai], [recommender systems], [frozen large video language models, micro-video recommendation, feature fusion, intermediate hidden states, dual feature fusion]
  - **authors:** Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du
  - **institution:** Nanjing University of Science and Technology, Singapore Management University, University of Wisconsin-Madison, GienTech Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.21863
  - **contributions:** 1. Conducted the first systematic empirical study on integrating frozen LVLMs into micro-video recommendation, evaluating feature extraction paradigms (captions vs. hidden states) and integration strategies (replacement vs. fusion) with ID embeddings. 2. Derived three key principles: intermediate hidden states outperform captions, ID embeddings are irreplaceable (fusion > replacement), and the effectiveness of hidden states varies across layers. 3. Proposed the Dual Feature Fusion (DFF) Framework, a lightweight plug-and-play method that adaptively fuses multi-layer LVLM representations with ID embeddings, achieving state-of-the-art performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp
  - **Simple LLM Summary:** This paper systematically studies how to best integrate frozen Large Video-Language Models (LVLMs) as feature extractors for micro-video recommendation. It finds that using intermediate decoder hidden states and fusing them with item ID embeddings is superior to using generated captions or replacing IDs. Based on these insights, the authors propose the Dual Feature Fusion (DFF) framework, which achieves state-of-the-art results on benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Frozen LVLMs for Micro-Video Recommendation") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("LVLM集成缺乏系统评估/Lack of systematic evaluation for LVLM integration")
        Method --> M1("系统实证研究/Systematic empirical study")
        Method --> M2("提出DFF框架/Propose DFF Framework")
        M1 --> M1a("比较特征提取范式/Compare feature extraction paradigms")
        M1 --> M1b("比较ID集成策略/Compare ID integration strategies")
        M2 --> M2a("自适应融合多层特征/Adaptively fuse multi-layer features")
        M2 --> M2b("轻量级即插即用/Lightweight plug-and-play")
        Results --> R1("中间隐藏态优于描述/Intermediate hidden states > captions")
        Results --> R2("ID嵌入不可替代/Fusion > replacement")
        Results --> R3("DFF实现SOTA性能/DFF achieves SOTA performance")
    ```

- **[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement**
  - **tags:** [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]
  - **authors:** Yiquan Gao, John See
  - **institution:** Heriot-Watt University
  - **link:** https://arxiv.org/pdf/2512.21944
  - **contributions:** 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --> Problem
        Root --> Method
        Root --> Results
        Problem("核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")
        Method("主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")
        Results("关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")
    ```
