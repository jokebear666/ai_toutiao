---
slug: /daily/csmm/20260105-20260111
---
# 20260105-20260111 (cs.MM)

## 2026-01-05

- **[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications**
  - **tags:** [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]
  - **authors:** Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen
  - **institution:** Qifu Technology, Fudan University
  - **link:** https://arxiv.org/pdf/2601.00150
  - **contributions:** 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[缺乏金融信贷领域专用多模态基准/Lack of domain-specific multimodal benchmark for financial credit]
        C --> C1[构建隐私合规的合成-采集管道/Build privacy-compliant synthesis-capture pipeline]
        C --> C2[设计三维评估框架/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]
        D --> D1[评估23个VLM/Evaluate 23 VLMs]
        D --> D2[Qfin-VL-Instruct性能最佳/Qfin-VL-Instruct achieves top score]
        D --> D3[鲁棒性挑战/Robustness remains a challenge]
    ```

- **[arXiv260105] Timed text extraction from Taiwanese Kua-á-hì TV series**
  - **tags:** [other], [music information retrieval], [OCR, Speech and Music Activity Detection, subtitle extraction, traditional Chinese, archival video processing]
  - **authors:** Tzu-Hung Huang, Yun-En Tsai, Yun-Ning Hung, Chih-Wei Wu, I-Chieh Wei, Li Su
  - **institution:** Academia Sinica, National Taiwan University, Music AI, University of Auckland
  - **link:** https://arxiv.org/pdf/2601.00299
  - **code:** https://github.com/z-huang/ocr-subtitle-editor
  - **contributions:** 1. Developed an interactive system for real-time OCR correction to handle low-quality archival video subtitles. 2. Proposed a two-step workflow integrating OCR-driven segmentation with Speech and Music Activity Detection (SMAD) to identify vocal segments. 3. Created a dataset of vocal segments and corresponding lyrics from Taiwanese opera TV series to support MIR tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9645950400de9f4b82b5447edc80e161ca3823766faec9633bb49af2f67eccfb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of extracting timed text and lyrics from low-quality archival Taiwanese opera TV series. The authors propose a method combining an interactive OCR correction tool with a two-step segmentation approach using OCR and SMAD to efficiently create a dataset of vocal segments and lyrics. The resulting dataset is intended to support Music Information Retrieval tasks like lyrics identification and tune retrieval.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Timed text extraction from Taiwanese Kua-á-hì TV series] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[低质量档案视频字幕提取困难/Low-quality archival video subtitle extraction is difficult]
        C --> C1[交互式OCR实时校正/Interactive real-time OCR correction]
        C --> C2[OCR分割与语音音乐活动检测两步法/Two-step OCR segmentation & SMAD]
        D --> D1[创建带歌词的人声片段数据集/Created dataset of vocal segments with lyrics]
        D --> D2[支持音乐信息检索任务/Supports MIR tasks]
    ```

- **[arXiv260105] MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality**
  - **tags:** [other], [Human-Computer Interaction (HCI) / Computer-Supported Cooperative Work (CSCW)], [Mixed Reality, Digital Audio Workstation, Collaborative Looping, Musical Metaverse, Speculative Design]
  - **authors:** Torin Hopkins, Shih-Yu Ma, Suibi Che-Chuan Weng, Ming-Yuan Pai, Ellen Yi-Luen Do, Luca Turchet
  - **institution:** University of Colorado Boulder, University of Trento, SolJAMM Research
  - **link:** https://arxiv.org/pdf/2601.00326
  - **contributions:** 1. Proposed and developed MR-DAW, a novel Mixed Reality system enabling multiple remote users to control a single, shared DAW instance while moving freely in their physical space. 2. Introduced a hands-free, collaborative interaction paradigm using physical foot pedals for remote, real-time looping control within the shared virtual session. 3. Conducted a qualitative study with 20 musicians to analyze current DAW practices, evaluate the MR-DAW system's usability, and provide a speculative outlook on the future of collaborative music-making in the "Musical Metaverse".
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates using Mixed Reality (MR) to overcome the limitations of traditional Digital Audio Workstations (DAWs), which tether musicians to a desk and hinder remote collaboration. The authors propose MR-DAW, a networked MR system that allows geographically dispersed musicians to control a shared DAW and use foot pedals for collaborative looping. The study with 20 musicians highlights MR's potential for unencumbered musical interaction and provides a speculative vision for future remote collaborative DAWs in the Musical Metaverse.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[DAWs束缚音乐家工作流/DAWs encumber musician workflow]
        Problem --> P2[远程协作困难/Remote collaboration is challenging]
        Method[主要方法/Method] --> M1[开发MR-DAW设计探针/Developed MR-DAW design probe]
        Method --> M2[使用脚踏板进行协作循环/Used foot pedal for collaborative looping]
        Method --> M3[定性研究与系统评估/Qualitative study & system evaluation]
        Results[关键结果/Results] --> R1[MR支持无束缚交互/MR affords unencumbered interaction]
        Results --> R2[展望音乐元宇宙未来/Speculative outlook on Musical Metaverse]
    ```

- **[arXiv260105] Effects of Limited Field of View on Musical Collaboration Experience with Avatars in Extended Reality**
  - **tags:** [other], [human-computer interaction], [extended reality, field of view, musical collaboration, co-presence, gesture recognition]
  - **authors:** Suibi Che-Chuan Weng, Torin Hopkins, Shih-Yu Ma, Amy Banic, Ellen Yi-Luen Do
  - **institution:** University of Colorado Boulder, University of Wyoming, SolJAMM Research
  - **link:** https://arxiv.org/pdf/2601.00333
  - **contributions:** 1. Conducted a comparative study investigating the specific impact of a limited field of view (FOV) on key aspects of musical collaboration in XR, such as co-presence and reaction time. 2. Proposed and evaluated a novel notification system ("Mini Musicians") designed to mitigate the negative effects of a limited FOV in AR-based musical collaboration. 3. Provided empirical evidence that while limited FOV degrades the collaborative experience, interface interventions like notifications can improve performance metrics like reaction time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b86b1bc6e18987027d6a96d6343c561f4f8304599001862d66c5108a3cb6f3ad_w640_q70.webp
  - **Simple LLM Summary:** This paper studies how the limited field of view (FOV) in XR head-mounted displays affects musical collaboration. It compares an unrestricted holographic setup (HoloJam) with a limited-FOV AR setup and tests a notification system (Mini Musicians) to improve awareness. The results show that limited FOV reduces co-presence and enjoyment, but notification systems can help improve reaction times.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Effects of Limited FOV on Musical Collaboration in XR<br/>XR中有限视场对音乐协作的影响] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Limited FOV in XR disrupts visual cues for musicians<br/>XR中的有限视场干扰了音乐家的视觉线索]
        C --> C1[Compared HoloJam (unrestricted FOV) vs. AR glasses (52° FOV)<br/>比较HoloJam(无限制视场)与AR眼镜(52°视场)]
        C --> C2[Tested AR notification system "Mini Musicians"<br/>测试AR通知系统"Mini Musicians"]
        D --> D1[HoloJam: higher co-presence & enjoyment<br/>HoloJam: 更高的共在感与愉悦度]
        D --> D2[Mini Musicians: reduced reaction time<br/>Mini Musicians: 降低了反应时间]
    ```

- **[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation**
  - **tags:** [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]
  - **authors:** Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang
  - **institution:** KAIST, NTU Singapore, DeepAuto.ai
  - **link:** https://arxiv.org/pdf/2601.00664
  - **code:** https://taekyungki.github.io/AvatarForcing
  - **contributions:** 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[缺乏真正互动/Lacks truly interactive communication]
        Problem --> P2[单向反应缺乏情感/One-way responses lack emotional engagement]
        Method[主要方法/Method] --> M1[扩散驱动框架/Diffusion forcing framework]
        Method --> M2[无标签直接偏好优化/Label-free direct preference optimization]
        Results[关键结果/Results] --> R1[低延迟实时交互/Low-latency real-time interaction (~500ms)]
        Results --> R2[6.8倍加速/6.8x speedup]
        Results --> R3[80%用户偏好/Over 80% user preference]
    ```
