# 20251215-20251221 (cs.NE)

## 2025-12-18

- **[arXiv251218] SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis**
  - **tags:** [mlsys], [others], [multi-agent system, structural plasticity, variational free energy, metabolic lagrangian, stochastic thermodynamics, unsupervised anomaly detection]
  - **authors:** Mustapha Hamdi
  - **institution:** InnoDeep
  - **link:** https://arxiv.org/pdf/2512.14708
  - **Simple LLM Summary:** This paper introduces SGEMAS, a bio-inspired multi-agent system that uses agent birth/death and a variational free energy objective to achieve energy-efficient, unsupervised anomaly detection in physiological signals. It validates the approach on the MIT-BIH Arrhythmia Database, showing that this physics-based, energy-constrained model can detect anomalies in a zero-shot setting, outperforming a standard autoencoder baseline.

- **[arXiv251218] Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning**
  - **tags:** [mlsys], [llm training], [surrogate modeling, large language model, sequence-to-sequence modeling, offline training, reinforcement learning fine-tuning, implicit q-learning, multi-task multi-objective optimization]
  - **authors:** Xian-Rong Zhang, Yue-Jiao Gong, Zeyuan Ma, Jun Zhang
  - **institution:** South China University of Technology, Nankai University, Hanyang University
  - **link:** https://arxiv.org/pdf/2512.15149
  - **Simple LLM Summary:** This paper proposes Q-MetaSur, a plug-and-play surrogate modeling scheme that uses a Large Language Model as a sequence-to-sequence surrogate for offline multi-task multi-objective optimization, trained with a two-stage strategy combining supervised tuning and RL fine-tuning. The method demonstrates superior objective approximation accuracy and helps evolutionary algorithms achieve better convergence and Pareto optimality on benchmark problems.

## 2025-12-19

- **[arXiv251219] Human-like Working Memory from Artificial Intrinsic Plasticity Neurons**
  - **tags:** [mlsys], [others], [neuromorphic computing, intrinsic plasticity, Magnetic Tunnel Junctions (MTJs), hardware-software co-design, near-sensor processing, working memory]
  - **authors:** Jingli Liu, Huannan Zheng, Bohao Zou, Kezhou Yang
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2512.15829
  - **Simple LLM Summary:** This paper introduces IPNet, a neuromorphic architecture that uses Magnetic Tunnel Junction (MTJ) neurons with intrinsic plasticity to physically emulate human-like working memory. The hardware-software co-designed system achieves high accuracy on dynamic vision tasks and significantly reduces energy consumption and footprint compared to traditional models like LSTMs. The work demonstrates that bio-inspired intrinsic plasticity can provide superior processing efficiency and performance for real-time applications.

- **[arXiv251219] Introduction to Symbolic Regression in the Physical Sciences**
  - **tags:** [ai], [symbolic regression], [symbolic regression, automated equation discovery, effective theories, surrogate models, complexity control, feature selection]
  - **authors:** Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira, Gabriel Kronberger
  - **institution:** University of Oxford, University of Portsmouth, University of Applied Sciences Upper Austria
  - **link:** https://arxiv.org/pdf/2512.15920
  - **Simple LLM Summary:** This paper introduces symbolic regression (SR) as a method for discovering interpretable mathematical equations from data in the physical sciences. It reviews SR's foundations, applications, methodological considerations, and challenges. The authors conclude that SR is a rapidly advancing and increasingly relevant tool for scientific discovery and empirical modeling.

- **[arXiv251219] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions**
  - **tags:** [ai], [optimization theory], [rank-based zeroth-order optimization, query complexity, CMA-ES, non-asymptotic analysis, smooth functions]
  - **authors:** Haishan Ye
  - **institution:** Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.16200
  - **Simple LLM Summary:** This paper provides a theoretical analysis of a simple rank-based zeroth-order optimization algorithm, which selects search directions based on the ordering of function evaluations. It establishes the first explicit, non-asymptotic query complexities for finding solutions on smooth strongly convex and nonconvex functions. The main conclusion is that the algorithm achieves efficient convergence rates, offering new insight into why rank-based heuristics are effective for zeroth-order optimization.

- **[arXiv251219] Hypernetworks That Evolve Themselves**
  - **tags:** [ai], [neuroevolution], [hypernetworks, graph hypernetworks, self-referential systems, evolutionary algorithms, adaptive mutation rates]
  - **authors:** Joachim Winther Pedersen, Erwan Plantec, Eleni Nisioti, Marcello Barylli, Milton Montero, Kathrin Korte, Sebastian Risi
  - **institution:** IT University of Copenhagen, Sakana AI
  - **link:** https://arxiv.org/pdf/2512.16406
  - **Simple LLM Summary:** This paper introduces Self-Referential Graph HyperNetworks, which embed evolutionary mechanisms like mutation and inheritance directly within neural networks, enabling them to evolve autonomously without external optimizers. The method demonstrates swift adaptation to environmental shifts and emergent population dynamics in reinforcement learning benchmarks, supporting the idea that evolvability can emerge from neural self-reference.

- **[arXiv251219] Topic Modelling Black Box Optimization**
  - **tags:** [ai], [hyperparameter optimization], [latent dirichlet allocation, black-box optimization, genetic algorithm, evolution strategy, preferential amortized black-box optimization, sharpness-aware black-box optimization]
  - **authors:** Roman Akramov, Artem Khamatullin, Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko
  - **institution:** Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2512.16445
  - **Simple LLM Summary:** This paper formulates selecting the number of topics in Latent Dirichlet Allocation (LDA) as a discrete black-box optimization problem. It compares evolutionary methods (GA, ES) against learned amortized optimizers (PABBO, SABBO), finding that the amortized approaches are substantially more sample- and time-efficient, with SABBO often finding a near-optimal topic number after essentially a single evaluation.

- **[arXiv251219] On the Universal Representation Property of Spiking Neural Networks**
  - **tags:** [ai], [neuromorphic computing], [spiking neural networks, universal approximation, sequence-to-sequence processing, spike train functions, circuit complexity]
  - **authors:** Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber
  - **institution:** University of Twente, Ruhr University Bochum
  - **link:** https://arxiv.org/pdf/2512.16872
  - **Simple LLM Summary:** This paper analyzes the representational power of Spiking Neural Networks (SNNs) by modeling them as sequence-to-sequence processors of spike trains. It establishes a universal representation property for a class of spike train functions, with quantitative and near-optimal bounds on the required neurons and weights. The main conclusion is that SNNs are particularly well-suited for representing functions with few inputs, low temporal complexity, or compositions thereof, providing a foundation for understanding spike-based neuromorphic systems.

- **[arXiv251219] The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading**
  - **tags:** [mlsys], [others], [deep reinforcement learning, evolutionary computation, LSTM, Transformer, genetic algorithm, high-frequency trading, multi-agent simulation]
  - **authors:** Yijia Chen
  - **institution:** Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.15732
  - **Simple LLM Summary:** This paper analyzes the failure of a hybrid trading system called "Galaxy Empire," which combines deep learning (LSTM/Transformer) for perception with evolutionary algorithms for agent survival in a high-frequency cryptocurrency market. Despite promising training results, the system suffered catastrophic live performance losses due to overfitting, survivor bias, and microstructure friction. The main conclusion is that increasing model complexity without true information asymmetry leads to systemic fragility in adaptive markets.
