---
slug: /daily/csne/20251229-20260104
---
# 20251229-20260104 (cs.NE)

## 2025-12-29

- **[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation**
  - **tags:** [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]
  - **authors:** Santhosh Kumar Ravindran
  - **institution:** Microsoft Corporation
  - **link:** https://arxiv.org/pdf/2512.21351
  - **contributions:** 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as "genomes" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp
  - **Simple LLM Summary:** CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM代码生成缺乏适应性，难以应对API变化/LLM code generation lacks adaptability to API changes]
        C --> C1[将RL轨迹视为基因组进行进化操作/Treat RL trajectories as genomes for evolutionary operations]
        C --> C2[在夜间回放阶段进行突变与选择/Mutation and selection during nocturnal replay]
        D --> D1[解决方案新颖性提升35%/35% higher novelty in solutions]
        D --> D2[适应速度加快25%/25% faster adaptation]
    ```

- **[arXiv251229] Conserved active information**
  - **tags:** [ai], [information theory], [conserved active information, No-Free-Lunch, KL divergence, search space, information conservation]
  - **authors:** Yanchen Chen, Daniel Andrés Díaz-Pachón
  - **institution:** University of Miami
  - **link:** https://arxiv.org/pdf/2512.21834
  - **contributions:** 1. Introduces conserved active information (I⊕), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I⊕ can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a new information-theoretic measure called conserved active information (I⊕) to quantify net information change in search problems while respecting conservation laws. It shows that I⊕ uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Conserved active information] --> Problem[核心问题/Problem: Limitations of average-focused information measures like KL divergence]
        Root --> Method[主要方法/Method: Introduce conserved active information I⊕, a symmetric extension respecting No-Free-Lunch]
        Root --> Results[关键结果/Results: I⊕ reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]
    ```

## 2025-12-30

- **[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks**
  - **tags:** [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]
  - **authors:** Yogeswar Reddy Thota
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.22206
  - **contributions:** 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --> B[核心问题/Problem: Modern residual networks perform redundant computation for all inputs]
        A --> C[主要方法/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]
        A --> D[关键结果/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]
    ```

## 2026-01-01

- **[arXiv260101] Identification of fixations and saccades in eye-tracking data using adaptive threshold-based method**
  - **tags:** [ai], [eye-tracking analysis], [adaptive thresholding, velocity threshold, dispersion threshold, K-ratio minimization, Markovian approximation]
  - **authors:** Charles Oriioma, Josef Krivan, Rujeena Mathema, Pedro G. Lind, Alexander Szorkovszky, Shailendra Bhandari
  - **institution:** OsloMet – Oslo Metropolitan University, Simula Research Laboratory, Kristiania University of Applied Sciences
  - **link:** https://arxiv.org/pdf/2512.23926
  - **contributions:** 1. Introduced an adaptive thresholding method for eye-tracking data that uses a Markovian model to find the optimal threshold by minimizing state transitions (K-ratio). 2. Systematically evaluated and compared the noise robustness of three common threshold-based algorithms (velocity, angular velocity, dispersion) with and without the adaptive optimization. 3. Provided practical guidance for algorithm selection, showing that while velocity thresholds have high baseline accuracy, adaptive dispersion thresholds offer superior robustness to high noise levels.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a34005335e56be821cea4646a8d8f8815324d67d0e2f0bb4fe8830262cf34e0_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of distinguishing fixations from saccades in noisy eye-tracking data, where fixed thresholds can introduce bias. The authors propose an adaptive thresholding method that models gaze dynamics as a Markov process and optimizes the threshold to minimize state transitions. Their evaluation shows that adaptive thresholds, especially for dispersion-based algorithms, significantly improve classification robustness under high noise conditions compared to fixed-threshold baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Identification of fixations and saccades in eye-tracking data using adaptive threshold-based methods] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Fixed thresholds in eye-tracking algorithms neglect variability and are noise-sensitive.]
        C[主要方法/Method: Adaptive threshold optimization via Markovian model and K-ratio minimization.]
        D[关键结果/Results: Adaptive dispersion is most noise-robust; velocity has high baseline accuracy but degrades with noise.]
    ```

- **[arXiv260101] Decoupling Constraint from Two Direction in Evolutionary Constrained Multi-objective Optimization**
  - **tags:** [ai], [evolutionary computation], [Constrained Multi-objective Optimization, Constraint Decoupling, Coevolutionary Algorithm]
  - **authors:** Ruiqing Sun, Dawei Feng, Xing Zhou, Lianghao Li, Sheng Qi, Bo Ding, Yijie Wang, Rui Wang, Huaimin Wang
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.23945
  - **code:** https://github.com/KFC-Grandpa/DCF2D-Decoupling-Constraint-from-Two-Direction
  - **contributions:** 1. Analysis of constraint coupling, showing the Constrained Pareto Front depends on individual constraint fronts and infeasible region boundaries., 2. Insight that CMOPs with different coupling types require different search directions., 3. Proposal of the DCF2D algorithm, which periodically detects constraint couplings and spawns auxiliary populations with appropriate search directions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9961a6ec373a0059008d366a3c457c71a3b16f2e308e3aca87ae29a142caf095_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitation of existing Constrained Multi-objective Evolutionary Algorithms (CMOEAs) that treat all constraints as a single aggregate, ignoring their couplings. The authors propose a novel algorithm called Decoupling Constraint from Two Directions (DCF2D), which detects constraint couplings and uses auxiliary populations with tailored search directions. Experiments on benchmark and real-world problems show DCF2D outperforms several state-of-the-art CMOEAs.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Decoupling Constraint from Two Direction in Evolutionary Constrained Multi-objective Optimization"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem<br>Existing CMOEAs ignore constraint couplings, treating them as a single aggregate."]
        Method["主要方法/Method<br>Propose DCF2D: periodically detects constraint couplings and spawns auxiliary populations with appropriate search directions."]
        Results["关键结果/Results<br>DCF2D outperforms five state-of-the-art CMOEAs on benchmarks and real-world CMOPs."]
    ```

- **[arXiv260101] TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems**
  - **tags:** [ai], [metaheuristics], [simulation optimization, tabu search, elite memory, noisy black-box, aspiration criterion]
  - **authors:** Bulent Soykan, Sean Mondesire, Ghaith Rabadi
  - **institution:** University of Central Florida
  - **link:** https://arxiv.org/pdf/2512.24007
  - **code:** github.com/bulentsoykan/TESO
  - **contributions:** 1. Proposes TESO, a novel metaheuristic framework that integrates adaptive search with memory-based strategies for simulation optimization. 2. Introduces a dual-memory mechanism combining a short-term Tabu List for diversification and a long-term Elite Memory for intensification. 3. Demonstrates the framework's effectiveness and reliability on a queue optimization problem, showing improved performance over benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00ac2df6705adb84c43e6fe1380cb528473aee412ea23024864f40a91dfd7ec9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces TESO, a metaheuristic framework for noisy, expensive black-box simulation optimization. It combines a Tabu List and an Elite Memory with an aspiration criterion to balance exploration and exploitation. The method is validated on a queue optimization problem, showing improved performance and reliability compared to benchmarks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[TESO: Tabu-Enhanced Simulation Optimization] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Noisy, expensive, multimodal simulation optimization] --> P1[挑战/Challenges: Noisy evaluations, high cost, complex landscapes]
        Method[主要方法/Method: Memory-based metaheuristic framework] --> M1[组件/Components: Tabu List, Elite Memory, Aspiration Criterion]
        Results[关键结果/Results: Validated on queue optimization] --> R1[结论/Conclusion: Improved performance & reliability]
    ```

- **[arXiv260101] Generalising E-prop to Deep Networks**
  - **tags:** [ai], [biologically plausible learning algorithms], [E-prop, eligibility traces, credit assignment, recurrent neural networks, backpropagation through time]
  - **authors:** Beren Millidge
  - **institution:** Zyphra
  - **link:** https://arxiv.org/pdf/2512.24506
  - **contributions:** 1. Extends the E-prop framework to handle arbitrarily deep networks, enabling credit assignment across both time and depth. 2. Derives a novel recursion relationship across depth that generalizes eligibility traces to deeper layers. 3. Demonstrates an online learning algorithm capable of training deep recurrent networks without backpropagation through time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6eda8a1b73fb9e52a5baf437a42dfe8578438c0dde5dfa6d166ba856f037f47_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the biological implausibility of Backpropagation Through Time (BPTT) for training recurrent neural networks. It proposes an extension of the E-prop algorithm to deep networks, enabling online credit assignment across both time and depth. The main conclusion is that this method allows for the training of deep recurrent networks without BPTT.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Generalising E-prop to Deep Networks") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("BPTT生物不可信/BPTT biologically implausible")
        Problem --> P2("RTRL计算复杂/RTRL computationally expensive")
        Problem --> P3("现有方法局限于单层/Existing methods limited to single layer")
        Method --> M1("扩展E-prop框架/Extend E-prop framework")
        Method --> M2("推导跨深度的递归关系/Derive depth-wise recursion")
        Method --> M3("推广资格迹到深层/Generalize eligibility traces to deep layers")
        Results --> R1("实现跨时空的在线信用分配/Online credit assignment across time and depth")
        Results --> R2("无需BPTT训练深度循环网络/Train deep recurrent networks without BPTT")
    ```

- **[arXiv260101] Evolutionary Discovery of Sequence Acceleration Methods for Slab Geometry Neutron Transport**
  - **tags:** [other], [computational physics, numerical methods], [genetic programming, sequence acceleration, neutron transport, slab geometry, discrete ordinates]
  - **authors:** Japan K. Patel, Barry D. Ganapol, Anthony Magliari, Matthew C. Schmidt, Todd A. Wareing
  - **institution:** Gateway Scripts, University of Arizona, Varian Medical Systems, Washington University in St. Louis
  - **link:** https://arxiv.org/pdf/2512.24559
  - **contributions:** 1. Applied genetic programming to automatically discover novel convergence acceleration methods for neutron transport problems, moving beyond classical methods with fixed assumptions. 2. Evolved a specific mathematical formula accelerator tailored to the convergence characteristics of discrete ordinates (SN) solutions in slab geometry. 3. Demonstrated the discovered accelerator's superior performance, achieving over 75% success rate in improving convergence, nearly double that of classical techniques on the tested problem set.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34d82f8c1e37e97ac83244d31b5fb8965de8b3a0c8f93a666dd8bc4b3585880_w640_q70.webp
  - **Simple LLM Summary:** This paper uses genetic programming to automatically discover new mathematical formulas for accelerating the convergence of numerical solutions to neutron transport problems in slab geometry. The evolved accelerator, which uses second differences and cross-product terms, significantly outperformed classical methods like Aitken's and Wynn's, showing the potential of AI to generate novel numerical methods in computational physics.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Evolutionary Discovery of Sequence Acceleration Methods for Slab Geometry Neutron Transport] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[经典加速方法泛化性差<br>Classical acceleration methods lack generalization]
        C --> C1[使用遗传编程进化公式<br>Use Genetic Programming to evolve formulas]
        D --> D1[发现新型加速器<br>Discovered novel accelerator]
        D --> D2[成功率 >75%, 性能翻倍<br>>75% success rate, nearly double performance]
    ```

- **[arXiv260101] Equivalence of Personalized PageRank and Successor Representations**
  - **tags:** [ai], [computational neuroscience], [personalized pagerank, successor representation, stationary distribution, random walk, hippocampus]
  - **authors:** Beren Millidge
  - **institution:** Zyphra
  - **link:** https://arxiv.org/pdf/2512.24722
  - **contributions:** 1. Demonstrates an isomorphism between Personalized PageRank (used for memory retrieval) and Successor Representations (used for planning/navigation). 2. Shows both algorithms utilize the same underlying representation: the stationary distribution of a random walk on a graph. 3. Proposes a unifying hypothesis that the core computational function of the hippocampus is to compute this stationary distribution representation on arbitrary input graphs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/130f6ae2c386370de68fa70e71e8766a30a8d563c1baa5dda86dea70938b910b_w640_q70.webp
  - **Simple LLM Summary:** This paper shows that two seemingly distinct algorithms proposed for hippocampal function—Personalized PageRank for memory retrieval and Successor Representations for planning—are mathematically equivalent, as both compute the stationary distribution of a random walk on a graph. The authors conclude that this shared representation suggests a unified computational role for the hippocampus in processing graph-structured information for both memory and navigation tasks.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[Equivalence of Personalized PageRank and Successor Representations] --> B(核心问题/Problem)
    A --> C(主要方法/Method)
    A --> D(关键结果/Results)
    B --> B1[海马体功能看似不同<br/>Hippocampus functions appear distinct]
    B1 --> B2[记忆检索 vs. 规划导航<br/>Memory Retrieval vs. Planning/Navigation]
    C --> C1[证明算法同构<br/>Demonstrate Algorithm Isomorphism]
    C1 --> C2[个性化PageRank<br/>Personalized PageRank]
    C1 --> C3[后继表示<br/>Successor Representations]
    D --> D1[共享基础表示<br/>Shared Underlying Representation]
    D1 --> D2[图上随机游走的平稳分布<br/>Stationary Distribution of Random Walk on Graph]
    D --> D3[统一计算假说<br/>Unified Computational Hypothesis]
    D3 --> D4[海马体核心功能<br/>Core Function of Hippocampus]
    ```

- **[arXiv260101] Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation**
  - **tags:** [cv], [monocular depth estimation], [adversarial attack, physics-in-the-loop optimization, sep-CMA-ES]
  - **authors:** Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono
  - **institution:** Kagoshima University
  - **link:** https://arxiv.org/pdf/2512.24792
  - **contributions:** 1. Proposes a projection-based adversarial attack method for monocular depth estimation models, using projected light as the perturbation. 2. Employs physics-in-the-loop (PITL) optimization to design perturbations in real-world environments, accounting for device specifications and disturbances. 3. Utilizes a distributed covariance matrix adaptation evolution strategy (sep-CMA-ES) for effective black-box optimization to generate adversarial examples.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a physical adversarial attack method for monocular depth estimation models. The method projects perturbation light onto a target object and uses physics-in-the-loop optimization with a distributed evolution strategy to create adversarial examples. Experiments confirmed the attack's success, causing depth misestimations that made parts of objects disappear from the scene.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: DNN-based monocular depth estimation models are vulnerable to adversarial attacks, threatening reliability in applications like autonomous systems.]
        Method[主要方法/Method: Proposes a projection-based attack using physics-in-the-loop optimization and sep-CMA-ES to generate adversarial light perturbations.]
        Results[关键结果/Results: Successfully created adversarial examples causing depth misestimation, making parts of objects disappear from the scene.]
    ```

- **[arXiv260101] Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks**
  - **tags:** [mlsys], [multi-modal training], [neural architecture search, self-supervised learning, multimodal fusion, contrastive learning, gradient-based search]
  - **authors:** Shota Suzuki, Satoshi Ono
  - **institution:** Kagoshima University
  - **link:** https://arxiv.org/pdf/2512.24793
  - **contributions:** 1. Proposes a self-supervised learning (SSL) method for neural architecture search (NAS) specifically for multimodal deep neural networks. 2. Applies SSL comprehensively to both the architecture search and model pretraining processes, eliminating the need for labeled data during search. 3. Demonstrates that the method can successfully design network architectures from unlabeled training data, achieving performance comparable to supervised NAS methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a6c1a495cc476572529c11ecd2d19b6e7849693fc7a5c6941d8e99e91599cc5_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem that neural architecture search (NAS) for multimodal deep neural networks typically requires large amounts of labeled data. The authors propose a self-supervised learning method that uses contrastive learning to perform NAS without labeled data. Experimental results show the method can successfully design effective multimodal network architectures using only unlabeled data.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Self-supervised Neural Architecture Search for Multimodal Deep Neural Networks] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Multimodal NAS requires substantial labeled data] --> P1[问题细化/Sub-problem: High labeling cost for multimodal data]
        Method[主要方法/Method: Self-supervised NAS] --> M1[方法基础/Foundation: Gradient-based NAS (BM-NAS)] --> M1_1[技术/Technique: Differential Architecture Search]
        Method --> M2[自监督机制/SSL Mechanism: Contrastive Learning (SimCLR)] --> M2_1[目标/Objective: Learn from unlabeled data]
        Results[关键结果/Results: Successfully designed architectures from unlabeled data] --> R1[评估/Evaluation: Comparable to supervised methods]
    ```

- **[arXiv260101] Generative Classifiers Avoid Shortcut Solutions**
  - **tags:** [ai], [generative models], [generative classifiers, spurious correlations, distribution shift, diffusion models, autoregressive models]
  - **authors:** Alexander C. Li, Ananya Kumar, Deepak Pathak
  - **institution:** Carnegie Mellon University, Stanford University
  - **link:** https://arxiv.org/pdf/2512.25034
  - **code:** https://github.com/alexlioralexli/generative-classifiers
  - **contributions:** 1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]
        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]
        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]
        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]
        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]
    ```

- **[arXiv260101] Spike-Timing-Dependent Plasticity for Bernoulli Message Passing**
  - **tags:** [ai], [computational neuroscience], [spike-timing-dependent plasticity, Bayesian inference, message passing, factor graphs, spiking neural networks]
  - **authors:** Sepideh Adamiat, Wouter M. Kouw, Bert de Vries
  - **institution:** TU Eindhoven, Lazy Dynamics B.V.
  - **link:** https://arxiv.org/pdf/2512.23728
  - **contributions:** 1. Bridging Bayesian inference and spike-based neural computation by designing spiking neural networks for Bernoulli message passing. 2. Employing spike-timing-dependent plasticity (STDP), a biologically plausible Hebbian learning rule, to train these networks. 3. Demonstrating the approach's versatility by applying it to a factor graph example from coding theory for signal transmission over an unreliable channel.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a449542e05c85b78a1c2c36d2343549e26e8c17a9edfccf34c692fd2512f710c_w640_q70.webp
  - **Simple LLM Summary:** This paper bridges Bayesian inference and spike-based neural activity by designing spiking neural networks that perform message passing for Bernoulli variables. The networks are trained using the biologically plausible spike-timing-dependent plasticity rule. The results show the network's performance matches the true numerical solution, and the method is demonstrated on a coding theory problem.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Spike-Timing-Dependent Plasticity for Bernoulli Message Passing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[连接贝叶斯推理与脉冲神经活动/Bridging Bayesian inference and spike-based neural activity]
        C --> C1[设计用于伯努利消息传递的脉冲神经网络/Designing SNNs for Bernoulli message passing]
        C --> C2[使用脉冲时序依赖可塑性进行训练/Using STDP for training]
        D --> D1[网络性能匹配真实数值解/Network performance matches true numerical solution]
        D --> D2[方法应用于编码理论因子图/Method applied to coding theory factor graph]
    ```

- **[arXiv260101] SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets**
  - **tags:** [ai], [sequence learning], [formal language theory, symbolic sequences, benchmark suite, cognitive modeling, sequence processing]
  - **authors:** Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte
  - **institution:** Jülich Research Centre, RWTH Aachen University, University of Groningen, University of Coimbra
  - **link:** https://arxiv.org/pdf/2512.24977
  - **contributions:** 1. Introduces SymSeq, a tool for the rigorous generation and analysis of structured symbolic sequences. 2. Introduces SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks for evaluating AI systems. 3. Provides a unified, domain-agnostic framework (SymSeqBench) based on Formal Language Theory to standardize experiments across cognitive science and AI.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces SymSeqBench, a unified software framework combining a symbolic sequence generator/analyzer (SymSeq) and a benchmark suite (SeqBench) for evaluating sequence learning. It is based on Formal Language Theory to provide a domain-agnostic, formal link between computation and cognition. The main conclusion is that this modular, open-source tool offers a versatile and standardized way to investigate sequential structure across diverse fields like psycholinguistics, cognitive psychology, and AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SymSeqBench: 统一框架] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[评估序列学习/Evaluating Sequence Learning]
        Problem --> P2[领域无关的评估/Domain-Agnostic Evaluation]
        Problem --> P3[连接形式理论与认知/Linking Formal Theory & Cognition]
        Method --> M1[SymSeq: 生成与分析/SymSeq: Generation & Analysis]
        Method --> M2[SeqBench: 基准测试套件/SeqBench: Benchmark Suite]
        Method --> M3[基于形式语言理论/Based on Formal Language Theory]
        Results --> R1[跨领域多功能/Versatile Across Domains]
        Results --> R2[标准化实验/Standardizes Experiments]
        Results --> R3[模块化开源工具/Modular Open-Source Tool]
    ```
