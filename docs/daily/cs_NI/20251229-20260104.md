---
slug: /daily/csni/20251229-20260104
---
# 20251229-20260104 (cs.NI)

## 2025-12-29

- **[arXiv251229] Physics-informed Diffusion Models for Multi-scale Prediction of Reference Signal Received Power in Wireless Networks**
  - **tags:** [mlsys], [diffusion models], [RSRP prediction, physics-informed generation, conditional diffusion models, multi-scale attenuation, prior guidance]
  - **authors:** Xiaoqian Qi, Haoye Chai, Yue Wang, Zhaocheng Wang, Yong Li
  - **institution:** Tsinghua University, Beijing University of Posts and Telecommunications
  - **link:** https://arxiv.org/pdf/2512.21475
  - **contributions:** 1. Proposes Channel-Diff, a novel RSRP prediction framework that uses physics-informed conditional diffusion models to jointly capture large-scale and small-scale signal attenuation characteristics. 2. Introduces a method to extract and integrate multi-scale physical priors (LS and SS) from network parameters and urban spatial maps to guide the generative model. 3. Designs a physical-prior-guided two-stage training scheme with a noise prior guidance mechanism to effectively fuse physical knowledge with the diffusion model, improving accuracy, transferability, and training efficiency.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c64fa6cd84fa5f5670ffbaf39db21089f232a03351b0a914dfcc15be65f6a771_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of accurately predicting Reference Signal Received Power (RSRP) in wireless networks, where existing methods struggle with multi-scale signal attenuation. The authors propose Channel-Diff, a framework that integrates physics-informed conditional diffusion models with extracted large-scale and small-scale physical priors from the environment. Evaluations show the model significantly outperforms baselines in accuracy and demonstrates strong transferability and training efficiency.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Physics-informed Diffusion Models for Multi-scale Prediction of RSRP] --> B(核心问题/Problem: Existing RSRP prediction methods lack accuracy and fail to model intertwined LS and SS signal attenuation.)
        A --> C(主要方法/Method: Proposes Channel-Diff framework using physics-informed conditional diffusion models guided by multi-scale physical priors.)
        A --> D(关键结果/Results: Achieves 25.15%-37.19% accuracy improvement over baselines, with strong transferability and training efficiency.)
    ```

- **[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities**
  - **tags:** [sys], [communication & networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]
  - **authors:** Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin
  - **institution:** Kyung Hee University, Ghent University
  - **link:** https://arxiv.org/pdf/2512.21717
  - **contributions:** 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp
  - **Simple LLM Summary:** This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"] --> Problem
        Root --> Method
        Root --> Results
        Problem["核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation"]
        Method["主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning"]
        Results["关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off"]
    ```

- **[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers**
  - **tags:** [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]
  - **authors:** Krishna Chaitanya Sunkara, Rambabu Konakanchi
  - **institution:** Oracle, Charles Schwab
  - **link:** https://arxiv.org/pdf/2512.21801
  - **contributions:** 1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]
        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]
        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]
    ```

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [distributed inference, block placement, request routing, performance modeling, resource allocation]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **contributions:** 1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]
        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]
        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]
    ```

- **[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN**
  - **tags:** [sys], [communication & networking], [Conditional Handovers, O-RAN, Meta-Learning, Mobility Management, xApp]
  - **authors:** Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers
  - **institution:** Delft University of Technology, Telefónica Research
  - **link:** https://arxiv.org/pdf/2512.22022
  - **contributions:** 1. Introduces CONTRA, the first framework to jointly optimize Traditional and Conditional Handovers within the O-RAN architecture. 2. Proposes a practical meta-learning algorithm for adaptive, on-the-fly handover type selection, guaranteeing universal no-regret performance. 3. Provides and analyzes unique, countrywide mobility management datasets from a top-tier mobile network operator, offering fresh insights into handover trade-offs.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the limitations of traditional and conditional handovers in mobile networks by proposing CONTRA, a meta-learning-based framework for O-RAN that dynamically selects and optimizes handover types. It is designed as a near-real-time xApp and is evaluated using real-world datasets. The results show that CONTRA improves user throughput and reduces switching costs, outperforming standard and RL-based baselines.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Meta-Learning-Based Handover Management in NextG O-RAN] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统切换延迟与失败/Traditional HO delays & failures]
        B --> B2[切换类型间的权衡/Trade-offs between HO types]
        C --> C1[CONTRA框架: 联合优化THO与CHO/CONTRA: Jointly optimizes THOs & CHOs]
        C --> C2[元学习算法动态选择/Meta-learning for dynamic selection]
        C --> C3[O-RAN xApp部署/O-RAN xApp deployment]
        D --> D1[提升用户吞吐量/Improves user throughput]
        D --> D2[降低切换成本/Reduces HO switching costs]
        D --> D3[优于3GPP与RL基线/Outperforms 3GPP & RL baselines]
    ```

- **[arXiv251229] Schwarz Information Criterion Aided Multi-Armed Bandit for Decentralized Resource Allocation in Dynamic LoRa Networks**
  - **tags:** [sys], [wireless networking], [LoRa, Multi-Armed Bandit, Schwarz Information Criterion, Decentralized Resource Allocation, Energy Efficiency]
  - **authors:** Ryotai Ariyoshi, Aohan Li, Mikio Hasegawa, Tomoaki Ohtsuki, Miao Pan, Zhu Han
  - **institution:** The University of Electro-Communications, Tokyo University of Science, Keio University, University of Houston
  - **link:** https://arxiv.org/pdf/2512.22089
  - **contributions:** 1. Proposes a lightweight, fully distributed learning method for LoRa EDs to autonomously select transmission parameters (channel, power, bandwidth) using the UCB1-tuned algorithm, optimizing for both success rate and energy efficiency. 2. Integrates the Schwarz Information Criterion (SIC) with UCB1-tuned to enable low-cost detection of changes in the dynamic communication environment, a key innovation for resource-constrained devices. 3. Introduces a reset mechanism where, upon SIC detecting an environmental change, the learning history of UCB1-tuned is cleared, allowing for rapid re-adaptation and improved performance in non-stationary conditions.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b39add198aa9e00b27583263765a3a7735e59d734218fea9cc0d752f93beeb_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of slow adaptation in decentralized LoRa network resource allocation under dynamic conditions. It proposes a method that combines the UCB1-tuned Multi-Armed Bandit algorithm with the Schwarz Information Criterion (SIC) to detect environmental changes and reset learning, enabling fast re-adaptation. Experimental results on real hardware show the proposed method outperforms standard UCB1-tuned in transmission success rate, energy efficiency, and adaptability.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Schwarz Information Criterion Aided Multi-Armed Bandit for Decentralized Resource Allocation in Dynamic LoRa Networks] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[动态LoRa网络中的资源分配/Dynamic LoRa Network Resource Allocation]
        B --> B2[UCB1-tuned在动态环境中适应慢/UCB1-tuned adapts slowly in dynamic environments]
        C --> C1[使用UCB1-tuned算法选择参数/Use UCB1-tuned to select parameters]
        C --> C2[集成SIC进行变化检测/Integrate SIC for change detection]
        C --> C3[检测到变化时重置学习历史/Reset learning history upon change detection]
        D --> D1[更高的传输成功率/Higher transmission success rate]
        D --> D2[更高的能源效率/Higher energy efficiency]
        D --> D3[更强的适应性/Stronger adaptability]
    ```
