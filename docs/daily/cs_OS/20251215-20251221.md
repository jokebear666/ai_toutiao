# 20251215-20251221 (cs.OS)

## 2025-12-18

- **[arXiv251218] EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving**
  - **tags:** [mlsys], [llm inference], [KV-cache management, lossy compression, adaptive eviction, utility function, multi-tier storage]
  - **authors:** Shaoting Feng, Yuhan Liu, Hanchen Li, Xiaokun Chen, Samuel Shen, Kuntai Du, Zhuohan Gu, Rui Zhang, Yuyang Huang, Yihua Cheng, Jiayi Yao, Qizheng Zhang, Ganesh Ananthanarayanan, Junchen Jiang
  - **institution:** University of Chicago, UC Berkeley, Tensormesh, Inc., MIT, UC Santa Cruz, Stanford, Microsoft
  - **link:** https://arxiv.org/pdf/2512.14946
  - **Simple LLM Summary:** EVICPRESS is a KV-cache management system that jointly optimizes lossy compression and adaptive eviction across multiple storage tiers using a unified utility function. It improves LLM inference efficiency by maximizing fast-tier cache hit rates while preserving generation quality through context-aware compression. Evaluations show it achieves up to 2.19x faster time-to-first-token at equivalent quality compared to baselines.
