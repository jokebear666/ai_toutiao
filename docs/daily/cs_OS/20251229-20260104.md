---
slug: /daily/csos/20251229-20260104
---
# 20251229-20260104 (cs.OS)

## 2025-12-29

- **[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems**
  - **tags:** [sys], [real-time systems], [lock-free, fault-tolerance, resource sharing, multicore, worst-case response time analysis]
  - **authors:** Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao
  - **institution:** University of York, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21701
  - **contributions:** 1. Proposes the LEFT-RS protocol, a lock-free design that allows concurrent read access to global resources and parallel entry into critical sections, improving efficiency. 2. Enhances fault resilience by limiting overhead and enabling tasks to complete earlier if others experience faults, reducing blocking. 3. Provides a comprehensive worst-case response time analysis to ensure timing guarantees for the proposed protocol.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb19a780f9199527b92c55981536e4b4108e6135efe187882739942a46ebf5ed_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes LEFT-RS, a lock-free and fault-tolerant resource sharing protocol for multicore real-time systems. It allows tasks to concurrently access resources and enter critical sections in parallel, improving efficiency and resilience to transient faults. Evaluation shows it significantly outperforms existing methods, achieving up to an 84.5% average improvement in schedulability.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Faults in critical sections cause error propagation; locking protocols lack fault tolerance, increasing blocking.]
        Method[主要方法/Method: LEFT-RS protocol enables concurrent read access and parallel critical section entry for fault resilience.]
        Results[关键结果/Results: Up to 84.5% average schedulability improvement over existing approaches.]
    ```

## 2025-12-30

- **[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers**
  - **tags:** [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]
  - **authors:** Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee
  - **institution:** Urmia University of Technology
  - **link:** https://arxiv.org/pdf/2512.23380
  - **code:** https://github.com/your-repo/CoLog (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)
  - **contributions:** 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --> Problem["核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]
        Root --> Method["主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]
        Root --> Results["关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]
    ```

## 2026-01-01

- **[arXiv260101] MSched: GPU Multitasking via Proactive Memory Scheduling**
  - **tags:** [mlsys], [memory & caching], [GPU multitasking, memory oversubscription, proactive scheduling, demand paging, context switching]
  - **authors:** Weihang Shen, Yinqiu Chen, Rong Chen, Haibo Chen
  - **institution:** Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.24637
  - **contributions:** 1. Proposes MSched, an OS-level scheduler that extends GPU context switching with proactive working set preparation to coalesce expensive page faults into a single efficient migration. 2. Introduces a template-based approach to predict GPU memory access patterns with near-perfect accuracy using kernel launch arguments. 3. Presents a co-design between the task scheduler and memory manager to enforce a globally optimal page placement policy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81aa7999cea2218215dd44a0f4318762fd750cf330b1155dde0203d7d1437b8c_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the bottleneck of limited GPU HBM capacity for multitasking by proposing MSched, a proactive memory scheduler. MSched predicts memory access patterns to prepare working sets during context switches, replacing fragmented demand paging with efficient bulk migrations. Evaluation shows it outperforms demand paging by up to 11.05x for DL workloads and 57.88x for LLM inference under memory oversubscription.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MSched: GPU Multitasking via Proactive Memory Scheduling"] --> Problem["核心问题/Problem: Limited HBM capacity bottleneck for GPU multitasking"]
        Root --> Method["主要方法/Method: Proactive OS scheduler with template-based prediction & co-design"]
        Root --> Results["关键结果/Results: Outperforms demand paging by up to 57.88x"]
    ```

- **[arXiv260101] Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search**
  - **tags:** [mlsys], [memory & caching], [heuristic synthesis, evolutionary search, instance-optimal, LLM code generation, cache eviction]
  - **authors:** Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.25065
  - **contributions:** 1. Proposes Vulcan, a framework that recasts heuristic design as an automated search problem using LLMs to synthesize instance-optimal heuristics tailored to specific deployment contexts. 2. Introduces LLM-friendly, task-agnostic interfaces that separate policy and mechanism, making the synthesis tractable and enabling even small LLMs to generate correct code. 3. Demonstrates the framework's effectiveness by synthesizing heuristics for cache eviction and memory tiering that outperform state-of-the-art human-designed algorithms.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8daf2ac9cf0548fb6b41e5cb643f8b78cc3001bcb2d8b95b8ade5ced5201e53a_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Vulcan, a framework that uses LLM-driven evolutionary search to automatically synthesize instance-optimal system heuristics, tailored to specific workloads and hardware. It introduces task-agnostic interfaces to separate policy from mechanism, enabling efficient code generation. The synthesized heuristics for cache eviction and memory tiering were shown to outperform existing state-of-the-art algorithms.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: Manual heuristic design is slow and cannot adapt to changing hardware and workloads.]
        Method[主要方法/Method: Use LLM-driven evolutionary search over task-agnostic interfaces to synthesize instance-optimal heuristics.]
        Results[关键结果/Results: Synthesized heuristics outperform state-of-the-art algorithms in cache eviction and memory tiering.]
    ```
