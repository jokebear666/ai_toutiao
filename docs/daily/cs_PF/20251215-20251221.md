# 20251215-20251221 (cs.PF)

## 2025-12-19

- **[arXiv251219] AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs**
  - **tags:** [mlsys], [llm training], [AdaGradSelect, gradient-guided selection, Dirichlet-based sampling, epsilon-greedy exploration, selective block update, Parameter-Efficient Fine-Tuning (PEFT)]
  - **authors:** Anshul Kumar, Gagan Raj Gupta, Manisha Chawla
  - **institution:** IIT Bhilai
  - **link:** https://arxiv.org/pdf/2512.15764
  - **Simple LLM Summary:** This paper introduces AdaGradSelect, an adaptive method for efficiently fine-tuning Small Language Models (SLMs) by selecting which transformer blocks to update based on gradient norms, using a combination of Dirichlet-based sampling and epsilon-greedy exploration. It achieves performance close to full fine-tuning while training about 12% faster and using 35% less GPU memory, outperforming methods like LoRA on benchmarks such as GSM8K.

- **[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models**
  - **tags:** [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]
  - **authors:** Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai
  - **institution:** Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.
  - **link:** https://arxiv.org/pdf/2512.15766
  - **Simple LLM Summary:** The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.

- **[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls**
  - **tags:** [mlsys], [llm inference], [speculative tool calls, tool cache, vLLM, prefix-caching]
  - **authors:** Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon
  - **institution:** Lawrence Livermore National Laboratory, University of Maryland
  - **link:** https://arxiv.org/pdf/2512.15834
  - **Simple LLM Summary:** This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new "tool cache" API to facilitate adoption of these optimizations.

- **[arXiv251219] XTC, A Research Platform for Optimizing AI Workload Operators**
  - **tags:** [mlsys], [GPU kernels], [scheduling language, code generation, performance evaluation, compiler, autotuning]
  - **authors:** Pompougnac Hugo, Guillon Christophe, Noiry Sylvain, Dutilleul Alban, Iooss Guillaume, Rastello Fabrice
  - **institution:** Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG
  - **link:** https://arxiv.org/pdf/2512.16512
  - **Simple LLM Summary:** The paper introduces XTC, a research platform that provides a unified scheduling language and API to decouple optimization strategies from compiler-specific code generation and measurement. This enables portable experimentation and fair performance comparison across different compiler frameworks. The main conclusion is that XTC accelerates research on AI operator optimization by providing a common, reproducible framework for scheduling and evaluation.
