---
slug: /daily/cspf/20251222-20251228
---
# 20251222-20251228 (cs.PF)

## 2025-12-22

- **[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping**
  - **tags:** [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]
  - **authors:** Yikang Yue, Yishu Yin, Xuehai Qian
  - **institution:** Tsinghua University, University of Illinois at Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2512.17570
  - **Simple LLM Summary:** The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.
