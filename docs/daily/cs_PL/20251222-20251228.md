---
slug: /daily/cspl/20251222-20251228
---
# 20251222-20251228 (cs.PL)

## 2025-12-23

- **[arXiv251223] Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs**
  - **tags:** TBD
  - **authors:** Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18134
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp
  - **Simple LLM Summary:** Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs

- **[arXiv251223] DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs**
  - **tags:** TBD
  - **authors:** Aleksandr Fedchin, Antero Mejr, Hari Sundar, Jeffrey S. Foster
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.18842
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865d9d3085c35dc9683acc6a3733dcc32fd4a99cf1bb506d84b85aa30412cf3e_w640_q70.webp
  - **Simple LLM Summary:** DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs

- **[arXiv251223] Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems**
  - **tags:** TBD
  - **authors:** Prathamesh Devadiga
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19250
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp
  - **Simple LLM Summary:** Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems

## 2025-12-24

- **[arXiv251224] A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows**
  - **tags:** TBD
  - **authors:** Ivan Daunis
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.19769
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e158baf642d33624c0967b1dcd509fbc3876a4bc52a539d4b6e7c800995b42_w640_q70.webp
  - **Simple LLM Summary:** A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows

- **[arXiv251224] Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)**
  - **tags:** TBD
  - **authors:** Philipp Schröer, Darion Haase, Joost-Pieter Katoen
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20214
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69d3cbfad240346e5e410cac1825291c7eadf08769c78985ee6d4b0712429906_w640_q70.webp
  - **Simple LLM Summary:** Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)

- **[arXiv251224] Symmaries: Automatic Inference of Formal Security Summaries for Java Programs**
  - **tags:** TBD
  - **authors:** Narges Khakpour, Nicolas Berthier
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.20396
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6bdfa1cdb9c42587497d38e68d6ea55a15a3372a5c197a241c9e2cf0c52caf1_w640_q70.webp
  - **Simple LLM Summary:** Symmaries: Automatic Inference of Formal Security Summaries for Java Programs

## 2025-12-25

- **[arXiv251225] AutoBaxBuilder: Bootstrapping Code Security Benchmarking**
  - **tags:** [sec], [code security evaluation], [automated benchmarking, LLM-generated code, security vulnerabilities, exploit generation, plausibility checks]
  - **authors:** Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev
  - **institution:** ETH Zurich, Snyk, INSAIT (Sofia University "St. Kliment Ohridski")
  - **link:** https://arxiv.org/pdf/2512.21132
  - **code:** https://github.com/eth-sri/autobaxbuilder
  - **contributions:** 1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp
  - **Simple LLM Summary:** The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.
  - **Mindmap:**

    ```mermaid
    graph LR
    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]
    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]
    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]
    ```
