---
slug: /daily/cssd/20251229-20260104
---
# 20251229-20260104 (cs.SD)

## 2025-12-29

- **[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression**
  - **tags:** [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]
  - **authors:** Liuyang Bai, Weiyi Lu, Li Guo
  - **institution:** NYU Shanghai
  - **link:** https://arxiv.org/pdf/2512.21653
  - **contributions:** 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --> Problem["核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]
        Root --> Method["主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]
        Root --> Results["关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]
    ```

- **[arXiv251229] Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning**
  - **tags:** [sec], [audio deepfake detection], [transfer learning, zero-shot inference, fine-tuning, Bengali audio, BanglaFake dataset]
  - **authors:** Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman
  - **institution:** Not explicitly stated in the provided content.
  - **link:** https://arxiv.org/pdf/2512.21702
  - **contributions:** 1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)
        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)
        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))
    ```

- **[arXiv251229] Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models**
  - **tags:** [nlp], [speech recognition & translation], [task vector, rare word recognition, catastrophic forgetting, speech-to-text, parameter arithmetic]
  - **authors:** Ruihao Jing, Cheng Gong, Yu Jiang, Boyu Zhu, Shansong Liu, Chi Zhang, Xiao-Lei Zhang, Xuelong Li
  - **institution:** Institute of Artificial Intelligence (TeleAI), China Telecom
  - **link:** https://arxiv.org/pdf/2512.21894
  - **contributions:** 1. Proposes a training-free paradigm for rare word handling using task vectors, eliminating the need for fine-tuning. 2. Introduces word-level task vector arithmetic for flexible composition and reuse of rare-word capabilities. 3. Demonstrates that the method matches or surpasses fine-tuning on target words, improves general performance (~5 BLEU), and mitigates catastrophic forgetting.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd10590bdc1608f8eae90820d97325d5b60e598c9d06e1275430238b0313b56_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the bottleneck of rare word recognition in speech-to-text systems. It proposes a training-free method based on task vector arithmetic to compose rare-word capabilities, which avoids the costs and forgetting issues of fine-tuning. Experiments show the method performs comparably to fine-tuning on target words while improving overall translation quality and reducing catastrophic forgetting.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[论文标题: Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models] --> B
        A --> C
        A --> D
        B[核心问题/Problem: Rare words are a bottleneck for speech-to-text systems. Fine-tuning is costly and causes forgetting.]
        C[主要方法/Method: Training-free paradigm using task vector arithmetic for flexible composition of rare-word capabilities.]
        D[关键结果/Results: Matches/surpasses fine-tuning on target words, improves general performance (~5 BLEU), mitigates forgetting.]
    ```

## 2025-12-30

- **[arXiv251230] Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification**
  - **tags:** [ai], [speaker verification], [Layer Attentive Pooling, Attentive Statistical Temporal Pooling, pre-trained speech models, multi-level features, speaker embeddings]
  - **authors:** Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.22148
  - **code:** https://github.com/sadPororo/LAP
  - **contributions:** 1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[重新思考利用预训练多层表示进行说话人验证<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem<br/>静态加权平均聚合多层特征的局限性<br/>Limitations of static weighted average for multi-layer feature aggregation] --> Problem_Detail[细节/Detail<br/>未充分利用高层表示<br/>Underutilization of high-level representations]
        Method[主要方法/Method<br/>提出层注意力池化<br/>Propose Layer Attentive Pooling (LAP)] --> Method_Detail1[细节/Detail<br/>动态多视角评估层重要性<br/>Time-dynamically assess layer significance from multiple perspectives]
        Method --> Method_Detail2[细节/Detail<br/>使用最大池化而非平均<br/>Employ max pooling instead of averaging]
        Method --> Method_Detail3[细节/Detail<br/>轻量级后端模型 (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]
        Results[关键结果/Results<br/>在VoxCeleb上达到SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --> Results_Detail[细节/Detail<br/>性能优越且大幅减少训练时间<br/>Superior performance and greatly reduced training time]
    ```

- **[arXiv251230] A Robust framework for sound event localization and detection on real recordings**
  - **tags:** [ai], [audio event detection], [SELD, data augmentation, test time augmentation, ensemble, ACCDOA]
  - **authors:** Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han
  - **institution:** Korea University
  - **link:** https://arxiv.org/pdf/2512.22156
  - **contributions:** 1. A robust training framework for SELD that mixes real recordings and emulated data with augmentation to improve generalization on real-world scenes. 2. A test time augmentation and clustering-based ensemble method to aggregate confident predictions and reject abnormal ones. 3. Application of the framework to a ResNet-based model, achieving competitive performance on the DCASE2022 challenge task.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12addf93fd64ffb5001f18188632e3d13c2b6e99ad52d2339022072d1c08696c_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a robust framework for sound event localization and detection (SELD) to improve performance on real-world recordings. The method combines data augmentation, a pipeline mixing real and emulated datasets, and a test-time clustering ensemble. Experimental results show it outperforms baselines and achieves competitive performance in the DCASE2022 challenge.
  - **Mindmap:**

    ```mermaid
    graph TB
    A[A Robust framework for sound event localization and detection on real recordings] --> B(核心问题/Problem: SELD generalization on real-world recordings)
    A --> C(主要方法/Method: ResNet-based model with augmentation, real+emulated data mixing, test time ensemble)
    A --> D(关键结果/Results: Outperforms baseline, achieves competitive performance)
    ```

- **[arXiv251230] Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation**
  - **tags:** [mlsys], [post-training (sft/rlhf)], [domain adaptation, fine-tuning, learning rate optimization, data augmentation, word error rate]
  - **authors:** Xuanfan Ni, Fei Yang, Fengping Tian, Qingjuan Li, Chenyang Lyu, Yichao Du, Longyue Wang, Weihua Luo, Kaifu Zhang
  - **institution:** Alibaba International Digital Commerce
  - **link:** https://arxiv.org/pdf/2512.22165
  - **contributions:** 1. A principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. 2. A learning rate optimization strategy based on performance metrics (e.g., WER) rather than just loss, to prevent overfitting and instability. 3. A domain-specific data analysis and augmentation pipeline to address data mismatch and linguistic variability.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edef09334bdd6f2c438c4ea7c62b2c8a134ebb61ef00ffed51f52a270a2fa141_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a principled fine-tuning framework to adapt large-scale ASR models like Whisper and Qwen2-Audio to specialized domains. The framework uses metric-driven learning rate optimization and domain-specific data augmentation. Empirical results validate the framework and establish practical protocols for improving domain-specific performance while preventing overfitting.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MARCO-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation"] --> Problem["核心问题/Problem: ASR模型在领域特定应用中性能下降/Degraded ASR performance in domain-specific applications"]
        Root --> Method["主要方法/Method: 基于指标的微调框架/Metric-driven fine-tuning framework with learning rate optimization & data augmentation"]
        Root --> Results["关键结果/Results: 框架验证与性能提升/Validated framework and improved performance while preventing overfitting"]
    ```

- **[arXiv251230] AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation**
  - **tags:** [mlsys], [multi-modal inference], [Generative Adversarial Networks (GANs), Single-Double-Triple (SDT) Attention, Time-Frequency Cross-Attention (TF-CA), contrastive losses, real-time generation]
  - **authors:** HaeChun Chung
  - **institution:** KT Corporation
  - **link:** https://arxiv.org/pdf/2512.22166
  - **contributions:** 1. Proposes AudioGAN, the first successful GAN-based framework for text-to-audio generation, enabling single-pass inference. 2. Introduces novel architectural components, Single-Double-Triple (SDT) Attention and Time-Frequency Cross-Attention (TF-CA), to enhance model capability. 3. Integrates multiple contrastive losses to overcome the inherent training difficulties of GANs, improving stability and performance.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb3eeaa0b35c7e10c5a5530b703d70eeb9b9309a57df9a97ae1234ec0c25981_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces AudioGAN, a GAN-based framework for efficient text-to-audio generation. It overcomes GAN training challenges with novel attention mechanisms and contrastive losses, achieving state-of-the-art results with 90% fewer parameters and 20x faster inference than diffusion models.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation"] --> Problem["核心问题/Problem: Diffusion-based TTA models are slow and computationally expensive"]
        Root --> Method["主要方法/Method: First successful GAN-based TTA framework with SDT Attention, TF-CA, and contrastive losses"]
        Root --> Results["关键结果/Results: SOTA performance, 90% fewer parameters, 20x faster inference (<1 second)"]
    ```

- **[arXiv251230] Chord Recognition with Deep Learning**
  - **tags:** [ai], [music information retrieval], [chord recognition, deep learning, generative models, pitch augmentation, beat detection]
  - **authors:** Pierre Mackenzie
  - **institution:** University of Edinburgh
  - **link:** https://arxiv.org/pdf/2512.22621
  - **contributions:** 1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp
  - **Simple LLM Summary:** This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root(Chord Recognition with Deep Learning) --> Problem(核心问题/Problem)
        Root --> Method(主要方法/Method)
        Root --> Results(关键结果/Results)
        Problem --> P1(进展缓慢/Slow Progress)
        Method --> M1(实验现有方法/Experiment with Existing Methods)
        Method --> M2(测试生成模型假设/Test Generative Model Hypotheses)
        Results --> R1(罕见和弦表现差/Poor Performance on Rare Chords)
        Results --> R2(音高增强提升准确率/Pitch Augmentation Boosts Accuracy)
        Results --> R3(节拍检测提升可解释性/Beat Detection Improves Interpretability)
    ```

- **[arXiv251230] Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study**
  - **tags:** [mlsys], [on-device ai], [DistilHuBERT, 8-bit quantization, cross-corpus validation, Leave-One-Session-Out (LOSO), model compression]
  - **authors:** Saifelden M. Ismail
  - **institution:** University of Science and Technology, Zewail City
  - **link:** https://arxiv.org/pdf/2512.23435
  - **contributions:** 1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a "theatricality effect" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.
  - **Mindmap:**

    ```mermaid
    graph TB
        A["Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study"] --> B["核心问题/Problem: SER部署受限于大模型的计算需求/SER deployment constrained by computational demands of large models"]
        A --> C["主要方法/Method: 使用蒸馏与8位量化的DistilHuBERT，并进行跨语料库训练/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training"]
        A --> D["关键结果/Results: 模型仅23MB，精度达基准91%，跨语料库训练提升泛化性/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization"]
    ```

- **[arXiv251230] Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models**
  - **tags:** [nlp], [spoken language understanding], [spoken language models, style amnesia, multi-turn conversation, paralinguistic features, instruction following]
  - **authors:** Yu-Xiang Lin, Cheng-Han Chiang, Hung-yi Lee
  - **institution:** National Taiwan University
  - **link:** https://arxiv.org/pdf/2512.23578
  - **contributions:** 1. Identifies and defines the "style amnesia" problem where spoken language models fail to maintain a user-specified speaking style across multiple conversation turns. 2. Provides a comprehensive evaluation across multiple proprietary and open-source SLMs, demonstrating the pervasiveness of the issue across different emotion, accent, volume, and speed styles. 3. Investigates mitigation strategies, finding that explicit recall prompts can partially alleviate the problem and revealing a counter-intuitive weakness when style instructions are placed in system messages.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the problem of "style amnesia" in spoken language models (SLMs), where models instructed to adopt a specific speaking style fail to maintain it over a multi-turn conversation. The authors evaluate several SLMs and find that explicitly prompting the model to recall the style instruction can partially mitigate the issue. The study concludes that current SLMs struggle with long-term style consistency, a critical challenge for natural spoken interactions.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("SLMs在多轮对话中无法维持指定的副语言风格/SLMs cannot maintain specified paralinguistic style in multi-turn conversation")
        Method --> M1("在多轮对话开始时指定风格并评估/Instruct style at conversation start and evaluate")
        Method --> M2("使用自动评估器测量指令遵循率/Use automatic judges to measure instruction-following rate")
        Method --> M3("测试不同的提示策略/Test different prompting strategies")
        Results --> R1("发现风格遗忘现象，指令遵循率随轮次下降/Style amnesia found, IF rate degrades over turns")
        Results --> R2("显式回忆指令可部分缓解问题/Explicit recall can partially mitigate")
        Results --> R3("系统提示中的指令效果不佳/Instructions in system prompts perform poorly")
    ```

- **[arXiv251230] PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech**
  - **tags:** [nlp], [speech recognition], [context-conditioned ASR, entity-aware evaluation, professional speech]
  - **authors:** Deepak Babu Piskala
  - **institution:** Independent Researcher (affiliation inferred from email domain: gmail.com)
  - **link:** https://arxiv.org/pdf/2512.23686
  - **code:** https://github.com/prdeepakbabu/ProfASR-Bench
  - **contributions:** 1. Introduces ProfASR-Bench, a benchmark for evaluating context-conditioned ASR in high-stakes professional domains (finance, medicine, legal, technology). 2. Identifies and defines the "context-utilization gap" (CUG), showing current promptable models underuse textual context for improving recognition. 3. Provides a standardized evaluation framework with a context ladder, entity/slice-aware reporting, and a reproducible testbed for comparing fusion strategies.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces ProfASR-Bench, a benchmark for evaluating Automatic Speech Recognition (ASR) in high-stakes professional settings. It tests models like Whisper and Qwen-Omni with various contextual prompts and finds a "context-utilization gap," where current systems fail to effectively use available side information to improve accuracy, despite being promptable. The benchmark provides tools for entity-aware and slice-wise evaluation to advance context-conditioned ASR.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PROFASR-BENCH: A Benchmark for Context-Conditioned ASR] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有基准低估专业场景挑战 / Existing benchmarks underplay professional challenges]
        B1 --> B2[密集术语, 正式语体, 关键实体零容忍 / Dense terminology, formal register, zero tolerance for entity errors]
        C --> C1[构建专业语音评估套件 / Build professional-talk evaluation suite]
        C1 --> C2[配对自然语言提示与目标话语 / Pair natural-language prompts with target utterances]
        C2 --> C3[支持实体感知和分片报告 / Support entity-aware and slice-wise reporting]
        D --> D1[发现上下文利用差距(CUG) / Uncover context-utilization gap (CUG)]
        D1 --> D2[轻量级上下文提示对WER改善甚微 / Lightweight textual context yields little WER change]
        D2 --> D3[对抗性提示不会可靠降低性能 / Adversarial prompts do not reliably degrade performance]
    ```

- **[arXiv251230] EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG**
  - **tags:** [ai], [brain-computer interface], [EEG-to-Voice, mel-spectrogram, domain adaptation, automatic speech recognition, language model correction]
  - **authors:** Hanbeot Park, Yunjeong Cho, Hunhee Kim
  - **institution:** Pukyong National University
  - **link:** https://arxiv.org/pdf/2512.22146
  - **contributions:** 1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[EEG-to-Voice Decoding of Spoken and Imagined speech] --> B
        A --> C
        A --> D
        B[核心问题/Problem: EEG-based speech reconstruction is challenging due to noise, low resolution, and lack of aligned targets for imagined speech.]
        C[主要方法/Method: Direct, open-loop EEG-to-mel-spectrogram generation with subject-specific generators, domain adaptation from spoken to imagined speech, and optional LM-based ASR correction.]
        D[关键结果/Results: Feasibility demonstrated for both speech types; stable acoustic/linguistic performance; LM correction reduces CER/WER without semantic distortion.]
    ```

- **[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers**
  - **tags:** [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]
  - **authors:** Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan
  - **institution:** Başkent University, Gaziantep University
  - **link:** https://arxiv.org/pdf/2512.22564
  - **contributions:** 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
    
        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]
        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]
    
        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]
        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]
        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]
    
        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]
        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]
    ```

## 2026-01-01

- **[arXiv260101] MiMo-Audio: Audio Language Models are Few-Shot Learners**
  - **tags:** [mlsys], [multi-modal training], [audio language model, few-shot learning, instruction tuning, scaling pretraining, speech continuation]
  - **authors:** Xiaomi LLM-Core Team, Dong Zhang, Gang Wang, Jinlong Xue, Kai Fang, Liang Zhao, Rui Ma, Shuhuai Ren, Shuo Liu, Tao Guo, Weiji Zhuang, Xin Zhang, Xingchen Song, Yihan Yan, Yongzhe He, Cici, Bowen Shen, Chengxuan Zhu, Chong Ma, Chun Chen, Heyu Chen, Jiawei Li, Lei Li, Menghang Zhu, Peidian Li, Qiying Wang, Sirui Deng, Weimin Xiong, Wenshan Huang, Wenyu Yang, Yilin Jiang, Yixin Yang, Yuanyuan Tian, Yue Ma, Yue Yu, Zihan Zhang, Zihao Yue, Bangjun Xiao, Bingquan Xia, Bofei Gao, Bowen Ye, Can Cai, Chang Liu, Chenhong He, Chunan Li, Dawei Zhu, Duo Zhang, Fengyuan Shi, Guoan Wang, Hailin Zhang, Hanglong Lv, Hanyu Li, Hao Tian, Heng Qu, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianguang Zuo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Linghao Zhang, Meng Chen, Nuo Chen, Peng Zhang, Qianli Chen, Qiantong Wang, Rang Li, Shaohui Liu, Shengfan Wang, Shicheng Li, Shihua Yu, Shijie Cao, Shimao Chen, Shuhao Gu, Weikun Wang, Wenhan Ma, Xiangwei Deng, Xing Yong, Xing Zhang, Xu Wang, Yifan Song, Yihao Zhao, Yingbo Zhao, Yizhao Gao, Yu Cheng, Yu Tu, Yudong Wang, Zhaojun Huang, Zhengju Tang, Zhenru Lin, Zhichao Song, Zhipeng Xu, Zhixian Zheng, Zihan Jiang
  - **institution:** Xiaomi
  - **link:** https://arxiv.org/pdf/2512.23808
  - **code:** https://github.com/XiaomiMiMo/MiMo-Audio
  - **contributions:** 1. Scaled audio language model pretraining to over 100 million hours, demonstrating emergent few-shot learning capabilities across diverse audio tasks. 2. Introduced a systematic evaluation framework and showed that the base model achieves SOTA performance among open-source models on speech intelligence and audio understanding benchmarks, with generalization to unseen tasks. 3. Developed an instruction-tuned variant with a curated corpus and thinking mechanisms, achieving open-source SOTA on multiple audio understanding, spoken dialogue, and TTS benchmarks, rivaling closed-source models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb9eb77f0c6bfe175c709a98be8909fd963e33e44e830db2cd023ecf616b62_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes MiMo-Audio, a large-scale audio language model pretrained on over 100 million hours of data, which demonstrates emergent few-shot learning capabilities for various audio tasks without task-specific fine-tuning. The instruction-tuned variant further achieves state-of-the-art performance on multiple benchmarks, showing strong generalization in audio understanding and generation.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["MiMo-Audio: Audio Language Models are Few-Shot Learners"] --> Problem["核心问题/Problem: Existing audio models require task-specific fine-tuning, lacking human-like generalization."]
        Root --> Method["主要方法/Method: Scale next-token prediction pretraining on 100M+ hours of audio; Use instruction-tuning with thinking mechanisms."]
        Root --> Results["关键结果/Results: Emergent few-shot learning; SOTA open-source performance; Generalizes to unseen tasks like voice conversion."]
    ```

- **[arXiv260101] Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack**
  - **tags:** [sec], [adversarial attacks], [universal adversarial perturbation, latent-space attack, audio-language models, encoder-level vulnerability, targeted attack]
  - **authors:** Roee Ziv, Raz Lapid, Moshe Sipper
  - **institution:** Ben Gurion University of the Negev, Deepkeep
  - **link:** https://arxiv.org/pdf/2512.23881
  - **contributions:** 1. Proposes a universal targeted latent-space attack against audio-language models, focusing solely on the audio encoder. 2. Introduces an attack method that learns a single perturbation effective across different inputs and speakers, without needing access to the downstream language model. 3. Demonstrates high attack success rates with minimal perceptual distortion on a state-of-the-art model, revealing a critical new attack surface in multimodal systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aec73155c062c3c66b8e33c0e7892e17d374292349cfa71e3389850584cf8195_w640_q70.webp
  - **Simple LLM Summary:** This paper identifies a security vulnerability in audio-language models where adversarial attacks can be launched by manipulating only the audio encoder's latent representations. The proposed method learns a universal perturbation that forces the model to generate attacker-specified text outputs, and experiments show it is highly effective and stealthy. This reveals a significant and previously underexplored attack surface at the encoder level of multimodal AI systems.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["Breaking Audio Large Language Models by Attacking Only the Encoder<br>仅攻击编码器来攻破音频大语言模型"] --> Problem["核心问题/Problem<br>Audio-language models have new security vulnerabilities.<br>音频-语言模型存在新的安全漏洞"]
        Root --> Method["主要方法/Method<br>Universal targeted latent-space attack on the encoder.<br>针对编码器的通用目标潜空间攻击"]
        Root --> Results["关键结果/Results<br>High attack success with minimal distortion.<br>高攻击成功率，最小感知失真"]
    ```

- **[arXiv260101] PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation**
  - **tags:** [cv], [audio-visual generation], [text-to-audio-video, physics-sensitivity, benchmark, audio-physics grounding, contrastive physical response score]
  - **authors:** Tianxin Xie, Wentao Lei, Guanjie Huang, Pengfei Zhang, Kai Jiang, Chunhui Zhang, Fengji Ma, Haoyu He, Han Zhang, Jiangshan He, Jinting Wang, Linghan Fang, Lufei Gao, Orkesh Ablet, Peihua Zhang, Ruolin Hu, Shengyu Li, Weilin Lin, Xiaoyang Feng, Xinyue Yang, Yan Rong, Yanyun Wang, Zihang Shao, Zelin Zhao, Chenxing Li, Shan Yang, Wenfu Wang, Meng Yu, Dong Yu, Li Liu
  - **institution:** HKUST(GZ), Tencent, Shanghai Jiao Tong University, Technical University of Munich
  - **link:** https://arxiv.org/pdf/2512.23994
  - **code:** https://imxtx.github.io/PhyAVBench/
  - **contributions:** 1. Introduces PhyAVBench, a novel benchmark for evaluating the audio physics-sensitivity of T2AV models., 2. Proposes the Audio-Physics Sensitivity Test (APST) paradigm using paired prompts with controlled physical variables., 3. Defines the Contrastive Physical Response Score (CPRS) to quantitatively measure a model's understanding of physical principles.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eec12bf437e946a08a88614c7454f1315e7f14f21f2ebe631265de8a77d352a_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that current text-to-audio-video (T2AV) models lack physical plausibility in generated sounds. To address this, it introduces PhyAVBench, a challenging benchmark designed to systematically evaluate models' audio physics grounding through a novel Audio-Physics Sensitivity Test (APST). The authors argue that this benchmark will stimulate progress in generating physically consistent audio-visual content.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[现有T2AV模型无法生成物理合理的声音 / Existing T2AV models generate physically implausible sounds]
        C --> C1[提出PhyAVBench基准与APST评估范式 / Propose PhyAVBench benchmark & APST evaluation paradigm]
        C --> C2[使用成对提示控制物理变量 / Use paired prompts with controlled physical variables]
        C --> C3[引入对比物理响应分数(CPRS) / Introduce Contrastive Physical Response Score (CPRS)]
        D --> D1[系统性评估模型对声学物理的理解 / Systematically evaluate models' understanding of acoustic physics]
        D --> D2[推动物理基础T2AV生成的研究 / Stimulate research in physically-grounded T2AV generation]
    ```

- **[arXiv260101] AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives**
  - **tags:** [ai], [multi-modal reasoning], [audio-language models, hallucination mitigation, counterfactual hard negatives, preference alignment, temporal reasoning]
  - **authors:** Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang
  - **institution:** Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley
  - **link:** https://arxiv.org/pdf/2512.24052
  - **code:** https://github.com/LLM-VLM-GSL/AHA
  - **contributions:** 1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem] --> P1[Large Audio-Language Models (LALMs) suffer from hallucinations / 大型音频语言模型存在幻觉问题]
        Method[主要方法/Method] --> M1[Propose AHA framework with counterfactual hard negative mining / 提出AHA框架，使用反事实硬负例挖掘]
        Method --> M2[Construct preference dataset for alignment / 构建用于对齐的偏好数据集]
        Results[关键结果/Results] --> R1[13.7% improvement on AHA-Eval benchmark / 在AHA-Eval基准上提升13.7%]
        Results --> R2[Gains on public benchmarks (MMAU-Test, MMAR) / 在公开基准(MMAU-Test, MMAR)上取得提升]
    ```

- **[arXiv260101] Environmental Sound Deepfake Detection Challenge: An Overview**
  - **tags:** [ai], [audio deepfake detection], [environmental sound, deepfake detection, anti-spoofing, acoustic scene understanding]
  - **authors:** Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Ting Dang
  - **institution:** KAIST, University of Melbourne, Fortemedia Singapore, Xi’an University of Posts & Telecommunications
  - **link:** https://arxiv.org/pdf/2512.24140
  - **code:** https://github.com/apple-yinhan/EnvSDD
  - **contributions:** 1. Introduced EnvSDD, the first large-scale curated dataset for environmental sound deepfake detection. 2. Organized the ICASSP 2026 ESDD Challenge with two novel tracks focusing on robustness to unseen generators and black-box, low-resource detection. 3. Provided a detailed analysis of the challenge results and summarized effective design choices from top-performing systems.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7537d7ea96f7125c1cdeae053e793ee0aabbb9a9ca6ce7fceaf6aa3e5967f959_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of detecting AI-generated fake environmental sounds by introducing the EnvSDD dataset and launching the ESDD Challenge. The challenge features two tracks to evaluate model robustness against unseen audio generators and in black-box, low-resource scenarios. The work aims to advance the field by providing a benchmark and analyzing the performance of submitted detection methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("ENVIRONMENTAL SOUND DEEPFAKE DETECTION CHALLENGE: AN OVERVIEW") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("AI生成环境音/AI-generated environmental sounds")
        Problem --> P2("存在滥用风险/Risk of misuse")
        Method --> M1("创建EnvSDD数据集/Create EnvSDD dataset")
        Method --> M2("组织ESDD挑战赛/Organize ESDD Challenge")
        Results --> R1("分析挑战赛结果/Analyze challenge results")
        Results --> R2("总结有效设计/Summarize effective designs")
    ```

- **[arXiv260101] AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels**
  - **tags:** [ai], [medical audio classification], [hierarchical classification, acoustic biomarkers, mel-spectrograms, voice disorders, sustained vowels]
  - **authors:** Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang
  - **institution:** Weill Cornell Medicine
  - **link:** https://arxiv.org/pdf/2512.24628
  - **contributions:** 1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework's potential as a scalable tool for early voice disorder screening and diagnostic triage.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[良性喉部嗓音疾病分类/Benign Laryngeal Voice Disorder Classification]
        C --> C1[三级分层机器学习框架/Three-Stage Hierarchical ML Framework]
        C1 --> C1_1[阶段1: 病理筛查/Stage 1: Pathological Screening]
        C1 --> C1_2[阶段2: 粗粒度分层/Stage 2: Coarse Stratification]
        C1 --> C1_3[阶段3: 细粒度分类/Stage 3: Fine-Grained Classification]
        C1_1 --> C1_1a[融合CNN梅尔谱特征与21种声学生物标志物/Integrates CNN Mel-Spectrogram & 21 Acoustic Biomarkers]
        D --> D1[性能优于平面多类分类器与预训练模型/Outperforms Flat Classifiers & Pre-trained Models (HuBERT, HeAR)]
        D --> D2[结合深度表征与可解释特征，增强临床可操作性/Enhances Transparency & Clinical Alignment via Deep & Interpretable Features]
    ```

- **[arXiv260101] AudioFab: Building A General and Intelligent Audio Factory through Tool Learning**
  - **tags:** [mlsys], [agent system], [audio agent, tool learning, modular design, few-shot learning, natural language interface]
  - **authors:** Cheng Zhu, Jing Han, Qianshuai Xue, Kehan Wang, Huan Zhao, Zixing Zhang
  - **institution:** Hunan University, University of Cambridge
  - **link:** https://arxiv.org/pdf/2512.24645
  - **code:** https://github.com/SmileHnu/AudioFab
  - **contributions:** 1. Introduces AudioFab, an open-source, modular agent framework designed to resolve dependency conflicts and simplify tool integration for audio processing. 2. Optimizes tool learning through intelligent tool selection and few-shot learning to improve efficiency and accuracy in complex tasks. 3. Provides a user-friendly natural language interface tailored for non-expert users, aiming to establish an open and intelligent audio-processing ecosystem.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37da2d211f7f1dcdae67f8b3dfe41860d2730828abf8f568b168e7809aca6b38_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AudioFab, a general and intelligent agent framework for audio processing. It addresses the fragmentation of audio tools and inefficient collaboration in existing frameworks through a modular design and optimized tool learning. The main conclusion is that AudioFab provides a stable, extensible platform to facilitate future research and development in audio and multimodal AI.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AudioFab: Building A General and Intelligent Audio Factory through Tool Learning] --> B[核心问题/Problem]
        A --> C[主要方法/Method]
        A --> D[关键结果/Results]
        B --> B1[音频工具碎片化/Fragmented audio tools]
        B --> B2[现有框架配置复杂且协作低效/Existing frameworks have complex config & inefficient collaboration]
        C --> C1[模块化设计解决依赖冲突/Modular design resolves dependency conflicts]
        C --> C2[智能工具选择与少样本学习优化工具学习/Intelligent tool selection & few-shot learning optimize tool learning]
        C --> C3[为非专家用户提供自然语言接口/Natural language interface for non-experts]
        D --> D1[构建开放智能的音频处理生态系统/Builds an open & intelligent audio-processing ecosystem]
        D --> D2[为未来音频与多模态AI研究提供稳定可扩展平台/Provides a stable & extensible platform for future audio & multimodal AI R&D]
    ```

- **[arXiv260101] SLM-TTA: A Framework for Test-Time Adaptation of Generative Spoken Language Models**
  - **tags:** [mlsys], [llm inference], [test-time adaptation, spoken language models, acoustic shift, parameter-efficient fine-tuning, robustness]
  - **authors:** Yuan-Kuei Wu, Yang Liu, Yiteng Huang, Zhaojun Yang, Haibin Wu, Ruizhe Huang, Yi-Te, Shuyu Kong, Ming Sun, Florian Metze, Li Wan
  - **institution:** National Taiwan University, Meta
  - **link:** https://arxiv.org/pdf/2512.24739
  - **contributions:** 1. Introduces the first test-time adaptation (TTA) framework specifically designed for generative Spoken Language Models (SLMs) that process interleaved audio-text prompts. 2. Proposes a method that updates only a small, targeted subset of model parameters during inference using a single incoming utterance, requiring no source data or labels, making it compute- and memory-efficient. 3. Demonstrates consistent performance gains across diverse tasks (ASR, speech translation, audio understanding) under various acoustic corruptions without degrading core task accuracy.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8a7defcaf1d981313d94ffcbed71bedbbb3c351330247513f7ac8c831945074_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the performance degradation of generative Spoken Language Models (SLMs) under real-world acoustic shifts like noise and reverberation. It proposes SLM-TTA, a test-time adaptation framework that efficiently updates a small subset of model parameters using only the incoming test utterance, improving robustness without extra data or labels. The method shows consistent gains across multiple speech tasks and is efficient enough for resource-constrained deployment.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[SLM-TTA: A Framework for Test-Time Adaptation of Generative Spoken Language Models] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: SLM性能在声学偏移下下降/SLM performance degrades under acoustic shift]
        Method[主要方法/Method: 仅用测试语音更新少量参数的TTA框架/TTA framework updating few parameters with test utterance]
        Results[关键结果/Results: 鲁棒性提升且高效/Robustness gains and efficiency]
    ```
