---
slug: /daily/csse/20251229-20260104
---
# 20251229-20260104 (cs.SE)

## 2025-12-29

- **[arXiv251229] Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey**
  - **tags:** [se], [software development tools], [Large Language Models, Survey, Industry, Empirical Study, Software Engineering Practices]
  - **authors:** Vítor Mateus de Brito, Kleinner Farias
  - **institution:** University of Vale do Rio dos Sinos
  - **link:** https://arxiv.org/pdf/2512.21347
  - **contributions:** 1. Provides empirical evidence on the adoption and impact of LLMs in professional software engineering practice through an industry survey. 2. Identifies key perceived benefits (e.g., faster problem resolution, better documentation) and concerns (e.g., cognitive dependence, security risks) associated with LLM use. 3. Bridges the gap between academic discourse and real-world development, offering actionable insights for responsible LLM integration.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fdebd67326ef0fe8cc1a2f2e7ff34382bc7ee9e314bfea976a0e99b4b0eda04_w640_q70.webp
  - **Simple LLM Summary:** This paper conducts an empirical survey of 46 industry professionals to understand the adoption and impact of Large Language Models (LLMs) in software engineering. The study finds that while LLMs are perceived to accelerate technical tasks and improve documentation, significant concerns about over-reliance and security risks persist. The results highlight the need for critical and supervised use of LLM-based tools in software development.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Understanding the Role of LLMs in Software Engineering<br>理解LLM在软件工程中的作用] --> B(核心问题/Problem: How are LLMs adopted and perceived in industry software engineering?<br>LLM在工业界软件工程中的采用和认知如何？)
        A --> C(主要方法/Method: Empirical survey of 46 industry professionals<br>对46位行业专业人员的实证调查)
        A --> D(关键结果/Results: Positive perceptions (speed, documentation) but concerns about dependence and security<br>积极认知（速度、文档）但对依赖性和安全性的担忧)
    ```

- **[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation**
  - **tags:** [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]
  - **authors:** Santhosh Kumar Ravindran
  - **institution:** Microsoft Corporation
  - **link:** https://arxiv.org/pdf/2512.21351
  - **contributions:** 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as "genomes" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp
  - **Simple LLM Summary:** CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[LLM代码生成缺乏适应性，难以应对API变化/LLM code generation lacks adaptability to API changes]
        C --> C1[将RL轨迹视为基因组进行进化操作/Treat RL trajectories as genomes for evolutionary operations]
        C --> C2[在夜间回放阶段进行突变与选择/Mutation and selection during nocturnal replay]
        D --> D1[解决方案新颖性提升35%/35% higher novelty in solutions]
        D --> D2[适应速度加快25%/25% faster adaptation]
    ```

- **[arXiv251229] Multi-Agent LLM Committees for Autonomous Software Beta Testing**
  - **tags:** [se], [automated software testing], [multi-agent system, large language model, vision-language model, consensus voting, beta testing]
  - **authors:** Sumanth Bharadwaj Hachalli Karanam, Dhiwahar Adhithya Kennady
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2512.21352
  - **contributions:** 1. A novel multi-agent committee framework that uses a three-round voting protocol for consensus-based decision-making in software testing. 2. Integration of vision-enabled LLMs and diverse testing personas to systematically explore and understand web application user interfaces. 3. Demonstrated significant performance improvements over single-agent baselines in task success, bug detection (F1 score), and security vulnerability coverage on established benchmarks.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the high cost of manual software beta testing and the limitations of single-agent LLM approaches by proposing a multi-agent committee framework. The method employs diverse, vision-enabled LLMs that collaborate through a structured voting protocol and persona-driven behavior to autonomously test web applications. The results show that this multi-agent approach significantly outperforms single-agent baselines in task success rates, bug detection, and security testing coverage, making it suitable for real-time CI/CD integration.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Multi-Agent LLM Committees for Autonomous Software Beta Testing] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[手动测试成本高，单智能体LLM存在幻觉/Manual testing costly, single-agent LLM hallucinates]
        C --> C1[多智能体委员会与三轮投票协议/Multi-agent committee & three-round voting]
        C --> C2[视觉LLM与角色多样性/Vision LLMs & persona diversity]
        D --> D1[任务成功率89.5%，超越基线/Task success 89.5%, beats baseline]
        D --> D2[动作延迟0.71秒，适合CI/CD/Action latency 0.71s, suitable for CI/CD]
        D --> D3[覆盖8/10 OWASP漏洞类别/Covers 8/10 OWASP Top 10]
    ```

- **[arXiv251229] Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software**
  - **tags:** [se], [software fairness], [correlation tuning, phi-coefficient, multi-objective optimization, pre-processing, bias mitigation]
  - **authors:** Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang
  - **institution:** King’s College London, National University of Defense Technology, Southern University of Science and Technology, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.21348
  - **contributions:** 1. Proposes a novel pre-processing bias mitigation method called Correlation Tuning (CoT) that adjusts data correlations. 2. Introduces the Phi-coefficient as an intuitive measure to quantify correlation between sensitive attributes and labels. 3. Employs multi-objective optimization to address proxy biases, demonstrating superior effectiveness over state-of-the-art methods in single and multiple attribute scenarios.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes Correlation Tuning (CoT), a novel pre-processing method to mitigate bias in ML software by adjusting data correlations using the Phi-coefficient and multi-objective optimization. It frames fairness as a core software quality issue. Extensive evaluation shows CoT significantly improves performance for unprivileged groups and reduces key bias metrics, outperforming existing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[传统公平研究忽视软件质量维度/Traditional fairness research neglects software quality dimension]
        B --> B2[预处理方法效果不足/Pre-processing methods lack effectiveness]
        C --> C1[提出相关性调优 (CoT)/Propose Correlation Tuning (CoT)]
        C --> C2[使用Phi系数量化相关性/Use Phi-coefficient to quantify correlation]
        C --> C3[采用多目标优化/Employ multi-objective optimization]
        D --> D1[提高弱势群体TPR 17.5%/Increase unprivileged group TPR by 17.5%]
        D --> D2[关键偏差指标降低 >50%/Key bias metrics reduced by >50%]
        D --> D3[超越SOTA方法 3-10个百分点/Outperform SOTA by 3-10 percentage points]
    ```

- **[arXiv251229] Reflection-Driven Control for Trustworthy Code Agents**
  - **tags:** [mlsys], [agent system], [reflection-driven control, secure code generation, trustworthy agents, reflective memory, safety control]
  - **authors:** Bin Wang, Jiazheng Quan, Xingrui Yu, Hansen Hu, Yuhao, Ivor Tsang
  - **institution:** Peking University, Xiamen University, Agency for Science, Technology and Research (A*STAR)
  - **link:** https://arxiv.org/pdf/2512.21354
  - **contributions:** 1. Introduces Reflection-Driven Control, a standardized and pluggable control module that integrates self-reflection as an explicit, internal step in an agent's reasoning process. 2. Instantiates the method for secure code generation, using a reflection loop to monitor decisions and retrieve repair examples/guidelines from an evolving reflective memory to inject constraints. 3. Empirically demonstrates that the approach substantially improves security and policy compliance of generated code while preserving functional correctness, with minimal overhead.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the lack of reliable safety controls in LLM agents by proposing Reflection-Driven Control, a module that makes self-reflection an explicit, continuous part of the agent's reasoning to monitor and constrain its decisions using evidence from a reflective memory. Evaluated on security-critical code generation tasks, the method significantly improves code security and compliance while maintaining functionality, offering a practical path toward trustworthy AI coding agents.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Reflection-Driven Control for Trustworthy Code Agents] --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[LLM代理缺乏可靠的安全控制/LLM agents lack reliable safety controls]
        Problem --> P2[可能产生有害输出/Can produce harmful outputs]
        Method --> M1[将自我反思作为推理的显式步骤/Elevates self-reflection to an explicit reasoning step]
        Method --> M2[内部反思循环监控决策路径/Internal reflection loop monitors decision path]
        Method --> M3[从反思记忆中检索修复示例/Retrieves repair examples from reflective memory]
        Results --> R1[显著提高生成代码的安全性和合规性/Substantially improves security & policy compliance]
        Results --> R2[基本保持功能正确性/Largely preserves functional correctness]
        Results --> R3[运行时和token开销最小/Minimal runtime & token overhead]
    ```

- **[arXiv251229] AInsteinBench: Benchmarking Coding Agents on Scientific Repositories**
  - **tags:** [se], [software engineering], [benchmark, scientific computing, code generation, pull requests, test-driven verification]
  - **authors:** Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng
  - **institution:** ByteDance Seed, Princeton University
  - **link:** https://arxiv.org/pdf/2512.21373
  - **code:** https://github.com/ByteDance-Seed/AInsteinBench
  - **contributions:** 1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp
  - **Simple LLM Summary:** The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents' ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI's role in computational scientific research.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[AInsteinBench: Benchmarking Coding Agents on Scientific Repositories] --> B[核心问题/Problem: Can LLM agents operate as scientific computing development agents?]
        A --> C[主要方法/Method: End-to-end evaluation using tasks from real scientific pull requests]
        A --> D[关键结果/Results: Measures ability beyond surface-level code generation]
    ```

- **[arXiv251229] What Makes a GitHub Issue Ready for Copilot?**
  - **tags:** [se], [ai-assisted software engineering], [GitHub Copilot, AI-agent, interpretable machine learning, pull request merge prediction, issue quality criteria]
  - **authors:** Mohammed Sayagh
  - **institution:** École de Technologie Supérieure, Université du Québec
  - **link:** https://arxiv.org/pdf/2512.21426
  - **contributions:** 1. Developed a set of 32 detailed criteria to measure the quality of GitHub issues for AI-agents like Copilot. 2. Built an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. 3. Identified key characteristics of successful issues (e.g., shorter, well-scoped) and those associated with failure (e.g., external references), providing actionable guidance for issue writing.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/998bd51b7cec267b5219b085eac5068f7a996cf57d816c17d8e06474fbae27f0_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates what makes a GitHub issue suitable for AI-agents like Copilot to successfully implement. The authors propose 32 quality criteria and build an interpretable machine learning model to predict if an issue will lead to a merged pull request. They conclude that successful issues are shorter, well-scoped, and provide clear implementation guidance, while issues with external references are less likely to succeed.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[What Makes a GitHub Issue Ready for Copilot?] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[AI-agent性能依赖输入质量/AI-agent performance depends on input quality]
        B --> B2[如何评估Issue对Copilot的适用性/How to evaluate Issue suitability for Copilot]
        C --> C1[定义32项质量评估标准/Define 32 quality criteria]
        C --> C2[构建可解释机器学习模型/Build interpretable ML model]
        C --> C3[比较合并与关闭的PR/Compare merged vs. closed PRs]
        D --> D1[成功Issue特征:简短、范围明确、指导清晰/Successful Issue traits: short, well-scoped, clear guidance]
        D --> D2[外部引用关联低合并率/External references linked to lower merge rate]
        D --> D3[模型AUC中位数72%/Model median AUC 72%]
    ```

- **[arXiv251229] Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors**
  - **tags:** [se], [software testing], [runtime error detection, coverage-guided testing, multi-agent reasoning, large language models, static analysis]
  - **authors:** Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.21431
  - **contributions:** 1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp
  - **Simple LLM Summary:** The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors] --> B(核心问题/Problem: Detecting runtime errors in code snippets without execution is crucial for software safety.)
        A --> C(主要方法/Method: Uses LLMs for execution-free, coverage-guided testing with a two-phase feedback loop.)
        A --> D(关键结果/Results: Outperforms conventional and learning-based frameworks by generating high-coverage tests and finding more errors.)
    ```

- **[arXiv251229] Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing**
  - **tags:** [se], [fuzz testing], [initial corpus generation, large language models, multi-agent framework, predictive code coverage, mutation-based fuzzing]
  - **authors:** Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen
  - **institution:** University of Texas at Dallas
  - **link:** https://arxiv.org/pdf/2512.21440
  - **contributions:** 1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp
  - **Simple LLM Summary:** The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[FuzzWise: Intelligent Initial Corpus Generation for Fuzzing] --> B[核心问题/Problem: 为模糊测试生成高质量的初始种子语料库/Generating high-quality initial seed corpus for fuzzing]
        A --> C[主要方法/Method: 基于LLM的多智能体框架，集成生成与预测性覆盖评估/LLM-based multi-agent framework integrating generation and predictive coverage assessment]
        A --> D[关键结果/Results: 用更少的测试用例实现更高的代码覆盖率和错误发现率/Achieves higher code coverage and bug detection with fewer test cases]
    ```

- **[arXiv251229] Code Clone Refactoring in C# with Lambda Expressions**
  - **tags:** [se], [code refactoring], [lambda expressions, extract method, behavior parameterization, code clone, C#]
  - **authors:** Takuto Kawamoto, Yoshiki Higo
  - **institution:** Osaka University
  - **link:** https://arxiv.org/pdf/2512.21511
  - **contributions:** 1. Proposed a C#-specific technique for code clone refactoring using lambda expressions for behavior parameterization, addressing a gap in language-specific research beyond Java. 2. Developed an analysis method to determine the refactorability of clone pairs detected by the NiCad clone detector. 3. Conducted an empirical evaluation on 2,217 clone pairs from 22 projects, measuring the success rate of the proposed consolidation approach.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c89087084c80383647290bcc69c8c950eca517f6ba4f10a7237d77a54477bdd_w640_q70.webp
  - **Simple LLM Summary:** This paper addresses the problem of consolidating code clones in C# programs using "Extract Method" refactoring. It proposes a novel technique that uses lambda expressions to parameterize behavioral differences between clones, which is tailored to C#'s language specifications. The evaluation on real-world projects showed that 35.0% of clone pairs were deemed refactorable by the approach, with 28.9% of those successfully refactored.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root("Code Clone Refactoring in C# with Lambda Expressions") --> Problem("核心问题/Problem")
        Root --> Method("主要方法/Method")
        Root --> Results("关键结果/Results")
        Problem --> P1("Clone Refactoring with Differences/存在差异的克隆重构")
        Problem --> P2("Language-Specific Techniques Needed/需要语言特定技术")
        Method --> M1("C#-Specific Lambda Expressions/C#特定的Lambda表达式")
        Method --> M2("Behavior Parameterization/行为参数化")
        Results --> R1("35.0% Pairs Refactorable/35.0% 可重构")
        Results --> R2("28.9% Successfully Refactored/28.9% 成功重构")
    ```

- **[arXiv251229] XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production**
  - **tags:** [sys], [mobile systems], [dynamic tracing, method interception, ART virtual machine, non-invasive proxying, runtime observability]
  - **authors:** Qi Hu, Jiangchao Liu, Xin Yu, Lin Zhang, Edward Jiang
  - **institution:** ByteDance
  - **link:** https://arxiv.org/pdf/2512.21555
  - **contributions:** 1. Proposes a novel non-invasive proxying paradigm for dynamic tracing that avoids modifying the ART VM's underlying data structures. 2. Achieves high-performance method interception by leveraging and optimizing the stable, built-in instrumentation mechanism of the Android ART virtual machine. 3. Demonstrates production-grade stability, minimal overhead, and broad compatibility through large-scale A/B experiments on a major app, successfully diagnosing severe online issues.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35b382ca224e5947c252f1f122f264a5993cf96e0069d9fecd115eb850c5ea49_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes XTrace, a non-invasive dynamic tracing framework for Android that intercepts arbitrary methods at runtime without app releases by leveraging the ART VM's instrumentation. It shows minimal performance impact and high stability in large-scale production use, significantly improving the efficiency of diagnosing online crashes and performance bottlenecks.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[XTrace: 一个用于生产环境的Android应用非侵入式动态追踪框架 / XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production]
        Root --> Problem[核心问题/Problem]
        Root --> Method[主要方法/Method]
        Root --> Results[关键结果/Results]
        Problem --> P1[移动应用复杂性 & 设备碎片化 / Mobile App Complexity & Device Fragmentation]
        Problem --> P2["传统方法(静态日志)缺乏实时上下文 / Traditional Methods Lack Real-time Context"]
        Problem --> P3["难以捕获'幽灵bug' / Difficulty in Catching 'Ghost Bugs'"]
        Method --> M1[非侵入式代理范式 / Non-Invasive Proxying Paradigm]
        Method --> M2[利用并优化ART内置插桩机制 / Leverage & Optimize ART's Built-in Instrumentation]
        Results --> R1[生产级稳定性 & 最小开销 / Production-Grade Stability & Minimal Overhead]
        Results --> R2[诊断严重线上崩溃 & 性能瓶颈 / Diagnosed Severe Online Crashes & Performance Bottlenecks]
        Results --> R3[根因定位效率提升>90% / Root-Cause Localization Efficiency Improved >90%]
    ```

- **[arXiv251229] Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code**
  - **tags:** [se], [type inference], [Entity Dependency Graph, co-evolution, type-checker-in-the-loop, LLM, repository-level]
  - **authors:** Shuo Sun, Shixin Zhang, Jiwei Yan, Jun Yan, Jian Zhang
  - **institution:** Institute of Software, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2512.21591
  - **contributions:** 1. An Entity Dependency Graph (EDG) model designed to capture repository-level type dependencies. 2. An iterative type inference approach where types and dependencies co-evolve in each iteration. 3. A type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly to reduce error propagation.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d664d948e35d5ccaf8cf1c7880d512bf91868eeaf908b4683c1db08768e3940_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes PyTIR, a novel approach for repository-level type inference in Python. It uses an Entity Dependency Graph (EDG) and an iterative co-evolution process between types and dependencies, enhanced by a type-checker-in-the-loop, to achieve accurate type annotations. The method significantly outperforms prior works, demonstrating a major improvement in automated type annotation for real-world Python code.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Python动态类型导致运行时错误/Python dynamic typing causes runtime errors]
        B --> B2[现有工具难以处理仓库级依赖/Existing tools struggle with repository-level dependencies]
        C --> C1[构建实体依赖图(EDG)/Construct Entity Dependency Graph (EDG)]
        C --> C2[类型与依赖协同进化迭代推理/Co-evolution iterative inference of types and dependencies]
        C --> C3[集成类型检查器循环验证/Type-checker-in-the-loop validation]
        D --> D1[TypeSim 0.89, TypeExact 0.84/TypeSim 0.89, TypeExact 0.84]
        D --> D2[相对基线提升27%和40%/27% and 40% relative improvement over baseline]
        D --> D3[减少92.7%的新类型错误/Reduced 92.7% of new type errors]
    ```

- **[arXiv251229] Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation**
  - **tags:** [sec], [software security], [backdoor attack, retrieval-augmented code generation, vulnerable code, supply-chain vulnerability, stealthy attack]
  - **authors:** Tian Li, Bo Lin, Shangwen Wang, Yusong Tan
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.21681
  - **contributions:** 1. Conducted the first systematic exploration of backdoor attacks targeting the retriever component in Retrieval-Augmented Code Generation (RACG), identifying it as a critical supply-chain vulnerability. 2. Proposed VenomRACG, a new class of potent and stealthy attack that makes poisoned code statistically indistinguishable from benign code, enabling realistic threat analysis. 3. Demonstrated the severe practical impact of the attack, showing that injecting only 0.05% poisoned data can manipulate the retriever and cause downstream models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while evading current defenses.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74c266d46865bfe9ae8b97f18b92cf9b8209ecd63852c6f32f3fe8533329fe2_w640_q70.webp
  - **Simple LLM Summary:** This paper investigates the security threat of backdoor attacks on the retriever in Retrieval-Augmented Code Generation (RACG). To enable a realistic analysis, the authors developed a stealthy attack called VenomRACG. Their findings reveal that this attack is highly effective and evades current defenses, posing a practical threat to the software development ecosystem.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[Retriever Backdoor: 供应链漏洞/Supply-Chain Vulnerability]
        C --> C1[VenomRACG: 隐蔽攻击/Stealthy Attack]
        D --> D1[低投毒率有效/Low Poisoning Rate Effective]
        D --> D2[下游模型生成漏洞代码/Downstream Model Generates Vulnerable Code]
        D --> D3[防御机制失效/Defenses Ineffective]
    ```

- **[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study**
  - **tags:** [se], [code optimization], [AI coding agents, performance optimization, empirical study, pull request analysis, AIDev dataset]
  - **authors:** Huiyun Peng, Antonio Zhong, Ricardo Andrés Calvo Méndez, Kelechi G. Kalu, James C. Davis
  - **institution:** Purdue University
  - **link:** https://arxiv.org/pdf/2512.21757
  - **contributions:** 1. Conducts the first empirical study comparing AI-agent-authored and human-authored performance optimization commits using real-world PR data. 2. Identifies a significant gap in explicit performance validation between AI-authored (45.7%) and human-authored (63.6%) PRs. 3. Finds that AI agents largely employ the same optimization patterns as humans, suggesting they learn from existing code but lack rigorous validation practices.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp
  - **Simple LLM Summary:** This paper presents an empirical study comparing how AI coding agents and humans perform code optimization by analyzing performance-related pull requests from the AIDev dataset. The study finds that while AI agents use similar optimization patterns as humans, they are significantly less likely to include explicit performance validation in their commits. This highlights a key limitation in current agentic code optimization and an opportunity for improvement.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[How Do Agents Perform Code Optimization? An Empirical Study] --> B[核心问题/Problem: AI coding agents' effectiveness on real-world performance optimization tasks is unknown.]
        A --> C[主要方法/Method: Empirical comparison of 324 agent-generated and 83 human-authored performance PRs from AIDev dataset.]
        A --> D[关键结果/Results: AI-authored PRs use similar patterns but include less explicit performance validation (45.7% vs 63.6%).]
    ```

- **[arXiv251229] The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX**
  - **tags:** [se], [software supply chain security], [Software Bill of Materials, SBOM, SPDX, CycloneDX, tool ecosystem]
  - **authors:** Abdul Ali Bangash, Tongxu Ge, Zhimin Zhao, Arshdeep Singh, Zitao Wang, Bram Adams
  - **institution:** Lahore University of Management Sciences, Queen's University, Indian Institute of Technology Ropar, University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.21781
  - **contributions:** 1. Conducted a quantitative comparison of use cases for 170 publicly advertised SBOM tools to identify enhancement areas for the SPDX and CycloneDX formats. 2. Compared health metrics of both ecosystems (171 CycloneDX vs. 470 SPDX tools) and analyzed 36,990 issue reports from open-source tools to evaluate robustness and identify challenges. 3. Investigated and compared the health metrics of the top 250 open-source projects using each tool ecosystem.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91bc1035c2c5b265034f618902937bd2da58ada7ea4b50d403537bd0f48a030c_w640_q70.webp
  - **Simple LLM Summary:** This paper conducts a comparative analysis of the two dominant Software Bill of Materials (SBOM) tool ecosystems, SPDX and CycloneDX. The authors quantitatively analyze tool use cases, ecosystem health metrics, issue reports, and project adoption. The findings reveal that CycloneDX tools show higher developer engagement in some areas, while SPDX benefits from a more mature ecosystem with broader tool availability and industry adoption.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX] --> B(核心问题/Problem)
        A --> C(主要方法/Method)
        A --> D(关键结果/Results)
        B --> B1[SBOM adoption depends on tool ecosystems<br/>SBOM采用依赖于工具生态系统]
        C --> C1[Quantitative comparison of tools, issues, and projects<br/>对工具、问题、项目进行定量比较]
        D --> D1[CycloneDX: higher developer engagement<br/>CycloneDX: 更高的开发者参与度]
        D --> D2[SPDX: more mature ecosystem & broader adoption<br/>SPDX: 更成熟的生态系统和更广泛的采用]
    ```

- **[arXiv251229] A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation**
  - **tags:** [se], [log parsing], [PMSS, label-free evaluation, silhouette analysis, Levenshtein distance, log parser]
  - **authors:** Qiaolin Qin, Jianchen Zhao, Heng Li, Weiyi Shang, Ettore Merlo
  - **institution:** Polytechnique Montreal, University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.21811
  - **contributions:** 1. Proposed PMSS, a novel label-free metric for evaluating log parser performance that does not require ground-truth data. 2. Demonstrated that PMSS is significantly correlated with existing label-based metrics (FGA and FTA) and can lead to comparable parser selection conclusions. 3. Provided guidelines and discussion on interpreting evaluation results with PMSS, addressing challenges and its application when labels are unavailable or inconsistent.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbe635df301e13b1acf1f17c8fb241f287195f74aae223daca138f58797fb87_w640_q70.webp
  - **Simple LLM Summary:** The paper identifies that existing metrics for evaluating log parsers rely on labeled data, which is often unavailable or inconsistent. To solve this, it proposes PMSS, a label-free metric based on medoid silhouette analysis and Levenshtein distance. The results show PMSS is strongly correlated with label-based metrics, offering a viable alternative for parser evaluation and selection without ground truth.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root["A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation<br>论文标题"]
        Root --> Problem["现有评估指标依赖标注数据，导致评估受限且结论不一致<br>Problem: Label-based metrics limit evaluation"]
        Root --> Method["提出PMSS，一种基于中心点轮廓分析和编辑距离的无标签评估指标<br>Method: Propose PMSS, a label-free metric"]
        Root --> Results["PMSS与FGA/FTA显著相关，为无标签场景提供有效替代方案<br>Results: PMSS correlates with label-based metrics"]
    ```

- **[arXiv251229] Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development**
  - **tags:** [mlsys], [agent system], [multi-agent system, code injection, threat model, security analysis agent, LLM]
  - **authors:** Brian Bowers, Smita Khapre, Jugal Kalita
  - **institution:** Loyola Marymount University, University of Colorado Colorado Springs
  - **link:** https://arxiv.org/pdf/2512.21818
  - **contributions:** 1. Proposed and evaluated LLM-based multi-agent architectures (coder, coder-tester, coder-reviewer-tester) for software implementation, assessing their accuracy, attack resilience, and efficiency. 2. Introduced a security analysis agent to mitigate code injection attacks, showing it improves resilience while recovering lost efficiency. 3. Demonstrated a vulnerability in the security analysis agent where embedding poisonous few-shot examples in injected code drastically increases attack success rate.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43b214bf9d80e3aaa2e2412bbf3ea1727db748cf0f33f818d81076fa53654f97_w640_q70.webp
  - **Simple LLM Summary:** This paper analyzes the vulnerability of LLM-based multi-agent systems in software development to code injection attacks. It proposes and evaluates several agent architectures, finding that adding a security analysis agent improves resilience and efficiency. However, the study concludes that even this security agent can be compromised by advanced attacks using poisoned few-shot examples, significantly increasing the attack success rate.
  - **Mindmap:**

    ```mermaid
    graph TB
        Root[Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems] --> Problem
        Root --> Method
        Root --> Results
        Problem[核心问题/Problem: LLM-based multi-agent systems for software development are vulnerable to code injection attacks] --> Problem_Detail[缺乏人在环/No Human-in-the-Loop]
        Method[主要方法/Method: Propose and evaluate multi-agent architectures, then add a security analysis agent] --> Method_Arch[架构评估/Architecture Evaluation: coder, coder-tester, coder-reviewer-tester]
        Method --> Method_Sec[安全代理/Security Agent: Add a security analysis agent for mitigation]
        Results[关键结果/Results: Security agent improves resilience but is itself vulnerable to advanced attacks] --> Results_Resilience[韧性提升/Improved Resilience: coder-reviewer-tester is more resilient]
        Results --> Results_Vulnerability[新漏洞/New Vulnerability: Poisonous few-shot examples increase attack success to 71.95%]
    ```

- **[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures**
  - **tags:** TBD
  - **authors:** Giuseppe De Palma, Saverio Giallorenzo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.22054

- **[arXiv251229] HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules**
  - **tags:** [sec], [binary analysis], [process hollowing, dynamic binary instrumentation, kernel module, fine-grained analysis, malware analysis]
  - **authors:** Zhangbo Long, Letian Sha, Jiaye Pan, Dongpeng Xu, Yifei Huang, Fu Xiao
  - **institution:** Nanjing University of Posts and Telecommunications, The University of New Hampshire
  - **link:** https://arxiv.org/pdf/2512.22043
  - **contributions:** 1. Proposes a new binary program analysis framework that uses a kernel module to extend the capabilities of traditional dynamic binary instrumentation. 2. Introduces a novel method to construct the analysis environment within a container process using process hollowing techniques, enabling decoupled analysis. 3. Demonstrates the framework's practical value through validation with benchmarks, actual exploit programs, and malicious code on the Windows platform.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19e32f77dc6c92df186a9244b45aa21db814a27ce5e27275643fa1e71537cf7_w640_q70.webp
  - **Simple LLM Summary:** This paper proposes HALF, a new binary program analysis framework designed to improve the usability and performance of fine-grained analysis. It combines kernel modules with process hollowing to decouple the analysis environment from the target program, reducing its impact. The framework is validated on Windows, showing effectiveness in analyzing exploits and malware.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[HALF: Process Hollowing Analysis Framework<br>HALF: 进程镂空分析框架] --> B(Problem: Fine-grained binary analysis has deployability and performance issues<br>问题: 细粒度二进制分析存在部署性和性能问题)
        A --> C(Method: Uses kernel modules & process hollowing for decoupled analysis<br>方法: 使用内核模块和进程镂空进行解耦分析)
        A --> D(Results: Validated on Windows, effective for exploit/malware analysis<br>结果: 在Windows上验证，对漏洞利用/恶意软件分析有效)
    ```

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [agent system], [root cause analysis, service dependency graph, program dependence graph, LLM agent, cloud incident]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **contributions:** 1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.
  - **thumbnail:** https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.
  - **Mindmap:**

    ```mermaid
    graph TB
        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B
        A --> C
        A --> D
        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]
        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]
        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]
    ```
